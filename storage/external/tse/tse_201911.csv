title,abstract
How Do Static and Dynamic Test Case Prioritization Techniques Perform on Modern Software Systems? An Extensive Study on GitHub Projects,"Abstract Test Case Prioritization (TCP) is an increasingly important regression testing technique for reordering test cases according to a pre-defined goal, particularly as agile practices gain adoption. To better understand these techniques, we perform the first extensive study aimed at empirically evaluating four static TCP techniques, comparing them with state-of-research dynamic TCP techniques across several quality metrics. This study was performed on 58 real-word Java programs encompassing 714 KLoC and results in several notable observations. First, our results across two effectiveness metrics (the Average Percentage of Faults Detected APFD and the cost cognizant APFDc) illustrate that at test-class granularity, these metrics tend to correlate, but this correlation does not hold at test-method granularity. Second, our analysis shows that static techniques can be surprisingly effective, particularly when measured by APFDc. Third, we found that TCP techniques tend to perform better on larger programs, but that program size does not affect comparative performance measures between techniques. Fourth, software evolution does not significantly impact comparative performance results between TCP techniques. Fifth, neither the number nor type of mutants utilized dramatically impact measures of TCP effectiveness under typical experimental settings. Finally, our similarity analysis illustrates that highly prioritized test cases tend to uncover dissimilar faults.Keywords Fault Diagnosis, Java, Program Testing, Regression Analysis, Regression Testing Technique, Static TCP Techniques, Dynamic TCP Techniques, Quality Metrics, Real Word Java Programs, Test Class Granularity, Test Method Granularity, Static Techniques, TCP Effectiveness, Static Test Case Prioritization Techniques, Dynamic Test Case Prioritization Techniques, Software Systems, Git Hub Projects, Testing, Measurement, Computer Bugs, Software Systems, Java, Fault Detection, Regression Testing, Test Case Prioritization, Static, Dynamic, Mutation Analysis"
Bellwethers: A Baseline Method for Transfer Learning,"Abstract Software analytics builds quality prediction models for software projects. Experience shows that (a) the more projects studied, the more varied are the conclusions; and (b) project managers lose faith in the results of software analytics if those results keep changing. To reduce this conclusion instability, we propose the use of “bellwethers”: given N projects from a community the bellwether is the project whose data yields the best predictions on all others. The bellwethers offer a way to mitigate conclusion instability because conclusions about a community are stable as long as this bellwether continues as the best oracle. Bellwethers are also simple to discover (just wrap a for-loop around standard data miners). When compared to other transfer learning methods (TCA+, transfer Naive Bayes, value cognitive boosting), using just the bellwether data to construct a simple transfer learner yields comparable predictions. Further, bellwethers appear in many SE tasks such as defect prediction, effort estimation, and bad smell detection. We hence recommend using bellwethers as a baseline method for transfer learning against which future work should be compared.Keywords Learning Artificial Intelligence, Project Management, Software Maintenance, Software Quality, Software Analytics, Quality Prediction Models, Software Projects, Transfer Learning Methods, Bellwether Data, Estimation, Software, Software Engineering, Task Analysis, Benchmark Testing, Complexity Theory, Analytical Models, Transfer Learning, Defect Prediction, Bad Smells, Issue Close Time, Effort Estimation, Prediction"
Microtask Programming,"Abstract Traditional forms of Crowdsourcing such as open source software development harness crowd contributions to democratize the creation of software. However, potential contributors must first overcome joining barriers forcing casually committed contributors to spend days or weeks onboarding and thereby reducing participation. To more effectively harness potential contributions from the crowd, we propose a method for programming in which work occurs entirely through microtasks, offering contributors short, self-contained tasks such as implementing part of a function or updating a call site invoking a function to match a change made to the function. In microtask programming, microtasks involve changes to a single artifact, are automatically generated as necessary by the system, and nurture quality through iteration. A study examining the feasibility of microtask programming to create small programs found that developers were able to complete 1008 microtasks, onboard and submit their first microtask in less than 15 minutes, complete all types of microtasks in less than 5 minutes on average, and create 490 lines of code and 149 unit tests. The results demonstrate the potential feasibility as well as revealing a number of important challenges to address to successfully scale microtask programming to larger and more complex programs.Keywords Groupware, Public Domain Software, Software Engineering, Open Source Software Development, Potential Contributors, Potential Contributions, Microtask Programming, Complex Programs, Crowd Contributions, Programming, Task Analysis, Crowdsourcing, Programming Environments, Public Domain Software, Collaborative Software, Programming Environments, Management"
Accurate and Scalable Cross-Architecture Cross-OS Binary Code Search with Emulation,"Abstract Different from source code clone detection, clone detection (similar code search) in binary executables faces big challenges due to the gigantic differences in the syntax and the structure of binary code that result from different configurations of compilers, architectures and OSs. Existing studies have proposed different categories of features for detecting binary code clones, including CFG structures, n-gram in CFG, input/output values, etc. In our previous study and the tool BinGo, to mitigate the huge gaps in CFG structures due to different compilation scenarios, we propose a selective inlining technique to capture the complete function semantics by inlining relevant library and user-defined functions. However, only features of input/output values are considered in BinGo. In this study, we propose to incorporate features from different categories (e.g., structural features and high-level semantic features) for accuracy improvement and emulation for efficiency improvement. We empirically compare our tool, BinGo-E, with the pervious tool BinGo and the available state-of-the-art tools of binary code search in terms of search accuracy and performance. Results show that BinGo-E achieves significantly better accuracies than BinGo for cross-architecture matching, cross-OS matching, cross-compiler matching and intra-compiler matching. Additionally, in the new task of matching binaries of forked projects, BinGo-E also exhibits a better accuracy than the existing benchmark tool. Meanwhile, BinGo-E takes less time than BinGo during the process of matching.Keywords Binary Codes, Java, Program Compilers, Program Diagnostics, Public Domain Software, Source Code Clone Detection, Similar Code Search, Binary Executables, Binary Code Clones, CFG Structures, Selective Inlining Technique, Structural Features, High Level Semantic Features, Bin Go E, Cross Architecture Matching, Cross OS Matching, Cross Compiler Matching, Intra Compiler Matching, Bin Go Tool, Compilation Scenario, Cross Architecture Cross OS Binary Code Search, Binary Codes, Semantics, Tools, Feature Extraction, Cloning, Syntactics, Emulation, Binary Code Search, Binary Clone Detection, Vulnerability Matching, Emulation, 3 D CFG"
Approximate Oracles and Synergy in Software Energy Search Spaces,"Abstract Reducing the energy consumption of software systems through optimisation techniques such as genetic improvement is gaining interest. However, efficient and effective improvement of software systems requires a better understanding of the code-change search space. One important choice practitioners have is whether to preserve the system's original output or permit approximation, with each scenario having its own search space characteristics. When output preservation is a hard constraint, we report that the maximum energy reduction achievable by the modification operators is 2.69 percent (0.76 percent on average). By contrast, this figure increases dramatically to 95.60 percent (33.90 percent on average) when approximation is permitted, indicating the critical importance of approximate output quality assessment for code optimisation. We investigate synergy, a phenomenon that occurs when simultaneously applied source code modifications produce an effect greater than their individual sum. Our results reveal that 12.0 percent of all joint code modifications produced such a synergistic effect, though 38.5 percent produce an antagonistic interaction in which simultaneously applied modifications are less effective than when applied individually. This highlights the need for more advanced search-based techniques.Keywords Optimisation, Power Aware Computing, Search Problems, Software Engineering, Optimisation Techniques, Genetic Improvement, Software Systems, Code Change Search Space, Search Space Characteristics, Output Preservation, Hard Constraint, Maximum Energy Reduction, Modification Operators, Approximate Output Quality Assessment, Code Optimisation, Source Code Modifications, Joint Code Modifications, Synergistic Effect, Advanced Search Based Techniques, Approximate Oracles, Software Energy Search Spaces, Energy Consumption, Energy Measurement, Software Systems, Optimization, Aggregates, Genetics, Search Based Software Engineering, Search Space, Energy Consumption, Genetic Improvement, Synergy, Antagonism, Oracle, Approximation"
