title,abstract
Automatic Identification and Classification of Software Development Video Tutorial Fragments,"Abstract Software development video tutorials have seen a steep increase in popularity in recent years. Their main advantage is that they thoroughly illustrate how certain technologies, programming languages, etc. are to be used. However, they come with a caveat: there is currently little support for searching and browsing their content. This makes it difficult to quickly find the useful parts in a longer video, as the only options are watching the entire video, leading to wasted time, or fast-forwarding through it, leading to missed information. We present an approach to mine video tutorials found on the web and enable developers to query their contents as opposed to just their metadata. The video tutorials are processed and split into coherent fragments, such that only relevant fragments are returned in response to a query. Moreover, fragments are automatically classified according to their purpose, such as introducing theoretical concepts, explaining code implementation steps, or dealing with errors. This allows developers to set filters in their search to target a specific type of video fragment they are interested in. In addition, the video fragments in CodeTube are complemented with information from other sources, such as Stack Overflow discussions, giving more context and useful information for understanding the concepts.Keywords Computer Aided Instruction, Data Mining, Internet, Meta Data, Pattern Classification, Query Processing, Software Engineering, Video Signal Processing, Code Tube, Video Tutorials Mining, Video Fragment, Software Development Video Tutorials, Automatic Identification, Tutorials, Java, Software, You Tube, Indexes, Androids, Humanoid Robots, Recommender Systems, Mining Unstructured Data, Video Tutorials"
Coverage-Based Greybox Fuzzing as Markov Chain,"Abstract Coverage-based Greybox Fuzzing (CGF) is a random testing approach that requires no program analysis. A new test is generated by slightly mutating a seed input. If the test exercises a new and interesting path, it is added to the set of seeds; otherwise, it is discarded. We observe that most tests exercise the same few “high-frequency” paths and develop strategies to explore significantly more paths with the same number of tests by gravitating towards low-frequency paths. We explain the challenges and opportunities of CGF using a Markov chain model which specifies the probability that fuzzing the seed that exercises path i generates an input that exercises path j. Each state (i.e., seed) has an energy that specifies the number of inputs to be generated from that seed. We show that CGF is considerably more efficient if energy is inversely proportional to the density of the stationary distribution and increases monotonically every time that seed is chosen. Energy is controlled with a power schedule. We implemented several schedules by extending AFL. In 24 hours, AFLFast exposes 3 previously unreported CVEs that are not exposed by AFL and exposes 6 previously unreported CVEs 7x faster than AFL. AFLFast produces at least an order of magnitude more unique crashes than AFL. We compared AFLFast to the symbolic executor Klee. In terms of vulnerability detection, AFLFast is significantly more effective than Klee on the same subject programs that were discussed in the original Klee paper. In terms of code coverage, AFLFast only slightly outperforms Klee while a combination of both tools achieves best results by mitigating the individual weaknesses.Keywords Markov Processes, Probability, Program Diagnostics, Program Testing, CGF, Random Testing Approach, Program Analysis, Seed Input, High Frequency Paths, Low Frequency Paths, Markov Chain Model, Code Coverage, Coverage Based Greybox Fuzzing, AFL Fast, Schedules, Markov Processes, Computer Crashes, Search Problems, Tools, Systematics, Vulnerability Detection, Fuzzing, Path Exploration, Symbolic Execution, Automated Testing"
Decomposition-Based Approach for Model-Based Test Generation,"Abstract Model-based test generation by model checking is a well-known testing technique that, however, suffers from the state explosion problem of model checking and it is, therefore, not always applicable. In this paper, we address this issue by decomposing a system model into suitable subsystem models separately analyzable. Our technique consists in decomposing that portion of a system model that is of interest for a given testing requirement, into a tree of subsystems by exploiting information on model variable dependency. The technique generates tests for the whole system model by merging tests built from those subsystems. We measure and report effectiveness and efficiency of the proposed decomposition-based test generation approach, both in terms of coverage and time.Keywords Formal Verification, Program Testing, Decomposition Based Approach, Model Based Test Generation, Model Checking, Testing Technique, Model Variable Dependency, Decomposition Based Test Generation Approach, Testing Requirement, Model Checking, Unified Modeling Language, Valves, Silicon, Explosions, Presses, Model Based Testing, Test Case Generation, Model Checking, State Explosion Problem, Decomposition"
Automatic Detection and Removal of Ineffective Mutants for the Mutation Analysis of Relational Database Schemas,"Abstract Data is one of an organization's most valuable and strategic assets. Testing the relational database schema, which protects the integrity of this data, is of paramount importance. Mutation analysis is a means of estimating the fault-finding “strength” of a test suite. As with program mutation, however, relational database schema mutation results in many “ineffective” mutants that both degrade test suite quality estimates and make mutation analysis more time consuming. This paper presents a taxonomy of ineffective mutants for relational database schemas, summarizing the root causes of ineffectiveness with a series of key patterns evident in database schemas. On the basis of these, we introduce algorithms that automatically detect and remove ineffective mutants. In an experimental study involving the mutation analysis of 34 schemas used with three popular relational database management systems—HyperSQL, PostgreSQL, and SQLite—the results show that our algorithms can identify and discard large numbers of ineffective mutants that can account for up to 24 percent of mutants, leading to a change in mutation score for 33 out of 34 schemas. The tests for seven schemas were found to achieve 100 percent scores, indicating that they were capable of detecting and killing all non-equivalent mutants. The results also reveal that the execution cost of mutation analysis may be significantly reduced, especially with “heavyweight” DBMSs like PostgreSQL. 1   Introduction For many different organizations, including large multinational firms such as Google and Facebook, data forms a strategic asset that must be carefully curated and protected [1]. Indeed, fields such as healthcare, science, and commerce often rely on information that is stored in databases [2]. While non-relational “NoSQL” systems have been gaining in popularity, relational databases remain pervasive. For instance, Skype, the widely used video-call software, uses the PostgreSQL database management system (DBMS) [3] while Google makes use of the SQLite DBMS in Android-based phones [4]. Moreover, relational databases form the backbone of Internet web browsers such as Chrome1 and Firefox,2 mobile applications [5], and even software powering political campaigns [6]. According to DB-Engines.com, the three most popular storage systems are relational in nature [7]. Another way to gauge the popularity of data management technologies is through an analysis of the tags assigned to questions posted on the popular Stack Overflow question and answer web site [8]. Examining the tags attached to the questions posted to Stack Overflow from January 2008 to August 2016 reveals that, while those about relational databases (e.g., “SQL”) are attached to between one and three percent of all questions on Stack Overflow, only one tag about NoSQL (i.e., “MongoDB”) is assigned to more than half a percent of questions. Indeed, the sum of the percentages for the top tags about relational databases (e.g., “Database”, “PostgreSQL”, and “SQLite”) are connected with nearly nine percent of all questions posted during the studied period. In contrast, the NoSQL tags (e.g., “Cassandra”, “HBase”, and “CouchDB”) are attached to less than one percent of Stack Overflow's questions. These results clearly indicate that relational databases, and their schemas that are the subject of this paper, are a technology that practicing programmers and database administrators frequently use and discuss. In addition to being favored because their schema clearly documents the structure of the data [9], relational databases are also commonly adopted because a schema protects the validity and consistency of the stored data through the specification and enforcement of integrity constraints . Integrity constraints encode logic ensuring that the data values are: distinct as dictated by an application (e.g., usernames); not absent from a database (e.g., a part must have an identification number); maintain referential integrity with other data values (e.g., the identifier in different parts of the schema must match if they refer to the same entity); and uphold other domain-specific conditions. Prior work has shown that real-world schemas are complex and often include features such as composite keys and multi-column foreign key relationships [10]. Given the importance of data and its consistency—and the role that these complex integrity constraints play in preserving its veracity—testing database schemas is a recommended industry practice [11]. This has led to the creation of testing strategies, coverage criteria, automatic test suite generators, and mutation analysis methods tailored for database schemas [12],[13],[14],[15]. Yet, it is important to ensure that any tests are sophisticated enough to find flaws in a relational database's schema. Although there are several methods for assessing the quality of a test suite (e.g., measuring how well the tests cover the entities in the relational schema [12],[16]), many of them may be limited in their capability to characterize a test suite's fault-exposing potential. As an alternative, mutation analysis is a method that estimates the fault-finding “strength” of a test suite by generating copies of an artifact under test and seeding small faults, known as “mutants”, into those copies [17]. Mutation analysis then repeatedly runs the test suite against each mutant to see if one or more of its test cases are capable of distinguishing between the mutant and the original—that is, whether a test case fails on the mutant that passed with the original. The intuition here is that if a test suite cannot reveal the difference between the mutant and the original then it cannot detect this fault if it appears in subsequent versions of this artifact [18]. The percentage of mutants killed is known as the “mutation score” of the test suite; the higher the mutation score, the stronger the suite is judged to be at trapping real faults [17]. Nevertheless, mutation analysis can result in the generation of many mutants that are useless for the purpose of evaluating a test suite, which we refer to as “ineffective” mutants in this paper. Mutants may be generated that are invalid, known as “stillborn” mutants, or are equivalent to the original artifact or some other generated mutant, called “equivalent” and “redundant” mutants, respectively [19]. Not only can such mutants reduce the usefulness of the final mutation score, they also incur an execution time overhead that is effectively wasted [17]. Moreover, some ineffective mutants, such as those that are equivalent, also have an associated human cost: following mutation analysis, testers often have to manually inspect test cases, mutants, and the original schema to determine why a mutant is still alive [17]. In the context of programs, where 45 percent of undetected mutants are equivalent, the manual study and classification of a mutant takes about fifteen minutes [20]. In summary, ineffective mutants have been a long-standing problem in program mutation [21], and, as this paper shows, they are also a concern for database schema mutation. In the context of relational database schema mutation, ineffectiveness can manifest itself in a variety of ways. For instance, PRIMARYKEY constraints ensure the uniqueness of database table rows, which is also a property of UNIQUE constraints—for example, this fact leads to a source of equivalent mutants in the SQLite DBMS. In this paper, we identify a wide range of representative root causes of ineffectiveness in the mutants of relational database schemas. We summarize these root causes into a number of patterns in database schemas that can be used for ineffective mutant detection. Not only do we identify sources of stillborn, equivalent, and redundant mutants (as has been previously done for program mutants), we find and classify a new type of ineffective mutant: the “impaired” mutant. Impaired mutants are similar to stillborn mutants, in that they represent infeasible database schemas, but are not damaged to the extent that they are completely invalid and as such automatically rejected by a DBMS. They are nevertheless of little worth in mutation analysis as they are always trivially killed by test cases that attempt to interact with them. On the basis of these representative patterns, we then present algorithms that are capable of statically analyzing mutants, identifying those that are ineffective and removing them from the mutant pool used in mutation analysis. We implemented them into our test generation and mutation analysis tool for database schema testing, the open-source system called SchemaAnalyst [22], and used it to perform an empirical study that incorporated 34 database schemas and three popular and widely used DBMSs—HyperSQL, PostgreSQL, and SQLite. The experiments focused on the testing of many real-world schemas, including those used in the Mozilla Firefox Internet browser and the database backend of the Stack Overflow web site. For the 34 schemas in this study, the experiments performed mutation analysis on a total of 186 tables, 1044 columns, and 590 constraints. The results of the experimental study show that, in practice, the presented algorithms are capable of detecting and removing large numbers of ineffective mutants. Excluding ineffective mutants from the mutant pool means that mutation scores obtained for test suites become more useful, because, for instance, mutants that are the same as the original artifact—and thus cannot be killed—no longer prevent test suites from achieving 100 percent mutation scores. Removing ineffective mutants also ensures that redundant mutants are not double counted. In this paper's study, we found that all but one of the schemas we studied had a test suite that experienced a change in mutation score following ineffective mutant removal. The test suites for the one remaining schema always killed all mutants, and as such had already attained a “perfect” mutation score that could not be improved upon. While only 15 percent of the schemas that we studied had at least one test suite with a perfect score before removing ineffective mutants, a further 21 percent of schemas had test suites—previously thought to have suboptimal scores—that achieve 100 percent scores after discounting ineffective mutants, primarily due to the elimination of equivalent mutants. We also investigated the efficiency of mutation analysis following the removal of ineffective mutants by the presented algorithms, finding that key parts of the analysis become significantly faster to run. In particular, eliminating stillborn mutants using our algorithms is always an order of magnitude faster than relying on the DBMS to “throw out” invalid schemas during the mutation analysis process. The improved efficiency of mutation analysis for other types of mutant depends on the numbers of that type of mutant involved, and whether the upfront time needed to detect and remove them is recouped by not having to consider them during mutation analysis. For instance, the time taken to identify and eliminate redundant mutants is rarely recouped, since the algorithms need to compare every mutant against every other mutant. While savings were indeed possible for several schemas, the overall process took longer for others. Nevertheless, the benefit in these cases is still, as discussed earlier in this section, the increased usefulness of the mutation score. These results also varied depending on the DBMS with which mutation analysis was performed. For fast, lightweight, and in-memory DBMSs, like SQLite, savings are harder to achieve, since tests can be processed quickly. Yet, for an enterprise, disk-based DBMS, such as PostgreSQL, significant time savings are often realizable. Therefore, the important contributions of this paper are: A study and taxonomy of ineffective mutants for relational database schemas—mutants that do not make a useful contribution during mutation analysis, because they are “stillborn” (i.e., invalid), equivalent to the original schema, equivalent to some other mutant, (i.e., “redundant”), or fall into a new class of mutants, those that are “impaired”. The study presents a collection of root causes that lead to ineffectiveness in database schema mutants, explicated as a series of 10 representative patterns common to mutant schemas (Section 3). A family of algorithms that statically analyze relational database schemas and remove ineffective mutants (Section 4) and are implemented as a part of our open-source testing tool called SchemaAnalyst (Section 5). The results of an empirical study, incorporating 34 diverse schemas and three well-known and representative DBMSs, that both evaluates the efficiency and effectiveness of the methods for detecting and removing the four types of ineffective mutants and reveals how their removal influences the final mutation score for a relational database schema's test suite. The study includes a manual analysis of the generated mutants that discerns whether any of those not detected by the automated methods are actually still ineffective (Section 6). This paper is organized as follows. We begin by detailing key background to database schemas, testing methods, and mutation analysis in Section 2. Then, Section 3 introduces a taxonomy of mutant types and a series of root causes and patterns that lead to a mutant schema being ineffective. Following this, Section 4 presents algorithms to detect each pattern of ineffectiveness, allowing these mutants to be removed from the mutant pool used in mutation analysis. Section 6 then presents the results of the empirical study, showing how the presented technique can detect large numbers of ineffective mutants, and how their removal increases the usefulness of mutation scores while potentially decreasing the execution costs of mutation analysis. Finally, we discuss related work in Section 7 and close with concluding remarks and avenues for future work in Section 8. 2   Background This section details the form and structure of relational database schemas, and the integrity constraints that form part of their definition. Since integrity constraints encode vital logic designed to protect the validity and authenticity of database data, it is important that they are tested. To this end, we discuss coverage criteria that have been previously proposed for this purpose, and further explain techniques for the automatic generation of a database-aware test suite. Finally, we introduce mutation analysis, initially in the context of program mutation, showing how it may be applied to relational database schemas for the purpose of estimating the “strength” of the tests used to exercise them. 2.1 Relational Schemas and Integrity Constraints The schema of a relational database defines the structure and type of data that will reside within it, declaring any relationships between pieces of data that may exist. A relational database is composed of two-dimensional tables. Tables are organized by columns, each of which have a specified data type. The schema may also include further restrictions on what data can be added to the database, expressed as one or more integrity constraints. There are five common types of constraints expressed in a schema [2]. PRIMARYKEY constraints ensure that the values in the given column(s) are unique, such that they individually identify each row. As only one PRIMARYKEY can be declared per table, UNIQUE constraints can also enforce additional row-uniqueness properties. A NOTNULL constraint specifies that a NULL value cannot be stored in a specific column. FOREIGNKEYs enforce that each row in one table must have a matching row in another table, connected according to the values in one or more corresponding pairs of columns. Lastly, CHECK constraints provide a means of defining arbitrary predicates that each row must satisfy. These can include boolean algebra operators like conjunction, disjunction, and negation, as well as relational operators and database operations, such as “x IS NULL”, “x > y” and “x IN (y, ...)”. Fig. 1 shows fragments of three different database schemas, highlighting each of the main five types of integrity constraints, and showing differences in declaration style. A segment of the relational database schema of the popular WordNet database, a large online lexical database of words in the English language,3 is shown by Fig. 1a. The snippet involves four tables (i.e., lexlinkref, linkdef, synset, and word) each declared by a separate CREATE TABLE SQL statement. Within each table declaration appear the definition of different columns (e.g., synsetid and word1id for the lexlinkref table). Each column is specified with a datatype (e.g., varchar(80), representing a variable length character string containing up to 80 characters). Fig. 1. Fragments of different real-world relational database schemas, showing differences in declaration style and highlighting different integrity constraint types (e.g., a CHECK constraint in the Station table, a FOREIGNKEY in the Similarity or Stats tables, a NOTNULL in the synset or word tables, a PRIMARYKEY in the lexlinkref or linkdef tables, and a UNIQUE in the word table). Note that the creators of these schemas declared some of the columns and tables with quotation marks surrounding the variable name, which is permitted by the SQL standard. The segment also shows a variety of integrity constraints declared by the relational database schema, which Fig. 1 also highlights. These include several NOTNULL constraints and a PRIMARYKEY for each table. For instance, the lexlinkref table has a primary key that involves all of its columns, meaning that the combination of values for every row must be unique. Alternatively, the word table defines the uniqueness of its rows through the wordid column. Data is inserted into relational database tables through SQL INSERT statements. Given the integrity constraints defined for the lexlinkref table, the following INSERT statement would be initially accepted by the DBMS for an empty database (i.e., the DBMS would admit the data); however, it would be rejected by the DBMS (i.e., the values would not be admitted) if it were attempted a second time, as the set of column values would no longer be unique: INSERT INTO lexlinkref (synset1id, word1id, synset2id, word2id, linkid) VALUES (0, 0, 0, 0, 0, 0); Instead, a distinct set of values would be needed, such as in the following INSERT statement: INSERT INTO lexlinkref (synset1id, word1id, synset2id, word2id, linkid) VALUES (0, 0, 0, 0, 0, 1); A database table can only have one primary key, so where further constraints are necessary to enforce distinctness of certain values for certain columns, the UNIQUE constraint can be used. For example, in the WordNet schema of Fig. 1a, a UNIQUE is defined on the lemma column in the word table. As such, following INSERT statement 3, INSERT statement 4 will be rejected since the value of lemma is repeated:  Fig. 1b shows a schema fragment from the freely available Million Song dataset [23], which contains 280 GB of data. This fragment involves two tables, artists and similarity, that contain three integrity constraints. There is a single PRIMARYKEY, defined on the artists table—and in a different style to primary keys declared for the WordNet schema, as this time it is declared inline with the column definition for artist_id. The schema also contains two FOREIGNKEY constraints designed to ensure that each target and similar value in the “source” table, similarity, refer to an existing artist_id value in the “referenced” table, artists. With these constraints, the following INSERT statements 5 and 6 would be accepted into a empty database, while statement 7 would be rejected. Accepted statement 6 uses a value for both target and similar that has already been inserted for artist_id in the artists table. Yet, rejected statement 7 uses a value for similar that does not refer to an existing artist_id value in the referenced table:  Finally, Fig. 1c shows the NistWeather database schema, a part of the NIST SQL conformance test suite [24]. This schema also contains a FOREIGNKEY, although declared inline to the ID column of the Stats table. This particular schema features a number of CHECK constraints. For example, the MONTH column of the Stats table has a CHECK constraint defined on it that ensures an integer MONTH value can only be between 1 and 12. Any INSERTs involving values for MONTH outside of this legal range will be rejected by the DBMS. 2.2 Mistakes Leading to Faults in Database Schemas Given that integrity constraints encode important logic used to protect the validity and consistency of data in a database, it is also important that these constraints are properly tested, in accordance with industry advice [11]. Broadly speaking, a database designer may make mistakes when specifying a relational database schema in two different ways. Different DBMSs have different implementations of the SQL standard and, additionally, may offer features not specifically required by the standard. A programmer moving from one DBMS to another is therefore open to making mistakes when specifying schemas, since the behavior of DBMSs varies greatly, as will be further demonstrated in Section 3. This is increasingly the case when an engineering team uses a different DBMS for development, in-house testing, and deployment. For example, programmers may prefer the speed and flexibility of a DBMS like SQLite for development, but choose a robust enterprise DBMS, such as PostgreSQL, for use with the deployed application. One instance of differences in DBMS behavior concerns how PRIMARYKEY constraints are handled by the PostgreSQL and SQLite DBMSs. With PostgreSQL, PRIMARYKEY constraints reject NULL values (as well as ensuring column values are distinct). Yet, for SQLite, NULL values may be admitted for primary key columns. As such, a programmer familiar with PostgreSQL may reasonably expect that the specification of a primary key in SQLite will defend the database against NULL values for the key. However, unless they remember to also additionally specify NOTNULL constraints on the columns of the key, this will not be the case. Thus, the behavior of the database schema must be tested to ensure it is consistent with what the developer intended. There are other ways in which a programmer may misunderstand SQL dialects. For instance, the treatment of NULL values in columns denoted as UNIQUE operates in one way for PostgreSQL, SQLite, and HyperSQL and in another manner for the MS SQL Server DBMS, which only allows one instance of a NULL value in a UNIQUE column, on the basis that it is not distinct from other NULL values. Yet, the three other aforementioned DBMSs treat NULL values as meaning “unknown” and therefore still distinct from one another. As such, PostgreSQL, SQLite, and HyperSQL permit multiple instances of NULL in columns constrained by a UNIQUE. Instead of misunderstanding the dialect of SQL that a DBMS supports, a developer may also make mistakes when specifying the schema by, for example, forgetting to add a PRIMARYKEY or UNIQUE constraint on a field for usernames that controls a system's login. If the database designer omits a PRIMARYKEY constraint on the username column in a database table, then the DBMS hosting this table would allow INSERT statements to create two users who have the same name. Or, if the designer of a schema neglects to add CHECK constraints on fields such as prices or product stock levels to ensure they can never be negative, then it may be possible for INSERT or UPDATE statements to corrupt the database's state. Finally, a designer could specify constraints on the wrong columns, thus, for instance, leading to a database table having the wrong PRIMARYKEY column. 2.3 Database Schema Testing It is important to perform testing to identify the two broad categories of faults described in Section 2.2. The goal of prior work has been to create a test case that consists of a sequence of SQL INSERT statements that aim to fulfill a test adequacy criterion [13]. The first work on testing integrity constraints, due to Kapfhammer et al. [12], introduced a search-based technique that automatically generates data for composing tests of INSERTs that exercise a database's schema. A test case “passes” when its INSERTs are accepted by the DBMS, as expected, and the data is admitted into the database since it satisfies the constraints of the relational schema. A test case may also pass when its INSERTs are, as anticipated, rejected by the DBMS because the data was generated with the goal of violating the schema's integrity constraints. 2.3.1 Coverage Criteria. McMinn et al. [13] followed up the work in [12] by defining a family of coverage criteria for testing relational database schema integrity constraints. Organized into subsumption hierarchies, these criteria range from simple measures with few coverage goals to more intricate criteria with substantially more test requirements. Each criterion centers on the reformulation of the integrity constraints of a database table as a boolean predicate, referred to as the acceptance predicate for the table. This is because the predicate evaluates to true when the data in an INSERT statement will be accepted for the table by a particular DBMS (i.e., the data is admitted into the database). Conversely, an INSERT statement will be rejected by a DBMS if the data within it causes the acceptance predicate to evaluate to false . “Acceptance Predicate Coverage” (APC), therefore, requires the acceptance predicate for each table to have been exercised as true and false by the test suite. As such, each table should have had data in an INSERT statement admitted to it at least once, and have had an INSERT statement rejected at least once [13]. APC does not, then, require that each particular integrity constraint has been properly exercised, because the INSERT statement may be rejected by the violation of just one of the integrity constraints defined for the table. “Active Integrity Constraint Coverage” (AICC) addresses this limitation. For this criterion, a test case is required that satisfies the acceptance predicate (i.e., all integrity constraints are satisfied), followed by tests that exercise the portion of the predicate corresponding to each integrity constraint as false , while ensuring the rest of the predicate evaluates to true (i.e., each integrity constraint is violated in isolation). “Clause-Based Active Integrity Constraint Coverage” (ClauseAICC) takes this further, requiring that each individual clause of the acceptance predicate be exercised as false [13]. A clause could correspond to a single aspect of a particular integrity constraint, for example the uniqueness of a column as part of a multi-column PRIMARYKEY constraint. Further criteria defined by McMinn et al. include “Active Unique Column Coverage” (AUCC), which requires that test cases be produced that exercise each column of each table with unique and non-unique values, while maintaining satisfaction of the acceptance predicate. Finally, “Active Null Column Coverage” (ANCC) requires that test cases be produced that exercise each column of each database table with NULL and non-NULL values, while also maintaining satisfaction of the acceptance predicate [13]. 2.3.2 Automatic Test Case Generation Kapfhammer et al. [12] presented an extension of Korel's Alternating Variable Method (AVM ) [25] for the automatic generation of data for INSERT statements that form part of test cases for schemas. McMinn et al. [13] extended this approach to generate test suites according to their coverage criteria. In that paper and the remainder of this one, a test suite is a collection of test cases, each of which contains its own INSERT statements designed to fulfill a testing objective. The paper also introduced a random approach, referred to as Random + , that utilizes constants mined from the database schema's definition. An empirical study conducted by McMinn et al. revealed that the AVM tends to reliably generate database-aware test suites that provide full coverage of the criteria, while Random + is more erratic and cannot guarantee such high levels of test coverage [13]. 2.4 Mutation Analysis of Schema Integrity Constraints Mutation analysis is a useful method for estimating the “strength” of a test suite—that is, its potential fault-finding capability [17]. The process of mutation works by producing copies of the artifact under test—traditionally a program—and making minor changes to them so as to simulate faults. The altered copies of the original artifact are called “mutants”. Fig. 2 shows two examples of mutants for a max function implemented in the syntax of a Java-like programming language. Part (b) of this figure shows how the relational operator in the conditional statement of the original function in part (a) is changed, resulting in the predicate being mutated from “if x > y” to “if x < y”. Fig. 2. An example of program mutation. This figure highlights the fact that equivalent mutants cannot be distinguished from the original program as there is no input to the max function for the mutant in part (c) that will produce a different output to the original program in part (a). Additionally, part (b) of this figure highlights a non-equivalent mutant in the max function that is semantically different from the original program. If a test suite can distinguish between the original artifact and the mutant (i.e., a test case fails on the mutant that previously passed on the original), then the mutant is said to be “killed”, else it is “live” [17]. For instance, the mutant in part (b) of Fig. 2 is easily distinguished from the original. A test case with the inputs x=1, y=2 gives the output 2 with the original program and 1 with the mutant, and so the test case kills the mutant. A test case with the inputs x=1, y=1 would not kill the mutant, however, since the result is 1 for both versions of the program. The percentage of mutants killed by a test suite is compiled into a metric known as its “mutation score”. The higher the mutation score a test suite has, the stronger it is estimated to be. A test suite is said to be mutation-adequate if it kills all mutants, that is, it achieves a “perfect” mutation score of 100 percent [17]. Intuitively, if a test suite cannot kill a mutant then this means that it would not be able to detect this type of programming error if it was subsequently introduced into the program under test [18]. While mutation analysis was originally proposed for traditional programs [17], it has recently been adopted for a wider range of software artifacts. For instance, Deng et al. and Lindström et al. proposed the use of mutation analysis to assess the adequacy of test suites for Android apps [26],[27]. Mutation testing has also recently been used to measure the effectiveness of test suites for web sites [28],[29],[30]. Additionally, mutation testing has been applied in other diverse domains such as mobile software agents (e.g., [31],[32]) and security policies (e.g., [33],[34]). In the context of databases, while Bowman et al. focused on the use of mutation testing to assess test suites for an entire database management system [35], Kapfhammer et al. [12] were the first to propose and evaluate mutation operators for the integrity constraints expressed in a relational database schema. These proposed operators created mutants by adding, removing, and replacing columns in the definitions of PRIMARYKEY and UNIQUE constraints, while also adding and removing NOTNULL constraints from other columns in the schema's tables. An operator was also proposed to remove CHECK constraints from schema definitions. Wright et al. [15] extended this set by adding operators that mutate the predicates of CHECK constraints (e.g., by replacing a relational operator such as > with >=) while also adding operators to mutate the columns featuring in a relational database schema's definition of FOREIGNKEY constraints. Fig. 3 shows an example of a mutation to a PRIMARYKEY. For the solitary table of the original schema, shown by part (a), the column x is the sole primary key column. For the mutant, shown by part (b), the column y is also a part of the key. With integrity constraint mutation, a mutant is “killed” when INSERTs made to a database instantiating the mutant schema behave differently compared to a database instantiating the original schema. As highlighted by the fact that the original and mutated schemas lead to different outcomes (i.e., an ✗ indicating rejection and a ✓ meaning acceptance, respectively), the following INSERT statements are capable of distinguishing between the original and the mutant for an initially empty database: Fig. 3. An example of a relational database schema integrity constraint mutant. As seen in part (b), the PRIMARYKEY on the table is mutated from the original schema (part (a)) to include the column y as well as x.  The data in statements (8) and (9) are successfully inserted into the database because the value of x is distinct, thereby satisfying the PRIMARYKEY constraint of the original schema, while the combination of x and y are distinct, satisfying the PRIMARYKEY of the mutant. For statement (10), however, the value for x is not distinct for the column, thereby causing the INSERT statement's rejection. The combination of x and y values is still unique for the mutated PRIMARYKEY, however, and thus statement (10) is accepted, leading to the mutant being killed, as indicated by the ✗ for the original schema and the ✓ for the mutated one. Like program mutation, mutation analysis of a relational database schema is a costly process that takes a long time to complete, due to the many mutants that may be created, and the fact that the test suite must be run against each mutant to determine if it is killed or remains live. Furthermore, mutation can result in many “ineffective” mutants that do not contribute to the mutation score or make it less useful, while still consuming valuable execution time. The most famous of these—and most widely studied for program mutation—is the “equivalent” mutant [17]. As with all mutants, equivalent mutants correspond to seeded changes—but they do not result in any change in behavior. An example of an equivalent mutant for program mutation is shown by Fig. 2c. The relational operator of the conditional statement has been changed, but the mutant program behaves exactly the same as the original one. That is, there is no input to the mutant that will produce an output different from that of the original. Thus, as previously noted, it is impossible for a test to distinguish between them. Equivalent mutants will always remain “live” following mutation, thereby preventing the tests from achieving a perfect mutation score [17]. In addition, there may be equivalence between pairs of mutants themselves. This means that the same mutants may be considered more than once; Just et al. [36] label these mutants as “redundant” while Papadakis et al. call them “duplicate” and note that they are problematic for mutation testing [37]. Even though the removal of these equivalent and redundant mutants would make mutation testing more efficient, the detection of equivalent mutants for programs is generally undecidable due to the halting problem [17]. Finally, mutation may introduce another type of ineffective mutant, known as the stillborn mutant. These mutants are ones where the seeded change has caused it to become invalid—for example, producing a program that does not compile [38]. Stillborn mutants slow down the process of mutation analysis, since there is an execution cost associated with finding them to be invalid (e.g., due to a compilation error) and removing them from the mutant pool so that they receive no further consideration. Overall, as these examples demonstrate, ineffective (i.e., equivalent, redundant, and stillborn) mutants are also a problem for the mutation of the integrity constraints in a schema. The next section explains how these mutants can arise, and, along with identifying a new type of ineffective mutant, define patterns common to schemas that are the direct cause of mutant ineffectiveness. 3   Classifying the Ineffective Mutants of Integrity Constraints in Database Schemas In this section we describe and define four different types of ineffective mutants produced during the mutation of the integrity constraints encoded in a relational database schema. We give examples of how they occur, and additionally identify common patterns in database schemas that summarize the root cause of their ineffectiveness. Three types of mutant that are ineffective for relational database schemas are also found in program mutation—equivalent, redundant and stillborn mutants [17],[36],[39],[40]. We further identify and explain a fourth type of ineffective mutant for relational database schemas, namely the impaired mutant. 3.1 Equivalent Relational Database Schema Mutants As with program mutation, equivalent mutants for relational database schemas are mutants that have the same behavior as the original artifact, and as such cannot be distinguished by a test. In SQL, it is possible to express the same two schemas by stating their definition, at the syntactic level, in a slightly different manner. As an example, Fig. 4 shows the definition of three schemas that are actually the same. Each schema consists of one table, t, with one column, c, with a PRIMARYKEY constraint defined on that column. Yet, the SQL declaration of the PRIMARYKEY constraint is expressed in three different ways. For the schema shown by part (a), the keyword “PRIMARYKEY” appears on the definition of the column. For the schema shown by part (b), the PRIMARYKEY declaration appears before the end of the table's definition. In the final schema of part (c), the PRIMARYKEY constraint definition appears after the creation of the table via an ALTER statement. We refer to schemas that are identical, but which are possibly declared in different ways, as structurally equivalent . We define this property as: Fig. 4. Three relational database schemas that are identical, and therefore equivalent, but declared in different, but valid, ways in the SQL. Definition 1 (Structural Equivalence). Two relational database schemas s 1 and s 2 are said to be structurally equivalent, if, following declaration, the tables, columns, and integrity constraints that exist for schema s 1 are identical to those of schema s 2 . It is also possible to express schemas that are structurally different but are functionally equivalent, and so also indistinguishable by a test case. This is because different types of integrity constraints have similar or identical behaviors, or can be combined to have the same effect as another. Since SQLite does not enforce the standard that a PRIMARYKEY should also imply a NOTNULL[41],PRIMARYKEY and UNIQUE constraints are, for this DBMS, identical in terms of accepting and rejecting the same INSERT statements. Fig. 5 shows an example of two schemas, which are the same but for the fact that one has a PRIMARYKEY constraint defined for the c column for the schema shown in part (a) of this figure, while the other, shown in part (b), has a UNIQUE constraint defined on the column instead. The functional behavior of these two schemas is the same: when distinct values for c are inserted into the table, the DBMS will accept them. Alternatively, when an INSERT statement contains a value that is already in the database for c, it will be rejected. Fig. 5. Two relational database schemas that are different, but functionally equivalent, for SQLite, since, for this DBMS, primary keys reject non-unique values in the same way as is done by UNIQUE constraints. However, these database schemas are not equivalent for most other DBMSs (e.g., HyperSQL and PostgreSQL), where PRIMARYKEY constraints also reject the insertion of NULLs in addition to non-distinct values. As such, the two schemas shown in Fig. 5 behave differently when managed by these DBMSs: one schema will be responsible for rejecting NULL values submitted for the c column (i.e., the schema in part (a) of the figure) while the other schema will admit them (i.e., the schema in part (b) of the same figure). Therefore, the equivalence of database schemas is a property that varies depending on the DBMS in question. This leads to the following definition of behavioral equivalence : Definition 2 (Behavioral Equivalence). Two relational database schemas s 1 and s 2 are said to be behaviorally equivalent for a relational database management system D if when, following their instantiation, for two initially empty (and separate) databases d 1 and d 2 , using D , no sequence of INSERT statements I = ⟨ i 1 , … , i q ⟩ exists such that there is an i j ∈ ⟨ i 1 , … , i q ⟩ that is accepted by d 1 but rejected by d 2 . Note that, according to this previous definition, structural equivalence is a type of behavioral equivalence: all database schemas that are structurally equivalent to one another are also behaviorally equivalent. As explained in the following definition, an equivalent mutant therefore refers to a mutant that is behaviorally equivalent with the original database schema from which it was created: Definition 3 (Equivalent Relational Database Schema Mutant). A mutant m e q v of a database schema s is said to be equivalent if s and m e q v are behaviorally equivalent. Where equivalent mutants exist, mutation scores are artificially deflated [19], thus, for instance, potentially compromising the comparison of different data generation techniques through mutation analysis. Equivalent mutants also have an associated human cost: following mutation analysis, testers often have to manually inspect test cases, mutants, and the original schema to determine why a mutant is still alive. In the context of programs, where 45 percent of undetected mutants are equivalent, the manual study and classification of a mutant takes about fifteen minutes [20]. Since it is impossible to kill an equivalent mutant, such diagnostic effort on the part of testers is essentially wasted. Combined with the execution cost per mutant, this makes the detection and discarding of these mutants, known as the equivalent mutant problem [17], an important issue for the mutation of both relational database schemas and programs. Since the large number of equivalent mutants and the high costs of human inspection make it infeasible to manually detect equivalent mutants [20], there are many approaches that attempt to automatically detect them for programs (e.g., [40],[42],[43]). This motivates our work to identify causes of equivalence for database schemas. Structural equivalence, which occurs at the syntactic level, is one source of equivalent mutants for database schemas that we defined in Definition 1. Behavioral equivalence following integrity constraint mutation is due to the functional equivalence of integrity constraints or between combinations of integrity constraints. We now identify six representative patterns that encapsulate the ways in which behavioral equivalence can manifest. Pattern BE-1: UNIQUE constraints and PRIMARY KEY s Pattern BE-1 expresses the form of equivalence demonstrated in Fig. 5, where, for DBMSs like SQLite, there is no behavioral difference between PRIMARYKEYs and UNIQUEs. Schemas that are identical but for a UNIQUE instead of a PRIMARYKEY defined on the same column set are equivalent. Pattern BE-2: PRIMARY KEY s and UNIQUE constraints paired with NOT NULL constraints For DBMSs where PRIMARYKEYs and UNIQUE constraints do not behave in the same way, because PRIMARYKEYs do not admit NULL values and UNIQUE constraints do (e.g., for DBMSs such as HyperSQL and PostgreSQL), the following two relational database schemas are behaviorally equivalent. If the columns involved in a UNIQUE constraint also have NOTNULL constraints defined on them, the combined behavior is the same as that of a PRIMARYKEY constraint: Pattern BE-3: PRIMARY KEY s and PRIMARY KEY s paired with NOT NULL constraints Following from the last rule, and for DBMSs where PRIMARYKEYs do not allow NULL values, NOTNULL constraints defined on primary key fields are superfluous. Thus, a schema without NOTNULL constraints on primary key fields is behaviorally equivalent to an identical schema but with additional NOTNULL constraints defined. That is, the following two database schemas are behaviorally equivalent: Pattern BE-4: Extraneous UNIQUE constraints If a set of columns C s u b is declared as UNIQUE, any further UNIQUE constraints involving the same columns (i.e., a set C s u p , C s u b ⊂ C s u p ) are extraneous. That is, the following two relational database schemas are behaviorally equivalent: Column values for c1 will be unique, due to the “UNIQUE(c1)” declaration. Therefore the combination of any further column value (i.e., c2) paired with a unique value for c1 will also be unique. This means that the additional constraint “UNIQUE(c1, c2)” in the right-hand schema is superfluous, and the two database schemas are equivalent. Note that removing “UNIQUE(c1)” from the right-hand schema would not have the same effect: The constraint “UNIQUE(c1, c2)” on its own does not guarantee that c1 is individually unique. It only guarantees that the combination of c1 and c2 are unique. As such, removing “UNIQUE(c1)” rather than “UNIQUE(c1, c2)” would change the behavior of the right-hand schema, and it would no longer be equivalent. It is also important to note that one of the integrity constraints could be a PRIMARYKEY (as it is not possible for a table to have two primary keys), since primary keys are equivalent to UNIQUE constraints under certain conditions, as already discussed in Patterns BE-1 and BE-2. An exception to the rule occurs when another table in the schema has a foreign key referencing the constraint with the greater number of columns (i.e., C s u p ). In this case, the superset constraint is not redundant, as it is preventing the schema from being invalid. We expand on the issue of foreign keys and schema validity in Section 3.3. Pattern BE-5: NOT NULL constraints and CHECK ... IS NOT NULL constraints The effect of a CHECK constraint of the form “CHECK c IS NOT NULL” for some column c is equivalent to defining a NOTNULL constraint on the column, as in the following example: Pattern BE-6: Behaviorally equivalent CHECK constraints Since CHECK constraints can encode arbitrary constraints, it is possible for them to be specified in different ways while being behaviorally equivalent, as in the following example. 3.2 Redundant Mutants In the context of program mutation, Just et al. describe a mutant of a conditional expression with one logical operator as being redundant if it leads to the same boolean outcome as other mutants that are better suited for efficiently assessing test suite effectiveness [36]. In this paper, we use the term more broadly: While equivalent mutants are behaviorally the same as the original artifact, a mutant is redundant with respect to another mutant if they are behaviorally equivalent to one another. This leads to the next definition: Definition 4 (Redundant Relational Database Schema Mutant). A mutant m r e d of a relational database schema s is said to be redundant with respect to some other mutant m of s if m and m r e d are behaviorally equivalent. Patterns of redundancy are the same as for equivalence, except that the relationship holds between mutants rather than between a mutant and the original artifact. When a redundant mutant pair is found, one of the mutants may be safely discarded, as it replicates the other mutant in the pair and only serves to artificially inflate mutation scores [37]. 3.3 Stillborn Mutants In the context of program mutation, stillborn mutants are programs that do not compile due to a mutation operator making it syntactically or semantically invalid [38]. Such mutants cannot be used during mutation analysis, since they do not represent an artifact against which any tests can be run. Stillborn mutants are also possible for relational database schema mutation, taking the form of syntactically invalid SQL declarations and also arising from semantic invalidity. The submission of SQL statements relating to the CREATE TABLE declarations for an invalid schema, and therefore a stillborn mutant, will be rejected by these DBMSs. Thus, we define the concept of a stillborn mutant4 as: Definition 5 (Stillborn Relational Database Schema Mutant). A mutant m s t b of a relational database schema s is said to be stillborn for a DBMS D if any SQL declaration relating to the definition of m s t b is rejected by D . We now define two patterns that are a source of semantically invalid database schemas during integrity constraint mutation, thus leading to stillborn mutants. Pattern SB-1: PRIMARY KEY and UNIQUE constraints Some DBMSs, such as HyperSQL, do not allow UNIQUE constraints to be defined on the same column sets as the table's primary key. An attempt to submit a database schema such as the following results in an error. That is, any mutant where UNIQUE constraint columns replicate those of the primary key will be stillborn, as in the following example: Pattern SB-2: Foreign key misalignment Many DBMSs require that, for foreign keys appearing in schemas, the column or columns in the referenced table must be the primary key of that table, or be declared in a UNIQUE. We refer to this property as foreign key alignment : Definition 6 (Foreign Key Alignment). A relational database schema s for a relational database management system D is said to exhibit foreign key alignment when for each foreign key f k = ( t , ⟨ t c 1 … t c n ⟩ , r , ⟨ r c 1 … r c n ⟩ ) , where t c 1 … t c n are columns of the table t on which the key is defined, and r c 1 … r c n are the columns of the referenced table r for the key, a PRIMARYKEY or UNIQUE constraint exists on r for the columns r c 1 … r c n , and the pairs of columns ( t c 1 , r c 1 ) … ( t c n , r c n ) have compatible types for a specific relational DBMS D . A relational database schema is said to exhibit foreign key misalignment when the foreign key alignment property does not hold. As stated by this definition, column pairs must have compatible types, a property that depends on the DBMS in use. For example, SQLite has a weak typing mechanism allowing any column type to be mapped to any other in a foreign key. In contrast, PostgreSQL is more strongly typed: It will allow a column of type INTEGER to be mapped to a column of type DECIMAL for example, but the pairing of VARCHAR and INTEGER types, for instance, is not allowed by this DBMS. An example of a database schema with correct foreign key alignment, and a mutant with foreign key misalignment is shown by Fig. 6. The original schema (part (a) of the figure) has a foreign key defined on the table t2, mapping the column id in table t2 to the id column of table t1. Since the id column in t1 is a primary key column, the schema is correctly aligned. However, the mutated version of the schema (part (b) of the figure) has had the primary key column changed from id to age. This schema is misaligned, since the foreign key in table t2 of the mutant is still referencing the non-primary key column id. Fig. 6. A relational database schema (in part (a)) and a mutant schema (in part (b)) with foreign key misalignment. With the mutant, the primary key column for t1 has changed (as highlighted) meaning the column referenced by the foreign key for t2 is no longer distinct. Mutants with foreign key misalignment are problematic for most database management systems. For instance, DBMSs such as PostgreSQL and HyperSQL, will reject the second CREATE TABLE statement in Fig. 6b. Other DBMSs, such as SQLite, do not reject database schemas with foreign key misalignment, but simply reject all data that is attempted to be inserted into the table with the misaligned foreign key definition. This leads to a fourth category of ineffective mutant, heretofore not mentioned in the literature and described in the next section. 3.4 Impaired Mutants For database schema mutation, impaired mutants can be created when an integrity constraint is mutated such that satisfaction of the collective system of integrity constraints for that table is not possible. That is, even though the schema will be accepted by the DBMS in use, no INSERT will ever result in a new row of data being added to the database's table. We now discuss how this can occur in terms of patterns found in a definition of a relational database schema. Pattern IM-1: Foreign key misalignment Database schemas with incorrect foreign key alignment, that are stillborn for most DBMSs, are impaired for others, such as for SQLite. This DBMS accepts the table definition as valid, but then refuses to accept data into the table with the misaligned foreign key when foreign keys are enabled for the DBMS. IM-1 is identical to SB-2 except that mutants are identified as impaired rather than stillborn. Pattern IM-2: Infeasible CHECK constraints For most DBMSs, a similar situation can occur with infeasible CHECK constraints. Infeasible CHECK constraints can occur as a result of schema mutation, as shown by the example in Fig. 7. The mutation of the relational operator in the second CHECK results in an infeasible set of constraints, and as a result, every INSERT will be rejected. Since infeasibility of constraints is generally undecidable [40],[44],[45], this type of impairment is hard to detect automatically. Fig. 7. A schema and an impaired mutant. The mutation changes the relational operator in the second CHECK (highlighted for the mutant in part (b)), rendering the constraint infeasible. No data can be inserted into the table of the mutant, and as such we describe it as “impaired”. We name these mutants “impaired” mutants. While they are valid relational schemas as far as the DBMS is concerned—and as such do not qualify as being “stillborn”—they have been damaged by the mutation process. Impaired mutants have little use in mutation analysis, due to the ease with which they are killed—essentially any syntactically valid test case will kill this type of mutant. We therefore categorize them as ineffective, and formally define them as follows: Definition 7 (Impaired Relational Database Schema Mutant) A mutant m i m p of a relational database schema s is said to be impaired for a relational database management system D if there is some table t defined for m i m p for which no INSERT statements are accepted by D . To the best of our knowledge, the concept of an “impaired” mutant has not been defined previously in the literature. An analogous ineffective mutant for program mutation might be a software component that is altered such that whenever it is used or accessed, it returns the same result or throws exceptions, and as such is trivially killed. 3.5 Ineffective Mutant Classification Summary Fig. 8 summarizes our categorization of mutants for database schemas. Out of those produced, only some will be “effective”. “Ineffective” mutants are ones that are either equivalent to the original; redundant , since they are the same as an already produced effective mutant; represent an invalid schema for the DBMS concerned, that is they are stillborn ; or, INSERTs will always fail for one or more tables of the mutant schema, that is, they are impaired . Additionally, this section defines representative patterns that describe the four different types of ineffective mutants: there are six patterns each for equivalent and redundant mutants and two patterns each for stillborn and impaired mutants, respectively. The next section explains how to use these patterns to automatically detect and remove ineffective mutants. Fig. 8. A taxonomy of database schema mutant types. In this figure, boxes with rounded corners represent a type of mutant and a box with non-rounded corners denotes an artifact that plays a role in determining whether or not a mutant is ineffective. The box with a double border shows that these effective mutants will be used in a subsequent mutation analysis. The light gray box highlights the fact that this paper is the first to draw attention to this type of mutant; boxes with a dark gray background correspond to types of ineffective mutant that have been previously reported in the mutation testing literature for programs. 4   Automatically Detecting and Removing Ineffective Database Schema Mutants Ineffective mutants decrease the usefulness of the mutation score, and may also increase the time taken to perform mutation analysis. This section describes our techniques for automatically removing certain classes of ineffective mutant that can be identified in advance of mutation analysis, thereby improving the usefulness of the mutation scores obtained and potentially decreasing analysis costs. The presented techniques rely on an abstract representation of relational database schemas, which greatly simplifies the analysis that needs to be performed, while not losing key information needed to identify the ineffective mutants. After describing this abstract representation, we then introduce our algorithms that use it when detecting and removing stillborn and impaired mutants. To avoid further unnecessary and potentially costly checks involving database schema comparisons, these types of mutants are automatically removed before applying the algorithms that identify and extract the equivalent and redundant mutants. 4.1 Abstract Representation of Database Schemas First, our technique parses the SQL statements that declare a relational database schema and creates a model, which we refer to as the “abstract representation” of a schema. This representation abstracts away the syntactic details of an SQL definition that also make the semantic analysis of schemas for ineffectiveness harder to undertake. This step is important both because, as discussed in Section 3.1, SQL can be used to express the same schema property in a variety of ways and, furthermore, SQL dialects vary across DBMSs In our model, a schema s is a sextuple s = ( T , C C , F K , N N , P K , U C ) , where T is a set of tables, C C is a set of CHECK constraints, F K is a set of FOREIGNKEY constraints, N N is a set of NOTNULL constraints, P K is a set of PRIMARYKEY constraints, and U C is a set of UNIQUE constraints. A table t ∈ T is a pair ( i d t , C ) where i d t is a unique string identifier (i.e., ∀ t ′ = ( i d ′ t , C ′ ) ∈ T , t ≠ t ′ , i d t ≠ i d ′ t ) and C is a set of columns. The function c o l s can be used to obtain the columns for a table (i.e., c o l s ( t ) = C ). A column c ∈ C is a pair ( i d c , t y p e ) , where i d c is a unique string identifier for the column in the table (i.e., ∀ c ′ = ( i d ′ c , t y p e ′ ) ∈ C , c ′ ≠ c , i d c ≠ i d ′ c ), and t y p e is a label indicating the data type of the column (e.g., INT). A CHECK constraint c c ∈ C C is a pair ( t c c , p ) , where t c c is the table to which the CHECK constraint applies, t c c ∈ T , and p is a predicate over the subset of columns c o l s ( t c c ) . A FOREIGNKEY constraint f k ∈ F K is a quadruple ( t f k , T C f k , r f k , R C f k ) , where t f k ∈ T is the table on which the key is defined, and r f k ∈ T is the table that it references. T C f k = ⟨ t c 1 , … , t c l e n ⟩ and R C f k = ⟨ r c 1 , … , r c l e n ⟩ are two lists of columns of equal length l e n , { t c 1 , … , t c l e n } ⊆ c o l s ( t f k ) and { r c 1 , … , r c l e n } ⊆ c o l s ( r f k ) . A NOTNULL constraint n n ∈ N N is a pair ( t n n , c n n ) where t n n and c n n are the table and column on which the constraint is defined, c n n ∈ c o l s ( t n n ) . A PRIMARYKEY constraint p k ∈ P K is a pair ( t p k , C p k ) where t p k and C p k are the table and columns on which the constraint is defined, where C p k ⊆ c o l s ( t p k ) . Only one primary key can be specified per table, that is, ∀ p k ∈ P K , ∄ p k ′ = ( t ′ p k , C ′ p k ) ∈ P K such that p k ≠ p k ′ ∧ t p k = t ′ p k . Finally, a UNIQUE constraint u c ∈ U C is a pair ( t u c , C u c ) where t_{uc} and C_{uc} are the table and columns on which the constraint is defined, C_{uc} \subseteq \textsc {cols}(t_{uc}) . 4.2 Stillborn Mutants As described in Section 3.3, stillborn mutants are mutants that will be rejected by the DBMS and may negatively influence the efficiency of mutation analysis. This paper's static analysis approach involves identifying stillborn mutants on the basis of different patterns. Following the parsing of the relational database schema into the abstract representation described in the previous section, the technique applies different checks to each mutant produced by each of the mutation operators. If the check passes, then the technique removes the mutant. The checks undertaken depend on the DBMS in use during mutation analysis and are as follows. Algorithm 1.   Detecting PRIMARYKEY and UNIQUE constraints on Identical Column Sets for a Schema s  function uniqueOnPrimaryKey(s = (\ldots, {PK}, {UC}))   for all {pk} = (t_{pk}, C_{pk}) \in {PK} do  for all {uc} = (t_{uc}, C_{uc}) \in {UC} do  if t_{pk} = t_{uc} \wedge C_{pk} = C_{uc} then return {true}   end if  end for  end for  return {false}   end function Check 1: PRIMARY KEY and UNIQUE constraints. This check applies to the mutant types characterized by Pattern SB-1 described in Section 3.3 (i.e., UNIQUE constraints defined on exactly the same column set as a PRIMARYKEY in the same table of a schema). These mutants can be detected at the level of the abstract representation using the function uniqueOnPrimaryKey shown by Algorithm 1. Mutants flagged by this detector can then be removed from the pool that is subsequently used during mutation analysis. Check 2: Foreign key misalignment. This check investigates mutants for possible foreign key misalignment according to Pattern SB-2 (Section 3.3) for DBMSs that reject these types of schemas (i.e., HyperSQL and PostgreSQL). This check is automatically performed using the abstract representation of the mutant schema and the function detectFKMisalignment in Algorithm 2. If there is misalignment, then the mutant can be removed from the pool used during the subsequent mutation analysis. 4.3 Impaired Mutants Our checks detect and remove impaired mutants according to Patterns IM-1 and IM-2. Pattern IM-1 (foreign key misalignment) is the same as Pattern SB-2, except it is applied at a different stage for DBMSs that regard mutants with malformed foreign keys as impaired rather than stillborn (e.g., SQLite). Therefore our check for IM-1 re-uses Algorithm 2. The problem of detecting infeasible CHECKs (Pattern IM-2) is generally undecidable [40],[44],[45], although some simple analyses, with limited generality, may be possible. As stated in Section 8, this task is outside of the scope of this paper and thus we leave it for future work. 4.4 Equivalent and Redundant Mutants Detection of equivalent and redundant mutants involves the same static analysis checks—that are applied to different mutants—since the basic problem is to detect whether two mutants behave identically (or are identical). In the case of equivalent mutants, the checks for equivalence take place between the original schema and a mutant, while for redundant mutants, the checks occur for each created mutant. Algorithm 2.   Detecting foreign key misalignment for a Schema s and a DBMS D (The function compatible returns true if two columns have compatible types for the DBMS D , else it returns {false} )  function detectFKMisalignment(s = (\ldots, {FK}, \ldots, {PK}, {UC}), D)   for all {fk} = (t_{fk}, \langle {tc}_1, \ldots, {tc}_{len} \rangle, r_{fk}, \langle {rc}_1, \ldots, {rc}_{len} \rangle) \in {FK} do  {compatible} \leftarrow {true}   for i = 1 \ldots {len} do  if \lnot \textsc {compatible}({D, {tc}_{i}, {rc}_{i}}) then {compatible} \leftarrow {false}   end if  end for  C_{fk} \leftarrow \lbrace {rc}_1, \ldots, {rc}_{len}\rbrace   {foundPK} \leftarrow {false}   for all {pk} = (t_{pk}, C_{pk}) \in {PK} do  if r_{fk} = t_{pk} \wedge C_{fk} = C_{pk} then {foundPK} \leftarrow {true}   end if  end for  {foundUC} \leftarrow {false}   for all {uc} = (t_{uc}, C_{uc}) \in {UC} do  if r_{fk} = t_{uc} \wedge C_{fk} = C_{uc} then {foundUC} \leftarrow {true}   end if  end for  if \lnot {compatible} \vee \lnot ({foundPK} \vee {foundUC}) then  return {false}   end if  end for  return {true}   end function As for stillborn and impaired mutants, the presented solution for detecting equivalence involves the comparison of the abstract representation for a pair of schemas s_1 and s_2 . This structural equivalence is trivial to detect as it is simply the check s_1 = s_2 . Finding behaviorally equivalent mutants is more challenging, however. The presented method converts schemas already in the abstract representation into a normalized form, aiming to produce a single common form of the schema such that behaviorally equivalent mutants will be structurally equivalent. Equivalent mutants can then be removed from the mutant pool used in a later mutation analysis. For a pair of identical mutants, one of the mutants is redundant and can be removed from the pool. Normalization of schemas involves a series of transformation steps, which are linked to the patterns of equivalence identified in Section 3.1. We describe these as follows, furnishing algorithms in terms of our abstract representation and illustrating the algorithms with examples, which, for ease of understanding, we demonstrate as if the abstract schema were written back out into SQL CREATE TABLEs. For clarity, we use before and after examples of database schemas to fully illustrate each of the transformation steps. Transformation Step 1: Conversion of PRIMARY KEY s The first transformation step corresponds to the equivalence patterns BE-1 and BE-2, converting primary keys to equivalent UNIQUEs. The transformation depends on the DBMS's “understanding” of how primary keys should behave. If, like HyperSQL and PostgreSQL, primary key column values should also be not NULL, the conversion involves also adding NOTNULL constraints to the columns concerned. If NULL values can be inserted into primary key columns, as for SQLite, this step is ignored, as Algorithm 3 shows. Algorithm 3   The Conversion of PRIMARYKEY constraints for a Schema s and a DBMS D (The nature of the conversion depends on the DBMS being used. If a DBMS D — such as HyperSQL or PostgreSQL—rejects NULL as a primary key value, the function pksAreNotNull returns {true} and NOTNULL constraints are added to each of the PRIMARYKEY constraints converted to UNIQUE constraints. For DBMSs that accept NULL as a primary key value (e.g., SQLite), the function pksAreNotNull returns {false} , and this particular step is ignored)  function convertPKs(s = (\ldots, {PK}, {UC}), D)   for all {pk} = (t_{pk}, C_{pk}) \in {PK} do  {PK} \leftarrow {PK} \setminus \lbrace {pk}\rbrace   {UC} \leftarrow {UC} \cup \lbrace (t_{pk}, C_{pk}) }  if \textsc {pksAreNotNull}({D}) then  for all {pkc} \in C_{pk} do {NN} \leftarrow {NN} \cup \lbrace (t_{pk}, {pkc})\rbrace   end for  end if  end for  end function The next example illustrates how, for SQLite, the following two schemas, which are behaviorally equivalent as described by pattern BE-1, are normalized into a structurally equivalent form by the transformation step. The right-hand database schema, involving a PRIMARYKEY constraint is affected by the change, and is normalized such that it is now structurally equivalent to the left-hand schema: The next two examples show database schemas that will be submitted to a DBMS that mandates primary key columns should not involve NULL values (e.g., HyperSQL and PostgreSQL). In the first example, the two schemas are behaviorally equivalent according to pattern BE-2. The right-hand schema is normalized by Algorithm 3, and becomes structurally equivalent to the left-hand schema: Algorithm 4.   Removing Extraneous UNIQUE constraints Involving a Superset of Columns for Some Existing UNIQUE constraint Defined on Some Table for a Schema s  function convertUCs(s = (\ldots, {FK}, \ldots, {UC}))   for all {uc} = (t_{uc}, C_{uc}) \in {UC} do  for all {uc}^{\prime } = (t_{uc}^{\prime }, C_{uc}^{\prime }) \in {UC}, {uc} \ne {uc}^{\prime } do  if \nexists {fk} = (t_{fk}, {TC}_{fk}, r_{fk}; {RC}_{fk}) \in {FK} , r_{fk} = t_{uc} \wedge {RC}_{fk} = C_{uc} then if C_{uc}^{\prime } \subset C_{uc} then {UC} \leftarrow {UC} \setminus \lbrace {uc}^{\prime }\rbrace  end if  end if  end for  end for  end function The second example involves a pair of relational database schemas that are behaviorally equivalent according to pattern BE-3. Again, the transformation step converts these schemas into structural equivalents. In this example, both schemas are affected. In the right-hand schema, a NOTNULL constraint is not added since one is already present for the column c in the set {NN} for the database schema. Transformation Step 2: Remove extraneous UNIQUE constraints. Transformation step 2 removes extraneous UNIQUEs defined on schemas—constraints that are superfluous since there is already some other UNIQUE constraint defined on the same table involving a subset of columns (Pattern BE-4 in Section 3.1). Algorithm 4 implements this step, with its third line ensuring that it does not remove a UNIQUE that is involved in a FOREIGNKEY. Note that this algorithm need not be concerned if one of the constraints is a PRIMARYKEY, since these will already have been converted in the previous step (transformation step 1). The following example shows two schemas that are behaviorally equivalent, as described by equivalence pattern BE-4. The right-hand schema has an extraneous UNIQUE that is removed by the algorithm, such that the two schemas become structurally equivalent: Transformation Step 3: Replace instances of CHECK ... IS NOT NULL with NOT NULL constraint As described by equivalence pattern BE-5, CHECK constraints of the form CHECK ... IS NOT NULL are behaviorally equivalent to NOTNULL constraints. Algorithm 5 describes how they may be removed in the abstract representation. The following example shows how such CHECK constraints are removed by the algorithm so that the two schemas involved become structurally equivalent: Finally, we do not handle behavioral equivalence pattern BE-6 in this paper, due to the undecidability of identifying equivalent constraint systems [40],[44],[45]. While simple cases of the problem could be handled by customizing the presented algorithms, we intend, as noted in Section 8, to more generally tackle this task as part of future work. Algorithm 5   Converting NOTNULL Predicates in CHECK constraints to NOTNULL constraints for a Schema s (The function assumes that the predicate of each CHECK constraint is in conjunctive normal form. The function remove removes a conjunct from a predicate, returning the modified predicate or \bot if no conjuncts remain)  function convertCheckNULLs(s = (\ldots, {CC}, \ldots, {UC}, \ldots))   for all cc = (t_{uc}, p = p_1 \wedge \ldots \wedge p_{max}) \in {UC} do  for all c \in \textsc {cols}\,({t_{uc}}) do  for {conjunct} = 1 \ldots {max} do if p_{{conjunct}} = “c IS NOT NULL” then p \leftarrow \textsc {removeClause}({p, {conjunct}})  {NN} \leftarrow {NN} \cup \lbrace (t_{uc}, c)\}  end if  end for  if p = \bot then {CC} \leftarrow {CC} \,\setminus \lbrace {cc}\rbrace   end if  end for  end for  end function 5   Mutation Analysis with SchemaAnalyst We implemented mutation analysis (i.e., the generation of mutants and the repeated execution of the test suite to determine the mutants’ kill status) and the ineffective mutant removal algorithms into our SchemaAnalyst tool [22], which supports the SQLite, PostgreSQL, and HyperSQL DBMSs. Although SchemaAnalyst also performs the automatic generation of test suites for relational schemas [22], since it is the primary focus of this paper, Fig. 9 only shows the different steps involved in mutant production with SchemaAnalyst , which we describe in the following sections. Fig. 9. The inputs and outputs of automatic mutation analysis in the SchemaAnalyst tool. In this figure, a dark square represents the tool and its constituent parts, an arrow stands for a process, a rectangle is a SQL representation, and circle symbolizes a relational database schema. 5.1 Automated Relational Schema Parsing SchemaAnalyst begins by parsing the SQL declarations of the relational schema (i.e., the CREATE TABLE statements) into the abstract, DBMS-independent schema representation described in Section 4.1 (Step 1 of Fig. 9). To control the threats to validity that may arise from incorrectly breaking down SQL commands, SchemaAnalyst performs parsing with the General SQL Parser (GSP),5 a commercial tool that handles SQL for a variety of database management systems, including the three used in the empirical study of Section 6. 5.2 Automated Generation of Mutants After the schema is parsed into the abstract representation, the tool applies mutation operators to produce mutant schemas (Step 2 of Fig. 9). Table 1 summarizes 13 different mutation operators that we apply in this paper and which are implemented into SchemaAnalyst . Designed to model the types of mistakes that database designers might make when specifying a schema, as outlined in Section 2.2, these operators were originally proposed by Kapfhammer et al. [12] and Wright et al. [15] for introducing synthetic faults into the integrity constraints of a relational schema. We designed these operators according to the following principles: Operators should make the smallest possible changes to the integrity constraints in a relational database schema. TABLE 1 The Mutation Operators Studied in This Paper  Operators should be as general as possible, applying to a wide range of DBMSs and vendor interpretations of the SQL standard. Thus, it is not an operator's responsibility to avoid the production of mutants that may be ineffective for one DBMS but effective for another. An operator should not create mutants that are trivially redundant with respect to its other produced mutants. An operator should be usable independently of other operators, thus enabling it to work in either a selective or a higher-order mutation strategy. It is therefore not an operator's concern as to whether the mutants it produces are redundant or not with respect to mutants that may or may not be produced by other operators. While prior work has defined mutation operators for other parts of a database application (e.g., the SQL SELECT statements created by a program [46]), SchemaAnalyst does not incorporate them since they do not adhere to the aforementioned design principles. Notably, operators that manipulate the SELECTs cannot directly process the CREATE TABLE statements that define a relational schema. Section 8 explains that, in future work, we will customize these SELECT-based operators so that they can manipulate schemas. For brevity and ease of identification, we assign each operator a name according to the constraint it targets and the modification it makes. For example, the “Primary KeyColumnAddition” operator is abbreviated to “PKColumnA”. The “addition” and “removal” operators add and remove components, respectively, while the “exchange” operators swap some component for another. The first three operators mutate CHECK constraints. The “CInListElementR” operator removes individual elements from the list of an IN expression (e.g., “CHECK month IN (1, 2, 3 ...)”). The second, “CRelOpE”, produces mutants by replacing the relational operator (i.e., =, <, >, <=, and >=) in an expression of a CHECK constraint with each other possible relational operator. Finally, the third operator, called “CR”, simply removes a CHECK from the definition of a schema. The next two operators mutate FOREIGNKEY constraints. Foreign key definitions require pairs of columns that map values in a column in the table on which the FOREIGNKEY is defined, referred to as the “source” table, to a column in the “referenced” table. “FKColumnPairR”, in contrast, performs the reverse operation, removing a pair of columns from an existing FOREIGNKEY constraint. The “FKColumnPairE” operator exchanges one of the columns in one of the pairs of the key (that is, the column that is changed can be on the source table side of the pair, or on the referenced table side of the pair). Wright et al. [15] also proposed an “FKColumnPairA” operator, which added a pair of columns to an existing foreign key. However, due to foreign key misalignment, this operator only produces non-stillborn or impaired mutants in a very narrow set of circumstances [47]. It does not result in any effective mutants for the representative schemas that we study in the experiments of Section 6 and thus, to forestall artificially inflating the significance of the results, we omitted it from the empirical evaluation. Two operators mutate NOTNULL constraints. The “NNA” operator adds a NOTNULL constraint to a column that did not previously have one, while the “NNR” operator removes an existing NOTNULL constraint from a table's column. Three additional operators mutate PRIMARYKEY constraints. The “PKColumnA” operator adds a column to an existing PRIMARYKEY constraint, or creates a new one from a column should a table not already have a primary key defined. The “PKColumnR” operator performs the reverse operation of removing a column from an existing primary key, while the “PKColumnE” operator exchanges a column in an existing key for another one in the table. The final three operators mutate UNIQUEs in much the same way as primary keys are mutated: adding columns to an existing UNIQUE constraint or creating new constraints (“UColumnA”), removing columns from existing columns (“UColumnR”), and exchanging them (“UColumnE”). In contrast to program mutation, which makes small changes to program code at the syntactic level, these operators apply mutation at a semantic level, automatically processing the abstract representation provided by the SchemaAnalyst tool. This method has clear advantages as it avoids the production of many kinds of ineffective mutants from the outset. Stillborn mutants that result from syntactical issues are not possible, while structurally equivalent mutants, such as those illustrated in Fig. 4, cannot be generated. By definition, each of these operators cannot produce a mutant that is structurally equivalent to the original schema. However, some operators (i.e., “UColumnE”) employ additional checks to ensure that structurally equivalent pairs of mutants (i.e., where one of the pair is redundant) are not produced. In adherence to our design principles, each operator does not know which other operators are being used together, nor does it have a notion of behavioral equivalence or invalidity as these concepts are DBMS specific. Therefore, ineffective mutants may be produced that are stillborn, equivalent, redundant, or impaired. As such, we implemented the algorithms described in the last section to automatically remove these ineffective mutants. The next section introduces the details of this implementation. 5.3 Automatic Removal of Ineffective Mutants Following the tool's automatic generation of mutants, the stage that is novel to this paper removes the ineffective (i.e., the stillborn, impaired, equivalent, and redundant database schema mutants), as discussed in Section 3, and according to the algorithms described in Section 4. The algorithms detailed in that section occupy steps 3–6 of Fig. 9. Step 3 (removal of stillborn mutants), consists of applying Checks 1 and 2 (described in Section 4.2) for HyperSQL. For PostgreSQL, SchemaAnalyst only applies Check 2, the only relevant check for this DBMS; for SQLite, no stillborn mutants can be identified, so the tool does not perform any checks. Any mutants found to be stillborn by these checks are removed from the mutant pool. In the Fig. 9 and in the experiments of Section 6, this is the set of mutants referred to as -S , since it contains all of the generated mutants, minus those the tool identified as stillborn. Step 4 (removal of impaired mutants) applies to SQLite, since SchemaAnalyst only needs to check for mutants with foreign key misalignment for this DBMS. In Fig. 9 and in Section 6's experiments, this is the set of mutants denoted -(S+I) , since it contains all of the generated mutants, minus those identified as being stillborn and impaired. Step 5 (removal of equivalent mutants) normalizes the remaining mutants according to the transformation steps described in Section 4.4. It then compares mutants with the (normalized) original schema for structural equivalence. SchemaAnalyst removes mutants identified as equivalent from the mutant pool. In Fig. 9 and in the experiments of Section 6, this is the set of mutants referred to as -(S+I+E) , since it contains all the mutants generated, minus those the tool identified as being stillborn, impaired as well as those equivalent to the original schema. Step 6 (removal of redundant mutants) uses the normalized mutants in checks for equivalence between the mutants themselves. Where two mutants are found to be structurally identical, one of the mutants is marked as redundant and removed from the mutant pool. In Fig. 9 and in the experiments of Section 6, this is the set of mutants referred to as -(S+I+E+R) , since it contains all the mutants generated, minus those automatically identified by the tool as stillborn, impaired, equivalent, and redundant with respect to some other mutant with which it is equivalent in the mutant pool. 5.4 Automated Mutation Analysis Once it completes the phases in Fig. 9,SchemaAnalyst then outputs the mutant schemas in the form of SQL CREATE TABLE statements, following a standardized SQL-writing process tailored to the DBMS in use in step 7. Mutation analysis can then begin, as described in Section 2.4. SchemaAnalyst applies its automatically generated suites of INSERT statements to the original and mutant schemas, checking whether the INSERTs are accepted or rejected by a DBMS in the same way for the two schemas. If there is any difference, then the mutant is killed, else it is deemed to be alive. Using this information, SchemaAnalyst computes the higher-is-better mutation score for the test suite. 6   Empirical Study In order to evaluate Section 4's technique that automatically detects and removes ineffective mutants for database schemas, we designed an empirical study with the aim of answering the following three research questions: RQ1: Ineffective Mutants Detected by Static Analysis. How many stillborn, impaired, equivalent, and redundant relational database schema mutants are detected using SchemaAnalyst 's automated static analysis approach? Do any ineffective mutants remain that were not identified by the presented method, and how are they characterized? RQ2: Efficiency of the Approach. How does the up-front time cost of statically identifying and removing impaired, equivalent, and redundant database schema mutants compare to the time savings made in not having to analyze them during mutation analysis? In other words, is mutation analysis more efficient overall with or without the use of automatic ineffective mutant identification and removal? RQ3: Impact on the Mutation Score. How does the automatic removal of impaired, equivalent, and redundant mutants influence the mutation score for the schema's tests? That is, how often does the removal of ineffective mutants cause a test suite's mutation score to increase or decrease? Does ineffective mutant removal ever enable a previously non-adequate test suite to achieve a perfect mutation score? 6.1 Methodology We now describe the methodology that we used to conduct our experiments with SchemaAnalyst , beginning with our choice of database schemas to use in mutation analysis with and without the removal of the ineffective mutants. 6.1.1 Subject Schemas In order to answer the aforementioned research questions, we constructed a representative set of 34 database schemas, over double the size of the set of subjects that featured in the conference version of this paper [15], and larger than in previous work on testing database schemas (e.g., [12],[13],[14]). Houkjær et al. notes that complex real-world relational schemas often include features such as composite keys and multi-column foreign-key relationships [10]. As such, the schemas chosen for this paper's study reflect a diverse set of features, from simple instances of each of the main types of integrity constraint (i.e., PRIMARYKEY constraints, FOREIGNKEY constraints, UNIQUE constraints, NOTNULL constraints, and CHECK constraints) to more complex examples involving many-column foreign key relationships. Additionally, the set of subjects that we used in this study involve database schemas drawn from a range of sources. Further details are shown by Table 2: the number of tables in each relational database schema varies from 1 to 42, with a range of just 3 columns in one of the smallest schema, to 309 in the largest. Collectively, the 186 tables and 1044 columns feature each of the main types of database schema integrity constraint that our mutation operators seek to manipulate. TABLE 2 The 34 Relational Database Schemas Studied in This Paper  Several schemas were taken from real-world projects. For example, ArtistSimilarity and ArtistTerm are schemas that underpin part of the Million Song dataset, a freely available research dataset of song metadata [23] (a fragment of which we introduced earlier in Fig. 1b). Cloc is a schema for the database used in a popular open-source application that counts the number of various types of lines in code for many different programming languages (http://cloc.sourceforge.net). IsoFlav_R2 belongs to a plant compound database from the U.S Department of Agriculture. JWhoisServer is used in an open-source, Java-based implementation of a server for the Internet WHOIS protocol (http://jwhoisserver.net). MozillaExtensions and MozillaPermissions were both extracted from SQLite databases that are a part of the Mozilla Firefox Internet browser. RiskIt is a database schema that forms part of a system for modeling the risk of insuring an individual (http://sourceforge.net/projects/riskitinsurance). StackOverflow is the underlying schema used by a popular programming question and answer website, as previously studied in a conference data mining challenge [48]. UnixUsage is taken from an application for monitoring and recording the use of Unix commands, while WordNet is the database schema used in a graph visualizer for the WordNet lexical database (a fragment of which was introduced earlier in Fig. 1a). While some of these database schemas are from real-world applications not used in prior experiments, we chose others because they featured in previous studies of various testing methods (e.g., RiskIt , UnixUsage [49], and JWhoisServer [50]). The six “Nist–” schemas are drawn from the SQL Conformance Test Suite of the National Institute of Standards and Technology (NIST) [24], and have featured in past studies such as those conducted by Tuya et al. [46] (the NistWeather schema in particular is shown by Fig. 1c). DellStore , FrenchTowns , Iso3166 , and Usda were taken from the samples for the PostgreSQL DBMS, available from the PgFoundry.org website. iTrust is a large schema that was designed as part of a patient records medical application to teach students about software testing methods. It previously featured in a mutation analysis experiment of Java code [51]. The remaining schemas (e.g., BankAccount , BookTown , CoffeeOrders , CustomerOrder , Person , and Products ) were extracted from the textbooks, assignments, and online tutorials in which they were provided as examples. While simpler than some of the other schemas used in our study, they nevertheless proved challenging for open-source database analysis tools such as the DBMonster data generator [12]. Since many of the database schemas studied in this paper's experiments contain many lines of complex SQL code, we do not include them in this paper. However, all of the schemas used as subjects are available from the web site for the SchemaAnalyst tool [22]. Moreover, the SchemaAnalyst tool parsed the SQL for each of these schemas into the abstract representation that was previously described in Section 5. Once in this abstract form, the tool wrote the SQL out again for each of the particular DBMSs featured in our study, regardless of minor differences in the version of SQL used; the abstract representation of each schema is also available for download from SchemaAnalyst 's web site [22]. 6.1.2 Subject DBMSs We performed experiments using the HyperSQL, PostgreSQL, and SQLite DBMSs. Each of these database management systems is supported by our SchemaAnalyst tool [22]; they were chosen for their performance differences and varying design goals. PostgreSQL is a full-featured, extensible, and scalable DBMS, while HyperSQL is a lightweight, small DBMS that supports an “in-memory” mode that avoids disk writing. SQLite is a lightweight DBMS that differs in its interpretation of the SQL standard in subtly different ways from HyperSQL and PostgreSQL. All three of these DBMSs are used in a wide variety of real-world programs from many diverse application domains. 6.1.3 Automatic Generation of the Example Test Suites To study the effect of removing ineffective mutants on the usefulness and cost of mutation analysis, we needed example test suites on which to perform mutation analysis. Since none of the chosen schemas are accompanied by a suite of tests that contain a sequence of INSERT statements, we generated suites with SchemaAnalyst using the approach described in our prior work [13]. In that paper, we detailed a series of coverage criteria and automated techniques that aim to generate tests to fulfill them. Importantly, the number of tests generated by the techniques from our prior work is a function of the chosen coverage criteria and not a parameter whose values are controlled by the empirical study's design. We used Random^{+} and AVM to automatically generate test cases with the aim of satisfying the coverage criteria combination of “ClauseAICC”, “AUCC”, and “ANCC”, as previously introduced in Section 2.3. Since our previous work showed that these two test generators satisfy these particular criteria to different degrees [13], resulting in tests with medium to strong mutant killing power, we deemed them highly suited to the task of assessing the relative usefulness and costs of mutation analysis with and without ineffective mutant removal. Since both methods rely on random number generation, we generated 30 test suites for each database schema using each of the two techniques and while always employing a different random seed [52]. 6.1.4 Experimental Procedure RQ1: Ineffective Mutants Detected by Static Analysis. To answer RQ1, we ran the automated static analysis approach for detecting ineffective mutants on each of the schemas when hosted by every DBMS, recording the numbers of mutants detected for each of the four category types—stillborn, impaired, equivalent, and redundant. Our static analysis procedure follows rules consistent with each DBMS such that false positives are unlikely to occur, unless there are bugs in our implementation of SchemaAnalyst or in the database management system itself (see Section 6.2 for more details about how we addressed the former in mitigating the threats to the validity of our experimental study). Although we judge that SchemaAnalyst is capable of identifying large numbers of mutants as ineffective, its checks are not exhaustive, and as such false negatives are still possible—that is, there may be mutant schemas that are ineffective, yet missed by our approach. For stillborn mutants, false negatives are easy to find in the course of standard mutation analysis—any mutant not identified as stillborn is rejected by the DBMS. For equivalent mutants, we manually analyzed the live mutants following mutation analysis, as a mutant that is killed by a test suite cannot be equivalent. For this purpose, a mutant is counted as “live” if it was not killed by any test suite. Since the number of live mutants is relatively few in number, an exhaustive manual analysis of these mutants is possible. For impaired and redundant mutants, however, no automated DBMS checks exist, nor is the set of mutants naturally reduced to a tractable number for manual checking through our implemented and tested mutation analysis procedure. Thus, we further checked our results through an intensive manual spot-check of the database schema mutants. To do so, we selected a subset of non-stillborn mutants produced by SchemaAnalyst for each schema and DBMS and manually generated INSERT statements with the aim of checking the classification of each mutant by our tool as either equivalent, redundant, impaired, or normal (i.e., effective). We selected an initial 50 mutants at random. We then added a further seven mutants to this pool to ensure that it contained at least one representative mutant for each schema, DBMS, mutation operator, and ineffective mutant pattern, as detailed in Section 3. Where possible, we selected these additional mutants at random from a constrained set (e.g., all mutants produced by a particular operator or for a schema, if fixing one of these aspects was an important property). Finding an exemplar mutant for Pattern IM-2 (i.e., infeasible CHECK constraints) was less straightforward, however, since we were unaware if such mutants existed in the set of mutants SchemaAnalyst generated for our schemas, and if they did, which ones they were. The process we adopted was therefore as follows: We manually reasoned about the 13 subject schemas involving CHECK constraints listed in Table 2, first concluding that removing elements of IN expressions through the CInListElementR operator, or complete constraints through the CR operator, would not result in infeasible constraints for any of these schemas. This left mutants produced by the CRelOpE operator, which was applicable to eight of our schemas (i.e., BookTown , BrowserCookies , Employee , Examination , NistXTS748 , NistXTS749 , Products , and StudentResidence ), as they involve CHECK constraints with relational expressions. Two of these schemas (i.e., BrowserCookies and Products ) have expressions of sufficient complexity that they could be sources of infeasibility following mutation. The rest involve expressions that simply compare a column with a constant. We performed an exhaustive manual analysis of the mutants produced by CRelOpE for BrowserCookies (15 mutants) and Products (20 mutants). We found three mutants for Products with infeasible CHECKs following mutation and selected one to use in our manual spot-check of mutants. The first author then produced a JUnit test suite for each of the 57 mutants for our manual spot-check analysis. Each test suite consisted of INSERT statements that could be automatically checked against mutants with the intention of asserting whether that mutant was correctly classified as “effective” or “ineffective”, and, if ineffective, what type of ineffective mutant it was. To rule out the possibility of a mutant being “impaired”, the first author devised INSERTs to show that data could be added to each table. To eliminate the possibility of manually classifying a mutant as equivalent to the original schema, INSERTs were crafted to show a difference in behavior for the original schema and the selected mutant (that is, a difference in the acceptance/rejection pattern of the INSERTs with the mutant compared to the original schema for the DBMS concerned). Finally, to rule out the conclusion that the mutant was equivalent to another mutant (i.e., it was redundant) further INSERT statements were written to ensure a difference between the mutant and each other mutant produced for the schema in question. To assist this process, the first author wrote utility methods that could be used by each JUnit test suite to automatically instantiate databases with the original and mutant schemas, submit INSERT statements, and compare the DBMS responses. If we could not construct INSERT statements to refute a particular type of ineffectiveness, the mutant was labeled accordingly, and the manually derived conclusion cross-checked against the mutant's classification as automatically computed by SchemaAnalyst using the static detection routines. As noted in Section 6.2, all of the aforementioned test suites, classifications, and crosschecks produced by the first author are available for download from a replication package accompanying this paper.6 RQ2: Efficiency of the Approach. To answer RQ2, we split up our analysis to specifically investigate (a) stillborn mutants and (b) impaired, equivalent, and redundant mutants. We treat these two sets of mutants separately since stillborn mutants may also be identified using the database management system, while the other three types of ineffective mutants cannot. If a mutant is stillborn, the DBMS will reject its CREATE TABLE statements. There are no such DBMS checks for impaired, equivalent, and redundant mutants. So, we compare the performance of our algorithms against the use of the DBMS for stillborn mutants, while for the other types of mutants, we compare the execution cost of mutation analysis with and without their inclusion. As background processes on the workstation could lead to small differences in the timings collected to answer this research question, we always ran 30 repeat trials for each experiment [52]. (a) Stillborn Mutants. For the first part of our investigation, relating specifically to stillborn mutants, we ran three experiments. First, we recorded the time taken to submit each of the CREATE TABLE statements for each mutant for each subject schema to every DBMS (i.e., HyperSQL, PostgreSQL, and SQLite). We then verified whether the DBMS accepted the mutant schema or rejected it as invalid. This particular scenario represents the simplest method of performing mutation analysis for database schemas, since schemas are identified during the process as stillborn by relying on the DBMS to report an error when the schema is invalid. Second, we recorded the time taken to perform the same process, but this time by wrapping the CREATE TABLEs for each mutant schema inside SQL transactions. This represents a potentially faster method of detecting stillborn mutants using the DBMS. Transactions leverage the “roll back” feature of a DBMS to remove any successfully created tables in the event of DBMS rejection of some later CREATE TABLE[53], rather than individually removing the parts of the schema that were successfully created. This is important since all fragments of a schema need to be removed from the DBMS in preparation for the analysis of the next mutant. Finally, we recorded the time taken to perform the stillborn mutant checking process using SchemaAnalyst 's automated static analysis approach. That is, the tool identified stillborn mutants ahead of the mutation analysis process, and then removed them from the mutant pool. Since, to our knowledge, stillborn mutants for database schema integrity constraints cannot be created for SQLite with the operators studied—and as a result there is no need for static analysis checks for this particular DBMS—we ran SchemaAnalyst for this analysis with only HyperSQL and PostgreSQL. (b) Impaired, Equivalent, and Redundant Mutants. To address the second part of the investigation, relating to impaired, equivalent, and redundant mutants, we first studied the time taken to run the detection and removal algorithms with each combination of schema and DBMS to achieve the four sets of mutants introduced in Section 5.3: “-S ”, which corresponds to all mutants that are produced except those identified by the algorithms as stillborn; “-(S+I) ”, which further excludes mutants identified as impaired; “-(S+I+E) ”, which additionally excludes equivalent mutants; and finally “-(S+I+E+R) ”, which also excludes redundant mutants, and as such excludes all ineffective mutants found by the algorithms. Fig. 9 showed the sequencing of these removals implemented in SchemaAnalyst . To find the times taken to produce every set of ineffective mutants, we timed how long SchemaAnalyst took to execute each of the steps 4–6 as described in Section 5.3 and depicted in Fig. 9. The time needed to produce the -(S+I+E+R) set of mutants corresponds to the complete time (i.e., for steps 4–6 inclusively). To obtain the time for -(S+I+E) , we subtracted the time spent in step 6. For -(S+I) , we subtracted the time spent in steps 5 and 6; while for -S the time is zero, since mutation analysis will always take place with stillborn mutants removed, regardless of which of the three different methods studied in part (a) of this research question is used to remove them. We ran SchemaAnalyst to perform mutation analysis with all non-stillborn mutants produced by its operators (i.e., the set “-S ”), recording the time taken to evaluate each individual mutant. Then it calculated the time taken to perform mutation analysis for each of the four different sets of mutants (i.e., -S , -(S+I) , -(S+I+E) , and -(S+I\ +E+R) ) by summing the evaluation times for each of the mutants in each of those particular sets. We repeated mutation analysis 30 times for each combination of schema, DBMS (i.e., HyperSQL, PostgreSQL, and SQLite) and test generation method (i.e., the AVM and Random^{+} ) using different tests generated with a different random seed, thus minimizing the possibility of random chance, during test generation, affecting the results [52]. To produce a total time to perform mutation analysis with each of the four mutant sets, we added the time to produce each respective set of mutants with the time needed to perform mutation analysis with it. RQ3: Impact on the Mutation Score. To answer RQ3, we used SchemaAnalyst to compute the mutation scores for each of the four sets of mutants (i.e., -S , -(S+I) , -(S+I+E) , and -(S+I+E+R) ) as evaluated during mutation analysis for the experiments that we conducted to answer RQ2. We performed all of the experiments with our SchemaAnalyst tool [12],[13],[15],[22], as described in Section 5, compiled with the Java Development Kit 7 compiler and executed with the Linux version of the 64-bit Oracle Java 1.7 virtual machine. Experiments were executed on a dedicated Ubuntu 14.04 workstation, with a 3.13.0-44 GNU/Linux 64-bit kernel, a quad-core 2.4GHz CPU, and 12GB RAM. All input (i.e., relational database schemas) and output (i.e., data files) were stored on the workstation's local disk. We used the default configuration of PostgreSQL version 9.3.5, HyperSQL version 2.2.8, and SQLite 3.8.2. HyperSQL and SQLite were used with “in-memory” mode enabled. 6.1.5 Evaluating the Impact on Timing and Mutation Score For each experiment, we computed the means of mutation scores and timings, over each of the experiment's 30 repetitions. To gauge the efficiency implications of ineffective mutant removal, we compared the time taken, and the mutation scores obtained, for mutation analysis with and without ineffective mutants. For timing data, where a type of mutant was removed from the mutant pool, we include the time required for the static analyses to run, detect, and remove ineffective mutants, thereby producing the different mutant sets described in Section 5.3 (i.e., “-S ” and “-(S+I) ”). Given two sets of data (obtained for either timing or mutation score, one set with an ineffective mutant type and one without), we checked for statistical significance with the Wilcoxon Rank-Sum test, using p < 0.05 as the significance threshold [52]. Then, we calculated the Vargha-Delaney Â statistic to measure effect size, thereby determining the average probability that one approach outperforms another [54]. In the tables of timing data and mutation scores (i.e., Tables 9–14), we annotate large effect sizes (that is, Â < 0.29 or > 0.71) with a “\star ”. Statistically significant decreases are annotated by a “\triangledown ” symbol, while statistically significant increases are annotated by a “\triangle ” symbol. If timings are subject to a significant decrease, this means that the process is more efficient with the removal of the ineffective mutants. Conversely, if timings are subject to a significant increase, the exclusion of ineffective mutants is slower than when it includes them. With mutation scores, a significant increase means that test suites killed a more favorable percentage of mutants following the removal of an ineffective mutant, while a significant decrease indicates that test suites killed a less favorable percentage of mutants. For assessing the implications of removing stillborn mutants, we only present standard deviations computed for the 30 runs of each experiment, as shown by Table 8. Due to the large differences in the means, and the relatively small standard deviations involved, the trend was clearly evident and thus further statistical analysis was not necessary. 6.2 Threats to Validity We now detail some threats to the validity of our empirical study, and explain how we sought to mitigate them as part of our experimental design. At the outset, it is important to note that the SchemaAnalyst tool and all of the relational database schemas used in this paper's study are available from the tool's web site.7 The availability of the data generation and mutation analysis tools, in addition to the SQL source code for each schema listed in Table 2, permits both the replication of this paper's experiments and the external confirmation that we correctly controlled many of the threats to validity discussed in this section. Also, all of the data sets and each of the data manipulation, statistical analysis, and table-creation routines—implemented separately by two different authors of the paper in two distinct programming languages—are available for download from the web site for this paper's replication package.8 Along with supporting the external confirmation that we appropriately controlled some of the validity threats mentioned in the remainder of this section, the availability of this replication package enables the recreation of all of the paper's data tables and statistical analyses [55]. In summary, along with releasing all of the software used to arrive at this paper's conclusions, we identified and handled the following validity threats for the experimental study. The schemas used may not generalize. While it impossible for us to claim that our schemas are representative of all of the characteristics of all possible relational database schemas, the set of subjects we have collected is larger than previously considered [12],[13],[14],[15] and contains schemas drawn from a wide range of sources, including the production systems detailed in Section 6.1.1. Table 2 shows the diversity captured by the 34 schemas that vary in size and their coverage of each of the main types of integrity constraint. The DBMSs used are not representative. While it is the case that there are some popular DBMSs that we did not include in the experiments, we note that our choice of DBMSs provides a good coverage of the different design goals (i.e., high performance through in-memory data storage or stability by keeping data on disk) and adherence to the SQL standard of many DBMSs used in practice, as we explained in Section 6.1.2. Although the results may vary for different DBMSs, the patterns observed for other management systems are likely to be similar to those seen for the chosen DBMSs as long as their features are similar—which several recent comparisons suggest is, in fact, largely the case.99. The web site available at http://goo.gl/7pzxeV provides a regularly-updated “DBMS comparison” table revealing that different database management systems now offer many of the same features. The test suites used may bias the results. To ensure a diverse set of tests in each of our test suites, we chose to generate test suites with two different test data generation techniques—the AVM and Random^{+} —with their differing approaches to obtaining coverage, as explained in Section 6.1.3. These two methods are stochastic, and so further diversity can be achieved by repeating experiments using a different random seed, which we did for each experiment and test data generator. Finally, since none of the chosen database schemas were accompanied by tests, we could not study how these types of test influenced the detection and removal of ineffective mutants; Section 8 notes that this may be a promising area for future work as more database designers start to test relational schemas. The mutation operators may not generalize. Since prior work has shown that real-world relational schemas are complex and often include features such as composite keys and multi-column foreign-key relationships [10], our operators specifically target these aspects of relational database schemas. Yet, different results may be obtained with different types of operators, and our results may not generalize to those operators. For instance, this paper does not focus on the identification and removal of ineffective higher-order mutants. However, Section 8 notes that we plan, as part of future work, to further control this threat by extending the set of mutation operators used by the SchemaAnalyst tool. The mutants are not representative of real faults. According to the “competent programmer” hypothesis [56], programmers are likely to produce programs that are nearly correct, implying that real faults will frequently be the result of small mistakes. By making small changes to each type of constraint, the mutation operators that we used were designed to model such faults in the context of relational database schemas. They implement operators for both the addition and removal of columns, and as such model faults of both omission and commission, further improving the range of mistakes in database schema that they can represent. Background tasks interfering with timings. The timing of the processes for detecting and removing ineffective mutants, and performing the mutation analysis itself, are subject to interference from background tasks. To minimize the impact of background tasks, we repeated the experiments and recorded all timings. Defects in the SchemaAnalyst tool. To mitigate this threat, we have implemented a JUnit test suite in parallel with the development of the SchemaAnalyst tool itself. Furthermore, we have extensively hand checked the results obtained to ensure that they are correct. In addition, as part of the methodology of the experiments, we manually-checked the classification of 57 mutants, further confirming the tool's correctness. Mistakes made as part of the manual analysis. Although the INSERT statements and JUnit tests used to find false positives in RQ1 of the experimental study were manually written by an author of this paper, they were automatically checked against the behavior of the DBMS and other mutants, and in each case our conclusions agreed with the result produced by the static analysis algorithms for ineffective mutant detection. The statistical tests used. We cannot be certain that our data is normally distributed, and as a result, we used non-parametric statistical tests, including the Wilcoxon Rank-Sum (Mann-Whitney U) Test and the Vargha-Delaney Â statistic for measuring effect size. These two statistical tests are commonly adopted for analysing results arising from the study of software engineering methods that employ randomness [52], thereby mitigating concerns that our conclusions are incorrect. Defects in the statistical analysis tools. Since it is possible that we made a mistake during the manipulation and statistical analysis of the empirical results, we took several steps to control this threat to validity. For instance, two authors of this paper separately implemented the data analysis routines and then compared the outputs, ultimately finding agreement in the final data tables and outcomes of the statistical tests. 6.3 Characterizing the Test Suites Since none of the database schemas came with a test suite, we automatically generated tests using the AVM and Random^{+} methods provided by the SchemaAnalyst tool. Table 3 characterizes the test suites created by both of these techniques, revealing that it is rare for the AVM to not achieve full coverage of the test requirements. In fact, in cases where AVM does not cover all of the requirements, we found that this was due to an infeasibility in the constraints that the test data generator must cover. This table also shows that test suites created by AVM are of a higher coverage—and often comprised of more tests—than those created by Random^{+} , thereby suggesting that they will also have a higher mutation score and a longer mutation analysis time than those that are produced by the random method. TABLE 3 Mean Coverage and Size of the Test Suites that were Automatically Generated by Random^{+} and the AVM  6.4 Answers to Research Questions RQ1: Ineffective Mutants Detected by Static Analysis Table 4 shows the number of mutants produced for each database schema, and the ineffective mutants identified for each. Table 5 breaks the data down by mutation operator. TABLE 4 Ineffective Mutants by Database Schema  TABLE 5 Ineffective Mutants by Mutation Operator  The number of mutants produced for each database schema depends on the number and type of integrity constraints it has, all information that is shown in Table 2. If a type of integrity constraint is not present for a schema, then certain operators cannot be applied (e.g., the NNR operator cannot be used to produce mutants with NOTNULL constraints removed, if there are no NOTNULL constraints in the first instance). Certain types of integrity constraints will yield more mutants with certain operators than others. For example, the PKColumnA operator will produce four different mutants for a table with a single column primary key with five columns, where each mutant is the original primary key with another column in the table added to it. Responding to this research question, we now discuss the results for each of the types of ineffective mutant. Stillborn Mutants. The tables show that SchemaAnalyst 's use of the abstract representation and static analysis checks leads to the identification of many stillborn mutants for the 34 schemas and the HyperSQL and PostgreSQL DBMSs. For stillborn mutants, it is possible to automatically verify the results by submitting each mutant to the DBMS and checking to see if was rejected as invalid. This process confirmed that the static analysis correctly identified all stillborn mutants. The stillborn mutants found are identified as a result of the foreign key misalignment rule (as discussed in Section 3.3) and the rule for HyperSQL that detects PRIMARYKEY constraints and UNIQUE constraints with identical column sets, which are disallowed for this DBMS. Since neither issue affects SQLite, no stillborn mutants were found for this DBMS. Table 4 shows that the NistDML182 schema resulted in the most stillborn mutants with the mutation operators. This schema has one foreign key with 15 columns, leading to many instances of foreign key misalignment when SchemaAnalyst applies the mutation operators. The RiskIt , UnixUsage , and CustomerOrder schemas also have high numbers of stillborn mutants. As Table 2 shows, these schemas have the highest number of foreign keys (10, 7, and 7, respectively), which again causes foreign key misalignment. As seen from Table 5, approximately 80 percent of mutants produced by the FKColumnPairE operator are stillborn. In order to create a mutant that maintains a correctly aligned foreign key, this operator has to exchange a column where the modified column set in the referenced table corresponds to an existing PRIMARYKEY or UNIQUE constraint so that the mutant is valid. However, this happened relatively infrequently. Moreover, approximately a third of mutants are stillborn for the FKColumnPairR operator. Since FKColumnPairR removes a pair of columns from the foreign key (i.e., a column in the foreign key table and its associated column in the referenced table), valid mutants tend not to be produced except for when the foreign key involves a single column pair, in which case the entire constraint is removed. Impaired mutants. For SQLite, mutant schemas with foreign key misalignment are impaired, rather than stillborn. Accordingly, many mutants that would have been classified as stillborn for HyperSQL and PostgreSQL are identified as impaired for SQLite. An example of this phenomenon is given in Table 5, where, for SQLite, operators like FKColumnPairE and PKColumnA, and PKColumnE produce no stillborn mutants and, respectively, 415, 102, and 111 impaired mutants. Also, there are fewer impaired mutants for SQLite than stillborn mutants for HyperSQL and PostgreSQL because SQLite does not regard mutants with UNIQUE constraints and PRIMARYKEYs on the same sets of columns as invalid, as does HyperSQL, and type mismatches between columns in mutated, yet correctly-aligned, FOREIGNKEYs are of no concern for this DBMS. (As discussed in Section 3.3, SQLite has a weak typing mechanism where, for example, a column of type TEXT can form a foreign key with a column of type INTEGER.) Since there is no need for static checks for impaired mutants with HyperSQL and PostgreSQL, no impaired mutants were identified for these DBMSs. Yet, our manual analysis did reveal impaired mutants that escaped the automated analysis. For the mutants of schemas with CHECKs, we found three that had constraints that were infeasible, and are therefore impaired. Involving Products , these mutants were produced by CRelOpE, which is responsible for changing the relational operator in a CHECK. This database schema has a table involving the three CHECK constraints shown by Fig. 10. The operator mutated the first constraint to price = 0, price < 0, and price <= 0, respectively. Because of the second constraint, mandating that discounted_price be greater than zero, the third constraint price > discounted_price can never be true. Fig. 10. CHECK constraints of the Products schema. It is worth noting that our manual analysis involved an exhaustive search for mutants with infeasible CHECK constraints, and resulted in us finding only three mutants—a very small percentage of the total number of mutants produced for the subject schemas. As reported in Tables 4 and 5, a total of 5223 mutants were produced, meaning that only \frac{3}{5233} = 0.06 percent of mutants escaped the automated analysis. Nevertheless, as discussed in Section 8, future work will automatically identify cases of infeasibility in CHECKs and remove them from the subsequent mutation analysis. Equivalent mutants. Tables 4 and 5 show that a significant number of mutants were identified as equivalent to the original schema by the automated analysis. Following the removal of stillborn and impaired mutants from the total of 5223 mutants produced for all subject schemas, SchemaAnalyst identified 162 (3), 271 (5), and 115 (2 percent) of these as equivalent for the HyperSQL, PostgreSQL, and SQLite DBMSs, respectively. Using the mutation operators with SQLite results in the fewest equivalent mutants of all three DBMSs detected by the static analysis checks (115 mutants). As shown by Table 5, the NNA and NNR operators do not produce equivalent mutants for SQLite—even though they do so with PostgreSQL and HyperSQL. This is because of the difference in PRIMARYKEY behavior between SQLite and the other two DBMSs. For PostgreSQL and HyperSQL, a NOTNULL can be added to or removed from a column that is already part of a PRIMARYKEY, and it will have no effect on the behavior of the PRIMARYKEY constraint, since for these DBMSs, primary key columns also have an implicit NOTNULL defined on them. As such, the NNA and NNR operators produce mutants that are indistinguishable in behavior from the original schema. However, for SQLite, values in primary key columns may be NULL, so adding and removing NOTNULL constraints on these columns changes the behavior of the schema. As shown in Table 4, the number of equivalent mutants detected for HyperSQL (162 mutants) is lower than that for PostgreSQL (271 mutants). This phenomenon is evident because many mutants that are equivalent for the PostgreSQL DBMS are stillborn for HyperSQL, and thus they were previously removed from the mutant pool. This set of mutants corresponds to schemas where a PRIMARYKEY and a UNIQUE constraint involve an identical set of columns, and, as Table 5 shows, is largely the result of the UColumnA operator, where a column is added to an existing UNIQUE constraint or a new single-column UNIQUE is created that is identical to the database table's primary key. With the goal of finding equivalent mutants that were not detected by our static analysis approach, we investigated mutants not killed following all of the mutation analysis runs, in adherence to the methodology detailed in Section 6.1.4. Table 6 summarizes this data, showing the numbers of remaining live mutants for each schema and operator after mutation analysis with each of the three chosen DBMSs. We manually studied each live mutant to try and ascertain whether it was a genuine equivalent mutant that was missed by our automated analysis, or whether the test suites used had simply failed to kill it. Following this investigation, we found that only three mutants were genuinely equivalent for each of the DBMSs. The first equivalent mutant is the one produced by the CR operator for Products , as listed in Table 6. This operator mutated the CHECK constraints shown by Fig. 10. For this schema, the first constraint is actually superfluous, since price must be greater than zero, if, according to constraint (2), discounted_price is greater than zero, and price must be greater than discounted_price as per constraint (3). Therefore, when the CR operator produces a mutant by removing constraint (1), the mutant is equivalent to the original. Two further equivalent mutants occur with the Products schema and CRelOpE, accounting for two of the three mutants listed for CRelOpE in Table 6. The first mutant changes the expression of CHECK constraint (1) to price != 0 while the second changes it to price >= 0. Again, these constraints add nothing further to constraints (2) and (3), and are thus equivalent to the original schema. TABLE 6 Live Mutants Following Mutation Analysis with All Test Suites  Manual analysis of the remaining live mutants revealed that the automatically generated test suites were incapable of distinguishing each of these mutants from their corresponding original schema. That is, the mutants were in theory killable by a test suite, and they were not actually equivalent. This is a shortcoming of the generated test suites and not the technique for detecting equivalent mutants. We refer the reader to our prior work on test data generation for relational database schemas [13] for a discussion of why the test suites generated with the chosen coverage criteria cannot kill all of the mutants that the operators produce. In summary, the automated analysis approach detects a significant number of equivalent mutants. However, due to the arbitrary nature of CHECK constraints, some mutants related to this type of constraint are not detected. As for impaired mutants, this is a small number (i.e., 3 of 5223 mutants), and is related to the fact that the current implementation does not analyze CHECK constraints. As mentioned in Section 8, we will address this issue in future work. Redundant mutants. Tables 4 and 5 show the number of schema mutants found to be redundant using the automated analysis. These tables show that, following the removal of stillborn, impaired, and equivalent mutants from the initial total of 5223 mutants produced for all subject schemas, SchemaAnalyst identified 171 (3), 179 (3) and 423 (7 percent) of these as redundant for the HyperSQL, PostgreSQL, and SQLite DBMSs, respectively. In practice, redundant mutants can be caused by the same or two different operators producing two identical mutants. One mutant is kept, while the other is removed from the mutant pool. Table 5 lists the mutants removed according the operator that produced them. Table 7 gives another view of redundant mutants, showing the pairs of operators that were responsible for producing the identical mutant pairs. The operators listed on each row are the ones that produced the mutant that was removed, while the operators listed on each column are those that produced the identical mutant that SchemaAnalyst retained. The cells of the table contain numbers of identical mutant pairs produced by each pair of operators for a particular DBMS. TABLE 7 Redundant Mutants by Pairs of Operators That Produced Them  Tables 4 and 5 show that more redundant mutants were produced for SQLite than for HyperSQL and PostgreSQL. Table 7 reveals that this was due to the overlapping effects of PKColumnA and UColumnA and the fact that SQLite does not force primary key values to also not be NULL. As such, these two operators can produce schemas with the same behavior for this DBMS. Since HyperSQL and PostgreSQL do require primary key values to not be NULL, the same effect does not occur, except when primary key columns are also declared as NOTNULL—that is, the addition of either a PRIMARYKEY or a UNIQUE constraint to a column would have had an identical effect in terms of the schema's behavior. Table 7 shows that redundant mutants can be placed into two categories: redundant mutants that were caused by two operators that mutate the (a) same or (b) a different type of integrity constraints. In category (a) are redundant mutants caused by operators that add, remove, and exchange columns from foreign keys, primary keys, and UNIQUE constraints have overlapping effects that are detected using the automated analysis. An example of this can occur when a table has two single-column UNIQUE constraints. The column of the first constraint is exchanged with that of the second by the UColumnE operator, which effectively removes the first constraint. This replicates UColumnR when it produces a mutant that removes the same constraint, resulting in behaviorally identical schemas. Yet, these mutants are a small proportion of the original pool of 5223, with the exact numbers depending on the DBMS used in each case. In category (b) are redundant mutants generated by PKColumnR and UColumnA. Again, these mutants happen in relatively rare situations (i.e., 23 to 30 mutants, depending on the DBMS), as Table 7 indicates. An example of such a situation is when a mutant is produced that removes a column from a multi-column primary key on columns (A, B) with PKColumnR, making it a primary key on just A. A behaviorally identical mutant is produced with UColumnA by adding a UNIQUE constraint to the column A. Although this mutant still has the primary key on (A, B), it now behaves the same as the UNIQUE constraint on A, for the reasons explained in Section 3's presentation of Pattern BE-4. Other mutants in this category are created by PKColumnA and NNA for PostgreSQL. This occurs when the operators add a primary key and a NOTNULL, respectively, to a column already declared as UNIQUE. (For HyperSQL, adding a primary key to a UNIQUE column makes it stillborn, whereas for SQLite, primary keys are not also required to be not NULL—hence the two schemas have different behaviors for this DBMS). Finally, our careful manual analysis of the 57 mutants chosen to verify SchemaAnalyst 's automated approach revealed no redundant mutants not already found by our tool. Conclusion for RQ1. The automated static analyses detected many ineffective mutants, the majority of which are stillborn or impaired, accounting for as many as 18 percent of mutants with HyperSQL. The tool also detected significant numbers of equivalent and redundant mutants. Our manual analysis of these mutants revealed that there were some ineffective mutants that our approach did not detect, but that they were relatively few in number and were associated with complex CHECKs. Analysis of arbitrary constraints for infeasibility and equivalence is undecidable in general [40],[44],[45], rendering these types of ineffective mutant hard to detect automatically. Yet, if the schema is free of CHECKs, the static analyses can reliably detect all ineffective mutants. If the schema does involve CHECKs, then some ineffective mutants may be missed, particularly where the constraints are relational expressions and there are multiple constraints involving the same columns, both of which contributed to mutants with infeasible constraints when applying the operators. Overall, our technique was able to identify approximately 24 percent of mutants as ineffective, regardless of the DBMS being used. RQ2: Efficiency of the Approach Stillborn Mutants. Table 8 shows the times taken for each of the three methods devised to identify stillborn mutants for HyperSQL and PostgreSQL (i.e., “DBMS”, “DBMS-Transacted”, and “Static”), as detailed by the methodology described in Section 6.1.4. Since no stillborn mutants are produced by our operators for SQLite—as confirmed by the answer to the last research question—there is no need for static analysis checks, and thus there are no results to report for this DBMS. TABLE 8 Mean Times Taken to Detect Stillborn Database Schema Mutants (in Milliseconds)  This table shows that using a DBMS is significantly more time consuming than using static analysis (in fact, due to the clarity of this result, we do not furnish a statistical analysis of these data points). This is the case even when attempting to use the DBMS to check schemas as efficiently as possible, by wrapping CREATETABLE statements in transactions. Static analysis only takes a fraction of a second for any of the schema and DBMS combinations. For HyperSQL, it requires two milliseconds or less for four schemas (i.e., Cloc , DellStore , StackOverflow , and Usda ) and, with PostgreSQL, less than one millisecond for 15 schemas (i.e., for just under half of the schemas). The longest time was recorded for NistDML182 with HyperSQL, at just 265 milliseconds. In contrast, relying on the DBMS to reject schemas takes several orders of magnitude longer, with the use of transactions only marginally decreasing the time overhead. The longest time recorded is with the non-transacted method for iTrust , which requires over eight seconds to process with HyperSQL, and over one hour with PostgreSQL. As Table 8 shows, more processing time was required for the database schemas when used in conjunction with PostgreSQL as opposed to HyperSQL, whether it be with the DBMS method (i.e., non-transacted) or with the DBMS-Transacted version. Further analysis of each technique's longest processing times sheds light on how schema characteristics influence running time. For the static analysis checks, the schemas with the longest processing times (for HyperSQL and PostgreSQL, respectively) are NistDML182 (257 ms and 265 ms), RiskIt (208 ms and 199 ms), UnixUsage (149 ms and 141 ms), and CustomerOrder (144 ms and 138 ms). These are also the schemas with the greatest number of foreign keys (c.f. Table 2) or the most complex foreign key relationships—as discussed in the answer to the last research question—and which, therefore, require the most foreign key misalignment checks needed for detecting the stillborn mutants. For the DBMS-based methods, the schemas with lengthy processing times tend to be those that are the largest and/or result in the greatest number of mutants—that is, the ones that will require the most setup on the host DBMS and/or the most DBMS-based validity checks. For example, iTrust , MozillaExtensions , RiskIt , and BookTown produce the longest times for the (non-transacted) DBMS method with PostgreSQL (46, 2.9, 2.6, and 2.4 \times 10^5 ms respectively). These are the schemas that produce the most mutants (first is iTrust , with 1458 mutants; second is MozillaExtensions with 364, as shown by Table 4), or are the largest in terms of the number of tables (first is iTrust , with 42 tables; second is BookTown with 22; third is RiskIt with 13, as shown by Table 2). While the times required for the DBMS-based methods are related to the schema's size or the number of mutants which result from it, the cost of the static checks is more closely related to the number of stillborn mutants produced by the operators. Impaired, Equivalent, and Redundant Mutants. Following the notational conventions given in Section 6.1.4,Tables 9 and 10 report mean mutation analysis times with tests generated using AVM and Random^{+} , respectively, with the exclusion of different sets of mutants to form the mutant pool used in mutation analysis. Each subsequent column in the table for a DBMS involves the removal of a particular type of ineffective mutant, and, for the purposes of statistical testing and effect size computation, can be compared with the left-most adjacent column for timing differences. For example, the -(S+I+E+R) column shows mean times when all ineffective mutants have been removed, and can be contrasted with the -(S+I+E) column to draw conclusions about the effect of removing redundant mutants. As given in Table 11, we summarize this information in the following discussion by counting the number of database schemas where times significantly improve (i.e., times decrease) or become significantly worse (i.e., times increase), highlighting the greatest increases and decreases in mutation analysis time for a database schema as appropriate. TABLE 9 Mean Mutation Analysis Times (in Milliseconds) for Test Suites Generated with Random^{+}  TABLE 10 Mean Mutation Analysis Times (in Milliseconds) for Test Suites Generated with the AVM  TABLE 11 Summary of Significance and Effect Size Results for Mutation Analysis Times  Impaired Mutants. Since only SQLite has automatic checks for impaired mutants, as originally discussed in Section 5.3,Tables 9 through 11 report figures for the -(S+I) set of mutants (i.e., the set of mutants following removal of impaired mutants) for this DBMS. To analyze the effect of removing impaired mutants, we contrast times in this column with the preceding -S column of the same table. For Random^{+} , SchemaAnalyst performs mutation analysis significantly faster for 14 schemas after removing impaired mutants, with a large effect size in each case. With the AVM , mutation analysis is significantly faster overall for the same 14 schemas as Random^{+} , and additionally, FrenchTowns . The effect size is large for 12 of these 15 schemas, of which RiskIt sees the greatest performance improvement. With SQLite, a comparison of the value for RiskIt in Table 9's “Impaired” column to its “Stillborn” column shows that SchemaAnalyst achieved a mean saving of 6 seconds when using Random^{+} . The mean saving for the AVM is just under 8 seconds, as observed from the corresponding values in Table 10. Table 4 shows that 19 schemas have impaired mutants, meaning that there were four schemas with impaired mutants (i.e., ArtistSimilarity , ArtistTerm , iTrust , and StudentResidence ) for which mutation analysis did not become significantly faster following their removal. In fact, Tables 9 and 10 reveal that mutation analysis time increased, in a statistically significant fashion and with large effect size, for two schemas (i.e., ArtistSimilarity and StudentResidence ) with Random^{+} and also for ArtistSimilarity with AVM . Notably for ArtistSimilarity , the cost of finding and removing its single impaired mutant leads to a time increase. It is clear that these schemas did not have a sufficient number of impaired mutants to make mutation analysis significantly faster. Equivalent Mutants. In contrast to impaired mutants, SchemaAnalyst supports, with the methods described in Sections 4 and 5, the removal of equivalent mutants for all three DBMSs. The -(S+I+E) columns of Tables 9 and 10 show mean mutation analysis times for the mutant pool with equivalent mutants removed. We compare this column with the preceding -S column (for HyperSQL and PostgreSQL) or the -(S+I) column (for SQLite) of the same data table to study the effect of removing equivalent mutants from the mutant pool on the time taken for mutation analysis. The summary information in Table 11 reveals that, for HyperSQL, mutation analysis times significantly improve for 6 schemas with test suites generated using Random^{+} , and an additional 6 with test suites generated by the AVM (i.e., 12 in total), with large effect sizes in each case. When SchemaAnalyst uses either Random^{+} or the AVM , the iTrust schema sees the greatest decrease in mutation analysis time for HyperSQL, with time savings of about 38 and 42 seconds, respectively. Yet, for HyperSQL with both the AVM and Random^{+} , times are significantly worse for 19 schemas. For PostgreSQL, mutation analysis times never become significantly worse, and become significantly better for 25 schemas with test suites generated by Random^{+} , and 27 schemas with the tests from AVM . For each configuration, significance is coupled with a large effect size. As with the HyperSQL DBMS, the iTrust schema demonstrates the greatest decrease in mutation analysis time, with SchemaAnalyst saving 72 and 89 minutes when it uses tests by Random^{+} and the AVM , respectively. Finally, for SQLite, mutation analysis times become significantly better for 3 schemas with tests from the AVM and Random^{+} , but significantly worse for 25. Yet, all three cases of significant improvement are coupled with a large effect size. Notably, SchemaAnalyst experiences the greatest decrease in mutation analysis time with the iTrust schema, respectively saving about 37 and 41 seconds with the test suites generated by Random^{+} and the AVM . Removing equivalent mutants involves comparing each mutated schema against the original schema. The cost of this comparison depends on the complexity of the schema under analysis, and tends to be slower than the automated checks that identify impaired mutants. As with impaired mutants, the differences by schema are explained by the number of equivalent mutants removed from the pool. Therefore, the timings vary depending on how many mutants SchemaAnalyst removes. When equivalent mutants are abundant, the overall time required for SchemaAnalyst to perform mutation analysis is reduced significantly. In fact, the schemas that experienced significant improvement in times for all three DBMSs (i.e., BookTown , iTrust , and RiskIt ) had some of the greatest numbers of equivalent mutants identified out of all the subjects (c.f. Table 2). Yet, when equivalent mutants are few in number, the mutation analysis is significantly slower. Yet, the choice of DBMS has the greatest effect on the mutation analysis times with or without the equivalent mutants. For PostgreSQL, mutation analysis times never become significantly worse, whereas for the other two DBMSs it depends on how many equivalent mutants are produced by the operators. This is primarily due to the way that the DBMSs are designed and work: PostgreSQL is an enterprise DBMS that uses disk-based storage, thus making it slow at evaluating mutants. The cost of evaluating extra ineffective mutants dominates that of detecting and eliminating them. In contrast, HyperSQL and SQLite store databases in memory, allowing for mutants to be evaluated quickly—meaning that the numbers of equivalent mutants involved must be high before the cost of running the removal algorithms may be recouped and additional time saved. Redundant Mutants. The -(S+I+E+R) columns of Tables 9 and 10 show the mean times for mutation analysis when the pool of mutants excludes those that are redundant. We compare this column with the preceding -(S+I+E) column of the corresponding table to study the effect of removing redundant mutants from the mutant pool on the time taken to perform mutation analysis with a schema. The summary information in Table 11 shows that, for HyperSQL and the removal of redundant mutants, only one schema (i.e., DellStore ) experienced a significant decrease in mutation analysis time with tests generated by either Random^{+} or AVM . While almost all of the other schemas saw a significant increase in mutation analysis time (i.e., 32 with Random^{+} and 31 with AVM ), the effect size for DellStore is large, representing a time savings of about one second for HyperSQL and the tests from either Random^{+} or AVM . For PostgreSQL, 14 schemas experienced a significant decrease in mutation analysis time with Random^{+} and 17 with the AVM . In contrast to mutation analysis with HyperSQL, no schemas are subject to a significant increase in time for this DBMS. With PostgreSQL, Usda saw the greatest reduction in time, saving about two minutes when it used tests from either Random^{+} or AVM . Finally, for SQLite, three schemas exhibit a significant improvement in mutation analysis time for Random^{+} (all with a large effect size), with five for AVM (again, all with a large effect size). Most of the remaining schemas (i.e., 29) saw a significant increase in time. Again, Usda sees the greatest reduction in mutation analysis time, with savings of about one and two seconds when it uses SQLite and tests from Random^{+} and AVM , respectively. The identification of redundant mutants is potentially more costly than for equivalent mutants: each mutant must be compared not just against one other schema (i.e., the original schema under test), but against every other mutant. Therefore, overall savings are less frequent with this type of ineffective mutant, since more mutants need to be removed to recoup the upfront cost of the analysis. For the majority of schemas, the overall mutation analysis process becomes significantly slower, except for when PostgreSQL is used. Here, as for equivalent mutants, the use of disk-based storage makes this DBMS slow at evaluating mutants, and thus the cost of performing the static analysis never leads to a significantly negative effect on mutation times. Finally, it is worth noting that when mutation analysis uses tests created by Random^{+} , SchemaAnalyst has fewer significant timing improvements than it does when it leverages AVM 's tests. This trend is evident because Random^{+} makes smaller test suites that cover fewer test coverage requirements than those suites created by the AVM , as shown in Table 3. Consequently, mutants are faster to evaluate with Random^{+} than AVM , which in turn renders the removal of ineffective mutants less beneficial—particularly for faster, memory-based DBMSs like HyperSQL and SQLite. Conclusion for RQ2. Checking for stillborn mutants by submitting mutated schemas to the DBMS is a time-consuming process, even when using transactions. In contrast, static analysis checks for invalid schemas are fast, taking on the order of milliseconds, rather than seconds, minutes, or even hours. The removal of impaired mutants, a step that the algorithms only perform for SQLite, is similarly fast. When mutation operators produce many impaired mutants for a schema, the speed of mutation analysis generally improves significantly when SchemaAnalyst removes them. Table 11 illustrates this trend, revealing that, for test suites created by either Random^{+} or AVM , the detection and removal of impaired mutants respectively reduces mutation analysis times, with a large effect size, for 14 and 15 database schemas. For tests generated by both Random^{+} and AVM , the summary in Table 11 makes it clear that, when using PostgreSQL, the removal of equivalent mutants decreases mutation analysis time, with a large effect size, for 25 and 27 of the 34 schemas. While the further removal of redundant mutants is less beneficial, this table shows that, for Random^{+} and AVM respectively, 12 and 17 schemas still see mutation analysis times drop with a large effect size. These trends are evident because PostgreSQL is a disk-based DBMS, meaning that the cost of the static checks for ineffective mutant detection is outweighed by the expense of creating the mutants and running tests during mutation analysis. The picture for equivalent mutants, and the HyperSQL and SQLite DBMSs, is mixed and depends on whether the mutation of a schema generates enough ineffective mutants so that the time taken to check for and remove them from the pool is recouped by not having to consider them later in mutation analysis. Table 11 reveals that, while there are more schemas that do not benefit from removing equivalent mutants than do, 6 and 12 schemas, respectively, see a decrease in mutation analysis time with HyperSQL and tests from Random^{+} and AVM . Yet, only three schemas, for tests generated by both Random^{+} and AVM , experience a reduction in mutation analysis time with SQLite. Table 11 shows that the further exclusion of redundant mutants, for both HyperSQL and SQLite, leads to few decreases in mutation analysis time. While Tables 9 and 10 point out that small reductions in mutation analysis time are possible, Table 11 reveals, for tests generated by both Random^{+} and AVM , that this happens for relatively few schemas. For instance, only 1 and 3 schemas, respectively, see decreased mutation analysis time with tests from Random^{+} when run with HyperSQL and SQLite. The trend is similar for tests generated by the AVM , with only 3 and 5 schemas, respectively, seeing time reductions with these two DBMSs. Overall, these results are evident since redundant mutant removal requires the comparison of a mutant to all others in the pool, making it expensive and thus less likely to decrease the overall cost of mutation analysis. In summary, Tables 9 through 11 point out that, while the removal of impaired and equivalent mutants often speeds up mutation analysis, there is a point of diminishing returns. Yet, importantly, there are other benefits to the removal of ineffective mutants. As explained in the answer to the next research question, finding and removing these mutants can also lead to desirable changes in the mutation score. RQ3: Impact on the Mutation Score Tables 12 and 13 show how the mean mutation score changes as a result of removing different types of ineffective mutants for the tests generated by Random^{+} and the AVM , respectively. The AVM achieves higher mutation scores than does Random^{+} . On average, its mutation score never drops below 85 percent over all schemas when stillborn mutants are not considered, regardless of the DBMS. In contrast, the average mutation score of Random^{+} 's test suites, over all schemas, never increases beyond 75 percent, regardless of the DBMS. TABLE 12 Mean Mutation Scores Obtained with Test Suites Generated by Random^{+}  TABLE 13 Mean Mutation Scores Obtained with Test Suites Generated by the AVM  We now discuss the effect of removing different types of ineffective mutant on the mutation scores for the database schemas, leveraging Table 14's summary data. At the outset, it is worth noting that a decrease in a test suite's mutation score is not a negative outcome per se—the lower score now gives a more useful understanding of its effectiveness. Since stillborn mutants do not contribute to mutation scores, we start by analyzing the effect of removing impaired mutants. TABLE 14 Summary of Mutation Score Changes  Impaired Mutants. Because impaired mutants artificially inflate the mutation scores, their removal can only lead to a decrease in the resulting scores. The algorithms are only designed to detect impaired mutants for the SQLite DBMS, and 19 of the subject schemas feature them, as shown by Table 4. With tests generated by the AVM , the mutation scores of 16 of these 19 significantly decreased (with large effect sizes) following SchemaAnalyst 's removal of impaired mutants, as summarized by Table 14 and seen in Tables 12 and 13 by comparing the -S (i.e., stillborn) and -(S+I) (i.e., impaired) columns for this DBMS. The mutation scores of the remaining three schemas with impaired mutants (i.e., NistDML181 , NistDML182 , and NistDML183 ) did not change, because their test suites killed all other mutants, and as such had perfect mutation scores (i.e., 100 percent) before (and after) their removal. The killed-to-total proportion remained the same for the other schemas without any impaired mutants. Tests suites generated by Random^{+} for NistDML181 and NistDML182 did not obtain perfect mutation scores before SchemaAnalyst removed impaired mutants (as can be seen by comparing their mutation scores in Tables 12 and 13 under the -S column for SQLite). Overall, 18 of the 19 schemas with impaired mutants saw a decrease in mutation score with this test generator. Of these 18 schemas, 16 saw significant decreases, with a large effect size. Equivalent Mutants. As equivalent mutants are impossible to kill, and thus artificially deflate mutation scores, schemas with these mutants saw their mutation scores increase for all of the test suites generated by both the AVM and Random^{+} . This effect can be seen by looking at Tables 12 and 13 and, for SQLite, comparing the scores in the -(S+I) (i.e., impaired) column to the -(S+I+E) (i.e., equivalent) column. Additionally, for HyperSQL and PostgreSQL, the comparison is between the scores in -S (i.e., stillborn) and -(S+I+E) (i.e., equivalent) columns. The summary in Table 14 shows that the tests for 15 to 23 schemas experienced a significant increase in mutation score when they were generated by Random^{+} , while test scores for 26 to 27 schemas significantly increased if the AVM generated them. The removal of equivalent mutants had a further interesting effect: when using the AVM , tests for seven schemas with HyperSQL and PostgreSQL, and five schemas with SQLite, which were previously thought to have suboptimal mutation scores, changed to scores of 100 percent. That is, every mutant was killed after removing equivalent mutants from the mutant pool. Redundant Mutants. The removal of redundant mutants cannot cause test suites to change to 100 percent mutation scores, even if all other mutants are killed. Either the redundant mutant pair is killed, and thus the score must already be 100%, or the pair is not killed, in which case one member of the pair will still remain alive in the mutant pool. When the pair is not killed, the mutation score always increases when one of the mutants in the pair is removed, as the total number of mutants (i.e., the denominator of the mutation score equation) is now less. When the pair is killed, the mutation score always decreases when a mutant in the pair is removed, as the numerator and denominator always decrease by one. Since the AVM 's tests are good at killing mutants, suites generated by this technique tend to experience a decrease in mutation score when redundant mutants are removed from the pool. Table 14 shows that test suites for 12 to 15 schemas experience a significant drop, depending on the DBMS in use. In contrast to the AVM , Random^{+} generates tests that are not as good at killing mutants and thus suites for 2 schemas exhibited a significant decrease in their score, with 5 to 14 seeing a significant increase in their score. Conclusion for RQ3. As shown in Table 14, the removal of impaired, equivalent, and redundant mutants generally changes the mutation score of a test suite. In particular, removing equivalent mutants always affects test suites, and can cause a test suite to “gain” a perfect mutation score, where previously it was thought to have one that was suboptimal, due to a mutant that was impossible to kill. In summary, this paper's technique can help testers of database schemas achieve a more precise understanding of the quality of their test suites. The effect of removing impaired and redundant mutants on a test suite's mutation score depends on the initial strength of the generated tests. If a test suite originally achieves a 100 percent score, then removing these mutants cannot improve the score any further; moreover, their removal cannot make it any worse. However, if a test suite has a sub-100 percent score, then it is likely to experience a change in its score. This means that the removal of these types of ineffective mutants has more of an effect on the weaker test suites generated by the Random^{+} test data generator than it does for the AVM 's test suites, which are stronger and more likely to therefore achieve 100 percent mutation scores. Overall, automatically finding and removing ineffective mutants lead to 33 of the 34 schemas having a significantly changed mutation score for at least one DBMS and one test generation method. The only schema that didn't undergo a significant change—NistDML183 —had a consistent 100 percent score before any ineffective mutants were removed. 7   Related Work Much like this paper, there has been an extensive amount of work that aims to make mutation analysis more efficient and/or more useful. Due to the voluminous nature of work in this area, we survey some representative papers that are related to this paper's technique and its experimental study; a more comprehensive treatment can be found in the survey by Jia and Harman [17]. To start, Wong and Mathur presented an early empirical study that evaluated different strategies for reducing the cost of mutation testing by randomly selecting certain mutants for analysis. Mresa and Bottaci [57] and Offutt et al. [58] followed up this study with new experiments that investigated how selective mutation could speed-up mutation testing by reducing the number of mutants subject to analysis; McCurdy et al. subsequently released an open-source tool to support further experimentation with these techniques [59]. Finally, Ma and Kim showed how clustering can reduce the cost of mutation testing by identifying similar mutants, facilitating the analysis of only the representative ones [60]. While all of these prior papers present ways to improve the efficiency of mutation analysis by discarding mutants, they may yield a mutation score that differs from the original; in contrast, the method presented in this paper will never compromise the mutation score because it only removes ineffective mutants. Many related papers have attempted to improve the performance of mutation analysis by using either specialized computer hardware or integrated software tools. In early work, both Offutt et al. [61] and Byoungju and Mathur [62] proposed a technique for high-performance mutation testing on a parallel computer. Attempting to improve the software that performs mutation analysis, both DeMillo et al. [63] and Just et al. [64] developed methods that were directly integrated into the compiler for a specific programming language. In an effort to make the generation of program mutants faster, Untch et al. [65], Ma et al. [66], and Just et al. [67] explored the use of configurable “templates” when manipulating the source code of program mutants. While each of these prior papers improves the efficiency of mutation testing, it does so at the expense of requiring either specialized hardware (e.g., a parallel computer) or customized software (e.g., a Java compiler). This paper's method is distinguished from these prior works in that it creates and executes mutants without needing to modify the DBMS or any other systems software. It is also worth noting that other prior work, like that of Zhang et al. [68] and Just et al. [69], obviate the need for a customized execution environment by applying regressing testing methods [70] that improve the efficiency of mutation testing by reordering the tests. While these methods could be customized for relational database schemas, to date and to the best of our knowledge, none of them can yet handle this new domain. In the context of using mutation analysis to compare test suite quality, the detection of equivalent program mutants is known to be generally undecidable [71]. Since there is a considerable human and computational cost associated with deciding if a mutant is equivalent [20], prior work has developed approaches that use genetic algorithms [42], compiler optimization [37],[39], constraint-based testing [40], and coverage analysis [20] to detect and remove some equivalent mutants. In addition, Hierons et al. explained how to use program slicing to reduce the computational and human effort needed to determine if a mutant is equivalent [72]. Applying it to the equivalent mutant problem, Hierons and Merayo also presented an algorithm for detecting equivalence between pairs of probabilistic stochastic finite state machines [73]. While these methods may be adapted for databases, none of them currently handle database schema mutants. Moreover, the term redundant mutant was previously used by Just et al. [36] to describe Java program mutants that should be removed because they are subsumed by other mutants. We use this term more generally to mean all mutants that are equivalent to other mutants. Finally, Papadakis et al. point out that ineffective mutants—like the ones that this paper's methods can identify and remove—are a validity threat for experiments using mutation analysis to assess the effectiveness of testing techniques [21]. As previously mentioned in Section 2, most work involving the implementation, improvement, and evaluation of mutation analysis methods was originally focused on traditional programs, like those written in programming languages such as Fortran, C, and Java [66],[74],[75]. However, mutation has recently been adopted for a wider range of software artifacts. For instance, the technique developed by Gligoric et al. considers concurrent programs [76]. Moving beyond traditional programs, work such as that of Deng et al. and Lindström et al., proposed the use of mutation analysis to assess the adequacy of test suites for Android apps [26],[27]. Others have recently applied mutation analysis to the measurement of test suite effectiveness for web sites [28],[29],[30]. Mutation testing has additionally been applied in other diverse domains such as mobile software agents (e.g., [31],[32]) and security policies (e.g., [33],[34]). Like these examples of related work, this paper considers mutation testing for a new domain—in this case, relational database schemas. Yet, unlike the aforementioned papers, this one's focus is on the automatic identification and removal of the ineffective mutants that may result in misleading mutation scores and an inefficient mutation analysis. Since many organizations maintain large databases [2] and the quality of the data in these databases is highly valued by consumers [77], it is worth noting that several examples of prior work have motivated the need for efficient and effective mutation analysis methods for relational database schemas. Experimentally observing that the schema of the database in real-world applications changes frequently, Qui et al. both demonstrated the important role that the relational database schema plays in ensuring the correctness of an application and motivated the need for extensive schema testing [78]. The empirical results of Qui et al. are amplified by Guz's remark that one of the key mistakes in testing database applications is “not testing [the] database schema” [11]. These aforementioned papers also stressed the importance of efficiently testing relational database schemas with adequate tests; this paper's automated technique for identifying and removing ineffective mutants help testers achieve this goal by making database schema mutation analysis both faster and more useful than it was when performed with prior methods (e.g., [12]). While Bowman et al. focused on using mutation analysis to assess test suites for an entire database management system [35], Kapfhammer et al. [12] were the first to propose mutation operators for the integrity constraints expressed in a relational database schema. These suggested operators created mutants by adding, removing, and replacing columns in the definitions of PRIMARYKEY and UNIQUE constraints, while also adding and removing NOTNULL constraints from other columns in the schema's tables. An operator was also proposed to remove CHECK constraints from database schema definitions. Wright et al. [15] extended this set with operators that mutated the predicates of CHECK constraints (e.g., by replacing a relational operator such as > with >=), while also introducing operators to mutate the columns in the definition of FOREIGNKEY constraints. Other prior work by this paper's authors furnished methods, such as mutant schemata and parallel execution, for speeding up the mutation analysis of database schemas [14]. Wright's dissertation presented a unified treatment of these approaches to efficient schema mutation testing [47]. While this dissertation, and the author's aforementioned work (e.g., [12],[14],[15]), focused on mutating the CREATE TABLE statements that produce the schema, other prior work has proposed mutation operators for the SQL SELECT statements used by applications to retrieve data stored in a database [46],[79]. The idea of mutation analysis for database queries was later incorporated into a tool for instrumenting and testing database applications written in the Java programming language, potentially mutating any executed SELECT statement [80]. Chan et al. also proposed mutation operators for the entity-relationship model managed by a database application [81]. Yet, unlike these aforementioned papers, this paper's methods concentrate on the database schema and are designed to remove the ineffective mutants that make mutation testing slower and less useful. 8   Conclusions and Future Work Since data is a key driver in business and science, its integrity is of obvious importance [53]. Relational database schemas help to ensure the validity of data through integrity constraints [1]. However, mistakes can be made while specifying schemas, or by misunderstanding the dialect of SQL understood by the DBMS of concern. Therefore, it is important to test relational schemas, as has been recently recommended by industrial practitioners [11]. Since test cases for database schemas may not be equally capable at finding faults, mutation analysis offers a way to evaluate the “strength” of test suites by inserting potential defects and then checking to see if the tests can find them [12],[13]. Although mutation analysis is known to effectively characterize the quality of tests for programs [82], it is subject to certain concerns [17], to which mutation analysis for schemas is also vulnerable. One issue is the production of useless, ineffective mutants. For instance, a mutant is ineffective if it is equivalent to the original program under test [83]. In the context of using mutation analysis to assess the quality of a program's tests, the detection of equivalent program mutants is known to be generally undecidable [71] and costly from a human perspective [20]. Since these concerns for program mutation also apply to the mutation analysis of database schemas, in this paper we have identified patterns of ineffectiveness in database schemas that lead to equivalent, redundant, and stillborn mutants. We have also discovered a new type of ineffective mutant not heretofore observed in program mutation: the impaired mutant. These impaired mutants are similar to stillborn mutants in that the schema is infeasible. However, instead of being rejected by the database management system (or failing to compile, as would be the case for stillborn mutants with program mutation), they are live until trivially killed by a test. This paper presented general-purpose algorithms, designed to be run before mutation analysis and implemented in the SchemaAnalyst tool, that statically analyze the mutants of database schemas to check if they are ineffective. In an empirical study, focusing on 34 representative database schemas comprising a total of 186 tables, 1044 columns, and 590 constraints that were hosted by the well-known HyperSQL, PostgreSQL, and SQLite DBMSs, we found that a significant number of ineffective mutants could be identified with this automated approach. We also discovered that removing them from the mutant pool often significantly decreased the time needed to perform mutation analysis. In particular, the prior identification and removal of stillborn mutants was shown to be an order of magnitude faster than relying on the DBMS to reject them during mutation analysis. The efficiency benefits of removing other types of mutant depended on their numbers, and whether the time taken to detect and eliminate them was regained in the course of not having to analyze significant numbers of them later, which could lead to further time savings. Finally, the results also revealed that the removal of ineffective mutants generally changed the mutation score, making it more useful to testers assessing the quality of their tests. In particular, the removal of equivalent mutants sometimes lead to a test suite achieving a perfect mutation score. Although this paper presents and empirically evaluates a comprehensive suite of methods for automatically detecting and removing ineffective mutants in database schemas, several avenues for future work remain. Yet, these methods could not identify some equivalent and infeasible impaired mutants due to the existence of arbitrary predicates in the database schema's CHECK constraints. Even though the equivalence and infeasibility of predicates is a generally undecidable problem [17],[40], future work will develop methods that can automatically detect simple forms and use them as the basis for removing such mutants—for example, by using a constraint solver. Moreover, even after the removal of ineffective mutants, many mutants remain that are costly to analyze. Future work needs to develop approaches that can speed up their analysis (e.g., through further investigation of virtual execution approaches [84]); or with techniques to reduce the number of mutants that need to be considered by selecting a representative sample, as has been previously proposed for program mutation (e.g., [42],[57],[58]) and preliminarily developed and evaluated for schema mutation [59]. Additionally, we will investigate how this paper's presented methods for the identification and removal of ineffective mutants could be applied to other domains, such as traditional programs and web sites. There are also many ways in which we intend to improve the empirical study presented in this paper. For instance, we will extend the experiments by considering new database schemas and database-aware mutation operators. This second extension will involve, along with the development of higher-order mutation operators for schemas, the customization of the mutation operators for SQL SELECTs [46] so that they ultimately work for database schemas. We will also more thoroughly investigate how both automatically and manually created tests influence the detection and removal of ineffective mutants. These new experimental configurations will serve to further control the threats to the validity of this paper's experimental study, leading to, for instance, further confirmation of the generalizability of the results. Once completed, we will integrate all of the new techniques into the existing repository for the SchemaAnalyst tool.10 Overall, the combination of this paper's automated method for handling ineffective mutants, and the improvements completed during future work, will yield an effective way to assess the quality of the test suites for the integrity constraints in a database schema. Ultimately, the use of the algorithms presented in this paper will support the production of better tests suites for schemas, leading to the creation of high-quality relational databases that store the data sets arising in fields such as science and business. Footnotes 1. https://www.google.com/chrome/browser 2. http://www.mozilla.org/firefox 3. https://wordnet.princeton.edu/ 4. In previous work we referred to stillborn mutants as “quasi” mutants [15], since it was always the case that, in practice, a mutant that was stillborn for one DBMS was not for another. In this paper, we revert to the original “stillborn” term, since we now know that the effective/ineffectiveness status of other types of mutants—for example, equivalent mutants—also varies across different DBMSs. 5. General SQL Parser is available at http://sqlparser.com. 6. https://github.com/schemaanalyst/ineffectivemutants 7. The web site https://github.com/schemaanalyst/schemaanalyst features a Git version control repository containing all of the relational database schemas used in the experiments in addition to the documented version of SchemaAnalyst 's source code and test suite. 8. Along with providing the source code for all of the data analysis and manipulation routines and all of the raw data files, https://github.com/schemaanalyst/ineffectivemutants also furnishes the manually created JUnit tests used in answering RQ1. 9. The web site available at http://goo.gl/7pzxeV provides a regularly-updated “DBMS comparison” table revealing that different database management systems now offer many of the same features. 10. https://github.com/schemaanalyst/schemaanalyst References  [1]P. Glikman and N. Glady, “What's the value of your data?” (2015). [Online]. Available: https://goo.gl/bFZKeR, Accessed on: -Dec.-2016. [2]G. M. Kapfhammer, “A comprehensive framework for testing database-centric applications,” Ph.D. dissertation, Univ. Pittsburgh, Pittsburgh, PA, USA, 2007. [3]PostgreSQL featured users. [Online]. Available: https://www.postgresql.org/about/users/, Accessed on: -Dec.-2016. [4]Well-known users of SQLite. [Online]. Available: https://www.sqlite.org/famous.html, Accessed on: -Dec.-2016. [5]K. Roukounaki, “Five popular databases for mobile.” (2014).[Online]. Available: https://goo.gl/rAUAe0, Accessed on: -Dec.-2016. [6]B. Butler, “Amazon: Our cloud powered Obama's campaign,” Netw. World, 2012. [7]DB-Engines DBMS ranking, (2016). [Online]. Available: http://db-engines.com/en/ranking/ [8]D. Robinson, “Releasing the StackLite dataset of Stack Overflow questions and tags.” (2016). [Online]. Available: http://varianceexplained.org/r/stack-lite/, Accessed on: -Dec.-2016. [9]C. Marisic, “With the recent prevelance of NoSQL databases why would I use a SQL database?” (2010). [Online]. Available: https://goo.gl/ivFu8h [10]K. Houkjær, K. Torp, and R. Wind, “Simple and realistic data generation,” in Proc. 32nd Int. Conf. Very Large Data Bases, 2006, pp. 1243–1246. [11]S. Guz, “Basic mistakes in database testing.” (2011). [Online]. Available: http://java.dzone.com/articles/basic-mistakes-database/, Accessed on: -Jan.-2014. [12]G. M. Kapfhammer, P. McMinn, and C. J. Wright, “Search-based testing of relational schema integrity constraints across multiple database management systems,” in Proc. 6th Int. Conf. Softw. Testing Verification Validation, 2013, pp. 31–40. [13]P. McMinn, C. J. Wright, and G. M. Kapfhammer, “The effectiveness of test coverage criteria for relational database schema integrity constraints,” ACM Trans. Softw. Eng. Methodology, vol. 25, no. 1, 2015, Art. no. 8. [14]C. J. Wright, G. M. Kapfhammer, and P. McMinn, “Efficient mutation analysis of relational database structure using mutant schemata and parallelisation,” in Proc. 8th Int. Workshop Mutation Anal., 2013, pp. 63–72. [15]C. J. Wright, G. M. Kapfhammer, and P. McMinn, “The impact of equivalent, redundant and quasi mutants on database schema mutation analysis,” in Proc. 14th Int. Conf. Quality Softw., 2014, pp. 57–66. [16]G. M. Kapfhammer and M. L. Soffa, “Database-aware test coverage monitoring,” in Proc. 1st India Softw. Eng. Conf., 2008, pp. 77–86. [17]Y. Jia and M. Harman, “An analysis and survey of the development of mutation testing,” IEEE Trans. Softw. Eng., vol. 37, no. 5, pp. 649–678, Sep.2011. [18]G. M. Kapfhammer, “Software testing, ” in The Computer Science Handbook. Boca Raton, FL, USA: CRC Press, 2004. [19]B. Grun, D. Schuler, and A. Zeller, “The impact of equivalent mutants,” in Proc. 4th Int. Workshop Mutation Anal., 2009, pp. 192–199. [20]D. Schuler and A. Zeller, “(Un-)covering equivalent mutants,” in Proc. 3rd Int. Conf. Softw. Test. Verification Validation, 2010, pp. 45–54. [21]M. Papadakis, C. Henard, M. Harman, Y. Jia, and Y. Le Traon, “Threats to the validity of mutation-based test assessment,” in Proc. Int. Symp. Softw. Testing Anal., 2016, pp. 354–365. [22]P. McMinn, C. J. Wright, C. Kinneer, C. J. McCurdy, M. Camara, and G. M. Kapfhammer, “SchemaAnalyst: Search-based test data generation for relational database schemas,” in Proc. 32nd Int. Conf. Softw. Maintenance Evolution, 2016, pp. 586–590. [23]T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and P. Lamere, “The million song dataset,” in Proc. 12th Int. Conf. Music Inform. Retrieval, 2011, pp. 591–596. [24]NIST SQL conformance suite. (2003). [Online]. Available: http://www.itl.nist.gov/div897/ctg/sql_form.htm, Accessed on: -Dec.-2016. [25]B. Korel, “Automated software test data generation,” IEEE Trans. Softw. Eng., vol. 16, no. 8, pp. 870–879, Aug.1990. [26]L. Deng, J. Offutt, P. Ammann, and N. Mirzaei, “Mutation operators for testing Android apps,” Inform. Softw. Technol., vol. 81, pp. 154–168, 2017. [27]B. Lindström, S. F. Andler, J. Offutt, P. Pettersson, and D. Sundmark, “Mutating aspect-oriented models to test cross-cutting concerns,” in Proc. 11th Int. Workshop Mutation Testing, 2015, pp. 1–10. [28]U. Praphamontripong, J. Offutt, L. Deng, and J. Gu, “An experimental evaluation of web mutation operators,” in Proc. 12th Int. Workshop Mutation Testing, 2016, pp. 102–111. [29]S. Mirshokraie, “Effective test generation and adequacy assessment for JavaScript-based web applications,” in Proc. Int. Symp. Softw. Testing Anal., 2014, pp. 453–456. [30]T. A. Walsh, P. McMinn, and G. M. Kapfhammer, “Automatic detection of potential layout faults following changes to responsive web pages,” in Proc. 30th Int. Conf. Autom. Softw. Eng., 2015, pp. 709–714. [31]A. A. Saifan and H. A. Wahsheh, “Mutation operators for JADE mobile agent systems,” in Proc. 3rd Int. Conf. Inform. Commun. Syst., 2012, Art. no. 16. [32]S. Savarimuthu and M. Winikoff, “Mutation operators for cognitive agent programs,” in Proc. Int. Conf. Autonomous Agents Multi-Agent Syst., 2013, pp. 1137–1138. [33]E. Martin and T. Xie, “A fault model and mutation testing of access control policies,” in Proc. 16th Int. World Wide Web Conf., 2007, pp. 667–676. [34]T. Mouelhi, F. Fleurey, and B. Baudry, “A generic metamodel for security policies mutation,” in Proc. 3rd Int. Workshop Mutation Testing, 2008, pp. 278–286. [35]I. T. Bowman, “Mutatis mutandis: Evaluating DBMS test adequacy with mutation testing,” in Proc. 6th Int. Workshop Testing Database Syst., 2013, Art. no. 10. [36]R. Just, G. M. Kapfhammer, and F. Schweiggert, “Do redundant mutants affect the effectiveness and efficiency of mutation analysis?” in Proc. 7th Int. Workshop Mutation Anal., 2012, pp. 720–725. [37]M. Papadakis, Y. Jia, M. Harman, and Y. Le Traon, “Trivial compiler equivalence: A large scale empirical study of a simple, fast and effective equivalent mutant detection technique,” Proc. 37th Int. Conf. Softw. Eng., 2015, pp. 936–946. [38]L. Bottaci, “Type sensitive application of mutation operators for dynamically typed programs,” in Proc. 5th Int. Workshop Mutation Anal., 2010, pp. 126–131. [39]A. J. Offutt and W. M. Craft, “Using compiler optimization techniques to detect equivalent mutants,” J. Softw. Testing Verification Rel., vol. 4, pp. 131–154, 1994. [40]A. J. Offutt and J. Pan, “Automatically detecting equivalent mutants and infeasible paths,” Softw. Testing Verification Rel., vol. 7, no. 3, pp. 165–192, 1997. [41]SQL as understood by SQLite. [Online]. Available: https://sqlite.org/lang_createtable.html, Accessed on: -May-2017. [42]K. Adamopoulos, M. Harman, and R. M. Hierons, “How to overcome the equivalent mutant problem and achieve tailored selective mutation using co-evolution,” in Proc. Genetic Evol. Comput. Conf., 2004, vol. 3103, pp. 1338–1349. [43]M. Kintis and N. Malevris, “MEDIC: A static analysis framework for equivalent mutant identification,” Inform. Softw. Technol., vol. 68, pp. 1–17, 2015. [44]R. A. DeMillo and A. J. Offutt, “Constraint-based automatic test data generation,” IEEE Trans. Softw. Eng., vol. 17, no. 9, pp. 900–910, Sep.1991. [45]A. Goldberg, T. C. Wang, and D. Zimmerman, “Applications of feasible path analysis to program testing,” in Proc. Int. Symp. Softw. Testing Anal., 1994, pp. 80–94. [46]J. Tuya, M. J. Suárez-Cabal, and C. de la Riva, “Mutating database queries,” Inform. Softw. Technol., vol. 49, no. 4, pp. 398–417, 2007. [47]C. Wright, “Mutation analysis of relational database schemas,” Ph.D. dissertation, University of Sheffield, Sheffield, U.K., 2015. [48]A. Bacchelli, “Mining challenge 2013: Stack overflow,” in Proc. 10th Work. Conf. Mining Softw. Repositories, 2013, http://2013.msrconf.org/challenge.php [49]K. Pan, X. Wu, and T. Xie, “Generating program inputs for database application testing,” in Proc. 26th Int. Conf. Automated Softw. Eng., 2011, pp. 73–82. [50]J. Cobb, G. M. Kapfhammer, J. A. Jones, and M. J. Harrold, “Dynamic invariant detection for relational databases,” in Proc. 9th Int. Workshop Dynamic Anal., 2011, pp. 12–17. [51]B. Smith and L. Williams, “An empirical evaluation of the MuJava mutation operators,” in Proc. Testing: Academic Ind. Conf. - Practice Res. Techn. Int. Workshop Mutation Anal., 2007, pp. 193–202. [52]A. Arcuri and L. Briand, “A hitchhiker's guide to statistical tests for assessing randomized algorithms in software engineering,” Softw. Testing Verification Rel., vol. 24, no. 3, pp. 219–250, 2014. [53]A. Silberschatz, H. F. Korth, and S. Sudarshan, Database System Concepts. New York, NY, USA: McGraw-Hill Education, 2010. [54]A. Vargha and H. D. Delaney, “A critique and improvement of the CL common language effect size statistics of McGraw and Wong,” J. Edu. Behavioral Statist., vol. 25, no. 2, pp. 101–132, 2000. [55]G. M. Kapfhammer, P. McMinn, and C. J. Wright, “Hitchhikers need free vehicles! Shared repositories for statistical analysis in SBST,” in Proc. 9th Int. Workshop Search-Based Softw. Testing, 2016, pp. 55–56. [56]R. DeMillo, R. Lipton, and F. Sayward, “Hints on test data selection: Help for the practicing programmer,” Comput., vol. 11, no. 4, pp. 34–41, 1978. [57]E. S. Mresa and L. Bottaci, “Efficiency of mutation operators and selective mutation strategies: An empirical study,” Softw. Testing Verification Rel., vol. 9, no. 4, pp. 205–232, 1999. [58]A. J. Offutt, G. Rothermel, and C. Zapf, “An experimental evaluation of selective mutation,” in Proc. 15th Int. Conf. Softw. Eng., 1993, pp. 100–107. [59]C. J. McCurdy, P. McMinn, and G. M. Kapfhammer, “mrstudyr: Retrospectively studying the effectiveness of mutant reduction techniques,” in Proc. 32nd Int. Conf. Softw. Maintenance Evolution, 2016, pp. 591–595. [60]Y.-S. Ma and S.-W. Kim, “Mutation testing cost reduction by clustering overlapped mutants,” J. Syst. Softw., vol. 115, 2016, pp. 18–30. [61]A. J. Offutt, R. P. Pargas, S. V. Fichter, and P. K. Khambekar, “Mutation testing of software using a MIMD computer,” in Proc. Int. Conf. Parallel Process., 1992, pp. II-257–266. [62]C. Byoungju and A. P. Mathur, “High-performance mutation testing,” J. Syst. Softw., vol. 20, no. 2, pp. 135–152, 1993. [63]R. DeMillo, E. Krauser, and A. Mathur, “Compiler-integrated program mutation,” in Proc. 15th Annu. Int. Comput. Softw. Appl. Conf., 1991, pp. 351–356. [64]R. Just, F. Schweiggert, and G. M. Kapfhammer, “MAJOR: An efficient and extensible tool for mutation analysis in a Java compiler,” in Proc. 26th Int. Conf. Automated Softw. Eng., 2011, pp. 612–615. [65]R. H. Untch, A. J. Offutt, and M. J. Harrold, “Mutation analysis using mutant schemata,” in Proc. Int. Symp. Softw. Testing Anal., 1993, pp. 139–148. [66]Y.-S. Ma, J. Offutt, and Y.-R. Kwon, “MuJava: A mutation system for Java,” in Proc. 28th Int. Conf. Softw. Eng., 2006, pp. 827–830. [67]R. Just, G. M. Kapfhammer, and F. Schweiggert, “Using conditional mutation to increase the efficiency of mutation analysis,” in Proc. 6th Int. Workshop Autom. Softw. Test, 2011, pp. 50–56. [68]L. Zhang, D. Marinov, and S. Khurshid, “Faster mutation testing inspired by test prioritization and reduction,” in Proc. Int. Symp. Softw. Testing Anal., 2013, pp. 235–245. [69]R. Just, G. M. Kapfhammer, and F. Schweiggert, “Using non-redundant mutation operators and test suite prioritization to achieve efficient and scalable mutation analysis,” in Proc. 23rd Int. Symp. Softw. Rel. Eng., 2012, pp. 11–20. [70]G. M. Kapfhammer, “Regression testing,” in Encyclopedia Softw. Eng., Taylor and Francis, Auerbach Publications, 2010. [71]T. A. Budd and D. Angluin, “Two notions of correctness and their relation to testing,” Acta Informatica, vol. 18, no. 1, pp. 31–45, 1982. [72]R. Hierons, M. Harman, and S. Danicic, “Using program slicing to assist in the detection of equivalent mutants,” Softw. Testing Verification Rel., vol. 9, no. 4, pp. 233–262, 1999. [73]R. M. Hierons and M. G. Merayo, “Mutation testing from probabilistic and stochastic finite state machines,” J. Syst. Softw., vol. 82, no. 11, pp. 1804–1818, 2009. [74]K. N. King and A. J. Offutt, “A Fortran language system for mutation-based software testing,” Softw.: Practice Experience, vol. 21, no. 7, pp. 686–718, 1991. [75]Y. Jia and M. Harman, “MILU: A customizable, runtime-optimized higher order mutation testing tool for the full C language,” in Proc. Testing: Academic Ind. Conf. - Practice Res. Techn. Int. Workshop Mutation Anal., 2008, pp. 94–98. [76]M. Gligoric, L. Zhang, C. Pereira, and G. Pokam, “Selective mutation testing for concurrent code,” in Proc. Int. Symp. Softw. Testing Anal., 2013, pp. 224–234. [77]R. Y. Wang and D. M. Strong, “Beyond accuracy: What data quality means to data consumers,” J. Manag. Inform. Syst., vol. 12, no. 4, pp. 5–33, 1996. [78]D. Qiu, B. Li, and Z. Su, “An empirical analysis of the co-evolution of schema and code in database applications,” in Proc. 21st Int. Symp. Found. Softw. Eng., 2013, pp. 125–135. [79]G. Kaminski, U. Praphamontripong, P. Ammann, and J. Offutt, “A logic mutation approach to selective mutation for programs and queries,” Inform. Softw. Technol., vol. 53, no. 10, pp. 1137–1152, 2011. [80]C. Zhou and P. Frankl, “JDAMA: Java database application mutation analyser,” Softw. Testing Verification Rel., vol. 21, no. 3, pp. 241–263, 2011. [81]W. K. Chan, S. C. Cheung, and T. H. Tse, “Fault-based testing of database application programs with conceptual data model,” in Proc. 5th Int. Conf. Quality Softw., 2005, pp. 187–196. [82]T. T. Chekam, M. Papadakis, Y. L. Traon, and M. Harman, “An empirical study on mutation, statement and branch coverage fault revelation that avoids the unreliable clean program assumption,” in Proc. 39th Int. Conf. Softw. Eng., 2017, pp. 597–608. [83]X. Yao, M. Harman, and Y. Jia, “A study of equivalent and stubborn mutation operators using human analysis of equivalence,” in Proc. 36th Int. Conf. Softw. Eng., 2014, pp. 919–930. [84]P. McMinn, G. M. Kapfhammer, and C. J. Wright, “Virtual mutation analysis of relational database schemas,” in Proc. 11th Int. Workshop Autom. Softw. Test, 2016, pp. 36–42. Phil McMinn is a reader (associate professor) in the Department of Computer Science, the University of Sheffield, where he has been researching and teaching software engineering since 2006. His main research interests include search-based software engineering, software testing, program transformation, and reverse engineering. Chris J. Wright received the PhD degree in computer science from the University of Sheffield, United Kingdom, in 2016. His research interests include mutation analysis and search-based software engineering, with a focus on the use of automated software testing techniques. Colton J. McCurdy received the BSc degree in computer science from Allegheny College, in 2017. He is a software engineer at StockX with research interests include the search-based software engineering and selective mutation testing. Gregory M. Kapfhammer is an associate professor in the Department of Computer Science, Allegheny College. In addition to teaching courses in many technical areas, he conducts research and develops useful tools in the fields of software engineering and software testing.Keywords Relational Databases, Algorithm Design And Analysis, Testing, Taxonomy, Google, Software, Software Testing, Software Quality, Software Tools, Relational Databases"
"The Good, the Bad and the Ugly: A Study of Security Decisions in a Cyber-Physical Systems Game","Abstract Stakeholders' security decisions play a fundamental role in determining security requirements, yet, little is currently understood about how different stakeholder groups within an organisation approach security and the drivers and tacit biases underpinning their decisions. We studied and contrasted the security decisions of three demographics-security experts, computer scientists and managers-when playing a tabletop game that we designed and developed. The game tasks players with managing the security of a cyber-physical environment while facing various threats. Analysis of 12 groups of players (4 groups in each of our demographics) reveals strategies that repeat in particular demographics, e.g., managers and security experts generally favoring technological solutions over personnel training, which computer scientists preferred. Surprisingly, security experts were not ipso facto better players-in some cases, they made very questionable decisions-yet they showed a higher level of confidence in themselves. We classified players' decision-making processes, i.e., procedure-, experience-, scenario- or intuition-driven. We identified decision patterns, both good practices and typical errors and pitfalls. Our game provides a requirements sandbox in which players can experiment with security risks, learn about decision-making and its consequences, and reflect on their own perception of security. 1   Introduction The security of any system is a direct consequence of stakeholders’ decisions regarding security requirements and their relative prioritization. Such decisions are taken with varying degrees of expertise in security. In some organizations—particularly those with resources—these are the preserve of computer (or information) security teams. In others—typically smaller organizations—the computing services team may be charged with the responsibility. Often managers have a role to play as guardians of business targets and goals. Be it common workplace practices or strategic decision making, security decisions underpin not only the initial security requirements and their prioritization but also the adaptation and evolution of these requirements as new business or security contexts arise. However, little is currently understood about how these various demographics approach cyber security decisions and the strategies and approaches that underpin those decisions. What are the typical decision patterns, if any, the consequences of such patterns and their impact (positive or negative) on the security of the system in question? Nor is there any substantial understanding of how the strategies and decision patterns of these different groups contrast. Is security expertise necessarily an advantage when making security decisions in a given context? Answers to these questions are key to understanding the “how” and “why” behind security decision processes. In this paper, we propose a tabletop game—Decisions and Disruptions (D-D)1—as a means to investigate these very questions. The game tasks a group of players with managing the security of a small utility company while facing a variety of threats. The game provides a requirements sandbox in which players can experiment with threats, learn about decision making and its consequences, and reflect on their own perception of risk. The game is intentionally kept short—2 hours—and simple enough to be played without prior training. A cyber-physical infrastructure, depicted through a Lego board, makes the game easy to understand and accessible to players from varying backgrounds and security expertise, without being too trivial a setting for security experts. The particular setting of a utility infrastructure is drawn from our prior experience of technical [26],[27] and non-technical investigations [28] as well as interviews with security experts, field engineers, IT users in such settings [29]. Our work complements existing work on gamification as a means to improve security awareness, education and training [9],[13]. While there is a definite educational and awareness-raising aspect to D-D (as noted consistently by players in all our subject groups), our focus in this paper is on contrasting the security decisions of the three demographics as they manifested in the game sessions. Existing work, e.g., [20], has demonstrated such use of games as an effective means to study decision processes of diverse stakeholders. Specifically, the tangible, physical board enables players to manipulate security features and observe the consequences of their decisions. Recording and analysis of these discussions and interactions provides a rich data source to study their decision strategies, processes and patterns. We report on insights gained from playing D-D with 43 players divided into homogeneous groups (group sizes of 2-6 players): 4 groups of security experts, 4 groups of non-technical managers and 4 groups of general computer scientists. Such observations should, of course, not be generalized, however, the substantial sample size enables in-depth qualitative analysis. Our analysis reveals a number of novel insights regarding security decisions of our three demographics: Strategies: Security experts had a strong interest in advanced technological solutions and tended to neglect intelligence gathering, to their own detriment: some security expert teams achieved poor results in the game. Managers, too, were technology-driven and focused on data protection while neglecting human factors more than other groups. Computer scientists tended to balance human factors and intelligence gathering with technical solutions, and achieved the best results of the three demographics. Decision Processes: Technical experience significantly changes the way players think. Teams with little technical experience had shallow, intuition-driven discussions with few concrete arguments. Technical teams, and the most experienced in particular, had much richer debates, driven by concrete scenarios, anecdotes from experience and procedural thinking. Security experts showed a high confidence in their decisions—despite some of them having bad consequences—while the other groups tended to doubt their own skills—even when they were playing good games. Patterns: A number of characteristic plays could be identified, some good (balance between priorities, open-mindedness and adapting strategies based on inputs that challenge one's pre-conceptions), some bad (excessive focus on particular issues, confidence in charismatic leaders), some ugly (“tunnel vision” syndrome by over-confident players). We document and discuss these patterns, showing the virtue of the positive ones, discouraging the negative ones, and inviting the readers to do their own introspection. The rest of this paper is structured as follows. In Section 2, we situate our work with respect to the literature on security decisions and security games. Section 3 presents D-D, its game model and rules. This is followed by a description of our subject groups and the analysis approach used to study their security decisions in Section 4. Section 5 presents the strategies that drove decisions of various groups. Section 6 discusses whether groups’ (and particular demographics’) approaches were procedure-, experience-, scenario- or intuition-driven. Section 7 presents decision patterns—the Good, the Bad and the Ugly—i.e., the patterns that yield better results than others and the clear mistakes and pitfalls to be avoided. Section 8 discusses threats to validity and limitations of our study. Finally, we discuss the possibilities offered by D-D beyond this particular experiment (Section 9). 2   Related Work 2.1 Security Decisions A key challenge faced by any organization is the need for optimal investment in security with respect to the threats it is likely to face. Consequently, balancing various factors, such as costs, against potential threats and their likelihood is a key concern. One of the initial metrics for measuring computer related risks was the Annual Loss Expectancy (ALE), which was developed by the U.S.- National Bureau of Standards in 1975 [17]. The ALE is an annual expected financial loss to an organization's information assets because of a particular threat occurring within that same calendar year. Several information security investment decision support methods have been proposed, e.g., [6],[12], some of them based on the ALE metric, e.g., [21]. Within D-D we aim to capture this realism of security decision-making—balancing priorities between various threats and investments is a key element in the gameplay. Bodin et al. [5] introduced the Perceived Composite Risk (PCR) approach. They used the Analytic Hierarchy Process (AHP) to weight and combine different risk measures into a single composite metric for risk analysis. This composite metric supports decision-makers by capturing and balancing the various risk measures that apply to their organization. Baker et al. [4] proposed an event-chain risk management model in which threats are “measured as rates per year and then converted into outcomes by specifying the number or extent per year.” While these works focus on providing decision support tools, the focus of our paper is to study and contrast such decisions between different demographics (with varying levels of expertise and knowledge in security). Research has also demonstrated that a better integration is necessary between business and security perspectives. Corriss [8] has shown that management usually considers information security governance as under the jurisdiction of their information technology department, separate from corporate governance. Coles-Kemp et al. [7] have highlighted the importance of relating security and business risk. They showed that many businesses do not have the tools to relate security risks to business risks and objectives, and that the use of a facilitator can help them understand and better communicate security risks and help embed security management into business practices. Similarly, Anderson [1] notes that an important step to protect an organization's data is for managers to promote security awareness by “creating a culture where the community has the knowledge (what to do), skill (how to do it), and attitude (desire to do it)”. These works demonstrate the value of improving security awareness, education and training while bridging corporate and security cultures. The insights resulting from our study are an important contribution in this direction. 2.2 Games Games have been used as research tools in various domains. Space Fortress [10] is a video game that was developed by cognitive psychologists to understand human cognition and performance. The famous Tetris game has been used to investigate the differences in the actions humans perform from a cognitive perspective [15]. Military organizations and cyber security companies have developed games to improve security awareness and education. Although the form may differ (tabletop, role playing, video game), they are based on a narrative similar to D-D: players are placed in an immersive environment where they must take decisions which require balancing business and technical constraints. Examples include CyberCIEGE [13]: a video game created by the US Naval Postgraduate School that tasks players with managing an IT organization, with the goal of maintaining user productivity while investing resources in necessary security protections against various attack scenarios. The Kaspersky Industrial Protection Simulation is another example: a board role playing game that defines itself as a “Security Monopoly” for maximizing enterprise revenue while building an industrial security capability and dealing with unexpected cyber events despite uncertain information and limited resources. Similar to model games such as miniature war games, D-D provides players with a physical replication of the context in appreciation: players are able to visualize and manipulate elements of the infrastructure, which facilitates immersion. Lego, in particular, has been used as a support for redesigning the organization of an industrial facility [24] and for teaching engineering principles [3]. Games have been used in education settings in numerous knowledge areas. Examples in software engineering include teaching software processes [18],[19], value-based software engineering [14], software process risk management [25], and requirements engineering good practices [23]. In [20], a jigsaw puzzle-based game is used to perform analysis and resolution of conflicts among stakeholders, showing how a game can involve players into activities usually considered boring or technical. In the security domain, Beckers et al. [2] propose a game to capture specific security requirements—in their case pertaining to social engineering. In contrast, D-D focuses on enabling stakeholders to manipulate security features and observe the consequences of their decisions, leading to an improved understanding of both security risks and the trade-offs resulting from particular decisions. Control-Alt-Hack [9] is a tabletop card game where players take on the role of white-hat hackers. The evaluation suggests that Control-Alt-Hack represents an effective model for disseminating ideas and encouraging interest in computer security. Although the primary focus of this paper is on contrasting security decisions of various groups, D-D also has a high potential for educational purposes as discussed in Section 9. 3   D-D: The Rules of the Game 3.1 Overview D-D is meant to be played by a team of 2 to 6 players, under the direction of a Game Master. The players act as the team in charge of cyber security in a small utility company, with the goal of minimizing security incidents. The Game Master enforces the rules and guides the players through the 4 rounds of the game. Each round—equivalent to 2 months in game time—is composed of the following steps: The Game Master describes the state of the company and the different systems in the infrastructure (cf. Section 3.2). The players are given a budget ($100,000) and a number of possible defenses to invest in, such as firewalls, antivirus, threat assessment (cf. Section 3.3). The players debate which defenses are more appropriate and decide by consensus where to invest their budget. The Game Master tells the players about the effects of their investments: whether their defenses deflect any attacks, and the effects of undefended attacks (cf. Section 3.4). In addition to technical consequences, the share price of the company can be affected by successful attacks. 3.2 The Game Board The game board represents the players’ infrastructure (cf. Figs. 1 and 2). It is composed of two parts: the field site (or plant) and the office. The field site is where the industrial process takes place. A couple of water turbines are controlled by a SCADA controller, operated by local technicians and engineers. A set of PCs used by local personnel and a database collecting production data sit on the field site's network. This local network is connected to the Internet in order to send strategic information to the office network, where the CEO, a part of the engineering team, and the human resources sit. The office network also hosts a number of PCs, as well as a server and a database: the company runs its own email service and website locally. Fig. 1. Overview of the game board. Fig. 2. The plant. 3.3 Defenses Each round, the players are given a budget of $100,000 plus any unspent money from the previous round. Initially, the players can choose to invest among the defenses shown in Table 1. When they choose to invest in an Asset Audit, the Game Master uncovers additional defenses shown in Table 2. TABLE 1 Initial Defenses Available to the Players  TABLE 2 Additional Defenses Available After an Asset Audit  It is explained up to the players that the costs of defenses in D-D do not reflect the actual costs of such defenses in practice. Instead, they are designed so that, with a $100,000 budget per turn and a standard price of $30,000 per defense, players can invest in 3 defenses each turn on average. Players must therefore prioritize their choices, hence enabling the study of their security decision strategies and processes. CCTV and Network Monitoring cost more ($50,000 each) to give these defenses an aura of “advanced technology”, that we later use to measure how much players are attracted to sophisticated security solutions (cf. Section 5). Threat Assessment and Encryption cost less ($20,000) for the sake of game balance, as they are perceived as less powerful than other defenses. 3.4 Attacks Each round, the Game Master runs a number of attacks against the players’ infrastructure, inspired by real-world threat reports [16],[22] and subsequently validated by the security experts who played the game. Attacks are carried out by three categories of attackers: Script kiddies using basic attacks (scans, DoS, phishing, server exploits) on public targets (the company web server and email addresses). Organized crime using more advanced techniques (spear phishing, infected USB drives, infiltration via an insecure wifi network) to achieve more advanced goals (data exfiltration from the offices and plant, ransom based on controller disruption). Nation states using the most advanced attacks to exfiltrate technical data from the plant and disrupt the controller. If the players invest in a Threat Assessment, the Game Master tells them about these three types of attackers and the type of attacks and goals associated with them. Script kiddies are “100 percent likely” to hit the company, Organized Crime attacks are “quite likely” whereas nation states attacks are “unlikely and nearly impossible to defend against anyway”. The players are, therefore, encouraged not to focus on high-profile attacks and to make sure that the organization and its infrastructure is properly secure against the most likely threats. Each attacker follows a particular attack progression, depicted in Table 3. It is important to note that the players do not have access to this table: a Threat Assessment gives them high-level information about the attackers, their methods and the associated likelihood, but no specific timings or progressions. Most attacks are initially silent unless the players have invested in the proper type of defense: for instance, Network Scans go undetected unless Firewalls are deployed, Phishing attacks are silently successful unless Security Training has been purchased for employees. Visible attack effects hit the infrastructure when it is too late: DoS attack paralyzing an un-firewalled server, viruses disrupting PCs, or a ransom for releasing stolen data. TABLE 3 Attacks Targeting the Infrastructure and the Corresponding Counters (Defenses) Noted × in the Table  TABLE 4 Detailed Investments by All Teams During the Game  The attacks are designed so that low-level attacks (DoS, simple virus by Script Kiddies at round 2 and 3) hit early to assess whether the players invested in security essentials against the most common threats. More sophisticated attacks follow an Advanced Persistent Threat (APT) life-cycle. These attacks hit later (data exfiltration, controller disruption in round 4) to assess whether players can prioritize between less frequent, sophisticated attacks and frequent, low-level threats (Script Kiddies in rounds 1, 2). The effect on the company's share price is also proportional to the sophistication of the attack: small bump when hit by a Script Kiddie, significant dip when hit by Organized Crime (along with mentions by the Game Master of press articles and headlines in the news). The Nation State attacks are revealed at the end of round 4 only, when the game finishes: the Game Master then mentions to the players that they were not expected to be able to defend against them. The possibility of adding an element of randomness to the attack scenarios was considered for its realism, but discarded as it would have biased the comparison between sessions, since groups would not have faced the same attacks. 3.5 Validation of the Game Model The models of the company's infrastructure and the attacks targeting it are central elements that determine the game's realism and fairness. The cyber physical infrastructure must include the essential elements of comparable real-life systems, despite the objective of making a simple game that non-experts can play. The attack scenarios were designed to be varied and representative of the current threat model for industrial control systems [16],[22]. Table 3 includes attacks of different natures—social engineering, cyber attacks, physical attacks—and different degrees of sophistication. This design choice favors players who are able to balance priorities between these different vectors over players focusing on a single type of threat: players are not rewarded for guessing the one particular attack they should be concerned with, but for identifying and countering as many different attacks as possible. From a study perspective, this also allows us to capture a wide variety of strategies that we can differentiate. The infrastructure model was elaborated based on our experience with industrial control systems. It was validated by all the computer scientists and security experts who played the game. The distribution of attacks took inspiration from recent threat reports (e.g., [16],[22]) and was also validated by the security experts who played the game. 3.6 Game Design Discussion The balance between theme and mechanics was a key design choice for D-D. The game was carefully designed in order not to encourage a mechanics-based play (or, in role-playing parlance, “meta-gaming”). The mechanics of the game are kept to a strict minimum from a player perspective: 4 rounds, $100,000 budget per round, 15 defense cards and the infrastructure are all the mechanics that they see. In particular, the players do not have access to the attack table, with which they would be able to understand the mechanics of the game and optimize their strategy accordingly. Instead, players must base their decisions on the thematic role of each defense and how they fit into the threat environment they are facing, which is entirely narrated. This design choice is also enforced by the Game Master: any meta-gaming attempt is discouraged. The Game Master de-emphasizes game mechanics and asks the players to focus instead on “what they would do in the real world”. The rulebook provides guidelines for Game Masters to encourage immersion and respond to players tempted by meta-gaming. For instance, a typical meta-gaming behavior would comprise second-guessing the Game Master (for instance: “I bet the GM will run exactly the attack that we won't have defended against...”). As an answer, the Game Master can emphasize that the attack scenarios are pre-determined and mimick real-world conditions, reconstructed from actual threat reports. In Section 8.1 we evaluate whether players’ decisions are indeed based on immersed, theme-driven thinking and to what extent do they rely on mechanics-based optimization. 4   Methodology We played D-D with a total of 43 players divided into 12 homogeneous groups: 4 groups of “security experts” with a background (i.e., skills, degree and/or professional experience) in cyber-security. 4 groups of “computer scientists” with a background in computer science but not in cyber security. 4 groups of “managers” with a management background and no skills in computer science or cyber-security. We ensured that our participants fell into these clear categories to avoid biasing the analysis. Therefore, when referring to players, “managers” or “computer scientists” refer to the backgrounds described above instead of an actual function. “Managers” were chosen as they play a major role in decisions about security and budgets, and their background is identifiable. The groups featured subjects from either academia or industry, the former being either academics, PhD students, postgraduate or undergraduate students in the corresponding areas. We cannot reveal affiliations for ethical reasons, but all “industry” players held a position or had previous work experience in industry. Each group is given a unique identifier shown in Table 5 (e.g., “CI2” for the second group of computer scientists from industry). For consistency, all games were run by the same Game Master with expertise in both security and games. TABLE 5 Group Names and Player Distribution  We advertised game sessions on our universities’ mailing lists, asking for volunteers, and we reached out to a number of our industry partners. Group selection was organised on a first-come first-served basis. A £10 compensation was offered to students. An ethics agreement was signed, guaranteeing the confidentiality of the study and of all personal statements recorded during the session. The recordings are kept in a secure, encrypted location. The transcripts are anonymized and kept confidential. The project received approval from the relevant ethics committee. This experimental sample is substantial for a qualitative analysis, much larger than existing literature on security games (e.g., [9] is based on a survey of 14 educators and observation of 11 players). We do not claim statistical significance: the quantitative values reported are to ground our qualitative insights and observations, in line with the general methodological approach taken in literature, e.g., [9],[23]. At the end of each round, the players were invited to write a short report detailing their investments and a short justification behind these. In addition to these logs of player decisions, we video-recorded all sessions with informed consent and transcribed them, then open-coded the transcripts [11]. We analyze these two data sources using several measurements as follows. 4.1 Game Score To measure how well the players defended their infrastructure, we marked each game with a Game Score that counts how many successful attacks the players successfully defended. In itself, the game score is not an absolute measurement of the security skills of the players: it is one indicator that must be considered in the context of the other qualitative observations that this study presents. We considered a total of 33 attacks from Script Kiddies and Organized Crime (cf. Table 3), each successful defense granting one point. State Attacks do not count as players should not be trying to defend against such high-profile threats: for instance, in round 1, defending a physical infiltration by a foreign spy with CCTV should not be considered a good play, as it comes to the detriment of essential defenses against more likely low-level threats. Fig. 3 shows the game score per demographics. Computer scientists achieved the best results as a demographic, manager teams had consistently average results, whereas some teams of security experts ended up at the bottom of the score sheet—surprising results that we discuss in more detail in Sections 6 and 7. Fig. 3. Game scores (red cross = average). 4.2 Measuring Player Strategies During the game, a record is kept of all players' investments, round per round: this record is used to measure the players' interest in different types of defenses (cf. Table 4). We assume that the earlier players invest in a defense, the more important this defense is to them. This assumption is clearly explained to the players by the Game Master: all defenses are useful and their budget is limited, therefore they are invited to prioritize the defenses they deem the most important for each round. We partition defenses into four categories and associated measurements that capture the interest of players: Data protection : How much importance is given to protecting the company's data (e.g., in databases, PC hard drives) from being stolen? Intelligence gathering : How much importance is given to evaluating the situation (threats, assets) before investing in actual defenses? Human factors : How much importance is given to addressing human vulnerabilities (bad security practices, social engineering)? Technological solutions : How much the players invest in technological solutions, as opposed to the first three categories? This category is further refined into three sub-categories: physical security (i.e., CCTV against physical intrusions), basic cyber security (essentials such as firewalls, antivirus, security patches) and advanced cyber security (highly sophisticated network monitoring and intrusion detection). To quantify the interest of players in defenses, we associate each defense with an Interest Score (IS) defined in Table 6. For instance, if a team invests in an Antivirus in the first round and a Security Training in round 3, the corresponding Interest Scores for this team are: I S ( A n t i v i r u s ) = 4 I S ( S e c u r i t y T r a i n i n g ) = 2. IS(Antivirus)=4IS(SecurityTr TABLE 6 Definition of Interest Scores in Defenses  The interest of players in the four categories of defenses is then measured via the following scores: Data Protection Score (DPS) D P S = I S ( E n c r y p t i o n P C s ) + I S ( E n c r y p t i o n D B s ) . DPS=IS(EncryptionPCs)+IS(En Intelligence Gathering Score (IGS) I G S = I S ( A s s e t A u d i t ) +  I S ( T h r e a t A s s e s s m e n t ) . IGS=IS(AssetAudit)+ IS(ThreatAsse Human Factors Score (HFS) H F S = I S ( S e c u r i t y T r a i n i n g ) . HFS=IS(SecurityTraining). Physical Security Score (PSS), Basic Cyber Security Score (BCS) and Advanced Cyber Security Score (ACS) P S S = I S ( C C T V P l a n t ) + I S ( C C T V O f f i c e ) PSS=IS(CCTVPlant)+IS(CCTVOffice) B C S = I S ( F i r e w a l l P l a n t ) + I S ( F i r e w a l l O f f i c e ) + I S ( A n t i v i r u s ) + I S ( P a t c h e s P C s ) + I S ( P a t c h e s S e r v e r & D B s ) + I S ( P a t c h e s C o n t r o l l e r ) BCS=IS(FirewallPlant)+IS(FirewallOffice)+IS(Antivir A C S = I S ( N e t w o r k M o n i t o r i n g P l a n t )   + I S ( N e t w o r k M o n i t o r i n g O f f i c e ) . ACS=IS(NetworkMonitoringPlant)  +IS(Netw Fig. 4 presents the measurements for each of these scores. Fig. 4. Detailed scores for each demographics (for clarity, only teams mentioned in the text are labeled). 4.3 Characterizing Decision Processes After measuring player strategies, we analyzed the decision processes themselves via two indicators: the type and richness of the arguments players used and the players’ confidence in their own decisions. These indicators were derived while open-coding the transcripts [11]. More precisely, each argument used by a participant is associated with one of the following categories, presented by decreasing levels of maturity: Procedure : a participant explicitly applies a methodological procedure. Example: “We should start with an asset audit, then we can know what we are protecting and invest accordingly. ” Experience : a participant bases a decision on relevant past experience with similar situations. Examples: “I have never seen an IT infrastructure without a firewall. ”, “Remember the news last week? They got owned by a phishing email, we should care about it. ” Scenario : a participant invents a hypothetical scenario, describing an attack or a potential situation, to illustrate a point to other players. Example: “What if someone got access to our database? We need to encrypt it. ” Intuition : a participant provides no additional evidence but their gut instinct. In the absence of one of the former justifications, this is the default code associated with arguments. Example: “I like the antivirus. ” Fig. 5 shows, for each team, how many arguments were used in each of these categories. Fig. 5. Count of arguments in the transcripts. Measuring self-confidence follows a similar protocol: each mention by participants of their confidence in their decisions—either positive or negative—is coded. Examples: “(To the Game Master:) you told us what we already knew. ” (positive); “I don't know, I'm not sure. ” (negative). We then computed the total number of positive and negative self-evaluations. The results are shown in Fig. 6. Fig. 6. Count of self-evaluation markers (no value means 0 markers were available in the transcripts). 5   Analysis of Player Strategies In this section we discuss player strategies in terms of priorities in their investments (Fig. 4) and their efficiency with respect to their game score (Fig. 3). In terms of background, we can summarize player strategies with the following tendencies: Security experts were strongly attracted by Advanced Cyber Protection and neglected Basic Cyber Protection and Intelligence Gathering. Computer scientists favored Intelligence Gathering and Human Factors while being less interested in Advanced Cyber Protection and Data Protection. Managers were technology-driven (Basic and Advanced) and focused more on Data Protection than other demographics while neglecting Human Factors. 5.1 The Best Players Are Not the Ones You Think Strikingly, security experts do not get better scores than the other two categories while providing the two worst performances of the panel (22 points for SA1 and 26 points for SI1 in Fig. 3). Security experts were the most interested in advanced cyber security solutions to the detriment of basic protections (average ACS = 4 and average BCS = 13.5 in Fig. 4). The discussions steered rapidly towards deploying “big shiny boxes” (i.e., network monitoring) in all groups of security experts. Interestingly, the most successful team of security experts (SI2, with a Game Score of 29 in Fig. 3) did not follow this tendency as much as other teams: they had the lowest interest in Advanced Cyber Protection and the highest interest in Basic Cyber Protection among all teams of security experts (ACS = 3 and BCS = 15 in Fig. 4). Security experts also tended to neglect intelligence gathering and in particular to skip threat assessments (average IGS = 5.5 in Fig. 4). A player from team SA1, who achieved the lowest score of the panel, stated: “We are security experts, we don't need a threat assessment. ”. Groups such as SI1 who did invest in a threat assessment noted that they had learnt little from it (“You told us what we already knew. ”). However, the detailed analysis of their decision processes (Section 7.3) shows that they were not able to capitalize on the very threat assessment they thought was obvious. 5.2 The Technology-Driven Managers were technology-driven: they were the most concerned with data protection (average DPS = 4.5 in Fig. 4) while having strong interest in cyber protection, both basic and advanced (average BCS = 8 and average ACS = 3.75 respectively in Fig. 4). This tendency is confirmed by in-game reflections such as the following: “I really need to have some software that can help me choose. I need some technicalsupport. ”. This technology focus comes at a price: managers were the least concerned about Human Factors (average HFS = 1.25 in Fig. 4) It should be noted that, despite this surprising trust in technology over humanity, none of the teams of managers provided a bad performance. Their results were actually quite regular, all teams scoring between 27 and 29 points (Fig. 3). 5.3 Balance Is a Key to Success Computer scientists were overall the most interested in non-technological defenses: they had the highest scores in Human Factors (average HFS = 7.5 in Fig. 4) and Intelligence Gathering (average IGS = 9 in Fig. 4). As summed up by a player from CI2: “You need to see what the problems are before you try and fix them. ”. Computer scientists also showed a bias against Advanced Cyber Solutions in favor of Basic Cyber Solutions (average ACS = 1.75 and average BCS = 16 respectively in Fig. 4), opposite to the strategy of security experts. Finally, they were the least interested in Data Protection (average DPS = 3.75 in Fig. 4). Such balanced, and not solely technology-driven, strategies yielded good results: the best score of the 12 teams was achieved by computer scientists (31 points for CI2 in Fig. 3) while the other 3 teams had average scores (27 or 28). Overall, of the three demographics, the group of computer scientists were the ones with the best results. 6   Decision Processes Considering the way teams took their decisions, in terms of arguments and self-evaluation, we identified two broad behaviors, depending on whether teams had both experience and technical knowledge or not. Interestingly, this classification does not necessarily correlate with good results, as shown in Section 5. 6.1 The Intuition-Driven n00bs Team CA2 (computer science students) and the four teams of managers (MA1 and MA2: management students, MI1 and MI2: industry managers) lacked either experience or a technical background, and sometimes both. As a result, their arguments were poor and mostly based on abstract intuitions: “I think we should go for firewalls ”, “I like the antivirus ”. This was particularly clear for MA1 and MA2 (management students) and MI2 (junior managers): they barely used any other form of argumentation, apart from a few attack scenarios. Team CA2 (computer science students) and MI1 (senior industry managers) used a higher proportion of scenarios and even some procedural thinking, such as “we should go first for an audit, then we will know what we are protecting ”. Notably, these two teams were also highly self-critical, team MI1 scoring a record number of “I'm not sure ” and “I don't know, what do you think? ” in the transcripts. 6.2 The l33t (or Are They?) Contrary to the previous category, security experts and experienced computer scientists mainly used rich, concrete scenarios to argue: “Imagine if someone compromised this box... ”. A clear difference can be seen between student groups (teams SA1 and SA2: security students) and teams with more experience (teams SI1 and SI2: senior security consultants, CA1: senior academics, CI1: junior IT engineers, CI2: senior IT engineers). The latter used their experience of the field much more by recalling past anecdotes. They also used procedural thinking much more often, referring explicitly to initial intelligence gathering before investing into any defenses. Procedural thinking was not a guarantee of quality however: team SI1, for instance, constantly referred to their initial threat assessment but exclusively as a justification for a budget increase, neglecting the actual content of this assessment and the threats they were facing—this “tunnel vision” syndrome is discussed in Section 7.3. Security experts showed a high degree of confidence, maybe due to a feeling of familiarity with security issues: their self-evaluation counters are very low and feature more positive mentions than other demographics. When realizing that they had been hacked, the reaction of security experts was in general to blame the lack of budget or complain that they had been put in a very unfavorable situation. Computer scientists and managers, on the other hand, acknowledged their lack of expertise in security much more: teams CA2 and MI1, for instance, were composed of highly self-critical players. At the end of the game, they were much more likely to acknowledge their mistakes; team CI2 was actually surprised by their excellent result, as they were constantly expecting a disaster to happen until the very end. 7   The Good, the Bad, and the Ugly The previous sections presented a number of patterns that we observed in players’ decision processes: in terms of their defensive focus, argument types and self-evaluations, and the overall game scores of the different teams. This section provides more detailed analysis of the transcripts, in order to identify both positive decision patterns that yield better results and mistakes the players made. We analyze several transversal patterns (good, bad or ugly) that we illustrate with characteristic plays. We invite the readers to be inspired by the virtuous behaviors described in this catalog and discourage bad habits (don't try this at home!). 7.1 Balance Is Key (Good) In D-D, finding a good balance between investments is critical to answer all threats appropriately. An inspiring example is team CI2, a group of experienced computer scientists from industry, who played a near-perfect game. The only way the team could have reached a higher game score would have been to skip the Threat Assessment (assuming the participants already had the appropriate knowledge) and to invest in an earlier Firewall, which would have deflected an inconsequential scan on the plant during the first round. A few characteristic features of this team: They were the only team to invest in a security training in round 1, deflecting 3 attacks at once (maximum Human Factors Score of 4, cf. Fig. 4). As one of the players said: “You can have all the technology in the world, if people are still going to click on a dodgy link in an email... ”. They correctly identified that the offices were more exposed than the plant, due to the public server, and prioritized their investments on this side (early office firewall and server patch in rounds 1 and 2). They delayed less critical investments (controller patch, CCTV) to later rounds while focusing explicitly on balancing their different defenses—technical and non-technical, plant side and office side—according to their evaluation of threats. The team had remarkably balanced discussions, every player expressing diverging opinions. Despite their experience, they were still self-critical (cf. self-evaluation markers on Fig. 6) and were genuinely impressed by their good performance at the end of the game. 7.2 A Little Knowledge Is a Dangerous Thing (Bad) Contrary to balanced approaches, excessive focus on particular threats leads to bad results. Team SA1 (security students) provided the worst performance of all teams, in terms of game score. They suffered from two major weaknesses: The team had an enthusiastic attraction for high-level threats: they invested straight into encryption for databases and PCs, by fear of data exfiltration, followed by a very early advanced technology—network monitoring—during round 2 (their Data Protection Score is the highest of all teams, cf. Fig. 4). Meanwhile, their untrained personnel sitting in a non-firewalled office with an unpatched server were hit by multiple Script Kiddie attacks—trojan-infected email, denial of service, compromised server—at the end of round 2 (low Basic Cyber Security Score, cf. Fig. 4). Despite their lack of experience, the team lacked self-criticism, one of the players notably stating that “We are security experts, we don't need a threat assessment ” (their self-evaluation markers are predominantly positive, cf. Fig. 6). Such a threat assessment would have precisely shifted their attention towards more likely low-level threats they finally cared about when it was too late. At the end of the game, realizing how poorly they had played, one of the players concluded: “Ignorance was bliss. ”. 7.3 The “Tunnel Vision” Syndrome (Ugly) When a team has strong pre-conceived assumptions about security, these can drive their decisions regardless of any contradictory information or feedback collected during the game. Team SA1, described in the previous section, were clearly affected by tunnel vision: their focus on data protection and advanced cyber defenses left them vulnerable to many low-level attacks. Team SI1, a group of engineers from a cyber security consulting firm, suffered from a similar syndrome and neglected data protection altogether, to a bitter end: The senior engineer of the group stated, at the beginning of round 1, that “this company's data has little value: you could publish it all ”. After investing in a threat assessment that described a number of potential data exfiltration attacks, the players signaled to the Game Master: “you told us what we already knew. ”. Yet they kept giving a very low priority to data encryption. At the end of round 2, they were hit by a minor data exfiltration attack, and still they did not change their plans and delayed encryption investments in favor of more protection against disruptive attacks. The senior engineer explicitly said: “I don't feel the encryption is any priority even though there has been a data breach. ”. It took two major data exfiltrations during round 3, from both the plant and office databases, for the players to concede that encryption was important: they finally encrypted their databases during round 4, too late to stop ongoing attacks. These two examples also show the risk of self-supported expertise: these two teams of experts had high confidence and little self-criticism. Therefore, they could not adapt to unforeseen attacks despite the relevant information being given to them. Interestingly, both of these teams expressed contempt for non-experts, using the exact same expression: “Users are idiots. ”. 7.4 Beware of the Champion (for Better or for Worse) Although teams take decisions collectively, individuals had a significant influence on the outcome of several games. Champions supporting their ideas with strong arguments were able to convince the rest of the team, for better or for worse. Conversely, some players failed to become effective champions for their cause and were silenced by stronger, yet wrong, arguments: Team CI2 (the perfect runner) was implicitly led by a senior engineer who pushed for an early security training, then directed the team's reflections according to his (correct) assessment of the risks for the infrastructure. One of the players in team SA1 (high-tech driven worst scorers) tried to argue in favor of investing in more basic defenses and considering human factors, yet his voice was not heard by his teammates. Team SI1 (second worst scorers) suffered from a tunnel vision syndrome partly because the senior engineer in the room disregarded the risks of data exfiltration. Team SI2 (second best scorer) played quite similarly to SI1, until one player with decades of experience in information assurance managed to convince the team to encrypt their databases, unknowingly preventing two catastrophic data exfiltration attacks—attacks that did hit SI1. Here, it should be noted that the Game Master ensured a certain fairness during the debates by trying to balance speaking times among players. In real-world contexts, some less-vocal players would not have got the exposure they were given during the game and their influence on their team's decisions would have been even weaker. 7.5 The Beginner's Syndrome (Good) A lack of experience can be compensated by open-mindedness and adaptability to the inputs provided by the game. Opposite to some teams of security experts falling for their pre-conceptions, all teams of managers and computer scientists reached at least a relatively good score. Despite their lack of expertise, they were able to capitalize on information gathering: All non-expert teams, MA1 excepted, went for an Asset Audit and Threat Assessment during the first two rounds and interpreted it correctly. They did not suffer from excessive tropisms that could have put their defenses off-balance—such as an immoderate focus on data protection (SA1), a complete lack of consideration for data protection (SI1) or high interest in advanced cyber solutions (all security experts). They were constantly critical about themselves and their approach to the game (cf. Fig. 6). Non-expert teams particularly praised the game for its educational value, which is discussed in more detail in Section 9. 8   Threats to Validity 8.1 Influence of Game Mechanics Players of a role-playing game such as D-D design their strategy based on two factors: Mechanics , i.e., investing in the defenses they think will optimally counter the game's attack scenario, in order to “win” the game. Theme , i.e., investing in the defenses they would invest in if they were facing the same situation in the real world. D-D explicitly encourages theme-based play so that player decisions reflect their understanding of real-world security. In order to assess how theme-driven and how mechanics-driven the players were, we asked them to justify each of their investments with a short written sentence. We gathered 117 such justifications across the 12 games we played, and we classified them as follows: 109 theme-driven justifications that unambiguously adopt an immersed, in-game perspective, for instance: “We need to identify what we are protecting. ” to justify an Asset Audit, “Data is the brain of the company and it shouldn't be vulnerable. ” to justify encrypting the databases. 5 mechanics-driven justifications that unambiguously leverage a game mechanic, namely the round-based structure of the game (for instance: “Can't afford all options on table - so do this first as gives benefit!! [sic] ”) or the attack scenarios (for instance: “[This defense is] more likely to stop insider threat and nation-state than CCTV. ”). 3 ambiguous justifications that all refer to ongoing attacks known by the players and for which we could not clearly determine whether the players were thinking in-game or meta-gaming, for instance: “Something is going on in the office and we need to understand what. ”. Overall, 93 percent of the justifications relied on a theme-driven strategy, which confirms that D-D does indeed achieve its goal of immersing the players and recording their real-world perception. In the future, we plan on studying the influence of varying game mechanics, for instance, by changing the price of defenses, changing the attack scenarios, changing the infrastructure. 8.2 Influence of Sample Size Although our sample size is significant for a qualitative analysis, it is not so for a quantitative analysis. We ran t-tests to assess whether there was a statistically significant difference between the distributions of the 6 Defence Scores and the Game Score for our three demographics. Results are shown in Table 7. There are no statistically significant differences, apart from two exceptions (out of 21 t-tests): security experts and computer scientists differ significantly on their AdvancedCyber score (p-value 0.029 < 0.05 0.029 ), and computer scientists and managers differ significantly on their Human score (p-value 0.033 < 0.05 0.033 ). These results hold after correcting for multiple testing, using Bonferroni correction. We do not claim statistical significance, and future work will focus on improving the statistical relevance of our results—namely by collecting and analysing a much larger set of games. TABLE 7 p-Values from Pairwise t-Tests Between Score Distributions from Different Demographics  Another limitation of our approach is the lack of scalability of our qualitative analysis. Transcribing, coding and interpreting a single game transcript requires a significant amount of manual work from several qualified researchers. We are exploring potential ways of speeding up the analysis phase. We are currently considering automating some parts of it, for instance, via Natural Language Processing tools, while preserving the in-depth understanding of player decision processes it provides. 9   D-D Beyond the Experiment Beyond its utility as a semi-controlled environment for experiments, D-D is intended to serve a number of purposes. All teams provided positive feedback after the game, although different backgrounds appreciated different aspects of D-D. 9.1 Educational Training Non-expert teams were extremely positive regarding the educational value of the game. Several management students reported to their teachers that they wished D-D had been part of their regular curriculum, as it provided them with an “informative and knowledgeable” introduction to cyber security, IT infrastructures and decision making. Before the game, players answered questions about their background and their familiarity with IT, security and industrial control systems (cf. Table 8). After the game they evaluated how much they had learnt during the session about these topics (cf. Table 9). As can be seen from the table, managers were extremely positive regarding the educational value of D-D. Making D-D a full-fledged tabletop game that can be used for educational purposes is the major objective for future work. TABLE 8 Pre-Session Background Assessment from Managers (11 Players in Total)  TABLE 9 Post-Session Feedback from Managers (11 Players in Total)  9.2 Corporate Practice and Communication Industry participants in general were all interested in having D-D played in their organization with mixed audiences: for instance, having a CEO, a board director, a CISO and an IT engineer play the game together to discover their different cultures and build a common understanding of cyber security. Participants with a governmental experience also praised the informative qualities of the game. Several teams made inquiries about future commercial versions of the game and expressed interest in the results of our study. Industry participants also praised D-D as a game: “very enjoyable and well-constructed game” supported by a “nice design” and a “good visual design” that delivered “good fun” are some examples of the verbal and written feedback provided by players. The board and its elements in particular were explicitly appreciated by players from all backgrounds, as it provided a support for visualization and helped focusing the debates. 9.3 Extending D-D In terms of the future objectives for D-D, a number of extensions to the game will be explored: A Game Master's guide for building new infrastructures and attack scenarios. Such an extension could increase the complexity of the game model so that companies can replicate their own infrastructure and threat environment for training purposes. This would also allow the game to be played several times by the same players in different settings (new infrastructure, new attack vectors, new attackers, new game objectives). A “red team versus blue team” version where a separate team of attackers is also given a budget and objectives (“exfiltrate the HR data”, “disrupt the SCADA controller”). The game then becomes an adversarial challenge between two teams that must handle partial information and anticipate their opponents’ next move. This extension was particularly popular among security experts and computer scientists: several players informally asked to be registered for the (potential future) tests of this extension. A software version that would allow single players to play D-D individually, or several players to play without the need for a Game Master. Removing the Game Master would lead to a different experience altogether, as the Game Master has a central role in directing and interacting with the players, answering their questions, providing additional information and background, and ensuring fairness in their debates. A software version, on the other hand, sacrifices this central human dimension for the sake of portability and convenience: many players would be able to play this version of the game with minimal constraints and investments. In order to catalyse the diffusion of D-D and encourage the development of new versions of it, the rules of the game have been made public under a Creative Commons licence. They can be freely downloaded at: http://decisions-disruptions.org 10   Conclusion In today's complex organizations and connected infrastructures, little is understood about security decisions and the impact of various factors and biases behind them. It is essential to promote cultural bridges that allow different demographics to build a common understanding of the “how” and “why” of the issues at play. In this paper, we proposed Decisions & Disruptions, a game that allows participants from various backgrounds to experiment with and reflect on their approaches to security decisions. The analysis of 12 games reveals several key insights: Strategic priorities and decision processes differed between demographics : Security experts had a strong interest in advanced technological solutions. They tended to neglect intelligence gathering, due to strong self-confidence in their knowledge and expertise. However, this self-confidence was often not balanced by a willingness to reflect and critique their decisions. Managers were also technology-driven and focused on data protection while neglecting human factors. Their debates were mostly driven by intuition but did not lead to disastrous decisions. Computer scientists tended to balance human factors and intelligence gathering with technical solutions, and demonstrated a strong willingness to question their decisions. The above insights can be valuable when conducting requirements gathering or prioritization workshops with stakeholders from different backgrounds within an organization—the requirements engineer can be cognisant of these cultural biases and, when such patterns manifest and take mitigating actions by exploring the rationale behind particular stakeholder decisions. Participants with a technical background and/or experience were not necessarily better players : Some teams of confident but over-focused experts achieved mediocre results, compared to inexperienced or non-technical players who better adapted to the various threats they were facing. Expertise was therefore not necessarily successful unless it had the willingness to question its pre-conceptions. This demonstrates the importance of incorporating so-called lay perspectives during security requirements engineering—non-experts possess invaluable business and operational knowledge that can contextualize the security risks and decisions pertaining to their mitigation. Various characteristic patterns and their influence on security decisions manifested across the 12 games : “balance is key” (good), “little knowledge” (bad), “tunnel vision” (ugly), “beware of the champion” (ambivalent), “beginner's syndrome” (good). Such patterns identify both good practices and typical errors and pitfalls to be avoided. D-D can, therefore, serve as a means for stakeholders to explore their perceptions and understandings of security risks, the trade-offs involved during decision-making and the impact of such decisions on the security of the overall system. Beyond the scope of this particular experiment, we invite readers to consider their own security decisions in the light of our findings: Do any of these patterns sound familiar? On which side of the spectrum does the reader belong? Promoting good approaches and, above all, discouraging bad habits and ugly mistakes is of paramount importance in a world where cyber security is becoming a concern for everyone. Through games such as D-D, one can experiment with one's own attitude towards cyber security, reflect as to which patterns manifest in one's decisions, and hopefully end up on the good side of the spectrum. Finally, D-D is a sandbox with clear educational value, as consistently noted by our participants. The game provides a didactic way of discovering cyber security for players with varying degrees of expertise. In corporate environments, D-D has the potential to become a strong communication and awareness-raising tool that allows players from different backgrounds—CEO, CISO, managers, IT engineers—to sit around a table, build a common understanding of security issues and bootstrap or consolidate their security requirements. Footnotes 1. Game rules available at: http://decisions-disruptions.org. Acknowledgments This work is supported by UK Engineering and Physical Science Research Council Grant, Mumba: Multi-faceted metrics for ICS business risk analysis (EP/M002780/1), part of the UK Research Institute on Trustworthy Industrial Control Systems (RITICS). The authors also wish to thank Alexander whose Lego play inspired the development of D-D. References  [1]A. Anderson, “Effective management of information security and privacy,” Educause Quart., vol. 29, no. 1, 2006, Art. no. 15. [2]K. Beckers and S. Pape, “A serious game for eliciting social engineering security requirements,” in Proc. IEEE 24th Int. Requirements Eng. Conf., 2016, pp. 16–25. [3]Auburn University. Lego lab, 2012. [Online]. Available: http://ocm.auburn.edu/featured_story/lego_lab.html. Last Accessed on: May 13th, 2016. [4]W. H. Baker and L. Wallace, “Is information security under control?: Investigating quality in information security management,” IEEE Secur. Privacy, vol. 5, no. 1, pp. 36–44, Jan./Feb.2007. [5]L. D. Bodin, L. A. Gordon, and M. P. Loeb, “Information security and risk management,” Commun. ACM, vol. 51, no. 4, pp. 64–68, 2008. [6]H. Cavusoglu, B. Mishra, and S. Raghunathan, “A model for evaluating it security investments,” Commun. ACM, vol. 47, no. 7, pp. 87–92, 2004. [7]L. Coles-Kemp and R. E. Overill, “On the role of the facilitator in information security risk assessment,” J. Comput. Virology, vol. 3, no. 2, pp. 143–148, 2007. [8]L. Corriss, “Information security governance: Integrating security into the organizational culture,” in Proc. Workshop Governance Technol. Inf. Policies, 2010, pp. 35–41. [9]T. Denning, A. Lerner, A. Shostack, and T. Kohno, “Control-alt-hack: The design and evaluation of a card game for computer security awareness and education,” in Proc. ACM SIGSAC Conf. Comput. Commun. Secur., 2013, pp. 915–928. [10]E. Donchin, “Video games as research tools: The space fortress game,” Behavior Res. Methods Instrum. Comput., vol. 27, no. 2, pp. 217–223, 1995. [11]A. Strauss and J. M. Corbin, Basics of Qualitative Research: Techniques and Procedures for Developing Grounded Theory, 2nd ed. Newbury Park, CA, USA: Sage, 1998. [12]L. A. Gordon and M. P. Loeb, “The economics of information security investment,” ACM Trans. Inf. Syst. Secur., vol. 5, no. 4, pp. 438–457, 2002. [13]C. E. Irvine, M. F. Thompson, and K. Allen, “CyberCIEGE: Gaming for information assurance,” IEEE Secur. Privacy, vol. 3, no. 3, pp. 61–64, May2005. [14]A. Jain and B. Boehm, “SimVBSE: Developing a game for value-based software engineering,” in Proc. 19th Conf. Softw. Eng. Edu. Training, 2006, pp. 103–114. [15]D. Kirsh and P. Maglio, “On distinguishing epistemic from pragmatic action,” Cogn. Sci., vol. 18, no. 4, pp. 513–549, 1994. [16]McAfee. Threats Report. 2015. [Online]. Available: http://www.mcafee.com/us/resources/reports/rp-quarterly-threats-aug-2015.pdf [17]National Bureau of Standards, Federal Information Processing Standards Publications (FIPS PUB) 65. Guideline for automatic data processing risk analysis, 1975. [18]E. O. Navarro, A. Baker, and A. V. D. Hoek, “Teaching software engineering using simulation games,” in Proc. Int. Conf. Simul. Edu., 2004, pp. 9–14. [19]E. O. Navarro and A. V. D. Hoek, “Comprehensive evaluation of an educational software engineering simulation environment,” in Proc. 20th Conf. Softw. Eng. Edu. Training, 2007, pp. 195–202. [20]M. Pinto-Albuquerque and A. Rashid, “Tackling the requirements jigsaw puzzle,” in Proc. IEEE 22nd Int. Requirements Eng. Conf., 2014, pp. 233–242. [21]R. K. RainerJr, C. A. Snyder, and H. H. Carr, “Risk analysis for information technology,” J. Manage. Inf. Syst., vol. 8, no. 1, pp. 129–147, 1991. [22]SANS Institute. The State of Security in Control Systems Today, 2015. [Online]. Available: https://www.sans.org/reading-room/whitepapers/analyst/state-security-control-systems-today-36042 [23]R. Smith and O. Gotel, “Gameplay to introduce and reinforce requirements engineering practices,” in Proc. 16th IEEE Int. Requirements Eng. Conf., 2008, pp. 95–104. [24]Sur-Seal. Lego plan model, 2013. [Online]. Available: http://www.sur-seal.com/whoweare/journey/Lego.html. Last Accessed on: May 13th, 2016. [25]G. Taran, “Using games in software engineering education to teach risk management,” in Proc. 20th Conf. Softw. Eng. Edu. Training, 2007, pp. 211–220. [26]R. Antrobus, S. Frey, B. Green, and A. Rashid, “SimaticScan: Towards a specialised vulnerability scanner for industrial control systems,” in Proc. 4th Int. Symp. ICS & SCADA Cyber Secur. Res., 2016, pp. 1–8. [27]W. Jardine, S. Frey, B. Breen, and A. Rashid, “SENAMI: Selective non-invasive active monitoring for ICS intrusion detection,” in Proc. 2nd ACM Workshop Cyber-Phys. Syst. Secur. Privacy, 2016, pp. 23–34. [28]S. Frey, A. Rashid, A. Zanutto, J. S. Busby, and K. Szmagalska-Follis, “On the role of latent design conditions in cyber-physical systems security,” in Proc. 2nd Int. Workshop Softw. Eng. Smart Cyber-Phys. Syst., 2016, pp. 43–46. [29]A. Zanutto, B. Shreeve, K. Follis, J. S. Busby, and A. Rashid, “The shadow warriors: In the no man's land between industrial control systems and enterprise IT systems,” in Proc. 3rd Workshop Secur. Inf. Workers, 2017. Sylvain Frey is a software engineer at Google UK. Prior to this he was a lecturer in cyber security with the University of Southampton. His research deals with complex, dynamic cyber-physical systems with critical safety and security requirements (e.g., robustness, resilience, availability, adaptability) such as the Internet, smart grids, industrial control systems, and critical infrastructures in general. Contact him at: frey.sylvain@gmail.com Awais Rashid is professor of cyber security with the University of Bristol, United Kingdom. He leads a number of research programmes on secure software development, security of cyber-physical systems, and human aspects of security. These include projects as part of two UK Research Institutes: Research Institute in Trustworthy Industrial Control Systems (RITICS) and Research Institute on Science of Cyber Security (RISCS). He heads a major international effort on developing a Cyber Security Body of Knowledge (CyBOK) and also co-leads the security and safety theme within the UK Research Hub on Cyber-Security of the Internet of Things (PETRAS). Contact him at: awais.rashid@bristol.ac.uk. Pauline Anthonysamy received the PhD degree in computer science from Security Lancaster, Lancaster University, United Kingdom. She is a privacy engineer in the Infrastructure Security & Privacy Team, Google. Over the years she has worked in the fields of software systems modeling, requirements engineering, use case modeling and transformation of natural language use case descriptions to petri nets. Her current interests include privacy engineering, applications of natural language processing and machine learning to privacy, modelling users’ privacy behaviour and preferences by leveraging large scale data analysis techniques. Prior to Google, she worked as a software engineer at Nortel Networks and later Ciena Corporation where she designed and implemented network traffic management systems. Contact her at: anthonysp@google.com. Maria Pinto-Albuquerque received the PhD degree in computer science from the University of Lancaster, United Kingdom. She is an assistant professor with the Instituto Universitario de Lisboa (ISCTE-IUL), Portugal. Her research interests include novel software engineering tools and techniques to address early stages decision support while reconciling the associated sociotechnical challenges. She is keen on using creativity and serious games in software development support. She is a member of the IEEE. Contact her at: maria.albuquerque@iscte-iul.pt. Syed Asad Naqvi received the PhD degree in computer science from Lancaster University, in 2015. He is a research associate in the School of Computing and Communications, Lancaster University. His research interests include cyber security, risk analysis, and mixed methods research. Contact him at: s.naqvi@lancaster.ac.uk.Keywords Computer Games, Cyber Physical Systems, Decision Making, Security Of Data, Cyber Physical Systems Game, Tabletop Game, Cyber Physical Environment, Decision Patterns, Security Risks, Decision Making, Games, Data Security, Cyber Physical Systems, Decision Making, Security Decisions, Security Requirements, Game, Decision Patterns"
