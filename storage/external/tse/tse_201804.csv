title,abstract
Hybrid Program Dependence Approximation for Effective Dynamic Impact Prediction,"Abstract Impact analysis determines the effects that program entities of interest, or changes to them, may have on the rest of the program for software measurement, maintenance, and evolution tasks. Dynamic impact analysis could be one major approach to impact analysis that computes smaller impact setsthan static alternatives for concrete sets of executions. However, existing dynamic approaches often produce impact sets that are too large to be useful, hindering their adoption in practice. To address this problem, we propose to exploit static program dependencies to drastically prune false-positive impacts that are not exercised by the set of executions utilized by the analysis, via hybrid dependence approximation. Further, we present a novel dynamic impact analysis called Diver which leverages both the information provided by the dependence graph and method-execution events to identify runtime method-level dependencies, hence dynamic impact sets, much more precisely without reducing safety and at acceptable costs. We evaluate Diver on ten Java subjects of various sizes and application domains against both arbitrary queries covering entire programs and practical queries based on changes actually committed by developers to actively evolving software repositories. Our extensive empirical studies show that Diver can significantly improve the precision of impact prediction, with 100-186 percent increase, with respect to a representative existing alternative thus provide a far more effective option for dynamic impact prediction. Following a similar rationale to Diver, we further developed and evaluated an online dynamic impact analysis called DiverOnline which produces impact sets immediately upon the termination of program execution. Our results show that compared to the offline approach, for the same precision, the online approach can reduce the time by 50 percent on average for answering all possible queries in the given program at once albeit at the price of possibly significant increase in runtime overhead. For users interested in one specific query only, the online approach may compute the impact set for that query during runtime without much slowing down normal program operation. Further, the online analysis, which does not incur any space cost beyond the static-analysis phase, may be favored against the offline approach when trace storage and/or related file-system resource consumption becomes a serious challenge or even stopper for adopting dynamic impact prediction. Therefore, the online and offline analysis together offer complementary options to practitioners accommodating varied application/task scenarios and diverse budget constraints.Keywords Performance Analysis, Runtime, Software, Java, Concrete, Software Measurement, Maintenance Engineering, Static Program Dependence, Dynamic Analysis, Impact Prediction, Online Impact Analysis, Precision, Efficiency"
Refactoring Inspection Support for Manual Refactoring Edits,"Abstract Refactoring is commonly performed manually, supported by regression testing, which serves as a safety net to provide confidence on the edits performed. However, inadequate test suites may prevent developers from initiating or performing refactorings. We propose RefDistiller, a static analysis approach to support the inspection of manual refactorings. It combines two techniques. First, it applies predefined templates to identify potential missed edits during manual refactoring. Second, it leverages an automated refactoring engine to identify extra edits that might be incorrect. RefDistiller also helps determine the root cause of detected anomalies. In our evaluation, RefDistiller identifies 97 percent of seeded anomalies, of which 24 percent are not detected by generated test suites. Compared to running existing regression test suites, it detects 22 times more anomalies, with 94 percent precision on average. In a study with 15 professional developers, the participants inspected problematic refactorings with RefDistiller versus testing only. With RefDistiller, participants located 90 percent of the seeded anomalies, while they located only 13 percent with testing. The results show RefDistiller can help check the correctness of manual refactorings.Keywords Manuals, Inspection, Testing, Computer Bugs, Transforms, Engines, Detectors, Refactoring, Refactoring Anomalies, Code Inspection"
Detecting Trivial Mutant Equivalences via Compiler Optimisations,"Abstract Mutation testing realises the idea of fault-based testing, i.e., using artificial defects to guide the testing process. It is used to evaluate the adequacy of test suites and to guide test case generation. It is a potentially powerful form of testing, but it is well-known that its effectiveness is inhibited by the presence of equivalent mutants. We recently studied Trivial Compiler Equivalence (TCE) as a simple, fast and readily applicable technique for identifying equivalent mutants for C programs. In the present work, we augment our findings with further results for the Java programming language. TCE can remove a large portion of all mutants because they are determined to be either equivalent or duplicates of other mutants. In particular, TCE equivalent mutants account for 7.4 and 5.7 percent of all C and Java mutants, while duplicated mutants account for a further 21 percent of all C mutants and 5.4 percent Java mutants, on average. With respect to a benchmark ground truth suite (of known equivalent mutants), approximately 30 percent (for C) and 54 percent (for Java) are TCE equivalent. It is unsurprising that results differ between languages, since mutation characteristics are language-dependent. In the case of Java, our new results suggest that TCE may be particularly effective, finding almost half of all equivalent mutants. 1   Introduction Mutation testing [1], [2] has attracted a lot of interest, because there is evidence that it is capable of simulating real faults [3], [4], [5] and subsuming other popular test adequacy criteria [6], [7], [8], [9]. It can also be used as a technique for generating test data [10], [11], as well as for assessing test data quality and can also explore subtle faults [12], [13] in the presence of fault masking and failed error propagation [14]. A mutant is a syntactically altered version of the program under test. The syntactic alterations are typically small, and are designed to reflect typical faults that might reside in the original program. A mutant is said to be killed, if a test case can be found that distinguishes between the mutant and the original program. The underlying idea of mutation testing is that test suites that kill many mutants will tend to be of higher quality than those that kill fewer. In this way, mutation testing can be used to assess the quality of a test suite, and can also be used to help the test case generation, by guiding the construction of test cases towards those that kill mutants. However, at the heart of mutation testing lies a problem that has been known to be undecidable for more than three decades [15]: the equivalent mutant problem. That is, mutation testing might produce a mutant that is syntactically different from the original, yet semantically identical. In general, determining whether a syntactic change yields a semantic difference is undecidable. As a result, the tester would never know whether he or she has failed to find a killing test case because the mutant is particularly hard to kill, yet remains killable (a ‘stubborn’ mutant [16]), or whether failure to find a killing test case derives from the fact that the mutant is equivalent. A related, newly identified problem, is the problem of mutant duplication. A duplicated mutant is simply a mutant that is semantically equivalent to some other mutant, although both duplicated mutants maybe semantically different from the original program. Duplicated mutants are also a problem for mutation testing, because they may artificially inflate the apparent mutant killing power of a test suite; a test case that kills two or more duplicated mutants is, all else being equal, no better than another test case that kills only a single non-duplicated mutant. Techniques such as mutant sampling [17], [18], [19], higher order mutation [20], [21], [22], and mutant execution optimisation [23], [24], [25], can be used to reduce the number of mutants that need to be considered, but not necessarily the proportion that remain equivalent, nor the proportion of those that are duplicated. Although theoretically undecidable, practical techniques may be able to significantly dent the equivalent and duplicate problems by detecting a proportion of equivalent/duplicated mutants. Equivalent mutant detection techniques have been extensively studied since 1979. Nevertheless, until now, no scalable, widely applicable technique has yet been found. Previous work on the detection of equivalent mutants has involved complicated program transformation techniques, which have proved difficult to scale and, thereby, have remained insufficiently practical to find implementation in current mutation testing tools and techniques. The equivalent mutant problem therefore remains the single most potent barrier to the wider uptake and exploitation of the potential power of mutation testing. In this paper we study Trivial Compiler Equivalence (TCE) as a simple, fast and widely applicable technique for detecting equivalent mutants. The paper is an extension of our previous ICSE conference paper [26], which studied the application of TCE to the detection of equivalent and duplicated mutants in the C programming language. The present paper extends this previous study to also consider the Java programming language, allowing us to compare TCE performance on these two widely-used languages. The extended results further confirm that TCE is a highly effective and readily applicable technique, with strong evidence to suggest that it may be even more effective when applied to Java than what is already known to be when applied to C. Specifically, while TCE finds, on average, approximately one third of the equivalent mutants in C programs, it finds approximately half of the equivalent mutants in Java. These new findings for Java are based on the study of a known equivalent ground truth set, which we have augmented for this study (and make available for replication and further study1). We also study the application of TCE to much larger Java programs, for which no ground truth is available, reporting results for the total number of equivalent and duplicated mutants found (using both the standard Java compiler2 and the Soot analysis framework3). Overall, we believe that the findings regarding TCE are extremely encouraging. It can ameliorate the adverse effects of the equivalent and duplicated mutant problems for both C and Java programs by removing such invaluable mutants (by an average of approximately 10 percent for Java and nearly 30 percent for C) and, as a consequence, reduces the overall work needed to develop mutation adequate test suites by approximately 37 percent, while, at the same time, improving the accuracy of the mutation score measurement by 0-18 percent for Java and 0-16 percent for C (depending on the ratio of the killed mutants). Furthermore, and fundamental to its success and importance, TCE is not a complicated technique; it can easily be implemented and added to any mutation testing study. It has already been included in the mutation testing tool MiLu (Version 3.2), and we were easily able to incorporate TCE analysis into the results produced by the C and Java mutation testing tools Proteum[27] and muJava [28]. The rest of the paper is organised as follows: Section 2 presents mutation testing and related approaches. Section 3 details our experiment and the studied research questions, while, Sections 4 and 5 analyse our results. Our findings are discussed in Section 6. Finally, the threats to validity are presented in Sections 7, while Section 8 concludes with potential directions for future work. 2   Background 2.1 Mutation Testing Mutation testing embeds artificial defects on the programs under test. These defects are called mutants and they are produced by simple syntactic rules, e.g., changing a relational operator from > > to ≥ ≥ . These rules are called mutant operators . By applying an operator only once, i.e., the defective program has only one syntactic difference from the original one, a mutant called a first order mutant is produced. By making several syntactic changes i.e., applying the operators multiple times, a higher order mutant is produced. In this paper we consider only first order mutants. These are generated by applying the operators at all possible locations of the program under test, as supported by the 3.2 version of MiLu and version 3 of muJava . Additional information about the corresponding operators can be found at Section 3.4. By measuring the ability of the test cases to expose mutants, an effectiveness measure can be established. Mutants are exposed when their outputs differ from those of the original program. When a mutant is exposed, it is termed killed , while in the opposite case, live . Of course, ideally, equivalent mutants should be removed from the test effectiveness assessment. Doing so gives the effectiveness measure called mutation score , i.e., the ratio of the exposed mutants to the number of the introduced, excluding the equivalent ones. 2.2 Equivalent Mutants Early research on mutation testing has demonstrated that deciding whether a mutant is equivalent is an undecidable problem [15]. Undecidability of equivalences means that it is unrealistic to expect all the equivalent mutants to be removed; the best we can have here is just effective algorithms that can remove most equivalent mutants. Currently, a large number of mutants must pass a manual equivalence inspection [16]. This constitutes a significant cost. In addition, effort is wasted when testers generate test cases, either manually or automatically, in attempting to kill equivalent mutants. Apart from the human effort, there is a computational cost: since equivalent mutants cannot be killed, they have to be exercised on the entire test suite, whereas killable mutants only require the executions until they are killed. Fortunately, partial and heuristic solutions exist [31]. However, tackling the equivalent mutant problem is hard. This is evident by the fact that very few attempts exist. According to a recent systematic literature review on the equivalent mutant problem [44], which identified 17 relevant techniques (in 22 articles), the problem is tackled in three ways. One is to address the problem directly by detecting some equivalent mutants, while, the other two try to reduce their effects by avoiding their creation of by suggesting likely non-equivalent ones to help with the manual analysis process. Following the terminology of Madeyski et al. [44], we refer to them as the Detect , Avoid and Suggest approaches, respectively. Table 1 summarises the current state-of-the-art techniques in chronological order by focussing on the most recent techniques. Specifically, it records: the publication details, column “Author(s) [Reference]”, the year of the publication, column “Year”, the studied programming language, column “Language”, the size of the largest program used, column “Largest Subject”, the number of equivalent mutants studied, column “#Eq. Mutants”, the existence of an automated publicly available tool, column “Publicly Av. Tool”, the category of the approach, i.e., detection, avoidance or suggestion, column “Category” and the main findings of the publication, column “Findings”. From this table it becomes evident that very few methods and tools exist. Regarding the equivalent mutant detection, only two publicly available tools exist with the largest considered subject being composed of 319 lines of code. It is noted that all the “large” subjects, i.e., having more than 1,000 lines of code, that were used in the previous research, involve a form of sampling. Mutants are sampled from the studied projects with no information about the relevant size of the component/class that these mutants are located. In these lines, in Table 1 we report the size of the projects that we consider. It is noted that the purpose of this table is to summarise the related work on equivalent mutants by focussing on the most recent advances. Further details on the subject can be found on the systematic literature review of Madeyski et al. [44]. TABLE 1 Summary of the Related Work on Equivalent Mutants  Acree [30] studied killable and equivalent mutants, and found that testers correctly identified equivalent mutants for approximately 80 percent of the cases. In 12 percent of the cases, equivalent mutants were identified as killable and in 8 percent, killable mutants were identified as equivalent. Therefore, indicating that detection techniques, such as the one suggested by the present paper, not only help in saving resources but also at reducing the mistakes made by the humans. The idea of using compiler optimisation techniques to detect equivalent mutants was suggested by Baldwin and Sayward [29]. The main intuition behind this technique is that code optimisation rules, such as those implemented by compilers, form transformations on equivalent programs. Thus, when the original program can be transformed by an optimisation rule to one of its mutants, then, this mutant is, ipso facto , equivalent. Baldwin and Sayward proposed adapting 6 compiler optimisation transformations. These transformations were then studied by Offutt and Craft [31] who implemented them inside Mothra, a mutation testing tool for Fortran. They found that on average 45 percent of the equivalent mutants can be detected. Our approach is inspired by this recruitment of compilers research to assist in equivalent mutant detection. As already discussed and demonstrated in the prior, conference version of this work [26], it is surprisingly effective for the case of the C programming language. However, we propose a truly simple (and therefore scalable and directly exploitable) use of compilers, which remained unexplored. Our TCE instead of deliberately implementing specialised techniques, it simply declares equivalences only for those mutants which their compiled object code is identical to the compiled object code of the original program. As indicated by our empirical findings, in Section 6, our approach is impressively effective, practical and scalable. Offutt and Pan [32], [33] developed an automatic technique to detect equivalent mutants based on constraint solving. This technique uses mathematical constraints to formulate the killing conditions of the mutants. If these conditions are infeasible then, the mutants are equivalent. Nica and Wotawa [42] implemented a similar constraint-based approach to detect equivalent mutants and demonstrated that many equivalent mutants can be detected. Voas and McGraw [34] suggested that program slicing can help in detecting equivalent mutants. Later, Hierons et al. [35] showed that amorphous program slicing can be used to detect equivalent mutants as well. Although potentially powerful, these techniques suffer from the inherent limitations of the constraint-based and slicing-based techniques. It is evident that the constraint-based approach, [32], [33], was assessed on programs consisting of 29 lines of code at maximum, while, the slicing technique remains unevaluated apart from worked examples. The scalability of these approaches is inherently constrained by the scalability of the underlying constraint handling and slicing technology. Furthermore, a new implementation is required for every programming language to be considered. By contrast TCE applies to any language for which a compiler exists and so is as scalable as the compiler itself. Kintis and Malevris [48], [49] used data-flow patterns and showed that a large proportion of equivalent mutants and partially equivalent mutants, i.e., mutants equivalent only under specific program paths, form data-flow anomalies. Bardin et al. [50] used static analysis techniques, such as Value Analysis and Weakest Precondition calculus, to detect mutants that are equivalent because they cannot be infected. Their results show that a significant number of those mutants can be detected. Although promising, these two methods have only been evaluated with less than 200 equivalent mutant instances and so their effectiveness, efficiency and practicality remain unknown. Hierons et al. [35] suggested using program slicing to reduce the size of the program considered during the equivalence identification. Thus, testers can focus on the code relevant to the examined mutants. Harman et al. [36] also suggested using dependence-based analysis as a complementary method to assist in the detection of equivalent mutants. Adamopoulos et al. [37] suggested the use of co-evolutionary techniques to avoid the creation of equivalent mutants. In this approach test cases and mutants are simultaneously evolved with the aim of producing both high quality test cases and mutants. However, these previous approaches have been evaluated only on case studies and synthetic data so their effectiveness and efficiency remains unknown. More recently, several studies sought to measure the impact of mutant execution. Instead of finding a partial but exact solution to the problem, as done by the Detect approaches, they try to classify the mutants to help identify likely killable ones and likely equivalent ones, based on their dynamic behavior. This idea was initially suggested by Grun et al. [38] and developed by the studies of Schuler et al. [39] and Schuler and Zeller [40], [41] who found that impact on coverage can accurately classify killable mutants. Kintis et al. [45], [46] further develop the approach, using the impact of mutants on other mutants, i.e., using higher order mutants. Papadakis et al. [47] proposed a mutation testing strategy that takes advantage of mutant classification. Finally, mutants belonging to software clones have been shown to exhibit analogous behaviour with respect to their equivalence [43]. Thus, knowledge about the (non-)equivalence of a portion of such mutants can be leveraged to analogously classify other mutants belonging to the same clones. Apart from the technical differences between TCE and the existing approaches, as discussed above, there is also a fundamental difference that is the identification of duplicated mutants. Existing approaches only aim at equivalent mutants while TCE tackles the general problem of mutant equivalences. 2.3 Reducing the Cost of Mutation Testing Mutant sampling has been suggested as a possible way to reduce the number of mutants. Empirical results demonstrate that even small samples [18] can be used as cost effective alternatives to perform mutation testing [17], [19]. Other approaches select mutant operators. Instead of sampling mutants at random, they select mutant operators that are empirically found to be the most effective. To this end, Offutt et al. [51] demonstrated that five mutant operators are almost as effective as the whole set of operators. More recently, Namin et al. [52] used statistically identified optimal operator subsets. Other cost reduction methods involve mutant schemata [23], [53]. This technique works by parameterizing all the mutants through instrumentation, i.e., introduce all the mutants into one parameterised program. However, apart from the inherent limitations of this technique [28] and the execution overheads that introduces, it also makes all the equivalent mutant detection techniques not applicable. Other approaches identify redundant mutants that fail to contribute to the testing process. Kintis et al. [54] defined the notion of disjoint mutants, i.e., a set of mutants that is representative of all the others (killing them implies killing all the others), and found that 9 percent of all mutants are disjoint. Ammann et al. defined minimal mutants using the notion of subsumption [55] and demonstrated that a small set of mutants, approximately 1.2 percent subsumes all the others. Based on these works, Papadakis et al. [56] demonstrated that redundancy among mutants has a very good chance ( > 60 p e r c e n t > ) to inflate mutation score and lead to biassed results. Along the same lines, Kurtz et al. [57] analysed the validity of selective mutation and found that selective mutants score relatively low with respect to subsuming mutants. Kaminski et al. [58], [59] and Just et al. [60] leverage the suggestions made by Tai [61] on fault-based predicate testing and demonstrated it possible to reduce the redundancy within the relational and logical operators. Higher order mutation can also reduce mutant numbers: Sampling [19], [44] and searching [13], [62], [63] within the space of higher order mutants both reduce the number of mutants and also of the equivalent mutants. 3   Experimental Study and Settings This section details the settings of our experiment. First, it presents the TCE approach ( Section 3.1) and the posed research questions ( Section 3.2). Next, the studied C and Java programs are described ( Section 3.3), along with the employed mutant operators ( Section 3.4), and, finally, the execution environment ( Section 3.5). 3.1 Detecting Mutant Equivalences: The TCE Approach Executable program generation involves several transformation phases that change the machine code. Different optimisation transformation techniques result in different executables. However, when there exist multiple program versions with identical source code, then there is no point in differentiating them with test data; it is safe to declare them as functionally equivalent. TCE realises this idea to detect mutant equivalences. It declares equivalent any two program versions with identical machine code. TCE simply compiles each mutant, comparing its machine code with that of the original program. Similarly, TCE also detects duplicated mutants, by comparing each mutant with the others residing in the same unit, i.e., function. As the reader will easily appreciate, the TCE implementation is truly trivial, hence its name: it is a compile command combined with a comparison of binaries. 3.2 Research Questions The mutation testing process is affected by the distorting effects of the equivalent and duplicated mutants on the mutation score calculation. Therefore, a natural question to ask is how effective is the TCE approach in detecting equivalent and duplicated mutants. This poses our first RQ: RQ1 (Effectiveness): How effective is the TCE approach in detecting equivalent and duplicated mutants? We answer this question by reporting the prevalence of the equivalent and duplicated mutants detected by the TCE approach using gcc4 and Soot.5 To reduce the confounding effects of different compiler configurations, we apply four and two popular options for gcc and Soot on the selected classes/packages, and report the number of the equivalent and duplicated mutants found. Soot does not support multiple levels of optimizations, thus, we only report its intra-procecural optimisations and report the equivalent and duplicated mutants found. The answer to this question also allows the estimation of the amount of effort that can be saved by the TCE method. The existing mutant equivalent detection techniques suffer from performance and scalability issues. As a result, the authors are unaware of any mutation testing system that includes a proposed equivalent mutant detection. By contrast, the TCE is static, and can be applied to any program that can be handled by a compiler. This makes TCE potentially scalable, but we need additional empirical evidence to determine the degree to which it scales. Hence, in the second RQ, we seek to investigate the observed efficiency and the scalability of the TCE approach: RQ2 (Efficiency): How efficient and scalable is the TCE approach? To demonstrate the scalability, we use selected classes/packages from 12 large open source projects, 6 for each studied programming language, and we report the efficiency of the mutant generation, equivalent mutant detection and duplicated mutant detection processes. For the case of gcc we also explore the trade-off between the effectiveness and efficiency using different compiler settings. To decide when it is appropriate to stop the testing process, testers need to know the mutation score. To this end, they need to identify equivalent mutants. The TCE approach improves the approximation by determining such mutants. However, to what extent? This is investigated in the next RQ: RQ3 (Equivalent Mutants) What proportion of the equivalent mutants can be detected? What types of equivalent mutants can be detected? To answer RQ3, we need to know the ‘ground truth’: how many equivalent mutants are there in the subjects studied? We therefore applied the TCE approach on two benchmark sets, one for each studied programming language, with hand-analysed, ground-truth data on equivalent mutants. The first benchmark,6 pertaining to the C test subjects, includes 990 manually-identified equivalent mutants over 18 small- and medium-sized subjects. The second one,7 for the Java programs, comprises 196 equivalent mutants selected over 6 small- and medium-sized subjects, detected with manual analysis. We report the proportion of the equivalent mutants found by TCE. We also analyse and report the types of the detected equivalent mutants. This information is useful in the design of complementary equivalent detection techniques. Mutation testers usually employ subsets of mutant operators. Therefore, knowing about the relationship between the operators and the equivalent and duplicated mutants found by TCE is useful in the sense that mutation testers can better understand the importance of their choices. Hence, the next RQ examines the extent of the equivalent and duplicated mutants found per mutant operator: RQ4 (Impact on Mutant operators): What is the contribution of each operator to the proportion of equivalent and duplicated mutants found by TCE? Among the several factors that can affect TCE is the program size. Thus, one might expect that in larger programs, the equivalent mutant identification would be harder, thereby impeding TCE’s effectiveness. Hence, we investigate whether the size of the programs or the number of mutants they contain correlate with the effectiveness of the TCE approach. RQ5 (Size Influence): Does program size or number of mutants affect TCE? We answer this question by investigating correlations between the number and proportions of both equivalent and duplicated mutants found by TCE with the program and mutant set size. Finally, since we have results for both C and Java, we investigate the similarities and differences between the two sets of programs. Thus we ask: RQ6 (Differences between programming languages): What are the similarities and differences between C and Java with respect to TCE? To answer this question we compare the results of C and Java and try to provide insights on the differences between C and Java as viewed by mutation testing. 3.3 Subject Programs We used two categories of subject programs for both C and Java. The first category is composed of 6 large to medium open source programs. In this set, we chose ‘real-world’ programs that vary in size and application domain. The second category of programs was taken from the studies of Yao et al. [16] and Kintis and Malevris [49]. We chose these sets because they are accompanied by manually-identified equivalent mutants. The availability of known equivalent mutants allows us to answer RQ3, because it provides a ‘ground truth’ on the undecidable equivalence question for a set of subjects. The rest of RQs are answered using the larger programs. Regarding the large programs, compiling all their mutants constitutes a time consuming task. This is due the increase of the mutants according to the size of the programs. It is evident by our reported results, presented in Section 4.2, where it took more than 50 hours to compile only the mutants involved in the Vim-Eval component (under -O3 ). TCE may be scalable in itself, but applying it to all possible mutants of a large program is clearly infeasible. Though there are techniques to reduce the number of mutants, i.e., by sampling, we prefer not to use them in case we unintentionally bias our sample of mutants. We prefer to sample, safer, over the code to be mutated in a systematic way so that we do not pre-exclude any mutants from our investigation. Therefore, in C we rank their source files according to their lines of code. Then, we select the two largest components (source code files). On these two components we apply mutation to all the functions they contain. In Java we followed a similar process by ranking all the project packages according to their size and selected the three largest classes that could be handled without a problem by muJava among the four largest packages. Tables 2 and 3 respectively present the information about the first category of subject programs for C and Java. Regarding Table 2 (large C subjects), the Gzip and Make are GNU utility programs. The first program performs file compression and the second one builds automatically executable files from several source code files. The two largest components of Gzip are the ‘trees’ and ‘gzip’. The former implements the source representation using variable-length binary code trees and the later implements the main command line interface for the Gzip program. The two largest components of the Make program are ‘main’ and ‘job’. The later implements utilities for managing individual jobs during the source building processes and the former implements the command line interface. The GSL (GNU Scientific Library) is a C/C++ numerical library, which provides a wide range of common mathematical functions. Its two largest components are ‘gen’ and ‘blas’. The ‘gen’ implements utilities that compute eigenvalues for generalised vectors and matrices. The ‘blas’ implements BLAS operations for vectors and dense matrices. TABLE 2 Details of C Subjects: ‘LoC’ Shows the Lines of Code of the Project; ‘Comp’ and ‘Comp-Size’ Show the Components Considered and Their Size; ‘Func’ and ‘Muts’ Show the Number of Functions and Mutants of the Components.  TABLE 3 Java Test Subjects’ Details: ‘LoC’ Shows the Source Code Lines of the Projects; ‘Package’ and ‘Class-Size’ Present the Packages of the Considered Classes and Their Size; the ‘Methods’ and ‘Mutants’ Columns Show the Number of Methods and the Corresponding Number of Generated Mutants  The program MSMTP is an SMTP client for sending and receiving emails. The components studied are the ‘smtp’ and the ‘msmtp’. The ‘smtp’ implements the coreutilities for exchanging information with SMTP servers and the ‘msmtp’ component implements the command line interface. The program Git is a source code management system and the components selected are the ‘refs’ and ‘diff’. The ‘refs’ implements the ‘reference’ data structure that associates history edits with SHA-1 values and the ‘diff’ component implements utilities for checking differences between git objects, for example commits and working trees. Finally, the program Vim is a configurable text editor. The selected components, ‘spell’ and ‘eval’, implement utilities for checking and built-in expression evaluation, respectively. The first two columns of Table 3 (large Java subjects) refer to the first category of programs and their size in terms of source code lines. The domains of the chosen subjects range from mathematics libraries (Commons-Math ) to build systems (Ant ). The application domains of the remaining subjects appertain to enhancements of Java’s core class (Commons-Lang ), bytecode manipulation (BCEL ), date and time manipulation (Joda-Time ) and database applications (H2 ). Finally, the size of the studied Java programs ranges between 16,753 and 104,479 source code lines. The next two columns of the table present the names of the utilised packages and the size of the considered classes, respectively. Finally, the last two columns of the table refer to the number of methods that belong to the examined classes and the number of the generated mutants. The second category of subjects contains 8 C and 6 Java programs. The C programs have lines of code ranging from 10 to 42 lines, 7 programs with 137 to 564 lines and 3 real-world programs with 9,564 to 35,545 lines. Additional details for these programs can be found in the work of Yao et al.  [16] . Details regarding the Java programs are given in Table 4. The first two columns of the table present the examined programs and the considered methods. Bisect is a simple program that calculates square roots, Commons-Lang and Joda-Time are enhancements to java core library and time manipulation libraries, Pamvotis is a wireless LAN simulator, Triangle is the classic triangle classification program and XStream is an XML object serialisation framework. The last two columns of the table present the number of the generated and manually-identified equivalent mutants. It is noted that for the purposes of the present study we extended the original set of programs by manually analysing approximately 400 additional mutants. Thus, in total the considered set is composed of 1,542 manually analysed mutants, out of which 196 are equivalent. TABLE 4 Manually-Analysed Java Test Subjects’ Details: ‘Program’ and ‘Method’ Columns Present the Examined Programs and the Considered Methods; ‘Mutants’ Shows the Number of the Generated Mutants and ‘Equivalent’ the Number of the Manually-Identified Equivalent Ones  3.4 Mutant Operators Based on previous research on mutant operator selection, we identify and use two sets of operators (one for C and one for Java). The C set of operators was based on the studies of Offutt et al. [51] and Andrews et al. [4], [64] and it is composed of 10 operators. A detailed description of the operators is reported in Table 5. TABLE 5 Mutant Operators of Milu  We detail exactly how these operators were applied since this is an important piece of information that differs from one tool to another. The ABS and UOI operators were only applied to numerical variables. The CRCR was applied to integer and floating numeric constants. No mutant operator was applied to the variables of the lefthand side of assignment statements; we only apply them to the right hand sides. This is an implementation choice that avoids the generation of duplicated mutants (as any variable on the lefthand side of assignment statements will be used (and mutated) later in the program). All operators are applied recursively to all sub expressions. With respect to the Java programming language, we used all the method-level operators of muJava (version 3) [28]. This means that we excluded all the object oriented related mutation operators. Previous research [65] has shown that object oriented mutation operators produce a small number of mutants and a rather low number of equivalent ones and thus, there is no need to investigate this case. muJava supports a wide range of mutant operators built based on the experience and studies of Offutt and colleagues, i.e., [28] and [51]. Table 6 describes the employed mutant operators: the first column of the table presents their names and the second one the mutation they impose. In total, 15 mutant operators were utilised which fall into 6 general categories: arithmetic operators, relational operators, conditional operators, shift operators, logical operators, and assignment operators. TABLE 6 Mutant Operators of muJava  We use these operators due to their extensive use in literature [2]. To generate the C mutants, we use MiLu[66], and for the Java mutants, muJava (version 3). Further details and the implementation of the tools and their operators can be found at the webpages of MiLu8 and muJava.9 3.5 Experimental Environment Two series of experiments were conducted. The first one was for programs written in C and the second one for programs written in Java. All the experiments of the C programs were undertaken on the Microsoft Azure Cloud platform using a A9 Compute Intensive Instance in the Ubuntu 14.04 operating system with gcc 4.8 compiler. To compile the mutants we used four configuration options. We compile with no optimisation settings, denoted as None , and with the three popular ones, as realised by the gcc compiler, denoted as -O , -O2 and -O3 . We use the Linux time utility to measure the CPU execution time of all the involved processes. To check whether two binaries are equivalent we use the ‘diff’ utility with the flag ‘--binary’. In short, we use a gcc -flag ’ combined with a ‘diff’. All the experiments regarding the Java language were performed on a physical machine running Fedora 22, equipped with an i7 processor (3.40 GHz, 4 cores) and 16 GB of memory. TCE relies on compiler optimisation to detect mutant equivalences. While in programming languages such as C or C++ many optimisation options have been embedded within the language compilers, e.g., gcc, this does not hold true for the standard Java compiler, i.e., javac. Despite the fact that javac does not possess advanced optimisation capabilities at the compilation time, it is able to detect some mutant equivalences. In order to successfully apply TCE to Java, compiler optimisations are required. To this end, we used Soot[67], a popular framework for analysing and transforming Java applications. Soot implements various analysis and transformation procedures. We utilised the -O option of the tool which performs intra-procedural optimisations. Such optimisations include the ‘elimination of common sub-expressions’ and ‘copy and constant propagation’, among others. As in the case of the C language, we used the diff command line tool, for the purposes of comparing the optimised classes. 4   TCE via gcc This section reports the results pertaining to the C programming language. Sections 4.1 and 4.2 respectively present results regarding the TCE effectiveness and efficiency. Sections 4.3 and 4.4 detail our results regarding the ground truth and the mutant operators. Finally, Section 4.5 investigates the impact of program size to TCE. 4.1 gcc: TCE Effectiveness To assess the effectiveness of the TCE approach, answering RQ1, we measure the number of the detected equivalent and duplicated mutants. We also measure the proportions of these mutants per program, computed as the percentage of the detected to introduced. When mutants are mutually equivalent to each other, i.e., they are duplicated, one of them should be kept, while, the other(s) should be discarded. In our results we only report the number of mutants that should be discarded. Table 7 reports our results per program and per considered optimisation option. Overall, these results indicate that TCE can detect in total 9,551 equivalent mutants, accounting for 7.4 percent of all mutants. TCE also detected 27,163 duplicated mutants, which account for 21 percent of all mutants. Overall, TCE can thus identify and remove approximately 28 percent of all mutants as being useless. TABLE 7 Equivalent and Duplicated Mutants Detected by TCE via gcc. ‘None’, ‘-O’, ‘-O2’ and ‘-O3’ Report the Fraction of All Identified Equivalent Mutants that Were Detected per Optimisation Flag  Fig. 1 depicts the proportions of both equivalent and duplicated mutants detected per program. The horizontal axis of the graph is ordered by the size of the components while the vertical axis records the proportions of mutants detected. From these results, it is evident that all the subjects have a reasonably high proportion of equivalent and duplicated mutants. The proportions of equivalent mutants detected varies from program to program. In the worst case it is 2 percent, while in the best, 17 percent. We observe a small variation in the proportions of the identified equivalent and duplicated mutants. The only exceptions are the Gsl-Blas and Gsl-Gen components. In the former case, TCE detects many equivalent mutants and very few duplicated ones, while, in the later case, it detects very few equivalent mutants and a similar to the other programs ratio of duplicated mutants. This divergence is mainly attributed to the internal structure and code characteristic of the component. Fig. 1. The proportion of equivalent and duplicated mutants detected by TCE per studied C program. Finally, Table 7 reveals that, depending on the options used, the detected equivalences differ. For instance, the -O3 option found on average 84 and 100 percent of the equivalent and duplicated mutants that are detected by applying all the options. Interestingly, with respect to equivalent mutants, among the different optimisation options, i.e., -O , -O2 and -O3 , there is no clear winner and their behaviour varies between programs. However, the overall differences between the options are relatively small. With respect to duplicated mutants, the results are clear and they show that the best options are the -O2 and -O3 . 4.2 gcc: TCE Efficiency To assess the efficiency of the TCE approach and answer RQ2, we report the CPU execution time. Table 8 summarises the execution time of TCE in total, average and per employed component, using the four studied compiler settings. The columns ‘Comp.’, ‘Eq.D.’ and ‘D.D.’ record the execution time with respect to the compilation process, the equivalent mutant detection and duplicated mutant detection, per considered compilation option, respectively. TABLE 8 Execution Time, Measured in Sec.: Compilation ‘Comp.’, Equivalent Mutant Detection ‘Eq.D.’ and Duplicated Mutant Detection ‘D.D.’  These results reveal that the execution time of the equivalence detection process is reasonably small compared to the compilation one. For instance, TCE requires on average 22 seconds, for all cases, to detect equivalent mutants, while, the average compilation cost is 5,942 seconds in the best case. A similar case arises when considering the costs for detecting duplicated mutants. While this is approximately an order of magnitude higher than the cost of detecting equivalent mutants, it is still reasonable; 225 seconds, and no more than 1/30 of the cheapest compilation cost. It is noted that our approach checks for equivalences only for the combinations of mutants that are located on the same function. Therefore, the reported time is analogous to the number of combinations between the mutants located at each function of the project and not between the whole combinations of all project mutants. Our results show that the compilation time of the -O3 option is almost 5 times higher than the None option. However, this is counterbalanced by the improved effectiveness of the option. In this case, the total time spend for compiling, detecting equivalent and duplicated mutants is 374,162, 260 and 2,744 seconds, respectively. Therefore, TCE analyzed 129,161 mutants in 377,166 seconds. This time accounts for less than 3 seconds per mutant suggesting that its application is reasonable. 4.3 gcc: Equivalent Mutants To determine the ratio of detected to all existing equivalent mutants, we applied TCE to the equivalent mutants identified by Yao et al. [16], using the accompanying website data. 10 This site is regularly updated, so data may differ slightly from those previously reported [16]. Additional details about these data can be found on the website. Table 9 reports the number and the proportions of equivalent mutants detected by TCE when using the different settings. The results are surprisingly good. They reveal that out of all the existing equivalent mutants, TCE can detect from 9 to 100 percent (with 30 percent on the average case) of them. With respect to the total number of mutants (killable and equivalent ones), the TCE equivalent ones are approximately 7 percent. These results are achieved within a few seconds with the potential to save considerable manual and computational resources. Together with the previously presented results, we conclude that TCE is effective and practically applicable on large real-world programs. TABLE 9 TCE Applied to Yao et al. [16] Benchmark Set: Number ‘No.’ and Proportion ‘%’ of Detected Equivalent Mutants  Regarding the types of the equivalent detected mutants, i.e., second part of RQ3, we recall that equivalent mutants are equivalent because: a) they reside in unreachable code, b) it is impossible to affect the program state that pertains immediately after mutant execution or c) there is no possible way to propagate the infection they introduce to the program output. Interestingly, the equivalent mutants detected by TCE reside within all of these categories. In particular, TCE detected 6, 25 and 45 percent of the equivalent mutants caused by a), b), and c), respectively. 4.4 gcc: Mutant Operators To determine the influence of the mutant operators on the effectiveness of TCE, answering RQ4, we measure the number of detected equivalent and duplicated mutants per operator. We also measure the ratios of detected to introduced mutants by the studied operators. It is noted that the choice of which mutants should be discarded when computing the duplicated mutants, can unfairly influence the reported numbers with respect to the mutant operators that they belong to. To avoid this, in this section we report the number and proportions of all the mutants that are duplicated and not the discarded ones. Table 10 reports the number and proportions of the equivalent and duplicated mutants found by TCE per program and operator. These results suggest that on different programs a similar proportion of equivalent and duplicated mutants can be detected by TCE. The only exceptions are the Gsl-Blas and Gsl-Gen components. TABLE 10 Number ‘No.’ and Proportion ‘%’ of Equivalent and Duplicated Mutants Detected by TCE per Operator  Fig. 2 depicts the proportions of equivalent and duplicated mutants detected per operator. The horizontal axis follows the presentation order of the operators from Table 10, while, the vertical axis records the proportions of detected mutants. These results reveal that the ABS and UOI operators introduce at least 15 percent equivalent mutants of all that they introduce. They also show that TCE detects more than 5 percent of equivalent mutants produced by the ABS, ROR, UOI and CRCR operators. Regarding the duplicated mutants, TCE detects large proportions, above 10 percent, on all of them but, the ABS, LCR and OAAA. Interestingly, the LCR operator seems to produce very few equivalent or duplicated mutants. Fig. 2. The proportion of equivalent and duplicated mutants detected by TCE per mutant operator in case of C. In conclusion, our results show that all but the LCR and OAAA operators produce a relatively high ratio of useless mutants, i.e., equivalent and duplicated. In practice this involves a huge overhead that, fortunately, can be saved by TCE. 4.5 gcc: Program Size and Mutant Equivalences To answer RQ5, we use the Spearman rank correlation coefficient ρ $ Z . T h i s i s a n o n − p a r a m e t r i c s t a t i s t i c a l t e s t t h a t m e a s u r e s w h e t h e r t w o v a r i a b l e s ′ r a n k s a r e r e l a t e d , i . e . , i t a s s e s s e s t h e m o n o t o n i c r e l a t i o n s h i p b e t w e e n t h e t w o v a r i a b l e s . T h e S p e a r m a n c o r r e l a t i o n g i v e s v a l u e s i n t h e r a n g e o f [ − 1 , + 1 ] w i t h 0 i n d i c a t i n g n o r e l a t i o n s h i p a n d + 1 i n d i c a t i n g a p e r f e c t o n e ( − 1 , a l s o i m p l i e s a p e r f e c t i n v e r s e r e l a t i o n s h i p ) . I n a d d i t i o n t o t h e t h e c o r r e l a t i o n c o e f f i c i e n t Z $ ρ $ Z w e r e p o r t t h e o b t a i n e d p − v a l u e s t h a t r e p r e s e n t t h e c h a n c e t h a t w e w o u l d o b s e r v e t h e Z $ ρ ρ value reported, were there, in fact, to be no correlation. We found a correlation between the number of mutants and the number of equivalent mutants detected ( ρ = 0.818 ρ , p-value = 0.002 = ). This suggests that more mutants lead to more equivalent ones. Similarly, a strong correlation between the number of mutants and the detected duplicated ones (Z_ $\rho = 0.930$_Z, p-value < 2.2 e − 16 < ) was also found. The correlation between the number of mutants and the proportion of TCE equivalent and duplicated mutants was found to be ( ρ = − 0.091 ρ , p-value = 0.783 = ) and ( ρ = 0.280 ρ , p-value = 0.379 = ) respectively. These results suggest that we have no evidence supporting the argument that mutants’ number can have a strong influence on the proportions of the equivalences detected by TCE. We also study the relation between the program size with the number of detected equivalences. We found a medium to small correlation in case of equivalent mutants ( ρ = 0.692 ρ , p-value = 0.016 = ). A slightly lower correlation was found between the size of program and the number of duplicated mutants (Z_ $\rho = 0.650$_Z, p-value = 0.026 = ). With respect to proportions, i.e., correlation between the program size and the proportion of the detected equivalences, we found ( ρ = − 0.035 ρ , p-value = 0.921 = ) and ( ρ = 0.084 ρ , p-value = 0.800 = ) for the cases of equivalent and duplicated mutants, which indicate that we have no data supporting the argument that program size impacts the ratios of the detected equivalences. Finally, we found a medium correlation between the size of program and the whole number of mutants ( ρ = 0.671 ρ , p-value = 0.020 = ), which indicates that larger programs have more mutants than smaller ones. In conclusion, we find no evidence of any correlation between the ratios of equivalent and duplicated mutants in any of the size indicators. This means there is no evidence that the proportion goes up or down as the size of the program or the number of mutants changes. However there is evidence that the number goes up with the size, as one would expect. Taken together based on the studied mutant set, these can be regarded as evidence suggesting that the number of TCE equivalent and duplicated mutants is a fairly consistent proportion, unaffected by the size of the program. These results may be explained by the fact that the compiler optimisations we use only apply “locally”, i.e., on the occurrences of code patterns, and not on the semantic of the entire system. 5   TCE via javac and Soot This section details our results for Java. Sections 5.1 and 5.2 respectively present results regarding TCE effectiveness and Efficiency. Sections 5.3 and 5.4 detail our results regarding the ground truth and the mutant operators. Finally, Section 5.5 investigates the impact of program size on TCE. 5.1 javac and Soot: TCE Effectiveness In an analogous manner to the results of Section 4.1, we present our findings that are pertinent to RQ1, i.e., the effectiveness of TCE. These results are illustrated in Table 11 and Figure 3. Fig. 3. The proportion of equivalent and duplicated mutants detected by TCE per studied Java program. TABLE 11 Equivalent and Duplicated Mutants Detected by TCE via javac and Soot  Table 11 presents the equivalent and duplicated mutants detected by javac and Soot per test subject. The ‘#Mutants’ column, which is divided into the ‘Eq.’ and ‘Dup’ sub-columns, presents the number of the detected equivalent and duplicated mutants (per tool). The ‘% of all Mutants’ column records their corresponding proportion to the generated mutants. From the depicted results, it is clear that Soot outperforms javac in both equivalent and duplicated mutant detection, managing to detect 3,904 equivalent mutants and 3,687 duplicated ones. Thus, code optimisations implemented in Soot appear to be superior to the ones of javac. Furthermore, it should be mentioned that the mutants detected by Soot form a superset of the ones detected by javac. Therefore, we conclude that Soot constitutes an appropriate tool for TCE. It is noted that most Java-to-bytecode compilers mainly perform runtime optimizations than static ones. Thus, class files are optimised by the Java virtual machine as they are interpreted and not at the compilation time. This explains why the Java stock compiler is infective. Fig. 3 illustrates the proportion of equivalent and duplicated mutants per test subject. The horizontal axis presents the corresponding proportions and the vertical presents the test subjects in ascending order, according to their size. By examining the figure, it becomes evident that TCE manages to detect a considerable number of equivalent and duplicated mutants, ranging between 1 and 18 percent for equivalent ones and 2 and 17 percent for duplicated ones. To summarise, in the case of Java, TCE managed to detect 6 percent of all mutants as equivalent and 5 percent of them as duplicated ones. 5.2 javac and Soot: TCE Efficiency In this section, we detail the empirical findings pertaining to TCE’s efficiency for the case of Java. To this end, Table 12 presents the CPU execution time that the equivalent and duplicated detection required per test subject and optimisation tool. TABLE 12 Execution Time, Measured in Sec., of Equivalent and Duplicated Mutant Detection per Considered Tool and Test Subject  Table 12 is divided into three columns: ‘Program’ refers to the names of test subjects; the ‘javac’ column reports the compilation time (‘Comp.’ sub-column), the equivalent mutant detection time (‘Eq.D.’ sub-column) and the duplicated mutant detection time (‘D.D.’ sub-column) of TCE via javac; and, ‘ Soot’ presents the corresponding results in the case of TCE via Soot. It should be noted that in this column the reported compilation time also includes the execution time of the tool. Finally, the last two rows of the table present the total and average time of the examined analyses. Regarding equivalent mutant detection, TCE via javac required in total 140 seconds to detect 124 equivalent mutants, while, TCE via Soot required 208 seconds for the identification of 3,904 equivalent mutants, with an average of 6 and 9 seconds per examined package, respectively. Given that the corresponding compilation time is 2,703 seconds for javac and 78,318 seconds (compilation and optimisation) for Soot, with an average of 113 and 3,263, it can be argued that TCE forms a practical approach for detecting equivalent mutants. Considering the duplicated mutants, the application of javac detected 3,123 mutants in 13,636 seconds and Soot detected 3,687 mutants in 19,399 seconds. It is noted that both equivalent and duplicated mutants that are detected by javac form a subset of those detected by Soot. Overall, the presented data suggest that TCE manages to automatically and safely discard a significant number of useless mutants in only a small fraction of the time, in less than 2 seconds per examined mutant. The question that is raised here is whether the time required by TCE is acceptable. While this depends on many uncontrolled parameters, we would like to underline that detecting equivalent mutants is a tedious and manual task. Previous research estimated the time of the manual identification of a single equivalent mutant to be approximately 15 minutes [40]. Assuming this is a fair approximation, identifying the TCE equivalent mutants pure manually would require 124 × 15 124 minutes or 111,600 seconds for the case of javac and Z_ $3,904\times 15$_Z minutes or 3,513,600 seconds for the case of Soot. Thus, it can be easily concluded that the execution cost of TCE is small when compared to the estimated manual effort. In fact, the total cost of TCE (optimisation phase + + detection phase) constitutes only 3 percent of the estimated manual effort. 5.3 javac and Soot: Equivalent Mutants This section, which answers RQ3, provides insights into the actual proportion of equivalent mutants that can be automatically detected by TCE. We perform this evaluation based on manually-identified sets of such mutants to gives us a ground truth. Table 13 describes the corresponding findings per utilised tool. As can be seen, javac failed to detect equivalent mutants on our ground truth benchmark. By contrast, Soot detected 105 out of 196 equivalent mutants, indicating that it can automatically weed out more than 50 percent of the studied equivalent mutants. These results provide strong evidence regarding the TCE’s effectiveness. Finally, it should be stated that these automatically-detected equivalent mutants correspond to 7 percent of all the studied ones, which is in line with the results of the large-scale experiment we report in Section 5.1. TABLE 13 TCE Applied to Java Benchmark Set: Number ‘No.’ and Proportion ‘%’ of Detected Equivalent Mutants.  A manual analysis of the types of equivalent mutants that are TCE equivalent reveals that all but one of the detected mutants belong to the third category, i.e., the corresponding mutant can be reached and can infect the program state locally but subsequently fail to propagate the corrupted state to the observable output. The one mutant not falling into this category is a mutant that can be reached but not infected. 5.4 javac and Soot: Mutant Operators In order to answer RQ4 for Java, this section reports the contribution of each mutant operator to the detected mutants. More precisely, Table 14 presents the number of the detected equivalent mutants per operator, along with their proportion to all the generated mutants by that specific operator, and Table 15 presents the respective results for the case of the duplicated mutants. For brevity, we only record the cases that the number of detected mutants was higher than 0 in the studied programs. TABLE 14 Number ‘No.’ and Proportion ‘%’ of Equivalent Mutants Detected by TCE per Operator  TABLE 15 Number ‘No.’ and Proportion ‘%’ of Duplicated Mutants Detected by TCE per Operator  By examining Table 14, it becomes clear that TCE (via Soot ) managed to detect equivalent mutants that belong to 7 out of the 15 utilised mutant operators, indicating that it can be effective across a wide range of operators. With respect to the duplicated mutants discovered, the findings of Table 15 show that these mutants belong to 8 operators, corroborating the previous statement. Fig. 4 visualises the proportions of detected mutants across the corresponding mutant operators. It can be seen that TCE manages to identify at least 4 percent of the equivalent mutants produced by LOR, AODS and AOIS and at least 24 percent of the duplicated ones generated by ROR and COI. Again, for brevity reasons we depict only those operators with higher than 0 percent detection rates. Fig. 4. The proportion of equivalent and duplicated mutants detected by TCE per program studied in case of Java. It is noted that the TCE equivalences are a special form of redundancy as they require mutual subsumption between mutants (mutant a a subsumes mutant b b and mutant b b subsumes mutant a a ). This is different from the redundant mutants studied by Kaminski et al. [59] and Just et al. [60] which consider non-mutual subsumptions (mutant a subsumes b and mutant b does not subsumes a). In view of this, it is normal that the COR operator produces redundant mutants that are not captured by TCE (results reported in Tables 14 and 15). Still, a stronger version of the COR operator may provide more chances for TCE equivalences. 5.5 javac and Soot: Program Size and Mutant Equivalences In order to answer RQ5, i.e., whether or not the number of generated mutants or the program size affects TCE, we examined the correlation between the program size and the number and proportions of the detected equivalent and duplicated mutants. All ρ ρ values, and p-values, were computed using the Spearman rank correlation test. Regarding the correlation between the number of mutants and the number of identified equivalent ones, a ρ ρ of 0.786, p-value = 8.536 e − 06 = , was obtained, indicating a strong correlation. The correlation between the number of mutants and the proportion of the equivalent ones was found to be ρ ρ of − 0.008 − , p-value = 0.972 = . In the case of duplicated mutants, i.e., correlation between the number of mutants and the number of duplicated ones, ρ = 0.657 ρ , p-value = 0.001 = and ρ = − 0.271 ρ , p-value = 0.199 = with respect to number and proportions of duplicated mutants detected. Based on these data, it can be concluded that the number of equivalent and duplicated mutants detected by TCE tends to increase as the number of the generated mutants increases. However, this does not appear to be the case when considering the detected proportions. With respect to the correlation of program size with the detected equivalent and duplicated mutants, the obtained results suggest that there is a very weak correlation in the case of the equivalent mutants ( ρ = 0.230 ρ , p-value = 0.277 = ) whereas, in the case of the duplicated ones, there is a strong one, i.e., ρ = 0.797 ρ , p-value = 3.081 e − 06 = . The correlations between program size and proportion of detected equivalent and duplicated mutants was found to be ρ = − 0.337 ρ , p-value = 0.107 = and ρ = 0.423 ρ , p-value = 0.041 = . It is noted that this last case, i.e., program size and duplicated mutants, is the only one where our data show a correlation. In conclusion, our data show that an increase in the program size is expected to increase the number of equivalent and duplicated mutants identified by TCE. However, the proportion of equivalent mutants detected is expected to be unaffected by the program size, while the proportion of duplicated ones is affected. Finally, we found a low but nontrivial correlation between the program size and the number of generated mutants, i.e., ρ = 0.417 ρ , with p-value = 0.044. 6   Discussion This section summarise our results and concludes the stated RQs. It also discusses the practical implications and constraints of applying mutation with the use of TCE. 6.1 Results Summary 6.1.1 TCE Effectiveness Our results suggest that TCE can reduce the total number of mutants by 11 percent for Java and 28 percent for C. In the case of C, TCE equivalent mutants range from 2 percent to 17 percent depending on the studied program and account for 7.4 percent of all mutants on average. In the case of Java, TCE, using Soot, revealed 5.7 percent equivalent mutants, on average, that range from 1 to 18 percent. TCE duplicated mutants range from 3 to 27 percent and account for 21.0 percent on average when considering C, while for Java, they range from 2 to 17 percent and they are 5.4 percent on average. 6.1.2 TCE Efficiency The time to detect equivalent and duplicated mutants, using the diff utility, ranges between programs and it is on average 22 and 225 seconds for C and 9 and 808 seconds for Java. This indicates that once the mutants have been compiled/optimised, the equivalence detection comes ‘almost for free’. This is an important finding because it suggests that TCE can be applied to remove equivalent and duplicated mutants before the application of other time consuming cost-reduction methods. Our results show that the total time spent for compiling, detecting equivalent and duplicated mutants is 374,162 and 95,222 seconds for C and Java respectively. Thus, a candidate mutant can be analyzed by TCE in less than 3.0 and 1.5 seconds for C and Java respectively. 6.1.3 Equivalent Mutants In an attempt to identify the prevalence of TCE equivalent mutants we estimated their ratio, with respect to all equivalent mutants, based on the studied benchmarks. We found that approximately 30 and 54 percent of the benchmark mutants are trivially equivalent with respect to C and Java. Here it should be noted that there is a large variation on the detected ratios among the studied programs. This is common for both C and Java subjects, indicating that program characteristics have a strong influence on the TCE equivalences. Another important finding regards the causes of mutant equivalences that are detected by TCE. Our results are surprising since they show that the majority of detected mutants are due to failed propagation, i.e, there is no possible way to propagate the mutant infection to the program output. This is true for both C and Java. In Java almost all, 99 percent, of the detected mutants are of this category, while in C these are 57 percent. In the case of C, 41 percent of the detected mutants fall in the second category, i.e, it is impossible to affect the program state that pertains immediately after mutant execution, and 2 percent to the first one, i.e., mutants reside in unreachable code. 6.1.4 Mutant Operators To better understand the nature of TCE mutants we identified their prevalence according to the considered mutant operators. Our results suggest that in C the ABS and UOI operators introduce more than 15 percent of trivial equivalent mutants, ROR and CRCR more than 5 percent and OAAA just 3 percent while, LCR, OBBN, AOR, OCNG and SSDL introduce a small fraction, less than 1 percent. Regarding Java most of the detected equivalent mutants are due to AOIS, 15 percent, and AODU, 11 percent. Also, LOR and AOIU introduce notable numbers that respectively account for 4 and 2 percent. The rest of the operators introduce none or non-significant numbers. With respect to duplicated mutants, all operators introduce a large number of such mutants in C. Most of them, account for more than 7 percent. Only LCR introduces a smaller fraction that is 3 percent. In the case of Java, the situation is a bit different. Only COI and ROR operators have large proportions of TCE duplicated mutants. These are 56 and 24 percent for COI and ROR. AOIS also produces a large number of duplicated mutants which accounts for 4 percent. The rest of the operators introduce none or small numbers. 6.1.5 Program Size and Mutant Equivalences We measured the correlation between the number of mutants and the size of programs. Our results reveal that in both cases there is medium level correlation which is stronger for C, i.e., Z_ $\rho = 0.671$_Z for C and ρ = 0.417 ρ for Java. Thus, programs of similar size can vary much in terms of number of mutants. By measuring the average number mutants per statement we get 1.90 and 1.69 for C and Java respectively. Hence, for the programs we studied, we conclude that C programs have approximately 10 percent more mutants and a stronger correlation, between mutants number and program lines of code, than the Java ones. With respect to equivalent mutants, our results indicate a strong correlation with the number of mutants, for both C and Java, i.e., ρ = 0.818 ρ and ρ = 0.786 ρ . This is getting weaker when considering program size, i.e., ρ = 0.692 ρ for C and ρ = 0.230 ρ for Java. However, in all cases we found no evidence indicating that the ratio of the detected equivalent mutants correlates with the number of mutants. Together these two results can be regarded as evidence suggesting that the number of the detected equivalent mutants is a fairly consistent proportion, unaffected by the size indicators of the program under analysis. With respect to duplicated mutants, our results suggest a strong correlation with the number of mutants, for both C and Java, i.e., ρ = 0.930 ρ and ρ = 0.657 ρ . However, both in C and Java we found no evidence indicating that the ratio of the duplicated mutants correlates with the number of mutants. Program size has medium to strong correlation with the number of TCE duplicated mutants, i.e., ρ = 0.650 ρ and ρ = 0.797 ρ for C and Java. In case of C we found no evidence indicating that the ratio of the duplicated mutants correlates with program size. In contrast a medium level correlation was found in the case of Java, Z_$\rho = 0.423$ _Z. 6.2 Differences between C and Java Our presentation this far has focused on our results as found by the two versions of TCE, i.e., for C and Java. Here we attempt to compare the results of the C with Java versions, answering RQ6, and highlight commonalities and differences between them. One first observation is that TCE detects more equivalences in C than in Java. This can be attributed to the compiler optimisations implemented in gcc that are way more advanced than that of Java and Soot. We took a close look at the analysis on the detected causes of equivalence and found that almost all TCE equivalent mutants detected in Java programs are those that cannot propagate, while, only the 57 percent of the C ones are due to the same reason. This suggests, that there is a 42 percent difference between the results of C and Java, mainly due to the lack of Java optimisations. The average detected ratios are 7.4 and 5.7 percent, for C and Java, that reflects the mentioned differences. Our results demonstrate that equivalent mutants are more prevalent in C than in Java. This is evident from our ground truth analysis which revealed that in C the equivalent mutants account for 23 percent, while, in Java for 12.7 percent of all mutants. Additionally, Java has a larger number of trivially equivalent mutants. This is also shown by our ground truth analysis, which revealed that 54 percent of all Java equivalent mutants are TCE equivalent. The same ratio for C is 30 percent. In this result, we should consider our first observation, i.e., that 42 percent of the TCE equivalent mutants cannot be detected by Soot due to lack of compiler optimizations, that a potentially high number of Java trivially equivalent mutants exists but not found by Soot. Thus, we can easily conclude that Java programs have considerably less equivalent mutants than the C ones and at the same time Java programs contain a much larger proportion of trivially equivalent mutants. Regarding duplicated mutants, we found TCE duplicated mutants in C are more prevalent than in Java programs. As our results shown that while in C a large proportion, of 21.0 percent on average, exists, in Java these mutants are considerably less and account for 5.4 percent on average. This difference is partly attributed to the lack of optimisations in Java and to language characteristics. Thus, characteristics, like the distinction of logical and arithmetic operators in Java, the typed conventions that are stronger in Java than in C and the use of pointers and arrays make C mutants more vulnerable to duplication. Another interesting point is that after removing the TCE equivalent mutants, a ratio of 5.8 percent of equivalent mutants remain in java, while in C the ratio of equivalent mutants that remain is 16.1 percent. Considering this observation together with the one regarding the number of mutants, that are approximately 10 percent less in Java than in C, we conclude that, based on the programs we studied, mutation analysis in C is harder than in Java. The efficiency differences between C and Java in detecting duplicated mutants is believed to be due to the language differences. Our results suggest that 0.028 sec are required per C mutant under analysis while 0.28 per Java one. C binary code tends to be smaller than Java bytecode. While the differences are not practically significant, these could be ameliorated by using some form of checksum, as done by md5 to improve substantially the performance of the diff comparisons. Considering other parameters, like the tools and operator sets used, could also lead to the differences in C and Java results. While, in C we have 10 operators and in Java 15, this difference is more conventional than actual. It is noted that the CRCR operator corresponds to many Java operators mainly due to the language differences, i.e., in C there are only arithmetic values while in Java logical operations are strictly of boolean types. Only two C operators, the ABS and SDL, are only partially implemented in Java; ABS is partially implemented by AODU and SDL by the various deletion operators like the COD. Three Java operators, SOR, AODS, and AORS, are not implemented in C. Comparing individual operators, C-ABS produces 24 percent of TCE equivalent mutants while Java-AODU 11 percent. Similarly, C-UOI 16 percent while Java-AOIS 15 percent. Interestingly, C-ROR, C-CRCR and C-OAAA account for 6, 5 and 3 percent respectively while their Java version for 0 percent. With respect to duplicated mutants, C-ROR produces 25 percent while the Java-ROR 24 percent. C-OCNG produces 49 percent and the Java-COI 56 percent. C-UOI produces 21 percent and the Java-AOIS 4 percent. All other C operators introduce many duplicated mutants not detected by the related Java ones. A manual inspection of the detected C mutants suggests that most of these mutants are due to a failed infection, i.e., mutant execution cannot result in a corrupted program state. As shown by our results, Java optimizations are ineffective for these cases and hence we get a reduced effectiveness. 6.3 Implications for Research Studies Our results have direct implications for research studies: the application of TCE can improve the accuracy of a study’s results when no manual analysis of equivalent mutants have been performed. To better understand these implications, Fig. 5 illustrates the range in which TCE can change the resulting mutation scores in the case of Java (left part of the figure) and C (right part of the figure), when assuming that our results are generalisable. Both parts present the mutation scores with no manual analysis (line “traditional”) and the improved mutation scores that could be obtained by applying TCE. We report the minimum and maximum number of detected equivalent mutants (lines “TCE_min” and “TCE_max”) to better reflect the impact of TCE. Note that the minimum and maximum values are based on the results of our large-scale experiment (see also Sections 4.1 and 5.1 ). By examining the figures, it can be seen that TCE can improve the accuracy of the obtained mutation scores. More precisely, in the case of Java, this improvement ranges between 0-18 percent and, in the case of C, it ranges between 0-16 percent. Fig. 5. Mutation score improvements by TCE, when no manual analysis of equivalent mutants has been performed, e.g., in large-scale experiments. While these results are only illustrative and have to be treated with a great deal of caution, they provide evidence that research studies will benefit from the application of TCE, by automatically improving the accuracy of the results reported. Consider for instance a study that compares two test generation methods, say methods X and Z which achieve a mutation score (without the analysis of equivalent mutants) of 60 and 67 percent respectively, and the study concludes that Z is better because it manages to achieve a better mutation score of 67 percent with an improvement of 7 percent over the previous method. TCE can be used to improve the accuracy of the study’s results: by applying TCE, the mutation score of X will range between 61 and 73 percent and the one of Z, between 68 and 82 percent. Thus, the application of TCE will result in more accurate mutation scores and will potentially reveal a greater difference, of 9 percent, between X and Z, improving the empirical evidence of Z’s superiority. 6.4 Practical Implications Practitioners use test criteria to develop test suites and to assess the level of test thoroughness. Thus, in practice, TCE affects the effort needed (required work) to develop test suites and the ability of the criterion to accurately measure the effectiveness of the test suites. This section investigates these two practical implications of TCE by examining its impact on the work required, when generating mutation adequate test suites, and by examining the improvements it makes when measuring the mutation score. To reliably investigate both the required work and the improvements of TCE we need to know which mutants are equivalent. We also need to have multiple test suites of various levels of test thoroughness, i.e., with low and high mutation scores. The benchmark set of Yao et al. , which we use to answer RQ3 (for C programs), is unfortunately short on both of the above two requirements. Thus, we used the benchmark of Papadakis et al. [47], which is an extension of the famous Siemens suite [68] and contains manually augmented test suites (mutation adequate) and analysed mutants. This benchmark was constructed using: a) the Proteum11 mutation testing tool to generate mutants, b) manual analysis to characterise these mutants as killable or equivalent and c) manual analysis to augment the test suites (generate tests that kill the identified killable mutants) [47]. In the case of Java, we used the mutation adequate test suites, which we generated when analysing the mutants of the ground truth set (used to answer RQ3). A summary of the mutants produced by Proteum, when applied to the Siemens suite and the results of TCE (using the -O option) are given in Table 16. From these data, it becomes evident that a non-trivial number of mutants has been detected by TCE. The numbers of the TCE equivalent mutants account for 30-48 percent (41 percent on average) of all the existing equivalent mutants. Interestingly, the results are very similar to those reported in our previous analysis (approximately 6.9 and 24 percent of all the Proteum mutants are TCE equivalent and duplicated) and thus, we are confident that they are representative. TABLE 16 TCE Applied to Siemens Suite and Proteum Mutants (Benchmark Set by Papadakis et al. [47]): Number ‘No.’ and Proportion ‘%’ of Detected Equivalent Mutants  6.4.1 Practical Implications: Required Work To measure the manual effort involved when performing mutation testing, we adopt the model used by the recent study of Kurtz et al. [57]. Thus, we define work as “the number of mutants that are examined by the engineer”, or equally, “the sum of the number of tests written to kill all non-equivalent mutants and the number of equivalent mutants identified” [57]. This metric in essence approximates the manual effort that a tester needs to perform when doing mutation testing. Equation (1) presents the work model. In order to compare the results across different programs, we normalise the recorded work by dividing with the overall required work, per subject. The corresponding formula is presented in Equation (2). w o r k = | t e s t C a s e s | + | E q u i v a l e n t M u t a n t s | work=|testCases|+|EquivalentMutants| n o r m a l i s e d _ w o r k = | t e s t C a s e s | + | E q u i v a l e n t M u t s | O v e r a l l W o r k R e q u i r e d normalised_work=|testCases|+|EquivalentMuts|Overa Algorithm 1 presents the procedure followed to calculate work, as suggested by Kurtz et al. [57]. First, a mutant is randomly selected from the generated set of mutants of the program under test. Next, if the mutant is equivalent, the work is increased by one and the process is repeated. If the mutant is killable, a test case that kills this mutant is randomly selected, the value of work is increased by one and the other mutants that can be killed by this test case are marked as killed. This process continues until every killable mutant of the considered mutants is selected/killed. Algorithm 1.   Calculating the Work Metric  Let muts represent the program’s generated mutants  Let tcs represent the program’s mutation test suite function WorkCalculation ( m u t s m , t c s t )  w o r k ← 0 w   while ∃  k i l l a b l e _ m u t a n t ∈ m u t s ∃ do  m u t ← m selectRandomAliveMutant ( m u t s m )  if isEquivalent( m u t m ) then  w o r k ← w o r k + 1 w   continue  end if  k i l l i n g _ t c ← k selectRandKillingTC( m u t , t c s m )  w o r k ← w o r k + 1 w   updateKilledMutants( k i l l i n g _ t c k , m u t s m )  end while  return w o r k w  end function As can be seen from the algorithm, it requires two inputs: a mutant set and a set of mutation adequate test cases. Thus, we calculate the work based on manually analysed test subjects. To avoid any bias from the selection process, we repeated the experiment 100 times. Figs. 6 and 7 illustrate the results obtained for both programming languages. These figures plot the normalised work (x -axis) against the subsuming mutation score [56] ( M S ∗ M , y -axis) realised at each step of Algorithm 1 with and without the application of TCE (denoted by the “TCE” and “Traditional” lines respectively) per test subject and programming language. Following the process of Kurtz et al. [57], we used the subsuming mutation score as effectiveness measurement. This measurement avoids the inflation effects caused by redundant mutants [56], [57]. Fig. 6. Work required for different effectiveness levels with and without the application of TCE in the case of Java. Fig. 7. Work required for different effectiveness levels with and without the application of TCE in the case of C. By examining Fig. 6, it can be seen that TCE manages to substantially reduce the work required to achieve a given test effectiveness level: for instance, in the case of Joda-Time, by applying TCE, the work required to achieve a 70 percent subsuming mutation score is reduced by 11 percent compared to the application of mutation without TCE, this reduction increases to approximately 20 percent when the subsuming mutation score reaches 80 percent and to 30 percent when the score reaches 90 percent; finally, at the 100 percent effectiveness level, TCE realises a 49 percent work reduction. This trend, i.e., the increase of the work reduction as the subsuming mutation score increases is present in most Java subjects.12 This fact can be justified by TCE’s equivalent mutant detection which, in turn, gives practitioners a higher chance of selecting killable mutants than equivalent ones, as the application of mutation progresses. Regarding the results for C depicted in Fig. 7, it can be seen that analogous conclusions can be drawn. To better portray TCE’s implications for work, Fig. 8 presents the overall work reduction when developing mutation adequate test suites per test subject and programming language. It can be seen that the application of TCE realises a work reduction between 0 and 51 percent, with an average of 37 percent, in the case of Java and a work reduction that ranges between 28 and 47 percent, with an average of 37 percent, in the case of C. These results suggest that the work of an engineer aiming at creating mutation adequate test suites can be substantially reduced by the application of TCE. Fig. 8. Overall work reduction after the application of TCE per test subject and programming language. 6.4.2 Practical Implications: Mutation Score Improvement This section investigates how the use of TCE improves the accuracy of the mutation score measurement. Consider the following example: an engineer applies mutation to a test subject based on the available test cases and obtains a value x x of mutation score; the question that is raised here is how much does the x x score differ from the true mutation score, i.e., the score computed by removing the equivalent mutants? We calculate the error of the measurement by comparing the true mutation score with the obtained one (with and without applying TCE). Equation (3) details the error of the computation. This metric quantifies the distance of our metric from the true one. Our results are depicted in Figs. 9 and 10 for Java and C, respectively. The y -axis of the figures refers to the aforementioned error and the x -axis to the effectiveness levels denoted by the subsuming mutation score. E r r o r = T r u e _ M S − O b t a i n e d _ M S Error=True_MS−Obtained_MS Fig. 9. Mutation score improvement after the application of TCE in the case of Java. Fig. 10. Mutation score improvement after the application of TCE in the case of C. By examining Figs. 9 and 10, it can be seen that the application of TCE results in a much lower error than calculating the mutation score without its application in most test subjects. For instance, in the case of the wrap method of the Commons test subject, at the 75 percent subsuming mutation score, the error in the mutation score’s calculation is 9 percent without the application of TCE; this error is reduced to 4 percent when TCE is applied; this difference remain approximately the same until the 100 percent subsuming mutation score is reached. Overall, in the case of Java, TCE reduces the calculation error of the mutation score by 1-10 percent. In the case of C, we find analogous results, with the calculation error reduction ranging between 0 and 4 percent. 6.5 Application Constraints The proposed technique is solely based on the use of compilers and their optimisation options, thereby avoiding the several limitations of other methods and tools, e.g., applicability and scalability. It does not require any sophisticated source code analysis techniques or any expensive test executions. Thus, it can be directly applied to real-world systems and can be easily incorporated within mutation testing tools. Interestingly, the detected mutant equivalences are partly dependent on the compiler options used. Although it is rather unlikely that equivalent mutants detected by one compiler option are not equivalent according to another, to be absolutely sure, beyond any doubt that TCE guarantees equivalence, we need to know which compiler settings are going to be used in the deployment environment. No previous research takes into account the particular compiler settings, but since we are using TCE, this cannot be ignored. All previous work implicitly assumes that there is only one compiler option, but actually there are as many options as the actual settings used by the deployed programs. When the deployed-code compiler settings are known, TCE can exploit this information. When they are unknown at mutation test time, we can investigate with a reasonable sample, checking for variance in equivalence behaviour. We investigated this issue by exploring the main gcc and Soot settings covering a wide range of optimisation options and found that all of them can be used to detect mutant equivalences (some are more effective than others of course). We also explored the trade off between effectiveness and efficiency using different settings. Our results suggest that the -O and -O2 options are reasonably good, because they consume less compilation time than the -O3 option. However, none of them is superior to the others in detecting equivalent mutants. Here it should be noted that there are many more optimisation options in the modern compilers, there might exist some combinations of them that can detect faster or more mutant equivalences. Thus, our future research is directed towards identifying the options that fit best to TCE. Detailed information about the performed optimizations can be found in the gcc13 and Soot14 websites. 7   Threats to Validity As it is usual in software engineering experiments, our subjects might not be representative. It is also possible that they might not hold for complete system analysis (as we only analysed sampled components of the large programs). To ameliorate this issue, we selected 12 real-world programs of varying size and application domain, 6 written in C and 6 written in Java, several orders of magnitude larger than those used in previous equivalent mutant detection studies. We also performed an additional evaluation using different sets of programs, composed of 31 manually-analysed benchmark subjects, taken from the literature. To further cater for this issue, we draw attention to strongly observed effects and present our results as ranges of expected values. The evaluation of our approach resulted in analogous findings in all studied sets. With reference to the C test subjects, it detected approximately 7.4 percent of all the mutants as equivalent ones for the large-scale experiment, and 7.2 and 6.9 percent of the mutants of the manually-analysed test subjects (for the Yao et al. [16] and Papadakis et al. benchmarks [47]), on average. In the case of Java, it identified 5.7 and 6.8 percent, accordingly. Regarding the range of the results (range between worst and best cases), a similar picture appears. Thus, we are confident that TCE can eliminate a considerable number of equivalences. Additionally, our results are in line with those reported in the literature15 providing confidence that they are realistic. We studied the mutants of the C and Java languages and TCE implemented using gcc and Soot. Therefore, some of our results might be a realisation of independent uncontrolled variables, such as the sample size, sample selection procedure (excluding classes not handled by muJava), programs’ internal characteristics, used software platforms and tools’ operation. Therefore, it is important to note that all our results form empirical observations that might not hold in the general case. However, our findings fit intuition and rely on the foundations set by previous studies [31]. Furthermore, we control the major factors that we believe can influence our results. Additional studies are needed to determine what influences the performance of TCE and its practical use on different languages and compiler optimisation techniques. Other threats are due to the use of software systems. For instance, the gcc compiler or the Soot optimisation framework may have defects. However, these systems are heavily tested and deployed. Thus, it is unlikely that the remaining defects would influence our results to a great extent. Implementation defects of MiLu, muJava and Proteum may also have an influence. To reduce this threat we carefully checked their results. However, we consider this as a minor threat since all the used tools have been used by several authors in recent studies, e.g., [6], [57], [70], [71], [72], independently of us. Furthermore, we utilised three equivalent mutant benchmark sets which were entirely built by hand. These results served as a ‘sanity check’ to reduce the threat to validity. Our results might be affected by our choice of mutant operators. As shown by other studies [73], [74] it is also possible that the realisation of the mutant operators by the employed tools may particularly affect the comparison between C and Java. To mitigate this threat, we detailed exactly how the operators supported by the used tools are realised ( Tables 5 and 6) and analysed the common C and Java operators. Based on these lines we draw some conclusions. Overall, we used a wide range of 75 mutant operators (realised by Proteum) and all the popular operators (included in most existing mutation testing tools and those empirically found to correlate with fault detection). In all cases we found large numbers of equivalences, which have a major impact on the application of mutation testing. The use of the equivalent mutants’ benchmarks may also pose another threat. This is due to the performed manual analysis: some killable mutants may have been mistakenly identified as equivalent. However, these studies were performed independently of the present one and hence, it is not likely that this kind of mistakes coincidentally match the results of TCE. Additionally, it is equally possible that such mistakes have also led to the underestimation of TCE’s effectiveness. Finally, all our subjects, tools and data are available in the accompanied website of the present paper. 16 This helps reducing all the above-mentioned threats [75] since independent researchers can check, replicate and analyse our findings. 8   Conclusion and Future Work We have presented the results of an extensive empirical analysis of the ability of Trivial Compiler Equivalence (TCE) to detect both equivalent and duplicated mutants in the C and Java programming languages. We have conducted an empirical study of TCE on 25 C and 6 Java benchmark systems, for which the programs under study are sufficiently small for their equivalent mutants to be determined manually. These systems provided us with the ground truth against which we can empirically assess the equivalent mutant detection power of TCE. We augmented this study with a much larger study for which no ground truth is possible. In total, we have experimented with over 1 million lines of code, consisting of the 31 smaller benchmark systems, together with 6 larger Java systems (with a total of 263,740 LoC) and 6 larger C systems (with a total of 750,157 LoC). Overall, we find that for both C and Java, TCE is a useful, fast and widely-applicable technique that can detect between 17-100 percent (30 percent on average) of C language equivalent mutants, and 0-94 percent (54 percent on average) of Java equivalent mutants (for the ground truth set). Furthermore, over all mutants studied in all large real-world programs, the detection of trivially equivalent and trivially duplicated mutants was found to reduce the total number of mutants by 5-23 percent for Java and 20-37 percent for C, which accounts for 11 and 28 percent on average. These achievements imply that a practitioner who applies mutation testing and is using TCE will spend 0-51 and 28-47 percent less manual effort in the case of Java and for C than without using it. TCE also improves the accuracy of the mutation score measurement by 1-10 and 0-4 percent for Java and C. Thus, future research should integrate compiler optimisations within mutation testing tools in order to avoid any generation of such trivial mutants and future research studies should consider applying TCE to reap the benefit of the technique. Our results revealed interesting findings that suggest topics for future work on mutation-based analysis of the semantic differences between programming languages. For example, it is intriguing that a larger proportion of Java’s equivalent mutants were found to be detectable using TCE than for C. Furthermore, if the proportion of equivalent mutants from the ground truth study is similar to that for mutants overall, then it would appear that the Java language suffers significantly less from the equivalent mutant problem than the C language does. One might conjecture that this is related to the relatively small size of Java methods when compared to the size of C functions. Alternative conjectures might revolve around the differing semantic features of these two languages (and the consequent mutation operators that are applicable). Of course, since we have insufficient data to make scientifically reliable statements on these conjectures, we have refrained from making any claims in the present paper and leave them as just that; conjectures. Nevertheless, our results suggest that future work might use TCE as one approach to tackle such conjectures, potentially leading to a better understanding of the difference between programming language semantics, based on mutation analysis. Footnotes 1. http://pages.cs.aueb.gr/~kintism/papers/tce/ and http://www0.cs.ucl.ac.uk/staff/Y.Jia/projects/compiler_equivalence/. 2. http://www.oracle.com/technetwork/java/index.html. 3. http://sable.github.io/soot/. 4. https://gcc.gnu.org/. 5. http://sable.github.io/soot/. 6. www0.cs.ucl.ac.uk/staff/Y.Jia/projects/equivalent_mutants/. 7. http://pages.cs.aueb.gr/~kintism/papers/tce/. 8. https://github.com/yuejia/Milu/tree/develop/src/mutators. 9. https://cs.gmu.edu/~offutt/mujava/. 10. www0.cs.ucl.ac.uk/staff/Y.Jia/projects/equivalent_mutants/. 11. We used the version 2.0 of the Proteum/IM tool [27]. 12. XStream is a clear outlier but it should be mentioned that it is the only program for which TCE did not detect any equivalent mutant. 13. https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html. 14. https://ssebuild.cased.de/nightly/soot/doc/soot_options.htm. 15. Offutt and Pan [33] reported that 9 percent of all the mutants are equivalent. Delamaro et al. [69] found 12 percent, Kintis et al. [46], Schuler and Zeller [40] 7-8 percent, Papadakis et al. [47] 17 percent, Yao et al. [16] 23 percent and Madeyski et al. [44] 4-39 percent. 16. http://pages.cs.aueb.gr/~kintism/papers/tce/. Acknowledgments Marinos Kintis is partly supported by the Research Centre of Athens University of Economics and Business (RC/AUEB). Mike Papadakis is supported by the National Research Fund, Luxembourg, INTER/MOBILITY/14/7562175 and by Microsoft Azure Grant 2015. Mark Harman is partly supported by the UK EPSRC projects EP/I033688 (GISMO) and EP/G060525, a ‘Platform Grant’ for the Centre for Research on Evolution Search and Testing (CREST) at UCL. Yue Jia is supported by the EPSRC project EP/J017515 (DAASE) and by Microsoft Azure Grant 2014. References  [1]T. A. Budd, R. A. DeMillo, R. J. Lipton, and F. G. Sayward, “The design of a prototype mutation system for program testing,” in Proc. AFIPS Natl. Comput. Conf., Jun. 5–8, 1978, pp. 623–627. [2]Y. Jia and M. Harman, “ An analysis and survey of the development of mutation testing,” IEEE Trans. Softw. Eng., vol. 37, no. 5, pp. 649–678, Sep./Oct.2011. [3]R. Just, D. Jalali, L. Inozemtseva, M. D. Ernst, R. Holmes , and G. Fraser, “Are mutants a valid substitute for real faults in software testing? ” in Proc. 22nd ACM SIGSOFT Int. Symp. Found. Softw. Eng., 2014 , pp. 654–665. [4]J. H. Andrews, L. C. Briand, and Y. Labiche, “Is mutation an appropriate tool for testing experiments?” in Proc. 27th Int. Conf. Softw. Eng., 2005, pp. 402–411. [5]M. Daran and P. Thévenod-Fosse, “Software error analysis: A real case study involving real faults and mutations ,” in Proc. ACM SIGSOFT Int. Symp. Softw. Testing Anal., 1996 , pp. 158–171. [6]R. Baker and I. Habli, “An empirical evaluation of mutation testing for improving the test quality of safety-critical software,” IEEE Trans. Softw. Eng., vol. 39 , no. 6, pp. 787–805, Jun.2013. [7]B. H. Smith and L. Williams, “On guiding the augmentation of an automated test suite via mutation analysis ,” Emp. Soft. Eng., vol. 14, no. 3, pp. 341 –369, 2009. [8]A. J. Offutt, J. Pan, K. Tewary, and T. Zhang, “An experimental evaluation of data flow and mutation testing,” Softw. Pract. Exper., vol. 26, no. 2, pp. 165–176, 1996. [9]P. G. Frankl, S. N. Weiss, and C. Hu, “All-uses versus mutation testing: An experimental comparison of effectiveness,” J. Syst. Softw. , vol. 38, no. 3, pp. 235–253, Sep.1997. [10]R. A. DeMillo and A. J. Offutt , “Constraint-based automatic test data generation,” IEEE Trans. Softw. Eng., vol. 17, no. 9, pp. 900–910, Sep.1991. [11]M. Harman, Y. Jia, and W. B. Langdon, “Strong higher order mutation-based test data generation,” in Proc. 8th Eur. Softw. Eng. Conf. ACM SIGSOFT Symp. Found. Softw. Eng., Sep. 5–9, 2011, pp. 212 –222. [Online]. Available: http://doi.acm.org/10.1145/2025113.2025144 [12]R. P. Mateo, P. U. Macario, F. Alemán, and J. Luis, “Validating second-order mutation at system level,” IEEE Trans. Softw. Eng., vol. 39, no. 4, pp. 570–587, Apr. 2013. [13]Y. Jia and M. Harman, “ Constructing subtle faults using higher order mutation testing,” in Proc. 8th IEEE Int. Working Conf. Source Code Anal. Manipulation, 2008 , pp. 249–258. [14]K. Androutsopoulos, D. Clark, H. Dan, M. Harman, and R. Hierons, “An analysis of the relationship between conditional entropy and failed error propagation in software testing,” in Proc. 36th Int. Conf. Softw. Eng., Jun. 2014, pp. 573–583. [15]T. A. Budd and D. Angluin, “Two notions of correctness and their relation to testing,” Acta Informatica, vol. 18, no. 1, pp. 31–45, 1982. [16]X. Yao , M. Harman, and Y. Jia, “A study of equivalent and stubborn mutation operators using human analysis of equivalence,” in Proc. 36th Int. Conf. Softw. Eng., 2014, pp. 919–930. [17]L. Zhang, S.-S. Hou, J.-J. Hu, T. Xie, and H. Mei, “Is operator-based mutant selection superior to random mutant selection?” in Proc. 32nd ACM/IEEE Int. Conf. Softw. Eng., 2010, pp. 435–444. [18]W. E. Wong and A. P. Mathur , “Reducing the cost of mutation testing: An empirical study,” J. Syst. Softw., vol. 31, no. 3, pp. 185–196 , Dec.1995. [19]M. Papadakis and N. Malevris, “An empirical evaluation of the first and second order mutation testing strategies ,” in Proc. ICST Workshops, 2010, pp. 90–99 . [20]M. Polo, M. Piattini, and I. Garcia-Rodriguez, “ Decreasing the cost of mutation testing with second-order mutants,” Softw. Test. Verif. Rel., vol. 19, no. 2, pp. 111–131, Jun.2008. [21]M. H. Bill Langdon and Y. Jia, “Efficient multi objective higher order mutation testing with genetic programming ,” J. Syst. Softw., vol. 83, no. 12, pp. 2416 –2430, Jul.2010. [22]M. Harman, Y. Jia, P. R. Mateo, and M. Polo, “Angels and monsters: An empirical investigation of potential test effectiveness and efficiency improvement from strongly subsuming higher order mutation ,” in Proc. ACM/IEEE Int. Conf. Automated Softw. Eng., 2014, pp. 397–408. [23]M. Papadakis and N. Malevris, “Automatic mutation test case generation via dynamic symbolic execution,” in Proc. IEEE 21st Int. Symp. Softw. Rel. Eng., 2010, pp. 121–130. [24]L. Zhang, D. Marinov, and S. Khurshid, “Faster mutation testing inspired by test prioritization and reduction,” in Proc. Int. Symp. Softw. Testing Anal., 2013, pp. 235–245. [25]P. R. Mateo and M. P. Usaola , “Reducing mutation costs through uncovered mutants,” Softw. Test., Verif. Reliab., vol. 25, no. 5–7, pp. 464 –489, 2015. [Online]. Available: http://dx.doi.org/10.1002/stvr.1534 [26]M. Papadakis, Y. Jia, M. Harman, and Y. L. Traon, “Trivial compiler equivalence: A large scale empirical study of a simple, fast and effective equivalent mutant detection technique,” in Proc. 37th IEEE/ACM Int. Conf. Softw. Eng., 2015, pp. 936–946. [Online]. Available: http://dx.doi.org/10.1109/ICSE.2015.103 [27]M. E. Delamaro, J. C. Maldonado, and A. M. R. Vincenzi, Proteum/IM 2.0: An Integrated Mutation Testing Environment. Boston, MA, USA: Springer US, 2001, pp. 91–101. [Online]. Available: http://dx.doi.org/10.1007/978–1-4757-5939-6_17 [28]Y.-S. Ma, A. J. Offutt, and Y.-R. Kwon, “MuJava: An automated class mutation system,” Softw. Test. Verif. Rel., vol. 15 , no. 2, pp. 97–133, Jun.2005. [29]D. Baldwin and F. G. Sayward , “Heuristics for determining equivalence of program mutations,” Yale University, New Haven, Connecticut, Res. Rep. 276, 1979. [30]A. T. Acree, “On mutation,” PhD Thesis, Georgia Institute of Technology , Atlanta, GA, USA, 1980. [31]A. J. Offutt and W. M. Craft, “Using compiler optimization techniques to detect equivalent mutants,” Softw. Test. Verif. Rel., vol. 4, no. 3, pp. 131– 154, 1994. [32]A. J. Offutt and J. Pan, “Detecting equivalent mutants and the feasible path problem,” in Proc. 11th Annu. Conf. Comput. Assurance Syst. Integrity Softw. Safety, 1996, pp. 224–236. [33]A. J. Offutt and J. Pan, “Automatically detecting equivalent mutants and infeasible paths,” Softw. Test. Verif. Rel., vol. 7, no. 3, pp. 165– 192, Sep.1997. [34]J. Voas and G. McGraw, Software Fault Injection: Inoculating Programs Against Errors. Hoboken, NJ, USA : Wiley, 1997. [35]R. M. Hierons, M. Harman, and S. Danicic, “Using program slicing to assist in the detection of equivalent mutants,” Softw. Test. Verif. Rel., vol. 9, no. 4, pp. 233–262, 1999. [36]M. Harman, R. Hierons, and S. Danicic, “The relationship between program dependence and mutation analysis,” in Mutation Testing for the New Century. Dordrecht, The Netherlands: Kluwer Academic Publishers, 2001, pp. 5–13. [37]K. Adamopoulos, M. Harman, and R. M. Hierons, “How to overcome the equivalent mutant problem and achieve tailored selective mutation using co-evolution ,” in Proc. Genetic Evol. Comput. Conf., 2004, pp. 1338 –1349. [38]B. J. M. Grün, D. Schuler, and A. Zeller, “The impact of equivalent mutants,” in Proc. ICST Workshops, 2009, pp. 192–199. [39]D. Schuler, V. Dallmeier, and A. Zeller, “Efficient mutation testing by checking invariant violations,” in Proc. 18th Int. Symp. Softw. Testing Anal., 2009, pp. 69–80. [40]D. Schuler and A. Zeller, “Covering and uncovering equivalent mutants,” Softw. Test. Verif. Rel., vol. 23, no. 5, pp. 353–374, 2013. [41]D. Schuler and A. Zeller, “(Un-)Covering equivalent mutants,” in Proc. IEEE Int. Conf. Softw. Test., 2010, pp. 45–54. [42]S. Nica and F. Wotawa, “Using constraints for equivalent mutant detection,” in Proc. 2nd Workshop Formal Methods Development Softw., 2012, pp. 1–8 . [43]M. Kintis and N. Malevris, “Identifying more equivalent mutants via code similarity,” in Proc. 20th Asia-Pacific Softw. Eng. Conf., Dec. 2013, pp. 180– 188. [44]L. Madeyski, W. Orzeszyna, R. Torkar, and M. Jozala, “Overcoming the equivalent mutant problem: A systematic literature review and a comparative experiment of second order mutation,” IEEE Trans. Softw. Eng., vol. 40, no. 1, pp. 23–42, Jan.2014. [45]M. Kintis, M. Papadakis, and N. Malevris, “Isolating first order equivalent mutants via second order mutation,” in Proc. IEEE 5th Int. Conf. Softw. Testing, Verification Validation, 2012, pp. 701 –710. [46]M. Kintis, M. Papadakis, and N. Malevris, “Employing second-order mutation for isolating first-order equivalent mutants,” Softw. Test. Verif. Rel., vol. 25, no. 5-7, pp. 508–535, 2014. [47]M. Papadakis, M. Delamaro, and Y. L. Traon, “ Mitigating the effects of equivalent mutants with mutant classification strategies,” Sci. Comput. Program., vol. 95, pp. 298–319, 2014 . [48]M. Kintis and N. Malevris, “Using data flow patterns for equivalent mutant detection,” in Proc. ICST Workshops, 2014, pp. 196–205. [49]M. Kintis and N. Malevris, “Medic: A static analysis framework for equivalent mutant identification,” Inf. Softw. Technol., vol. 68, pp. 1–17, 2015. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0950584915001329 [50]S. Bardin, et al., “Sound and quasi-complete detection of infeasible test requirements,” in Proc. 8th IEEE Int. Conf. Softw. Testing Verification Validation, 2015, pp. 1–10. [Online]. Available: http://dx.doi.org/10.1109/ICST.2015.7102607 [51]A. J. Offutt, A. Lee, G. Rothermel, R. H. Untch, and C. Zapf, “An experimental determination of sufficient mutant operators ,” ACM Trans. Softw. Eng. Meth., vol. 5, no. 2, pp. 99–118, Apr.1996. [52]A. S. Namin, J. H. Andrews, and D. J. Murdoch, “ Sufficient mutation operators for measuring test effectiveness,” in Proc. 30th Int. Conf. Softw. Eng., 2008, pp. 351–360. [53]R. H. Untch, A. J. Offutt, and M. J. Harrold, “ Mutation analysis using mutant schemata,” in Proc. ACM SIGSOFT Int. Symp. Softw. Test. Anal., 1993, pp. 139–148. [54]M. Kintis, M. Papadakis, and N. Malevris, “ Evaluating mutation testing alternatives: A collateral experiment,” in Proc. Asia Pacific Softw. Eng. Conf., 2010, pp. 300–309. [55]P. Ammann, M. E. Delamaro, and J. Offutt, “ Establishing theoretical minimal sets of mutants,” in Proc. IEEE Int. Conf. Softw. Testing Verification Validation, 2014, pp. 21 –30. [56]M. Papadakis, C. Henard, M. Harman, Y. Jia, and Y. Le Traon, “Threats to the validity of mutation-based test assessment ,” in Proc. 25th Int. Symp. Softw. Test. Anal., 2016, pp. 354–365. [57]B. Kurtz, P. Ammann, J. Offutt, M. E. Delamaro, M. Kurtz, and N. Gökçe, “Analyzing the validity of selective mutation with dominator mutants ,” in Proc. 24th ACM SIGSOFT Int. Symp. Found. Softw. Eng., 2016 , pp. 571–582. [58]G. Kaminski, P. Ammann, and J. Offutt, “Better predicate testing,” in Proc. 6th Int. Workshop Autom. Softw. Test, 2011, pp. 57–63. [59]G. Kaminski, P. Ammann, and J. Offutt, “Improving logic-based testing,” J. Syst. Softw., vol. 86, no. 8 , pp. 2002–2012, 2013. [60]R. Just, G. M. Kapfhammer, and F. Schweiggert, “ Using non-redundant mutation operators and test suite prioritization to achieve efficient and scalable mutation analysis,” in Proc. IEEE 23rd Int. Symp. Softw. Rel. Eng. , 2012, pp. 11–20. [61]K.-C. Tai, “Theory of fault-based predicate testing for computer programs ,” IEEE Trans. Softw. Eng., vol. 22, no. 8, pp. 552–562, Aug.1996. [62]W. B. Langdon, M. Harman, and Y. Jia, “Efficient multi-objective higher order mutation testing with genetic programming,” J. Syst. Softw. , vol. 83, pp. 2416–2430, 2010. [63]M. Harman, Y. Jia, P. Reales Mateo, and M. Polo, “Angels and monsters: An empirical investigation of potential test effectiveness and efficiency improvement from strongly subsuming higher order mutation,” in Proc. 29th ACM/IEEE Int. Conf. Automated Softw. Eng., 2014, pp. 397–408. [64]J. H. Andrews, L. C. Briand, Y. Labiche, and A. S. Namin, “Using mutation analysis for assessing and comparing testing coverage criteria,” IEEE Trans. Softw. Eng., vol. 32, no. 8, pp. 608–624, Aug.2006. [65]Y. Ma , M. J. Harrold, and Y. R. Kwon, “Evaluation of mutation testing for object-oriented programs,” in Proc. 28th Int. Conf. Softw. Eng, 2006, pp. 869–872. [Online]. Available: http://doi.acm.org/10.1145/1134437 [66]Y. Jia and M. Harman, “ MILU: A customizable, runtime-optimized higher order mutation testing tool for the full C language ,” in Proc. Testing: Academic Ind. Conf Practice Res. Techn., 2008 , pp. 94–98. [67]P. Lam , E. Bodden, O. Lhotak, and L. Hendren , “The soot framework for Java program analysis: A retrospective,” in Proc. Cetus Users Compiler Infastructure Workshop, 2011, pp. 35–42. [68]M. Hutchins, H. Foster, T. Goradia, and T. J. Ostrand, “Experiments of the effectiveness of dataflow- and controlflow-based test adequacy criteria,” in Proc. 16th Int. Conf. Softw. Eng., 1994, pp. 191–200. [69]M. E. Delamaro, L. Deng, V. H. S. Durelli, N. Li, and J. Offutt, “Experimental evaluation of SDL and one-op mutation for C ,” in Proc. IEEE Int. Conf. Softw. Testing Verification Validation , 2014, pp. 203–212. [70]L. S. Ghandehari, J. Czerwonka, Y. Lei, S. Shafiee, R. Kacker, and D. R. Kuhn, “ An empirical comparison of combinatorial and random testing,” in Proc. ICST Workshops, 2014, pp. 68–77. [71]M. Patrick, R. Alexander, M. Oriol, and J. Clark, “Probability-based semantic interpretation of mutants,” in Proc. IEEE 7th Int. Conf. Softw. Testing Verification Validation Workshops, Mar.2014, pp. 186– 195. [72]L. Deng, J. Offutt, and N. Li, “Empirical evaluation of the statement deletion mutation operator,” in Proc. IEEE 6th Int. Conf. Softw. Testing Verification Validation, Mar. 2013, pp. 84 –93. [73]L. Madeyski and N. Radyk, “Judy—a mutation testing tool for Java,” IET Softw., vol. 4, no. 1, pp. 32–42, 2010. [Online]. Available: http://dx.doi.org/10.1049/iet-sen.2008.0038 [74]M. Kintis, M. Papadakis, A. Papadopoulos, E. Valvis, and N. Malevris, “Analysing and comparing the effectiveness of mutation testing tools: A manual study,” in Proc. 16th IEEE Int. Working Conf. Source Code Anal. Manipulation, 2016, pp. 147–156. [75]C. Wohlin, P. Runeson, M. Höst, M. C. Ohlsson, and B. Regnell, Experimentation in Software Engineering. Berlin, Germany : Springer, 2012. Marinos Kintis received the PhD degree from the Department of Informatics of the Athens University of Economics and Business, in 2016. He is a research associate with the Interdisciplinary Centre for Security, Reliability and Trust at the University of Luxembourg. The main topic of his dissertation was the introduction of effective techniques to ameliorate the adverse effects of the Equivalent Mutant Problem when testing software with Mutation. His main research interests include: software analysis and testing, model-based testing and security testing. He was awarded a Best Paper Award at the 16th International Working Conference on Source Code Analysis and Manipulation (SCAM 2016). He is a member of the IEEE. Mike Papadakis received the PhD diploma in computer science from the Athens University of Economics and Business. He is a research scientist with the Interdisciplinary Centre for Security, Reliability and Trust (SnT), the University of Luxembourg. His research interests include software testing, static analysis, prediction modelling, mutation analysis and search-based software engineering. He is a member of the IEEE. Yue Jia is a lecturer in the Department of Computer Science, University College London. His research interests cover mutation testing, app store analysis and search-based software engineering. He is director of the MaJiCKe, an automated test data generation start up and also co-founder of Appredict, an app store analytics company, spun out from UCL’s UCLappA group. He is a member of the IEEE. Nicos Malevris received the BSc degree in mathematics from the University of Athens, Greece and the MSc degree in operational research from the University of Southampton, United Kingdom. He received the PhD degree from the University of Liverpool, United Kingdom, where he also served as a member of staff. He is a professor in the Department of Informatics, Athens University of Economics and Business (AUEB). He has been with the Department of Informatics at AUEB, since 1991. His research interests include software quality assurance and in particular software testing and software reliability. He has gained experience in that area for more than 25 years, having been involved in research projects and having published a significant number of papers in journals and conferences. He serves at the Editorial Board of high quality International journals and has also been on the program committees of international conferences. Yves Le Traon is professor with University of Luxembourg where he leads the SERVAL (SEcurity, Reasoning and VALidation) research team. His research interests within the group include (1) innovative testing and debugging techniques, (2) Android apps security and reliability using static code analysis, machine learning techniques and, (3) model-driven engineering with a focus on IoT and CPS. His reputation in the domain of software testing is acknowledged by the community. He has been general chair of major conferences in the domain, such as the 2013 IEEE International Conference on Software Testing, Verification and Validation (ICST), and Program Chair of the 2016 IEEE International Conference on Software Quality, Reliability and Security (QRS). He serves at the editorial boards of several, internationally-known journals (STVR, SoSym, IEEE Transactions on Reliability) and is author of more than 140 publications in international peer-reviewed conferences and journals. He is a member of the IEEE. Mark Harman is currently an engineering manager with Facebook and a professor of software engineering in the Department of Computer Science at University College London, where he directed the CREST centre for ten years (2006-2017) and was head of the Software Systems Engineering (2012-2017). He is widely known for work on source code analysis, software testing, app store analysis and Search Based Software Engineering (SBSE), a field he co-founded and which has grown rapidly to include more than 1,600 authors spread over more than 40 countries. His SBSE and testing work has been used by many organisations including Daimler, Ericsson, Google, Huawei, Microsoft and Visa. He is a co-founder (and was co-director) of Appredict, an app store analytics company, spun out from UCL’s UCLappA group, and was the chief scientific advisor to Majicke, an automated test data generation start up. In February 2017, he and the other two co-founders of Majicke (Yue Jia and Ke Mao) moved to Facebook, London, in order to develop their research and technology as part of Facebook. He is a member of the IEEE.Keywords Java, Program Compilers, Program Testing, Mutation Testing, Test Case Generation, Java Programming Language, TCE Equivalent Mutants, Trivial Mutant Equivalences Detection, Fault Based Testing, TCE Duplicated Mutants, Java Mutants, Compiler Equivalence, Java, Testing, Optimization, Syntactics, Program Processors, Electronic Mail, Mutation Testing, Equivalent Mutants, Duplicated Mutants, Compiler Optimisation"
Understanding Diverse Usage Patterns from Large-Scale Appstore-Service Profiles,"Abstract The prevalence of smart mobile devices has promoted the popularity of mobile applications (a.k.a. apps). Supporting mobility has become a promising trend in software engineering research. This article presents an empirical study of behavioral service profiles collected from millions of users whose devices are deployed with Wandoujia, a leading Android app-store service in China. The dataset of Wandoujia service profiles consists of two kinds of user behavioral data from using 0.28 million free Android apps, including (1) app management activities (i.e., downloading, updating, and uninstalling apps) from over 17 million unique users and (2) app network usage from over 6 million unique users. We explore multiple aspects of such behavioral data and present patterns of app usage. Based on the findings as well as derived knowledge, we also suggest some new open opportunities and challenges that can be explored by the research community, including app development, deployment, delivery, revenue, etc. 1   Introduction The release of iPhone in 2007 has opened a new era of mobile computing. Currently, smart devices such as iPhones, iPads, and Android devices have played an indispensable role in our daily lives. The increasing popularity of mobile devices and apps has induced an evolution of software industry. One of the currently inspiring trends is the emergence of online app stores (e.g., the Apple App Store, Google Play, and Microsoft Marketplace) for distributing mobile software applications (a.k.a., apps) [1]. For the first time in the history of software development, individual developers and small companies can access distribution infrastructures that allow them to sell (mobile) apps to millions of potential customers at the tap of a finger. The emergence of mobile apps and online app stores has been a game changer to breed a new “mobile app ecosystem ” [2] constituting stakeholders such as app developers, marketplace operators, end-users, network service providers, and advertisers. Such an ecosystem also creates new opportunities and challenges for software engineering research. Recently, a few efforts have been proposed, covering aspects including requirement analysis [3], [4], code/library analysis [5], [6], [7], [8], [9], version evolution  [10], and system/tool supports [11] , [12]. Other than the preceding efforts, in software engineering research, understanding user behaviors is a natural and effective channel to align software development activities with user requirements. However, in practice, app developers have quite limited communications with their users, and thus have difficulties to comprehensively identify target users and understand their needs. Although developers can receive user ratings and reviews towards their apps, the reviews and ratings can be quite sparse and even low-quality for some apps  [13], and only very few successful apps can receive useful user feedback  [3]. Previous in-field user studies have made efforts to understand user behaviors towards using apps  [3], [14], [15], [16], [17], but most of these studies were conducted using rather limited or biased datasets, typically based on subjects such as college students and questionnaire volunteers. Other behavioral signals were collected through a monitoring app voluntarily installed on the subjects’ devices. Such a study cannot be widely applied by a variety of crowds because of security and privacy concerns. Certainly, app stores and network-service providers can have a lot of data on app usage, but no evidence shows that such data has been accessed and studied by external researchers. Due to the absence of user behavioral data, it is currently difficult for the research community to directly extrapolate existing results and make representative understandings of how, where, and when the apps are actually used at scale, and thus explore what knowledge can be derived for apps development, maintenance, revenue, etc., accordingly . The main goal of the work presented in this article is to bridge this knowledge gap. We are fortunate to collect a variety of user behavioral data from millions of users with a leading Android app-store service provider. We then conduct our study from three main folds. First, we make an empirical study to characterize the diverse app-usage behaviors. To this end, we propose various research questions in terms of app popularity, management activities, and network usage. We conduct a series of measurements over the dataset covering 17 million users, 0.28 million apps, with various metrics. Second, we exploit the derived knowledge from our measurement study and provide some helpful implications to current app-centric software engineering research, ranging from better understanding user requirements and needs, to improving the workload and ranking system of app stores, and to affecting the app’s development activities and revenue strategies, etc. Last, although this article does provide some primitive answers and implications, researchers can go much deeper into each of these research questions and conduct a much more in-depth study. In addition, we expect that our dataset can establish a valuable resource 1 for the research community to explore more potential research topics and opportunities. Our dataset comes from a leading Android app store in China, called Wandoujia . 2 Similar to Google Play and Apple AppStore, Wandoujia provides its own services with a native management app that facilitates users to search, browse, download, install, update, and uninstall apps. Additionally, compared to Google Play and AppStore, Wandoujia can provide advanced monitoring services and features that can be optionally enabled by its users. Once these features are enabled, the Wandoujia management app runs as a background system-wide daemon service but without requiring the “root” privilege, and is able to collect network-activity information per app, e.g., data traffic and access time under Wi-Fi and cellular, respectively. Our dataset covers millions of active users who frequently use Wandoujia. The dataset contains two types of service profiles reflecting user behaviors: (1) app-management activities (i.e., installation, updating, and uninstallation) from over 17 million (17,303,122) unique users; (2) app-network usage from 6 million (6,632,303) unique users. Based on this extensive dataset, we conduct a systematic analysis that includes the distributions of app popularity, possible reasons of app selection, life cycles of abandoned apps, fine-grained network analysis of apps, and the impact of device-specific factors on app usage. In total, our dataset covers over 0.28 million (283,922) Android apps. Although the apps provided by Wandoujia are all free, such a large-scale dataset can provide useful knowledge on app usage patterns. Part of this work was previously presented in our IMC 2015 conference paper  [18]. The main extension presented in this article is that we employ a larger dataset spanning five months (May 1, 2014 to September 30, 2014) instead of the previous one-month data. The two datasets have the same kinds of information, but the new dataset contains more users (17 million instead of 8 million) and more apps (0.28 million instead of 0.26 million) and thus can enable more comprehensive analysis. In this way, our new dataset can overcome some limitations of the previous one-month data such as impacts by release time and update frequency of apps. We conduct the same measurements proposed in our IMC 2015 conference paper [18] over our new dataset, i.e., the app popularity, app management, and network usage. Additionally, we extend some entirely new statistical measurements, i.e., how the user reviews can correlate to app popularity ( Section 5.4), and how app usage is affected by the choice of device models (Section 7). Most of results are quite consistent with those made over the old dataset. In addition, the new longer-timespan dataset also enables us to explore more insights. This article includes new suggestions on how our findings and implications can help explore open opportunities and problems for software engineering research (Section 8). We also provide more discussions (Section 9), such as limitations of our dataset and threats to validity of our study. Based on the unique dataset, we conduct a systematic empirical study from various perspectives. We not only confirm some results derived from previous studies that were conducted over relatively small datasets or limited users, but also derive some new knowledge and implications from diverse user behavioral patterns on app usage. More specifically, this article makes the following main contributions: We characterize the popularity of apps with various ranking metrics including the number of downloads, the number of unique users, the volume of data traffic, and the length of network-access time. We validate the Pareto-like principle and further explore the power-law of apps’ popularity distribution. Additionally, we also find some “clustered” apps that are frequently to be requested together, indicating their locality on the servers. These findings suggest the significant improvement for cache placement where the copy of some apps can be placed on the app store’s servers. We then simulate the request traces on various typical cache mechanisms and devise an adaptive mechanism for optimizing the app store’s workload. We describe how users perform app-management activities such as downloading, updating, uninstalling, and rating their apps. We reveal the diurnal regularities of app-management activities and the lifecycle of those abandoned apps. We demonstrate how to find the apps having possible “fake” downloads, e.g., some apps have an abnormal number of downloads. In particular, we surprisingly find that the user ratings of an app are not always consistent with the numbers of downloads and unique users of this app, especially for apps with very few ratings. These findings suggest that app-store operators should incorporate new ranking mechanisms to predict the adoption of an app. We investigate the network usage including the volume of data traffic and the length of access time under cellular and Wi-Fi as well as at foreground and background, respectively. We find that apps from specific categories (e.g., VIDEO ) can account for substantial traffic under different networks. We are especially surprised to find that numerous apps keep “long-live” TCP connections and produce data drain at background after they are launched but without user interaction. These findings suggest that both developers and end-users need to justify whether such dynamic behaviors (especially those at background) and potentially extra cost are really reasonable with respect to the regular functionality of the apps. We reveal that the choice of device models can lead to significantly diverse usage patterns of apps. We find that the device models are heavily “fragmented”, i.e., the number of users per device model varies significantly. Additionally, the user behaviors of app download & uninstall and network-access time are affected by the choice of device models. These findings suggest that Android developers need to carefully prioritize device models. Furthermore, the users holding different device models have quite distinct preferences of selecting “competing apps”. For instance, lower-end users prefer the Opera Mini browser while higher-end uses prefer Chrome, since the latter claims to reduce data traffic with advanced compression services. These findings suggest that app developers should take into account the device-specific features in releasing their apps to gain more users and potential revenues. We approach our study from perspectives with the intention that interested readers could focus on parts relevant to their research. In addition, various open opportunities can be explored over our dataset and findings. Although most users studied in our work are from China, the measurement approach and derived knowledge from such an extensive dataset can be generalizable to the populations from other app stores. The remainder of this article is organized as follows. Section 2 describes the dataset. Sections 3 presents the measurement approach by proposing some research questions. Sections 4, 5, 6, and 7 describe the inferred diverse app-usage patterns in four aspects: app popularity patterns, management patterns, network patterns, and device-sensitive patterns, respectively. Section 8 summarizes the findings and implications to different stakeholders in the app-centric ecosystem. Section 9 discusses some possible limitations of our study. Section 10 makes comparisons with related work, and Section 11 concludes the article. 2   Dataset In this section, we briefly introduce the Wandoujia app store and describe the information covered by our dataset. To protect the user privacy and assure the academic ethics of our research, we also discuss how the data is processed with a rigorous workflow. 2.1 Wandoujia Wandoujia3 is a free Android app store in China. Wandoujia was founded in 2009 and has grown to be a leading Android app store [19]. Similar to other app stores, third-party app developers can upload their apps to Wandoujia and get them published by passing Wandoujia’s authentication system. 4 Wandoujia also provides a categorization system, in which each app is annotated with a category tag, such as COMMUNICATION , GAME , MEDIA , MUSIC . Developers can choose the category by themselves or Wandoujia can annotate the app’s category information. 5 Compared to other app stores such as Google Play, apps on Wandoujia are all free, but apps are fully allowed to have “in-app purchase ”. Our dataset comes from Wandoujia’s management app. The Wandoujia management app is a native Android app that provides various services, by which people can manage their apps, e.g., searching, downloading, updating, and uninstalling apps. The logs of these management activities are all automatically recorded. Besides these basic features, the Wandoujia management app is developed with some advanced but optional services that can monitor and optimize a device. These services include network activity statistics, permission monitoring, content recommendation, etc. All services are developed upon standard Android system APIs and do not require the “root” privilege. Users can decide whether to enable these features, as shown in Fig. 1. However, these services are supported only in the Chinese version. Fig. 1. Screenshots of advanced settings in the Chinese version of the Wandoujia management app (the advanced settings is not supported in the current English version). (a) is the homepage of the Wandoujia management app, where users can navigate to “settings” by clicking the text circled by red; (b) refers to the background management service setting, which is highlighted by the red rectangle; (c) is to toggle whether to allow Wandoujia to collect the data of network activities. When installed, the Wandoujia management app is automatically launched and it works as a system-wide service after the device starts up. The data collected by such a service per device are uploaded to the Wandoujia server when Wi-Fi is available. 2.2 Data Collection As of 2014, Wandoujia has over 350 million users.6 Each user is actually identified by a unique Android device, which could be either a smartphone or tablet computer. In the study described in this article, we collected five-month usage data from May 1, 2014 to September 30, 2014. To avoid “zombie” users who contribute little to the analysis, our five-month dataset chooses only users who are actively using Wandoujia. To this end, the users should launch and use the Wandoujia app for more than 120 days, according to the service profile logs generated by the Wandoujia management app. The data reflecting user behaviors consist of two types of service profiles: (1) the profile of app-management activities (i.e., installation, update, and uninstallation) (2) the profile of app-network usage (i.e., the traffic and access time per app). Finally, we obtain the behavioral data from more than 0.28 million (283,922) Android apps. The overall statistical information of our dataset is described in Table 1, which contains aggregated information including the number of apps, users, traffic, and network access time per category. Such a dataset occupies about 5.6 TB disk space. TABLE 1 Chosen Apps by Category  As the network activity monitoring is an optional service of the Wandoujia management app, we distinguish the two kinds of service profiles as “App-Management Activities” and “App-Network Usage Activities”. In addition, we collect the user rating (against an app) and device model information. 2.2.1 App-Management Activities App-management activities consist of downloading, updating,7 and uninstalling apps. The monitoring of app-management activities is always enabled when the Wandoujia management app is installed on device. When an app is installed or updated via the Wandoujia management app, its installation counter is automatically incremented by one and a log entry of this activity is created. The logs of uninstallation via the Wandoujia management app are processed similarly, and the count of uninstallations is incremented automatically. Unlike the Apple App Store, the Android platform allows users to install various app stores other than Google Play; 8 hence, our dataset focuses on only the apps that are operated by the Wandoujia management app. To this end, we maintain a list of popular app stores in China such as 360, Baidu, Tencent, and Xiaomi, and filter out all users who install these app stores out of our dataset.9 Finally, we collect the management activity logs from 17,303,122 unique users (unique devices in fact). We denote the dataset as “Universal User Set .” The logs of management activities are used to explore an app’s popularity, and can implicitly reflect the app’s quality. 2.2.2 App-Network Usage Activities When the advanced features are enabled, the Wandoujia management app collects daily network statistics of each app, when the app is connected to network either from Wi-Fi or cellular (2G/3G/LTE). If an app is never launched or generates no network connections, the app is not recorded in the network statistic logs. To reduce the overhead of runtime monitoring, the Wandoujia management app does not record network details of each session of an app. Instead, it summarizes the total daily traffic drain and access time of an app by examining flows at the TCP level. The traffic drain and access time are accounted for Wi-Fi and cellular, respectively. In particular, the traffic drain and access time generated from foreground and background are accounted separately. To this end, the Wandoujia management app determines whether an app is running at “foreground” by probing the Android system stack for every 2 seconds. In this way, the “foreground” access time can imply how long the user interacts with an app. The Wandoujia management app checks whether an app running at “background” every minute. If any network activity is detected during this interval, this app is regarded to be “online” and its “background” access time is increased by a minute. Such a time interval is reasonable to initiate and release a TCP connection. In summary, the statistic of network activities provides 8 dimensions of information, i.e., 2 metrics (access time and traffic) * 2 networks (Wi-Fi and Cellular) * 2 states (foreground and background). As the statistic of network activities is an optional feature for end-users, the covered users are a subset of the “Universal User Set .” We take into account only the users who successively contributed the statistics for more than three weeks. In our five-month dataset, the network activities cover 6,632,303 unique users from our 0.28 million apps. We denote such a dataset as “Networked User Set .” 2.2.3 User Ratings Most app stores allow users to commit their user reviews and make ratings to an app. Compared to Google Play where an app is ranked by a 5-star model, Wandoujia’s users can simply rank an app with a binary-metric voting model of “like ” or “dislike ”, i.e., a user who installs an app can vote “like” if he/she is in favor of the app, or “dislike” otherwise. To encourage user participation, Wandoujia allows multi ratings of an app per user. But, to prevent possible “fake” ratings, Wandoujia now restricts that each user can rate only once every 3 days. Note that currently Wandoujia does not associate the rating towards a specific version of the app, but just the app generally. Hence, it is possible that users installing an older version of an app can still rate this app even after the app has released updates. In practice, on most app stores such as Apple App Store and Google Play, users can have the information of ratings of an app from its profile page, but it is observed that the ratings are usually provided as an overall score, not specific to versions. In this way, we aggregate the ratings given to an app in our five-month dataset. In addition, users can optionally commit textual reviews. In this article, we collect only the number of “like” (and “dislike”) that can be publicly collected from the profile page of an app on Wandoujia. 2.2.4 Device-Model Information and Price The Wandoujia management app also records the manufactural information of each device, e.g., Samsung Galaxy Note 2, Google Nexus. We employ the information of device models to classify users. There are 19,147 different device models in total. Such a result immediately implies the heavy fragmentation of Android device models. To better organize these models, we collect their on-sale price information when they were first put onto market. We provide details of clustering the device models against their subscribers in Section 7. 2.3 Ethical Considerations Undoubtedly, collecting user behavioral data always needs to be very careful. As an app-store service provider, Wandoujia explicitly declares what and why the preceding data should be collected in its privacy policy statement. We take a series of steps to preserve the research ethics and user privacy of involved users in our dataset. First, all raw data collected for this study are kept within the Wandoujia’s data warehouse servers, which are placed behind the company firewall. Second, our data-collection logic and analysis pipelines are completely governed by three Wandoujia employees10 to ensure compliance with the commitments of Wandoujia privacy policy in the Term-of-Use statements. Finally, the Wandoujia employees take charge of anonymizing the user identifiers. The dataset includes only the aggregated statistics for the users covered by our study period. No actual users can be traced at all. 2.4 Limitations Some limitations should be also addressed before we perform our measurement, as they can have potential impacts on the analysis and may narrow the generalization of results. First, the collected data come from only a single app store in China, and the covered users are mainly Chinese. As a result, the demographical differences can occur in other app stores or other countries. Second, the dataset currently cannot trace an app’s versions where the data come from, and thus cannot capture the impact of app release, which is proposed to be a factor to affect the app’s success [20]. Third, apps published on the Wandoujia app store are free, and hence we cannot infer the users’ payment behaviors on the paid apps. Indeed, it is reported that user behaviors can be a bit different on free and paid apps [2]. However, our dataset is quite unique in terms of both the scale of users and behavior dimensions. As presented in the subsequent sections, our study not only validates or contradicts some results derived from a small scale of users, but also provides new findings. In addition, we discuss how to alleviate the preceding limitations in Section 9. 3   Research Questions Starting from this section, we demonstrate how our dataset can be useful for general research on the diverse user-behavior patterns of Android apps. In particular, we propose a series of research questions, which are concerned with the popularity, management activities, and network characteristics of using apps. We show that these research questions are rather interesting and can inspire various potential research projects; if answered, these research questions can provide many insights on app-usage behaviors. Although in this article we do provide preliminary answers, researchers can go much deeper into each of these research questions and conduct a much more in-depth study. Our goal is to provoke the interest of the research community to study these questions (based on such a dataset). 3.1 RQ1: How Can We Identify the Multi-Dimensional Popularity Distribution of Apps? For app-store operators, a fundamental task is to identify and determine which apps are actually popular or unpopular, so that the operators can improve their ranking and recommendation system, allocate server-side resources, and place ads for the most popular apps. Currently, the popularity of an app is usually measured by the number of downloads of this app. Such a metric should not be always sufficient or accurate enough in some cases. For example, it is quite possible that some apps can be purposely re-downloaded by automatic programs. In addition, downloading an app does not mean that the app could be actually used. To this end, we aim to make a comprehensive study of app popularity from multiple aspects, i.e., the download times, the unique subscribers, and the network usage. Indeed, all these aspects are meaningful, but no previous study can synthesize them comprehensively . Hence, we aim to understand the distribution of app popularity from various indicators. Additionally, we can further examine the consistency of these indicators, and thus provide a representative genre of the apps that can substantially account for all indicators. Such a genre can be released to researchers who can explore further topics. 3.2 RQ2: How Do the Users Manage Their Apps? The app-management activities include downloading, updating, and uninstalling apps. Exploring app-management activities is motivated for various reasons. Essentially, the management activities can reflect the user attitudes towards an app, i.e., downloading and updating can mean that the user needs this app, while uninstalling can imply that the user does not need or even dislikes this app. In addition, we can associate the app-management activities with the publicly available user ratings, and infer users’ attitudes towards the app in a more comprehensive way. On the other hand, as the app-management activities are usually made manually, they can implicitly reflect the density and frequency of user interactions with the app stores. This information can help app-store operators better understand when a large number of concurrent user requests arrive, so that the operators can optimize their servers for faster app delivery and more reliable network bandwidth. Therefore, we plan to break down RQ2 to the following questions. RQ 2.1: How do the users perform their diurnal management activities of apps? RQ 2.2: What apps are more likely to be selected and liked by users? RQ 2.3: How can we identify an app that is more likely to be disliked by users? RQ 2.4: Are the user ratings of an app consistent with the app-management activities, with respect to the user attitude towards this app? 3.3 RQ3: How Do Apps Perform in Terms of Access Time and Traffic Drain Over Network? Understanding network activities of apps is always a highly interesting topic. Undoubtedly, most of current smartphone apps heavily rely on the network to function and provide features. End-users would like to know how their data plan is spent by an app, and whether some potentially unnecessary “hidden” or even “stealthy” network behaviors may occur. To avoid the low ratings or even user loss, the developers should carefully check the design/implementation such as improperly granting permissions, or fix some possible bugs. As our dataset contains the detailed information of access time and traffic volume generated at foreground/background under cellular/Wi-Fi, respectively, we decompose the network characteristics of apps by exploring the following questions. RQ 3.1: Which apps are the users likely to interact with, when these apps are under Wi-Fi and cellular networks, respectively? RQ 3.2: Which apps are more “traffic-intensive” and how much traffic is generated by these apps? RQ 3.3: How much “hidden” traffic is consumed when using an app? 3.4 RQ4: How Does the Choice of Device Models Affect the App Usage? Intuitively, the answers to RQs 2-3 can reflect the overall knowledge of app usage from millions of users. To break down whether the classification of users can have impact on the app usage, we categorize the users according to the device models that they hold, i.e., high-end, medium-end, and low-end, ranked by the on-sale price. Hence, we revisit some important aspects of RQ2 and RQ3 , such as app selection, app abandonment, and network usage. As an empirical study, we employ descriptive and statistical analysis of our dataset to answer the preceding research questions. We apply some well-established statistical metrics such as the Spearman correlation coefficient and linear regression model. The large-scale dataset enables our findings to be comprehensive. 4   App Popularity Patterns with Different Metrics In this section, we first analyze the apps’ popularity distribution. Essentially, the app store can be viewed as a special kind of system for sharing web contents and resources. Indeed, it is demonstrated that analyzing the exact form of popularity distribution not only helps understand the underlying mechanisms (i.e., cache, bandwidth, etc.), but also helps improve important design solutions in other systems of sharing web contents and resources such as search engines, online video systems, e-commerce systems [21]. For instance, the scale-free nature of web requests has been used to improve search engines, advertising policies, and recommendation systems. In a previous study [2], app popularity is usually measured by only the number of app downloads on the app store. To make a comprehensive analysis, we employ four metrics for an app in this article: (1) the number of unique devices that ever used the app; (2) the number of downloads of the app; (3) the aggregated data traffic generated by the app; (4) the aggregated access time that users interacted with the app. The former two metrics can indicate how widely an app is owned by users, and the latter two can indicate how much an app is really used. 4.1 Popular Apps by Downloads We investigate the most intuitive metric of app popularity, i.e., the number of downloads of an app. Many app stores take the number of downloads (i.e., accumulatively total, monthly, or weekly) as the key indicator to rank app popularity. We then investigate app downloads from the management activities of the Universal User Set . Various data points show that the Pareto principle exists in networked application domains such as web content, audio, and video downloads [21], i.e., 20 percent of objects account for about 80 percent number of downloads. In practice, the Pareto principle can be extended as “a small proportion of the objects account for a substantial proportion of downloads. ” Fig. 2a demonstrates the cumulative distribution function (CDF) of the percentage of app downloads against app ranking by downloads. It indicates that the distribution of app downloads exactly follows the “Pareto-like” principle, or more specifically, 3 percent of apps account for about 95 percent downloads of all apps. Fig. 2. App popularity by unique users and downloads. Figs. 2a to 2d demonstrate that the number of downloads and the number of unique users for a single app are observed to follow the “Pareto-like” principle and power law. Fig. 2e indicates that users do not very frequently update their apps. Other than the Pareto principle, the power-law distribution was discovered to be one basic law of the networked systems [22], [23], and has been increasingly used to explain various statistics appearing in computer science and networking applications, such as Youtube [24] and search engines  [21]. Therefore, we next explore whether the number of app downloads can follow the power-law. A distinguished feature of power law is a straight line in the log-log plot of views versus frequency. However, there are other distributions (e.g., log-normal) with a very similar shape. In the real world, the shape of the natural distribution can be affected for various reasons. In fact, it has been found that many distributions whose underlying mechanism is power law fail to show clear power-law patterns, especially at the two ends of the distribution: the most popular and the least popular items [23] . Hence, the distribution of app downloads is yet another typical presence of power law in collective behaviors. The easiest way to spot a power law is observing a straight line on a log-log plot: the power-law exponent is essentially the slope of the straight line. As illustrated in Fig. 2b, the apps are ranked by the number of their downloads (in X -Axis). The result indicates that the main trunk follows a quite linear slope, and is truncated at both ends. While there are many techniques to estimate the slope, the best way of estimating the power-law exponent is using a maximum likelihood estimator  [25]. In practice, such an exponent (r = 1.699) can be obtained by Python.11 At the curve’s head, some extraordinarily popular apps, such as WeChat (having over 6 million downloads) and QQ (having about 7 million downloads), gain substantial proportion of downloads than other apps. In contrast, about 80 percent of the apps are downloaded fewer than 10 times, and 30 percent of the apps are even downloaded only once. We can then validate that the “fetch-only-once ” principle [24] still applies for app stores. In a sense, the power law can be a guiding indicator for content-service providers to allocate the server resources and bandwidth [26]. Therefore, a straightforward implication that can be immediately taken away from the popularity of apps is that app-store operators can optimize the resource allocation (e.g., more bandwidth or servers) to the apps that are more frequently downloaded and updated. Indeed, it is quite possible that most app stores have already found and applied such knowledge. However, in Section 8, we synthesize the power law and the app-management patterns to further help the design and optimization of an app store’s workload in terms of cache mechanisms. 4.2 Popular Apps by Unique Users Usually, many app stores take the number of an app’s downloads to indicate the app’s popularity. However, it is a common observation that people can update their installed apps, and the updates can be affected by the app’s release time [20]. In addition, some apps can be possibly downloaded or updated with “faking ” behaviors. For example, apps can be downloaded and updated by automated programs to increase their ranks on an app store. In our opinion, the number of an app’s unique users is an intuitive and straightforward indicator that cannot be affected by the preceding factors. We then aggregate the unique users (devices) that ever downloaded, updated, or uninstalled an app in our dataset, and each user is counted once and only once. Such processing can involve more users who ever used this app, because some apps were downloaded in advance to the starting time of our dataset but the updates and uninstallations can still be captured. Fig. 2c shows the correlation between the number of downloads & updates and the number of unique users per app; the correlation is linearly positive. In other words, apps having more users can gain more downloads & updates. Note that there are some outliers at the left top. These outliers actually refer to the apps that have substantially more downloads against the number of the unique users that the apps have. Fig. 2d plots the distribution of the number of apps that a user installs on the device. Intuitively, many users use only a few apps, while a few others try out a large number of apps. The distribution obeys the power law in its tail distribution. We need to mention that the number of apps installed on a device is likely to be underestimated as a lot of devices have pre-loaded apps, and users can install apps directly from the app developers’ websites other than Wandoujia. As a result, the distribution somewhat does not strictly follow the power law. We then investigate how frequently an app is downloaded and updated by its users. Computed by Formula  (1), we can observe the user preferences of an app and the possibility of adopting its released new versions. As shown in Fig. 2e, we are surprised to find that more than 95 percent apps received only one download & update in our five-month dataset. Such a finding indicates that users do not tend to update apps very frequently,12 \begin{equation} Avg.D\& P per app = \frac{\mathrm{Number\; of\; Downloads \;\&\; Updates}}{\mathrm{Number\; of\; Unique\; Users}}. \end{equation} This simple metric can help identify some apps that receive an extraordinary number of downloads & updates. We find that 408 apps receive more than 5 downloads & updates per user in our dataset, i.e., at least one operation per month. These apps include some popular apps such as QQ. Such a finding confirms that more-popular apps usually have more updates [20]. However, we are surprised to observe that some apps can have extremely abnormal behaviors. For example, an app has only 18 unique users, but receives 3,581 downloads & updates, and 3,563 downloads & updates come from only one user. In addition, we find that some apps receiving an extraordinary number of downloads & updates per user can share quite similar behaviors: (1) the management activities are mostly “downloading” but very few “updating”; (2) the user reviews are quite sparse, but most user ratings are marked as “like”. Such a finding indicates that some app developers may purposely increase the number of downloads in possibly “faking” ways, e.g., by automatic programs. We plan to release the detailed information of these apps, including the apk name and the exact number of downloads & updates per unique user. Some of these apps can still be accessed on Wandoujia, as we cannot make sure that they are problematic ones. However, these apps are moved to the watchlist, and their rankings are tuned down accordingly. 4.3 Popular Apps by Network Usage The preceding analysis can identify popular apps based on their numbers of downloads and their unique users. But at the same time, we do not want to discriminate against those apps with few users but with a significant impact on the network, i.e., generating a lot of traffic or accessing the network for long time periods. Indeed, either the number of downloads or that of unique devices of an app can indicate only that this app is downloaded and installed, but we cannot judge whether the app is “really” used by users. From the logs of the Networked User Set , an app cannot generate network logs if it is never launched by users. Hence, we use the network activities to examine whether the app is really used. Although we may miss some apps that are usually used offline, e.g., PDF readers or dictionaries, it is a common sense that most of current smartphone apps heavily rely on network. To illustrate the usage of network, we distinguish the aggregated data traffic and the network-access time from all users that an app owns, respectively. The data traffic comes from both foreground and background. In contrast, we take into account only the access time from foreground, because such a metric indicates how long users really interact with the app when they are connected to the network. Figs. 3 Fig. 3. App popularity by traffic drain. a and 4 Fig. 4. App popularity by access time. a illustrate the distribution of aggregated traffic/access time of apps, respectively. We can find that the Pareto-like principle still holds for the network activities of investigated apps. We can observe that about 90 percent apps consume less than 100 MB traffic volume in five months, and about 94 percent apps are used less than 100 hours. Considering that our dataset comes from five months, we can regard that most of apps are not quite active over network. Intuitively, the more users an app has, the more traffic and access time the app accounts for. Such an intuition can be reflected in Figs. 3b and 4 b, respectively. Furthermore, if the number of unique users is a good metric for filtering, the top X apps based on the number of unique users should contribute similar amount of data traffic and access time as the top X apps based on the data traffic and access time. We compare the contribution of the top X apps based on these two metrics. Fig. 3c compares the contribution of the top X apps based on the number of unique users against the top X apps based on the data traffic. We can observe that the cumulative contributions of the top X apps based on traffic and the top X apps based on number of unique users are quite close, by comparing the “data traffic from all apps” and the “data traffic from top apps.” Likewise, the contributions of the top X apps based on the network-access time and the number of unique users are still close in Fig. 4c although a little difference does exist. We note that over 90 percent of the total data traffic and access time is accounted for around the top 2,500-3,000 apps based on the number of unique users. The preceding analysis studies the app popularity from various metrics including the number of downloads, the number of unique users, data traffic, and access time. The immediate finding is summarized as follows. Finding (F1). The popularity of apps can typically follow the Pareto principle. Furthermore, the distributions of the numbers of downloads and unique users even follow the power law. By exploring the average number of downloads & updates of an app per user, some possibly faking behaviors can be detected. 4.4 Released Popular Apps Indeed, for researchers who are interested in our dataset, it is not realistic or necessary to make the data for all 0.28 million apps released, as a substantial percentage of them have a very limited number of downloads or unique users. We plan to release the information of some representative apps. Hence, we should define a reasonable threshold instead of releasing all the apps. We choose the intersection of apps having at least 50 downloads, 50 unique users, aggregated 100-hour foreground network access time, and 100-MB traffic from users, as the genre of “ Popular App Set .” In total, we have around 3,500 apps in this set. Indeed, it is known that app sampling can have selection bias [27]. Nevertheless, the released data of the chosen apps have sufficient information that can help explore more research topics. 5   App Management Patterns In this section, we study how users manage their apps, i.e., which apps are frequently installed, and when they are installed, which apps are more likely to be uninstalled, etc. In addition, we also explore whether the users’ app-management activities are consistent with the ratings of the apps. 5.1 Diurnal Patterns of App Management We first investigate RQ 2.1 , i.e., how do the users perform their diurnal management activities of apps? To this end, we investigate the diurnal downloading, updating, and uninstallation distribution. We aggregate the activities of downloading and updating per app, because these two kinds of activities both reflect the users’ interest towards this app and access to the app store. Each entry of activity log is associated with the smartphone’s local timestamp to indicate when the activity is performed. We align the timestamp to avoid the inconsistencies caused by different time zones. As shown in Fig. 5, the app-management activities are “ periodically and regularly ” performed during a day . The extent of app downloading and updating activities keeps growing from 6:00 am and reaches the first peak around 11:00 am. The downloading and updating activities decline slightly between 11:00 am to 12:00 pm. It is not very surprising because users may take lunch at this time. The same observation can be found between 4:00 pm to 6:00 pm, i.e., the time on the way back home or at dinner. We can also find that about 32 percent of downloading and updating activities are performed from 7:00 pm to 11:00 pm, where they reach the maximum around 8:00 pm-9:00 pm. Such a distribution is quite consistent with human regularity. After 9:00 pm, the downloading and updating activities decline quite sharply, and reach the minimum around 5:00 am. However, at midnight, downloading and updating activities occupy about 7 percent in total, implying that there are still a considerable number of active users at this time. Fig. 5. Distribution of diurnal app-management activities. Each point on the curve represents the percentage of activities performed during the one-hour time interval against the total activities, during the whole day. For example, activities during 10:00 am-11:00 am account for about 6 percent of all activities. The preceding results indicate the temporal patterns when users access the app store. Hence, the app-store operators should reserve sufficient bandwidth at the peak to reduce user-perceived latency. In addition, app developers can leverage this finding in their release planning, e.g., pushing the update notifications to their users at the right time. Activities of uninstalling apps present a similar distribution to those of downloading/updating apps. However, knowing when users uninstall apps may be less useful, because the uninstallation activities do not have interactions with app-store operators or app providers. Finding (F2). The app-management activities can reach fixed peaks and are performed quite periodically during a day. 5.2 App Selection Patterns Then we explore RQ 2.2 , i.e., what apps are more likely to be selected and liked by users? Such activities can imply the user interests and needs towards apps. For app-store operators, such information can help improve the recommendation systems. In fact, some previous studies investigated how users select apps [2], [14], [28], [29], and some findings were reported. A straightforward metric is to check the “cluster effect ”: which apps are more likely to be selected together. We adopt a similar metric, but explore the study at two levels: the micro-level of co-installed apps, and the meso-level of correlated app categories. From our previous analysis, a substantial percentage of apps are rarely downloaded and updated. Therefore, we choose only the top 3,500 apps mentioned in Section 4.4. 5.2.1 Clustering Co-Installed Apps We study the frequently co-installed apps. Given two apps app_{m}$ _Z and Z_$app_{n} , we employ the Jaccard Similarity Coefficient (denoted as \lambda ) to measure the possibility of how they are installed together by users. We denote the number of unique devices that install either app_{m} or app_{n} as \mathbb {D} (app_{m} \cup app_{n}) , and the number of unique devices that install both app_{m} and app_{n} as \mathbb {D} (app_{m} \cap app_{n}) . We compute \lambda as \frac{\mathbb {D}(app_{m} \cap app_{n})}{\mathbb {D} (app_{m} \cup app_{n})} . Fig. 6 shows the Jaccard Similarity Coefficient of the top-N apps, where N varies from 100 to 3,500. With the increasing number of N , the value of \lambda decreases significantly. The CDF indicates that the \lambda value of over 95 percent of app pairs is lower than 0.1. In other words, there are a very small fraction of apps that are frequently co-installed together. Fig. 6. Jaccard coefficient of co-installed apps. To better demonstrate which apps are frequently co-installed, we employ the metric of Point-wise Mutual Information (PMI), which is widely used in information retrieval to identify co-occurrence of objects. Formally, let us assume that n_i represents the number of downloading and updating activities that contain app_{i} , n_{ij} represents the number of activities that contain both app_{i} and app_{j} , and N denotes the total number of activities, and thus the PMI is computed as follows: \begin{equation} PMI(i,j) = \log\left(\frac{p_{ij}}{p_i * p_j}\right) =log\left(\frac{n_{ij} / N}{(n_i / N) * (n_j / N)}\right) \end{equation} \begin{equation} =log\left(\frac{n_{ij} * N}{n_i * n_j}\right) \end{equation} \begin{equation} =log(N) + log\left(\frac{n_{ij}}{n_i * n_j}\right) \end{equation} The larger PMI that two apps hold, the more probably they are co-installed. We visualize the network structure of co-installed apps based on the PMI, and employ a force vector algorithm [30] to detect the community structure. As shown in Fig. 7, there exist some significant clusters, i.e., the apps WeChat, QQ, QQMusic, Dianping are more likely to be co-installed. To make further exploration, we find that apps from some big clusters share some characteristics, i.e., developed by the same vendor or from the same category . Fig. 7. Network structure of co-installed apps. The vendor information of an app can reflect who develops this app. Usually, the naming rules of an Android app can reflect the vendor information. For example, the package com.tencent.mm can be processed by removing the general stopword “com” and the app’s feature word “mm”, and the vendor information “tencent” is extracted. We find that a number of co-installed apps with high Z_ $\lambda$_Z values come from the same vendor . For example, the pair of Z_$<$ _Z Huawei Backup (used by 555,332 devices), Huawei Account Manager (used by 151,541 devices)> has the \lambda value of 0.274, and these two apps are both provided by Huawei. Furthermore, the \lambda value can be much higher, if two apps developed by the same vendor also belong to the same category . For example, the pair of < WeChat (used by 3,048,557 devices), QQ (used by 7,225,074 devices)> holds the \lambda value of 0.43, and the two apps are provided by Tencent. There are many possible reasons why apps from the same vendor are often co-installed. A vendor usually focuses on a specific application domain, e.g., Tencent is the largest messaging service provider in China. Tencent QQ is the most popular instant messaging app in China; WeChat not only supports instant messaging, but also provides social communication features such as content sharing. Another reason is that there might be “in-app bundled installation ” in some apps. For example, when users install an app, the app’s vendor may implicitly or explicitly recommend the users to install their other apps. For simple validation, we make field studies by selecting 50 apps from well-known app developers such as Qihoo, Baidu, and Tencent, and install them manually. 14 apps out of the 50 apps recommend installing other apps in their installation wizard, and 8 apps of these 14 “bundled” installations are provided by the same vendor. 5.2.2 Correlation of App Categories The category information of an app indicates the functionality and application domains of the app. We can infer the users’ needs and interests according to their selected app’s category. Given two app categories M and N , we denote the number of unique users who install an app either from M or N as \mathbb {D} ({M} \cup {N}) , and the number of unique devices that install apps from both M and N as \mathbb {D} ({M} \cap {N}) . We then compute \frac{\mathbb {D}({M} \cup {N})}{\mathbb {D} ({M} \cap {N})} to indicate how likely that the apps in M and N are installed together. We also take into account the special case where M=N , indicating how many users install more than one app in the same category. Fig. 8 shows the probability distribution that the apps from different categories are selected together. The categories are sorted by the descending order of the number of apps (in X -axis). Apps providing related functionalities are more likely to be selected together. For example, users may want to share a video from a video-player app to friends in a communication app (e.g., correlation between COMMUNICATION and VIDEO is 0.77), or use a viewer app to open a document that is received by an instant messenger app (e.g., correlation between TOOL and COMMUNICATION is 0.88 ). Fig. 8. Heatmap of the category-level relationships of co-installed apps. It is not surprising that users may install more than one app in the same category. For example, in GAME and COMMUNICATION , the correlations are both more than 0.8. The result suggests that users have more interests and needs in these categories. Finding (F3). In terms of app installation, apps from some app categories are frequently installed together such as COMMUNICATION and TOOL . Additionally, apps that come from the same vendor or category are more likely to be installed together. 5.3 Uninstallation Patterns We next explore RQ 2.3 , i.e., how can we identify an app that is more likely to be disliked by users? Such a question is quite crucial to app developers and app-store operators. App developers can know whether their apps are appreciated by users or not, so that they can examine their apps in time to avoid losing users. App-store operators can improve their recommendation systems to filter unpopular, low-quality, or even malware apps. However, to answer the question, only the absolute number of unistallations of an app may not be a good indicator. For example, apps with a high number of uninstallations may also have a high number of downloads. So we compute the metric of installation/uninstallation ratio , or “I/U ratio” for short, denoted as \Omega (app_{i}) , to indicate how likely an app could be abandoned. Given an app app_{i} , we compute \Omega (app_{i}) as \frac{\sum \mathbb {I}_{device_{i}}}{\sum \mathbb {U}_{device{i}}} , where \sum \mathbb {I}_{device_{i}} and \sum \mathbb {U}_{device_{i}} represent the number of devices that install and uninstall app_{i} , respectively. We extract all devices that appear in both installation and uninstallation activity logs of app_{i} from the Universal User Set . Note that there may be some biases when computing Z_$\Omega (app_{i})$ _Z, i.e., we cannot capture how many installations have already existed for an app before the starting time of our dataset, nor can we know how many uninstallations would be performed for this app after the ending time of our dataset. However, our analysis aims to derive the overall trend that an app would be abandoned by users during our dataset’s time span, we still simply rely on the preceding computation. The lower value of \Omega (app_{i}) an app holds, the higher likelihood that this app could be abandoned. \Omega can tell how much an app is actually abandoned by users. For better illustration, Fig. 9 Fig. 9. Lifecycle of abandoned app. a shows the scattered distribution of \Omega (app_{i}) . The mean and median values of \Omega are 7.89 and 5.875, respectively. However, the value of \Omega (app_{i}) seems to be irrelevant to the number of downloads, indicating that \Omega (app_{i}) is at least not a good signal to comprehensively reflect how much an app is disliked by users, because users may not always uninstall an app even if they do not need the app any longer. To further infer the users’ attitude towards apps, we evaluate the lifecycle of abandoned apps by combining the temporal information with \Omega . Such an evaluation is motivated by an intuition that an app is likely to be a disliked one if it is uninstalled shortly after being installed. To this end, we compute the app’s lifecycle by the timestamps of installation and uninstallation. We have two immediate observations. First, from Fig. 9b, if an app is uninstalled, its lifecycle can be identified. About 60 percent of abandoned apps can “survive” for only less than 1.5 days, and about 80 percent of abandoned apps can “survive” for less than a week . Such results are largely consistent with the ones derived from the one-month data in our previous work  [18], i.e., 60 percent abandoned apps can “survive” for only less than two days, and about 93 percent abandoned apps can “survive” for less than a week . Second, from Fig. 9c, we can find a quite weak positive correlation between \Omega and the lifecycle of abandoned apps . In other words, apps with a lower \Omega seem to be a bit more probably uninstalled within a shorter interval. However, such a finding implies that we should find more meaningful indicators. In practice, we have devised some new signals to more accurately predict how an app could be adopted by users [31]. Finding (F4). An app’s installation/uninstallation ratio exhibits a weakly positive correlation to its lifecycle. Most “abandoned” apps are often uninstalled within 1.5 days after they are installed. 5.4 User Rating Patterns Usually, on most app stores such as Apple App Store and Google Play, the users’ attitudes towards an app can be somewhat reflected by the ratings of the app. End-users may be simply attracted by the overall ratings of an app at their first sight, before scrolling down the page to see textual user reviews. To some extent, the ratings of an app can imply the quality of the app. For example, Google Play allows users to rate apps with a 5-star model, where 1-star refers to the lowest rating and 5-star refers to the highest rating. In contrast, Wandoujia allows users to simply tag an app with a binary metric, i.e., “like ” or “dislike .” Intuitively, an app is considered to be of higher quality if it receives a higher average rating from its users. Previous study reported that the score of ratings can have positive correlation with the app rank by downloads  [32]. In practice, there are many issues of directly using this simple and straightforward metric. Online ratings can suffer from the sparseness of some apps. To this end, we argue that the management activities may be more objective, e.g., downloading and updating an app can reflect positive attitudes of this app, while uninstalling the app can reflect negative attitudes. In this way, we are interested in investigating whether user ratings of an app are typically consistent with management activities of the app. To this end, we then move to RQ 2.4 , i.e., are the user ratings of an app consistent with the app-management activities, with respect to the user attitude towards this app? We compute the average rating of an app on Wandoujia, namely likerate , as denoted in Equation  (5). In other words, the higher likerate an app holds, the more possibly it is preferred by the users. We then correlate the number of downloading and updating activities (actually the installations) to the likerate of apps. Indeed, the likerate metric reflects only the general attitude of an app by its users, but suffers from the absence of the attitude towards a specific version of the app, \begin{equation} \mbox{likerate}=\frac{\mbox{number of likes}}{\mbox{number of likes} + \mbox{number of dislikes}}. \end{equation} 5.4.1 Correlation between Rating and Selection In Fig. 10, we rank all apps that have received at least 5 ratings during our five-month period, split them into equally-sized bins,13 and demonstrate the mean and standard deviations of their likerates. As the number of downloads follows the power law, we plot the results at the log scale (X -axis). We then run a regression process to derive the correlation between the likerates and the number of downloads, and use the seaborn package of Python to plot the results, as shown in Fig. 10. Fig. 10. The number of downloads against likerate . The number of downloads is weakly correlated with likerate for popular apps and negatively correlated for unpopular apps. Some immediate observations can be reached. Surprisingly, when the number of downloads is less than 1,000, it is negatively correlated with the likerate. In other words, the more times an app is downloaded, the more likely it is disliked by users. Such an observation is rather counter-intuitive. One explanation is that the apps not frequently downloaded may be sensitive to fake “like ” ratings, while some frequently downloaded apps may be maliciously rated down by their competitors. It reminds that app-store operators should pay attention to address such an issue in their ranking and recommendation system. When the number of downloads exceeds 10,000, the correlation becomes quite weakly positive. In either case, the result indicates that only the number of downloads is weak as a guiding indicator for ranking apps. In particular, it may not be valid for the apps that are not popular, with few ratings, or newly published on an app store. One may argue that the ratings are given to all versions of an app rather than a specific version. To alleviate the bias, we explore the likerate and the number of users (devices) that ever used the app in our five-month dataset. From Fig. 11, we can observe that the number of users (devices) installing an app is even negatively correlated with its likerate, although the correlation is quite weak either. From a macro perspective, it indicates that the app’s versions do not have very significant impact on its ratings. Fig. 11. The number of unique users against likerate of apps . The number of unique users seems not to be a positive indicator of likerate. 5.4.2 Correlation between Rating and Abandonment Another intuition is that an uninstallation may indicate that a user dislikes an app. Although most app stores usually do not report this statistic, we can compare the number of uninstallations of an app with its user ratings. Instead of using the raw number of uninstallations, we still use the metric of \Omega , which is computed as the total number of downloads divided by the total number of uninstallations. Intuitively, the lower \Omega an app holds, the more likely the app is disliked. We plot the correlation of \Omega of an app and the corresponding likerate in Fig. 12. Again, we rank all apps with at least 5 ratings by \Omega and split them into equally-sized bins. Fig. 12. I/U ratio against likerate of apps . I/U ratio is not a promising indicator of likerate. Interestingly, when \Omega is below 1, a quite weakly positive correlation is observed between \Omega and the likerate. However, when \Omega is over 10, the correlation becomes more negative. In either case, the correlation is not significant and presents a long error bar. Apparently, the I/U ratio is also at best a weak indicator of user preferences. To infer user preferences from activities, new signals need to be conducted. In practice, our recent work [31] explored how app-management sequences can be a promising indicator with machine learning algorithms. Due to page limit, the details are not included in this article. Finding (F5). Neither the number of unique users nor the users’ attitudes towards an app (installing and uninstalling) can exhibit significant correlation of users’ ratings towards this app. Such a finding is somewhat inconsistent with a previous study [32], which reported that the score of user ratings can be positively correlated with the app ranking determined by the number of downloads of the app. For apps that have few or spare reviews, new indicators are required to judge the user attitudes. 6   Network Activity Patterns Previous studies have already revealed some observations on network usage of apps, e.g., by the TCP flows on tier-1 network [28], or usage logs by field studies  [15], [33]. In contrast, our study is performed at a much finer granularity. First, we distinguish the daily data traffic and access time from Wi-Fi and cellular network, respectively. Second, we distinguish the daily data traffic and access time from foreground and background, respectively. Based on the granularity of network activities, our study aims to explore some issues that are not covered by previous efforts. End-users can know which apps are network-intensive, and thus result in more data traffic and battery consumption. In this way, end-users can identify the apps that generate “undesirable ” traffic, pay attention to granted network permissions, or even uninstall these apps. App-store operators can identify some potentially problematic apps. App developers can fix possible bugs, and OS vendors can patch their frameworks to avoid potential threats. 6.1 Access Time Patterns First, we aim to answer RQ 3.1 , i.e., which apps are the users likely to interact with, when these apps are under Wi-Fi and cellular networks, respectively? We investigate the access time of the network activity log. Intuitively, access time may reflect two important insights. First, the foreground access time of an app indicates how long a user stays in this app when he/she is connected to the Internet. Therefore, such a metric can somewhat imply how much the user likes or needs the app. Second, similar to the background data traffic, the background access time indicates how long an app connects to network when users do not interact with it. Therefore, the background access time can imply the “liveness” of the app after it is launched. We illustrate the access time distribution among app categories, as shown in Table 1 (in Section 2). When the foreground access time is explored, it is not surprising that the COMMUNICATION apps account for 48.32 percent of cellular time and 46.08 percent of Wi-Fi time against all apps. It is also interesting to find that users spend a lot of time on BEAUTIFY (11.96 percent under cellular and 11.33 percent under Wi-Fi) to personalize their smartphones (typical BEAUTIFY apps include choosing themes, background, icon types, and rings), and TOOL (12.16 percent under cellular and 11.06 percent under Wi-Fi) to optimize the smartphones (typical TOOL apps can include battery manager, third-party input method, and weather). We then break down the access time spent at foreground and background under cellular and Wi-Fi of all apps of a category, respectively. From Table 2 (Columns 2-5), we are surprised to observe that foreground time accounts for only less than 2 percent (by aggregating W -Time(F) and C -Time(F)) in most categories, but the background time occupies more than 98 percent (by aggregating W -Time(B) and C -Time(B)). Even for the COMMUNICATION apps, the background time accounts for more than 94 percent of all network time. In other words, most apps still keep “long-and-live” TCP connection at background after being launched, even users do not interact with them . The background time may be reasonable for the apps that heavily rely on network, e.g., COMMUNICATION and SOCIAL . Most of these apps require auto synchronization or notification. However, it is hard to confirm whether many apps from other categories should have “reasonable” continuous network connection at background. TABLE 2 Network Summary of All App Categories  Indeed, the background TCP connection does not always indicate data traffic loss, as our dataset can monitor the actual data transmission. In other words, the live TCP connection can probably generate no data traffic. This finding also indicates that currently most Android apps could stay silently in memory even when they are switched to background. Such a mechanism can be useful, as the app can be quickly “waken up” and the network connection can be fast restored. However, it can also lead to large memory occupation and have side effect of system performance. In fact, many Android users complain that their devices become too sluggish to respond to user interaction, and the user experiences really are unsatisfying. It would be interesting to explore whether keeping “long-live” network connection at background could be a potential factor of such unsatisfactory user experiences. Finding (F6). A large number of apps keep long-lived TCP connection when they are not currently “used” by users. It is not quite sure whether the background connection is always reasonable. 6.2 Data Traffic Patterns We then move to RQ 3.2 , i.e., which apps are more “traffic-intensive” and how much traffic is generated by these apps? We identify the apps that consume substantial data traffic. We aggregate apps by their categories and summarize the total traffic consumption (in GB) from Wi-Fi and cellular, respectively. As shown in Table 1, VIDEO apps are the most “traffic-intensive ”. Apps from VIDEO category consume 61.56 percent of Wi-Fi traffic and 12.02 percent of cellular traffic against all apps. Interestingly, apps from TOOL and SYSTEM_TOOL consume a lot of data traffic. The apps in these two categories include input method, anti-virus, and app management. It indicates that Android users heavily rely on these apps to manage, optimize, and personalize their devices. We then classify data traffic into two dimensions: (1) Wi-Fi and cellular; (2) foreground and background. Such classifications can provide more details of traffic consumption. 6.2.1 Traffic from Wi-Fi and Cellular As shown in Table 2 (Columns 6-9), in most categories, it is not surprising that the data drain generated under Wi-Fi accounts for more than 70 percent of total traffic. In the categories of TOOL , MUSIC , SYSTEM_TOOL , SHOPPING , and EDUCATION , more than 75 percent of data traffic is from Wi-Fi. For VIDEO , almost 97 percent of traffic drain comes from Wi-Fi. A possible reason is that most of these apps are usually used in places with stable Wi-Fi connection, e.g., at home or cafe. The situation is a bit different in COMMUNICATION, GAME, LIFESTYLE, FINANCE , and TRAFFIC , where the traffic from cellular network accounts for more than 30 percent. Such results can be consistent to the purposes of the apps. For example, users may use COMMUNICATION apps such as instant messaging and LIFESTYLE apps such as searching restaurant whenever a network connection is available, and need to synchronize to the servers for latest stock information when using FINANCE apps. 6.2.2 Traffic from Foreground and Background We then move to RQ 3.3 , i.e., how much “hidden” traffic is consumed when using an app? . We distinguish the foreground and background traffic of an app, respectively. Often, the foreground traffic is generated when users interact with the app. In the Android OS, a foreground app can be identified if the app is currently at the top of the activity stack. In contrast, the background traffic implies that the app is still connecting to network even when users do not interact with it. From Table 2 , the foreground traffic accounts for more than 60 percent in many categories. Foreground traffic accounts for less than 50 percent in some categories, i.e., SYSTEM_TOOL (12.62 percent), TRAVEL (39.2 percent), LIFESTYLE (38.33 percent), TRAFFIC (39.42 percent), BEAUTIFY (23.97 percent), PRODUCTIVITY (20.89 percent), and IMAGE (28.27 percent). It indicates that some apps in these categories keep consuming a large amount of traffic, when users switch to use other apps, or the screen-off traffic occurs with device sleeping . Hence, the background traffic of these apps could be necessary. Some apps can reasonably have background network activities. For example, the SYSTEM_TOOL management apps such as anti-virus apps often need downloading or updating activities at background. Compared to the data generated at background under Wi-Fi (abbreviated as WBD ), the data generated at background under cellular (abbreviated as CBD ) can bring potential loss of data plan for end-users. We demonstrate the CBD according to the app’s category in Fig. 13. The median values of CBD of COMMUNICATION and VIDEO apps are relatively higher than the ones of apps from other categories. The results provide strong evidence of “hidden” data drain generated at background. Hence, we focus on the average daily CBD of an app from all users that use this app. From our dataset, we have 2,697 apps that produce at least 2 MB daily CBD per user. Given the average daily CBD of 2 MB, the monthly data drain can reach up to 60 MB, which cannot be ignored for those who have rather limited data plan. Fig. 13. The background data traffic generated under cellular. The immediate take-away message of this study reminds end-users to alert or kill background network activities after launching an app. Indeed, there are various possible reasons why an app produces data drain at background. One reason is that ad-libraries are widely used in a lot of apps, and may download and update advertisements according to user contexts [34]. Another reason is that the background data drain is required by the apps’ features, e.g., VIDEO and NEWS_AND_READING apps may download contents and cache them locally. Such behaviors are reasonable under Wi-Fi, but are not desired under cellular, especially for users who have limited data plan. Last but not the least, the misuses or even malicious granting of network permissions cannot be neglected. For example, as found in our conference paper  [18], some TOOL apps such as flashlight and namecard scanning were detected to collect the location information of their users. However, it is really very difficult to justify whether the background data drain generated by an app is really necessary with respect to the functionalities of this app. To the best of our knowledge, some preceding efforts such as CHABADA [7] and WHYPER  [5] can help check whether the app’s granted permissions or behaviors are abnormal against their descriptions. However, existing efforts are not adequate to validate whether the background data drain is reasonable for apps, as background traffic is a dynamic behavior that can be monitored only at runtime. In addition, background data drain from in-app ads are needed for developers’ revenue, but end-users may be annoyed by such a loss and thus annotate low rating of the app or even uninstall it. Another immediate outcome of this study is that we can find some apps consuming exceptionally high cellular data at background. For example, on average, the GAME app “DJMax Ray” with 134 users generates the daily CBD of 58 MB per user, and the TRAFFIC app “ N5 Navigator” with 72 users generates the daily CBD of 43 MB. Undoubtedly, such a large volume cannot be ignored under cellular network, as end-users have to pay for the data plan. We believe that these apps can be an interesting genre for the research community to explore possible reasons why such substantial CBD occurs and assess whether the background data drain is “really” necessary. Finding (F7). Some apps can consume a considerable amount of traffic at background, but it is challenging to determine whether such dynamic behaviors are really reasonable or necessary. 7   Device-Specific Patterns The preceding measurements provide information on some patterns of app selection and network usage. However, the patterns are derived from the global distribution instead of the classification of users. It would be interesting to explore the diverse preferences of different users. In practice, the users can be categorized from different aspects, i.e., gender, age, country, and economic background. In this article, we select a different signal, i.e., the price of the device model that a user holds. Such a signal is motivated by two aspects. First, the price of a device model can generally reflect the hardware specifications of a device model when it is released onto market. Second, such a metric can somewhat imply a user’s economic background, which can influence the user behaviors at the demographic level. Indeed, such an assumption cannot be always reliable, as it cannot identify the users who buy second-hand “high-end” devices, or those who use two (or more) devices, e.g., one is low-end while the other is high-end. However, from a general trend, we can rely on the device models’ price as an indicator to classify users in our dataset. We then categorize the device models according to their on-market price, and revisit the research questions RQ 2 and RQ 3 , respectively. 7.1 Device Model Fragmentation First, we compute the distribution of the number of unique users that a device model has. The result is shown in Fig. 14. In the treemap of Fig. 14 a, each colored block corresponds to a specific device model, and its area’s size depends on the number of users. Given over 19,000 distinct device models in our dataset, such a visualized result indicates the heavy “fragmentation” of Android devices. In addition, for the distribution of users per device model in Fig. 14b, we can find that more than 83 percent device models have only fewer than 100 unique users, while about 1 percent device models own more than 10,000 unique users. Fig. 14. Heavy fragmentation of Android devices. Finding (F8). The on-market Android device models present a heavy fragmentation, i.e., more than 19,000 distinct device models in our dataset. 7.2 Device Model Clustering We then study the first-release price of a device model so as to classify users. In China, on the price systems of popular e-commerce web sites such as Jd, Amazon, and Taobao , the price of device models is usually segmented at every 1,000 RMB level, i.e., \leqslant 1,000 RMB, 1,000 RMB-2,000 RMB, 2,000 RMB-3,000 RMB, 3,000 RMB-4,000 RMB, and \geqslant 4,000 RMB. Hence, we roughly categorize the device models into three groups according to their on-sale price information that is published on Jd, i.e., the High-End (Z_$\geqslant$ _Z3,000 RMB, about 500 USD), the Medium-End (1,000 RMB-3,000 RMB, about 150-500 USD), and the Low-End (\leqslant 1,000 RMB, about 150 USD). We choose the top 500 device models according to their number of unique users, and manually check their price-history evolution on Jd as well as looking up some third-party data sources such as Dong-Dong14 and Xitie.15 Most of the device models were first released to market after 2012, and can still fall into the preceding coarse-grained groups as of May 1, 2014 (the starting time of our data set). Very few device models cannot meet this criterion, e.g., the first-release price of Galaxy S2 was 4,399 RMB, but the price fell down to about 2,700 RMB as of July 2014. For this case, we still categorize the device models by the first-release price. Luckily, only 19 exceptions out of 500 device models occur in our dataset. We list the categorization results in Table 3. TABLE 3 Categorization of Device Models  We then revisit RQ2 and RQ3 , by considering the choice of device models. We adopt the Spearman correlation coefficient.16 Statistically, the Spearman correlation coefficient is a non-parametric metric of statistical dependence between the ranking of two variables, X and Y . Such a metric assesses how well the relationship between two variables can be described, and it does not make any assumption on the distribution of the data. Commonly, the Spearman correlation coefficient is represented by the Greek letter \rho . For a sample of size n , the n raw scores X_i , Y_i are converted to ranks rg X_i , rg Y_i . Then, \rho is computed from the following: \begin{equation*} \rho =\frac{cov(rg_X, rg_Y)}{\sigma _{rg_X}\sigma _{rg_Y}}. \end{equation*} where cov is the covariance, and \sigma is the standard deviation. In our measurement, X represents the price of device models, and Y represents the usage patterns including the numbers of downloads, updates, and uninstallations, the traffic volume, and the access time, respectively. For each pattern, we compute the Spearman correlation coefficient with packages provided by Python. 17 The results are shown in Table 4. TABLE 4 The Spearman Correlation Co-Efficient of Every Singe App Category  7.3 Apps Selection Against Device Models We investigate whether the choice of device models can impact the app selection. From our previous study of the global distribution of apps [18], we find that users can have quite high overlap in selecting the popular apps, such as WeChat, QQ, and Map, by counting the number of unique users of these apps relative to the total number of users in the dataset. Hence, we explore the diverse requirements of apps. For simplicity, we cluster the apps according to their category information provided by Wandoujia, e.g., Game , NEWS_AND_READING . We compute the contributions of downloads and updates from every single device model relative to a specific app category. For example, if there are 1,000,000 downloads of GAME apps and 50,000 of these downloads and updates come from the device model Samsung S4, we assign the contributions made by this device model as 5 percent. Then we make the correlation analysis of app selection and the price of device, by means of the Spearman correlation co-efficient. We find that as the price of device models increases, the users are more likely to choose apps from the categories of TRAFFIC (r = 0.542, p = .000), LIFESTYLE (r = 0.565, p = .000), NEWS_AND_READING (r = 0.552, p = .000), SHOPPING (r = 0.659, p = .000), FINANCE (r = 0.655, p = .000), and TRAVEL (r = 0.719, p = .000). In contrast, the correlation analysis indicates that as the price of device models increases, the users are less likely to choose the apps from GAME (r = −0.707, p = .000) and MUSIC (r = −0.477, p = .000). Such observations imply that the choice of device models could significantly influence the app selections, and infer the characteristics and requirements of the users. For example, users with high-end smartphones are more likely to care about the apps from NEWS_AND_READING , FINANCE , TRAVEL , and SHOPPING . Users holding lower-end device models care more about the entertainment apps such as GAME and MUSIC . Finding (F9). The selection of device models has significant correlations with the selection of apps, implying the various user needs and requirements. 7.4 App Abandonment Against Device Models The uninstallation can indicate the users’ negative attitudes towards an app, i.e., the users do not like or require the app any longer. We then perform the correlation analysis in a similar way of downloads and updates. In most categories, the correlation is not quite significant. Although the uninstallation does not take significant correlation to the choice of device models at the level of app category, investigating the individual apps that are possibly abandoned by a specific device model is still meaningful, as such investigation can help developers identify some device-specific problems. To this end, we explore the apps that have been uninstalled for more than 500 times in our dataset, and obtain 6,736 apps. We then examine the distribution of uninstallations according to the device model. An interesting finding is that the manufacturer-customized or preloaded apps are more possibly uninstalled on the lower-end device models. For example, the app Huawei News Reader is a preloaded app on almost all device models produced by Huawei. This app has received 20,985 uninstallations, while 17,461 uninstallations come from the lower-end devices of Huawei. The similar findings can be found in other device models produced by Samsung, Lenovo, and ZTE. Such an observation implies that the lower-end users are less likely to adopt these customized or preloaded apps. Besides the preloaded apps, some apps are also more likely to be uninstalled by a specific device model. For example, two device models Samsung Galaxy S5 and Motorola Defy account for 72 percent of the uninstallations of an HD-Video calling app called CIPSimple (com.hh.csipsimple). Such a finding implies that these apps can probably suffer from device-specific incompatibilities or bugs. Although our current finding cannot tell the root causes for such abandonments, it can help the app developers better locate some “attention-needing ” device models where their apps may encounter possible loss of users. Finding (F10) . Some apps are more likely to be uninstalled on specific device models, indicating users’ negative attitudes towards preloaded or vendor-customized apps, or some potential device-specific problems. Such findings can help developers accurately locate some device models with more care. 7.5 Access Time Against Device Models Figs. 15a and 15b describe the distribution of daily access time at foreground under Wi-Fi and cellular, respectively. For access time at foreground, we are surprised to find that users rely less on the cellular network when the price of device model increases. In other words, the higher-end users typically spend less time under cellular network. For the average daily access time at foreground, the low-end users (\leqslant$ _Z 1,000-RMB device models) spend about 20 minutes more than the high-end users (Z_ $\geqslant 3,000-RMB device models) under cellular, while the high-end users spend 2 hours more than the low-end users under Wi-Fi. Immediately, we can infer that the network conditions could probably vary a lot among different users, i.e., the lower-end users are less likely to stay in the places with stable Wi-Fi connections. In contrast, the higher-end users tend to have better Wi-Fi connections. Fig. 15. Daily network activity comparison among user groups. We then investigate whether the choice of device models can affect the usage of “network-intensive” apps. Similar to the preceding analysis of the management activities, we compare the distribution of access time at foreground against the device models over each app category, under cellular and Wi-Fi, respectively. As shown in Table 4, the cellular time of apps has no significant correlation with the price of device models, except the categories of PRODUCTIVITY (r = 0.508, p = .000 ) and SHOPPING ( r = 0.640, p = .000 ). It is interesting to see that users holding lower-end smartphones are more likely to use EDUCATION (r = -0.382, p = .000 ) apps under cellular network. Such a finding suggests that a considerable proportion of lower-end users may be in-school students. Finding (F11). The selection of device models can have significant correlations with the spent access time under different networks. For example, higher-end device users heavily rely on Wi-Fi. In contrast, lower-end device users are likely to use more cellular than lower-end device users do. The correlation between the choice of device models and the access time at foreground under Wi-Fi does not seem be quite significant, either. Only in the category of SHOPPING (r = 0.590, p = .000 ), the choice of device models seems to take positive correlation with the price of device models. Such an observation can be expected, as higher-end users are supposed to have better economic background and spend more time on shopping. 7.6 Traffic Volume Against Device Models The distribution of daily Wi-Fi and cellular traffic consumption among device models is shown in Figs. 15c and 15d, respectively. Interestingly, although the higher-end users are observed to spend the least time under cellular network, they spend the most traffic. In other words, we can infer that the higher-end users are more likely to use those “traffic-intensive” apps. On average, a high-end user can spend 100 MB data plan more than a low-end user in a month. For carries such as China Mobile, such a difference of data plan can lead to 15-RMB extra data plan fee. It is quite meaningful to identify which apps consume more traffic on specific device models. Similar to the preceding analysis, we compute the correlation coefficients between the choice of device models from every single user and the apps on which the traffic is consumed. The cellular data traffic consumed over the apps from SHOPPING (r = 0.601, p = .000 ) and TRAVEL ( r = 0.589, p = .000 ) presents a quite significantly positive correlation to the choice of device models. In contrast, the correlations seem to be significantly negative in GAME (r = -0.429, p = .000 ) and MUSIC ( r = -0.411, p = .000 ) apps. In these app categories, users with lower-end smartphones tend to spend more cellular traffic. The traffic generated under Wi-Fi presents significant correlations with the device models in some categories. The lower-end users tend to spend a large number of Wi-Fi traffic on the VIDEO apps. In contrast, the higher-end users are more likely to rely on the apps of COMMUNICATION , PRODUCTIVITY , SYSTEM_TOOL , TRAVEL , BEAUTIFY , NEWS_AND_READING , LIFESTYLE , and SHOPPING under Wi-Fi. Such a difference in the Wi-Fi traffic usage can indicate the requirements and preferences of users holding different device models. 7.7 Competing Apps Against Device Models Finally, we study how the choice of device models impacts the selection of “competing” apps with the same or similar functionalities. We choose three typical apps: News Reader , Video Player , and Browser , as these apps are observed to be commonly used in daily life. For each app, we select the top apps according to the access time at foreground that the users spend on them. The reason why we employ the access time at foreground instead of the number of downloads is that access time at foreground can be computed only when the users interact with the app. The selected competing apps are as follows. The News Reader contains Phoenix News, Sohu News, Netease News, Today’s Top News, and Tencent News; the Video Player contains QVOD, Lenovo Video, Baidu Video, Sohu Video , and iQiyi Video; the Browser contains more apps, i.e., Chrome, UC Web, Jinshan, Baidu, Opera Mini, Sogou, Aoyou, FireFox, Tencent, and 360. First, we want to figure out the distribution of the user preferences against the app according to the device model. We employ the cumulative distribution function (CDF) to demonstrate such distributions, as shown in Fig. 16. For each app, the X -Axis represents the price of device models that are sorted in the ascending order, and the Y -Axis refers to the percentage of the app’s users holding such a device model. An app tends to be used by more higher-end users if the curve is close to the right bottom. Fig. 16. Similar app preferences among user groups. We can observe that the choice of device models significantly impacts the selection of competing apps. For the News Readers, we can see that the Phoenix News and Netease News are more likely to be adopted by higher-end users. In contrast, the Sohu News tends to be more preferred by the lower-end users, possibly because Sohu is famous for its entertainment channels in China. The impact of device models is even more significant for the Video players. The Lenono Video takes a very significant difference compared to other 4 apps, indicating most of its users are lower-end. One possible reason is that the Lenovo Video is a preloaded app that is used mainly on smartphones manufactured by Lenovo, and most of these smartphones are categorized into medium-end and low-end groups. Finally, in the Browser group, the similar findings can be observed. The most preferred browser of higher-end users is the Chrome browser, followed by the FireFox browser, and the Jinshan browser. The Opera Mini and the Baidu browser are more likely to be adopted by the low-end users. One reason leading to the diversity is that an app can provide specific features beyond common functionalities that its competitors also provide, so as to meet requirements of specific user groups. For example, when examining the textual profile of the ten browsers, we find that the Opera Mini is said to save traffic by offloading computation onto cloud. As a result, more than 77 percent of its users are those who hold medium-end and low-end devices. In summary, such a finding implies that the choice of device model has impacts on competing-app selection and usage, and probably reflects the different user interests and needs . Finding (F12). Users holding different device models could have various needs of specific apps or preferences against some “competing” apps. For example, lower-end device users prefer the Opera Mini browser as it is said to save traffic. 8   Implications and Suggestions So far we have investigated the user behaviors from a large-scale dataset and inferred some patterns. Besides confirming and validating some findings that have been reported in previous studies based on a relatively small scale of users, our study results can further imply some new, open challenges and opportunities of the development, maintenance, and management of apps. In this section, we discuss some implications and suggestions that can be taken away by relevant stakeholders in the mobile-app ecosystem, including app-store operators, app developers, end-users, and network-service carriers. For ease of presentation, we denote the preceding findings as \mathbf{F1, F2, \ldots, F12} , and discuss the problems and opportunities resulted from these findings. 8.1 Efficient App-Store Management App stores play as the core in the whole app ecosystem to connect all related stakeholders. We provide some implications to improve the recommendation quality and performance of an app store. In particular, we intend to point out some gaps that may not be well addressed by app-store operators. 8.1.1 Improving Workloads of App Stores From the preceding macro-level finding of popularity distribution (F1 ), we can conclude that the app popularity generally follows the “Pareto-like” principle. Such a finding confirms the results reported in previous work [28]. In addition, we also validate that the numbers of downloads, updates, and unique users per app can follow the power law. Indeed, the distribution of user requests to servers can help optimize the workloads of servers for better content delivery. However, to the best of our knowledge, so far very few published papers have comprehensively discussed the current practice of managing an app store’s workload [35]. One reason can be the lack of actual user-request traces of app stores. A recent study  [2] assumed that the time submitting a user review can be an immediately subsequent behavior of downloading an app. Based on such an assumption, this effort clustered the apps whose comments were posted in a given time interval, and simulated the possible workload for predicting which apps will be downloaded. However, such an assumption may not be always reliable. It is reported that users may not write reviews when/after downloading an app [3]. Additionally, users may not immediately submit their reviews when downloading an app, but submit their reviews after they try or use the app sometime later. Although we do not have knowledge of how app stores other than Wandoujia design their workload models, the traces of app download and update activities can help us improve the current design of the Wandoujia app store’s workload, e.g., placing the .apk files of most popular apps in the server-side fast memory or local network cache. In practice, Wandoujia currently relies on the derived power law to place the .apk files on its Content Delivery Network (CDN) servers. We then simulate the workload performance from the server’s point of view, by synthesizing power law and co-installation patterns. For simplicity, we assume that the size of each app is the same, and assign the size of cache as the number of apps. Certainly, the size of every single app can vary in practice. We consider the following three conventional cache schemes, and simulate these schemes by replaying a successive one-week request traces of app download and update. We use request traces of the final week (September 24-September 30) in our dataset. The first mechanism is the power-law-driven static finite cache. At day zero, we fill the cache with the .apk copies of the most popular apps. We populate the static cache with the apps accounting for the most downloads and updates in Fig. 2a. We use the power-law exponent (1.69) for overall app ranking against the number of downloads and updates, and obtain 8,260 apps accounting for 95 percent of downloads and updates. The static cache is not changed during the one-week trace period. Such a cache is rather similar to the current design of Wandoujia, i.e., only the .apk files of the most popular apps are put in the cache, and the cache size is not tuned very frequently. The second mechanism is the dynamic infinite cache. At day zero, the cache is filled with all apps that are ever requested before day zero (not limited to the 0.28 million apps in our dataset but those who had ever existed on Wandoujia server), and thereafter accumulatively stores any other apps requested during the trace period. The third mechanism is the co-installed and LRU-based finite cache. At day zero, the cache is populated with the top pairs of co-installed apps (before day zero), whose Jaccard co-efficient exceeds 0.01. Suppose that two apps \mathbb {A} and \mathbb {B} both have 2,000 unique users, and such a value indicates that they have 20 shared users. As the co-installation pattern essentially implies the locality of accessing two apps, it indicates that these two apps are accessed subsequently for 20 times. After day zero, we compute the daily top co-installed apps whose Jaccard coefficient is over 0.01, and replace those who are “Least Recently Used ” at the previous day. In Table 5, we report the cache size (the number of apps placed in cache) and hit ratio of each cache design. Note that the “hit ratio” refers to how many requests are exactly matched instead of the matched apps. Undoubtedly, the dynamic infinite cache reaches the highest hit ratio, but the cache size is tremendously large as well. In contrast, we can see that the simple static finite cache with the top popular apps can reach a very high hit ratio (around 95 percent). In contrast, we see a significantly lower hit ratio (around 80 percent) for the cache built upon the co-installation with LRU. It indicates that, although the typical LRU cache can exploit locality of user downloads and updates to improve the performance, it does not work very well with the co-installation patterns. Some new replacement mechanisms should be exploited to fit the behaviors of co-installation. TABLE 5 Synthetic Cache Performance  To balance the hit ratio and the cache size, we then aim to improve the LRU mechanism for app-store workloads by synthesizing the knowledge of power law and co-installation. We design a cache mechanism called adaptive hybrid finite . At day zero, the cache is first populated with the top popular apps just like the static cache, and such a space will be tunable during the trace period. At day one, the cache reserves space for the top popular apps, and allocates some extra space for two kinds of apps: (1) the apps from the daily popular apps accounting for 95 percent of downloads and updates; (2) the apps from the daily app pairs whose Jaccard co-efficient is over 0.01. If an app appears in either of the preceding cases, we assign its cache for only once. After day one, the cache daily reallocates the cache by replacing the apps that were “Least Recently Used ” at the previous day. From Table 5, we can find that this mechanism requires the cache size up to around 15,000, which is nearly doubled compared to the static cache and co-installated LRU-based cache . However, the hit ratio can be significantly improved to over 97 percent. Such a hybrid cache is efficient, as it compensates the limitations of co-installation patterns that can miss some very popular apps but preserves the benefits of locality. Suppose that the average size of .apk file is about 10 MB, and such a cache usually requires about 150 GB space for one CDN node. As there can be various concurrent requests for the same app, we need to assign more copies for the most frequently accessed apps, and the actual cache size should be a bit larger. Indeed, the preceding simulation can be further explored by leveraging the diurnal patterns reported in F2 . The cache can be more dynamically adapted when more concurrent requests arrive at the server at a fixed point, e.g., 9:00 pm in our preceding findings. In practice, app-store operators can optimize the cache adaptation on CDN servers for specific areas. In addition, we plan to further explore the timestamp of user download/update sequences for possible temporal patterns of accessing the server. In this way, the popular apps or the most probably “co-installed” apps that have not been downloaded by the user can be prefetched to a nearby place so as to improve user experience and app-delivery performance. We plan to test these workload mechanisms on Wandoujia and evaluate the efficiency in real-world practices [36]. 8.1.2 Improving App Recommendation The fundamental responsibility of an app store is to recommend proper and high-quality apps to end-users. A recommendation system can benefit apps and developers by identifying the apps that need to be recommended to increase their popularity, and by identifying users’ interests in the respective app category. If an app is downloaded by most users in a group, then it is likely to be of interest for another user (in the same group) who has not yet downloaded it. In this way, finding F3 can suggest the most “co-installed” pairs of apps that can be pushed to users. Additionally, finding F3 also suggests that some frequently co-installed apps may come from the same category and vendor. Although such a pattern could be reasonable from the vendor’s aspect and thus increase downloads, it may not be always desirable or necessary for end-users, as app vendors can purposely “induce” the bundling downloads of some apps. Another useful implication for app recommendation can be derived from F9 , i.e., users with different device models can have quite various preferences towards apps with similar functionalities. For example, lower-end users prefer the “Opera Mini” browser while higher-end users prefer the “ Chrome” browser. However, based on our investigations of the most popular app stores such as Wandoujia, Google Play, and Tencent, device-specific recommendation is not well explored. As most app-store operators can gather information of device models, our finding can help achieve more accurate recommendation of apps for specific users. From F1 , we can also detect that some possibly “fake” downloads can exist by the metric of “average download/update per user .” For example, an app has only 18 unique users but receives 3,581 downloads & updates, and 3,563 downloads come from only one user. This metric can be generally applied for any app stores that collect similar information. The apps that are downloaded by a limited number of unique devices in a short interval could be alerted to app-store operators and end-users. 8.1.3 Predicting the App Ranking An immediate result can be taken away from our study is the “I/U” ratio and the lifecycle of an app presented in F4 . Indeed, we do not have the knowledge of other app stores except Wandoujia. Previously, Wandoujia used to rank an app according to the number of its downloads. However, only the download count can possibly prioritize the ranking of an app with “bursting” downloads in a short time interval. When adding the uninstallation count, Wanoudjia now leverages the “I/U” ratio and the lifecycle to more accurately evaluate an app. Indeed, other app stores recording uninstallation information can extrapolate the ranking accordingly. Generally, the sentiment of an app’s user ratings can reflect its users’ attitudes towards the app, and may provide useful information for app ranking. As a result, the rating of an app can influence the user downloads and thus is quite significant to the revenue of the app developers and the ranking strategy of app-store operators. In a previous study [32], it was observed that an app can probably gain higher ranking if it receives a number of “good” ratings. However, from F5 , we are surprised to find that such an observation may not always hold for large-scale users, at least in China. Additionally, we find that the abandonment of apps does not have strong correlation with the number of “bad” (negative) ratings. Such findings enable us to design new indicators for predicting an app’s popularity. An important understanding about user activities is that the activities are not independent but always appear as sequences of events. Indeed, when search engines utilize user activities as the implicit feedback about the relevance of documents, the sequences of actions are usually more indicative than single clicks. Analogically, we anticipate that some sequential patterns of the app-management activities may be better indicators of app quality than downloading actions alone. In practice, our recent work [31] made the first step to mine some sequential patterns of app-management activities that are actually correlated with online ratings of apps. Multiple time-aware management sequences were combined with several machine-learning algorithms such as Lasso Regression (Lasso) [37], Random Forrest Regression (RF)  [38], and Gradient Boosted Regression Tree (GBRT)  [39]. From the activities, we derived some patterns that are surprisingly accurate to be used to produce the general rankings of an app, and may be used to effectively predict those new and high-quality apps. Given that the user reviews are sparse or even do not exist in some apps, such an effort can be useful for both app-store operators to rank an app, and for app developers to estimate the potential revenues. Due to page limit, we do not discuss more details in this article. However, this experience shows that “mandatory” app-management activities can be a promising metric to reflect user attitudes and intensions. It would be interesting to further apply recent deep learning techniques over our user-behavioral data. 8.2 Avoiding Unexpected Cost From the finding F7 , it is observed that the background data traffic pervasively exists, and may not be always reasonable with respect to the app’s claimed functionalities. More seriously, F6 suggests that most apps can keep “long-live” TCP connections at background. This finding indicates that currently most Android apps could stay silently in memory even when they are switched to background. The background network connection can be useful, as the app can be quickly “awaken” and the network connection can be fast restored. However, these apps can also lead to memory occupation and have side effect of system performance. In fact, it is reported that many Android users complain that their devices become slower and slower to respond to user interaction [40]. It would be interesting to explore whether such “long-live” network connection could be a factor. In addition, our recent work  [41] made an empirical study of 1,000 apps from three third-party app stores (including Wandoujia, Baidu, and Tencent), and surprisingly found that some apps can have “collusion” behavior, i.e., they can awake one another at background but users are never aware of such behavior. The hidden cost of collusion behaviors, including computation resources and energy drain, can be too significant to be ignored. Hence, such finding reminds that end-users should employ some tools to periodically “clean up” their devices or terminate threads of unused apps running at background. The findings also imply that these apps can bring hidden and unexpected cost for users, such as the loss of data-plan traffic and energy. The findings call for efficient solutions to determine whether the undesirable cost is really necessary for an app. Preliminary efforts revealed that the undesirable cost can originate from the improper granting of permissions [5], the use of third-party ad-network libraries [9], or the unreasonable API usage  [7], etc. However, only static analysis is not sufficient  [42]. Determining whether the additional cost is “really ” unreasonable or malicious is quite difficult. For example, the additional data drain may not be always “purposely” malicious, as the developers may want to collect some information such as location in order to push context-aware ads. A possible way can be combining the user-behavioral data with other analytic techniques such as natural language processing, static code analysis, library-dependency analysis, and network-trace analysis, to deeply understand the app semantics and comprehensively evaluate the cost against the functionality of apps. Additionally, it would be also appreciated to provide lightweight system services that can identify the cost of the necessary functional features from other features, respectively. Hence, we can display such information to end-users who launch the app, and let themselves decide whether to prevent some unnecessary cost. However, such a “ separation-of-concern ” solution is not easy and must be performed very carefully, because preventing some features may also affect the normal functionality. Our findings can provide a genre of apps that have substantial background traffic drain, and researchers can focus on these apps for further study. In practice, the findings have already motivated us to apply the metric of CBD in the current Wandoujia app store. We can help identify some “suspicious” apps to which both app-store operators and end-users need to pay attention, because users may suffer from a lot of unnecessary loss of data-plan traffic and the possible overhead leading to unexpected CPU and energy cost [40]. For example, our previous study [18] found that an alarm clock app daily consumes about 13 MB CBD and an LED flashlight app daily consumes about 7 MB CBD . These apps were put onto the watchlist of the Wandoujia app store, and then evaluated more comprehensively and rigorously. In fact, some of these apps have been forced off the shelf accordingly. Additionally, the undesirable cost of apps suggests that developers should optimize their apps, because users can give low rating and even abandon these apps when perceiving the undesirable cost. Developers need to configure the use of network connection at a finer granularity, e.g., disabling background data transfer under cellular. Developers also need to explicitly make end-users aware of the potential cost, e.g., popping up alert information at the installation wizard or displaying on the app’s information webpage. 8.3 Addressing Device-Specific Features From F10 , we can find that some apps are more frequently uninstalled on some device models. Such a finding implies that there may be some problems such as compliance with hardware, device-specific drivers, or API evolution. As reported on the StackOverflow [43], some camera-related bugs have been found on Samsung Galaxy Note2. Indeed, the finding currently cannot comprehensively trace the root causes of these problematic issues; however, we can at least locate a genre of device models and help developers explore possible device-specific problems or bugs, especially for apps that have sparse user reviews and cannot be well amenable to existing techniques [44]. Other than knowing only the device-specific problems, app developers have to face the challenge introduced by Android fragmentation. According to our finding F8 , there are more than 19,000 device models in our dataset. Such a fragmentation brings significant challenges to software engineering practices for mobile apps, such as app design, development, maintenance, quality assurance, and revenue generation  [45], [46], [47], [48]. A recent study [49] (in 2013) showed that 94 percent of mobile-app developers who avoid the Android platform cite fragmentation as the main reason. Android app developers need to identify the major device models out of the wide selection space to validate an app’s functionality, usability (such as the GUI effects), and even revenues. As a result, prioritizing device models can be an important topic, especially before the release of an app. Developers need to buy some device models or use some cloud-based emulators such as Appthwack 18 and Testin.19 In fact, due to the lack of usage data, most developers currently rely on popular device models from public market-share reports such as AppBrain [50]. However, such reports are too coarse-grained to be precise enough, and the major device models for different individual apps can be quite different. A recent study [47] showed that major device models from which the users may post positive/negative reviews of a specific app can vary a lot. Given very limited resources to buy device models or cloud services, developers have strong desire to more accurately locate and invest those major device models. As our dataset contains detailed usage data of an app per distinct device model, we can design some prediction techniques for prioritizing device models. In practice, our recent PRADA work  [51] made a first step to prioritize Android device models for individual apps, based on mining large-scale usage data from our dataset. PRADA adapts the concept of operational profiling  [52] and includes a collaborative filtering technique to predict the usage of an app on different device models, even if the app is entirely new (without its actual usage in the market yet), based on the usage data of a large collection of apps. Compared to the coarse-grained market-share based metric, PRADA can accurately predict the device models where a new app could be used. In addition, the preceding analysis indicates that the distribution of uninstallation behaviors of an app among device models can be a more objective indicator compared to user reviews or ratings. We are currently extending the PRADA approach to prioritize device models where an app is more likely to be uninstalled. 8.4 Addressing Various Requirements Our findings F9 -F12 demonstrate that users holding different device models may have quite various requirements. Given that a device model used by a user can imply possible preferences of the user to some extent, the diversity of user needs towards apps must be further explored. First, the preference of selecting “routine apps” can be diverse. For example, the users holding higher-end device models tend to more use the FINANCIAL and NEWS_AND_READING apps, while the users holding lower-end device models tend to more use the GAME and EDUCATION apps. The developers have to identify which users are more worth focusing on, and provide optimal services or customized features. Second, the preference of selecting “competing apps” can be diverse. Here, we refer to the “competing apps” as the apps that have similar functionalities. For example, the users holding lower-end device models prefer the Baidu browser and the Opera Mini browser (rather than the Chrome browser). We inspect the textual profiles of these two browser apps to identify attractive features provided by the apps. For example, the Opera Mini browser claims that it can save the traffic by compressing the content and resizing the images in a front-end cloud before the page is loaded on the user side. Such a diversity can motivate the developers to address more personalized features in their release planning so as to retain the user base. Note that F9 to F12 are derived from only the “device-model” based categorization of users. In practice, the users can be categorized in different ways, e.g., by region, sex, and age. Additionally, the diversity of requirements can be more complex. As a result, addressing the various user requirements is challenging. The traditional product-oriented software delivery model often provides a large number of features that aim to meet all potential user requirements. However, mobile apps are published and delivered in a quite different “user-oriented” model. There is a strong need of new requirements-engineering approaches to better explore the specific requirements of a user. One of the significant issues is to build up a precise, fine-grained, and extensive user profile. To this end, an important trend in requirements elicitation is to rely on the comprehensive data analytics of user reviews, bug reports, social networks, and app stores such as our Wandoujia dataset. In addition, as the release of new versions or features of an app can be quite frequent, e.g., weekly or monthly, the gap between requirements elicitation and app development needs to be greatly shortened. If the app developers can know that some users holding a specific device model spend more cellular time rather than Wi-Fi, the app developers can provide some customized features to these users. To this end, some approaches such as end-user configuration/programming and context awareness can be useful. For example, apps can be developed to enable end-users to manually “turn-on” and pay for only the features or contents that the users need (commonly implemented via in-app purchase), or to disable the downloading of contents under cellular network. Also, an app can prefetch contents under Wi-Fi network for the lower-end users who may have limited cellular data plan. 8.5 Exploring Potential Revenues from App A large number of popular apps, especially Android apps, are free instead of paid ones. Mobile apps heavily rely on other revenue channels such as in-app purchase and ads, especially for specific types of apps (e.g., game and media apps). App developers should accurately target the users who are likely to use/buy their apps and increase potential revenue. For example, finding F10 suggests that the users holding lower-end device models are more likely to select GAME apps. As it was reported that a lot of GAME apps are paid ones  [14], the GAME app developers could provide the “ try-out-first ” feature to encourage these lower-end users to use their apps. Indeed, in practice, a lot of Game app developers have adopted such a strategy, i.e., making the first-level play free. Besides the “try-out-first” strategy, our findings can further recommend app developers how to better place their in-app advertisements, which are the major revenue channel for mobile apps. From finding F11 , the choice of device models can correlate to the network access time, so app developers can know which users more possibly spend time in their apps. From the experiences of advertisement on the Web  [53], longer staying time usually implies more possibility to click advertisements. In this way, developers can more precisely target potential users and increase ads-clicking opportunities. Here, one feasible way is to negotiate with the device manufacturers and make their apps preloaded on these devices. Furthermore, by learning the access-time usage among different device models from F11 , our recent PRADA work [51] has designed efficient machine learning techniques to predict which users are more likely to spend more time on a specific app. In addition, although we cannot make very strict hypothesis testing, it might be a common sense that the choice of device models can possibly reflect the economic status or other background of users. For example, users with a better economic status are more likely to use higher-end device models. In this way, developers can further provide more “personalized ” advertisements to fit the users’ interests. Similarly, ad network providers can also leverage our findings to know which apps a specific group of users are more likely to spend time on, i.e., users with higher-end device models tend to use the NEWS_AND_READING and TRAVEL apps. Therefore, ad network providers can negotiate with these developers to import their ad-network libraries. We can also observe that the users holding lower-end device models are more likely to pay more traffic on the MUSIC apps under the cellular network. Given that these users may have relatively low economic background, their data plan could be a bit limited. However, the network service providers can leverage this finding and provide “app-specific ” data plans. For example, some carriers in China make a special data-plan contract with the MUSIC apps (such as Baidu Music com.ting.mp3.oemc.android and Kuwo Music cn.kuwo.player) and video apps (such as Youku com.cibn.tv), and users can pay for this data plan independently and enjoy unlimited cellular data-traffic to download video/audio files. Indeed, such a new “app-specific” data plan requires supports such as independent traffic accounting. 9   Limitations and Discussions As an empirical measurement study, considerable care and attention should be given to ensure the rigor. However, as with any chosen research methodology, it is hardly without limitations. In this section, we discuss about the major limitations and threats to validate and generalize the results of our study. Single Dataset . The first limitation is that our dataset is collected from only a single app store. Such a limitation may have introduced some selection biases caused by the app store’s specific policies. In this way, some of our results, such as the popularity distribution of apps, may not always hold in other app stores. For example, the same app can be ranked to be quite popular on the Wandoujia app store, but may be unavailable on other app stores such as Google Play. In addition, the features provided on different OSes can be various. One feature of an Android app may not be available in the corresponding iOS and Windows version of this app. As a result, care should be given to generalize our findings to other app stores or platforms. To address this issue, we plan to validate some results by leveraging the public data such as the number of downloads, user ratings, and reviews from other app stores such as Google Play and Apple AppStore. For example, for a specific app, we can investigate the difference of user attitudes towards the same app. However, the information of network traffic, online time, and device model, cannot be easily captured on other app stores, and thus the limitation caused by a single app store cannot be completely solved. In a sense, due to the uniqueness of our dataset, some potentially useful results can be further leveraged by the research community. Demographical Differences . Another major limitation is the demographical differences. The users under study are mainly from China, so the regional differences should be considered. For example, it is reported that users from different countries can perform variously in giving reviews against apps  [3]. However, the same limitation also exists in most of previous studies that were conducted over users from a specific region, e.g., the study conducted on some states in US  [28]. In practice, collecting multi-dimensional usage data of large-scale users from various regions can be quite difficult. As our study was made over millions of users, the derived findings can still be useful. In addition, we can find that the usage patterns from only Chinese users can share commonalities with the users from other countries. For example, the power law distribution of app popularity and the user interests of co-installed apps, could also exist in other app stores, and can be further generalized to improve the design of app/content delivery. Time and Versions . Another mentionable limitation is the time sensitivity. Because the analysis of our conference version [18] is done on only one-month usage data, some of our findings may not be promisingly generalized to the latest released versions of some apps. It is well-known that the mobile apps are updated very frequently, and some potential bugs of some apps, e.g., misuse or over-granting of network permissions, might be already fixed in the up-to-date app versions. Indeed, we realize that the one-month data is not sufficient to comprehensively capture app quality and user behaviors. As a result, this article employs a new extensive five-month dataset to reproduce the research questions, and most of results keep consistent. In this way, the time limitation is somewhat alleviated. However, some limitations need to be further addressed, e.g., the user ratings are not identified to a specific version of an app. It is possible to align the time when a rating is committed to an app with the release time of this app, so that one can determine whether the ratings are given to a specific version. However, in fact, on some app stores, one can arbitrarily commit ratings to an app, even though he/she doesn’t install the latest version of this app. In the future, we plan to consider adding other data sources such as the bug reports and textual user reviews to judge the user attitudes of a specific app version. User Classification . We categorize users according to the device models they hold, or more specifically, the price of device models. Indeed, such an indicator cannot be always reliable for some users. It is true that we cannot well address some limitations, including the users holding second-hand or multiple device models and the inaccurate estimation of device model’s prices regarding the release date. To alleviate the threats, we employ various sources to segment the device models into coarse-grained ranges. Given the large-scale of users involved, such a classification can possibly reflect the diversity of usage patterns. We plan to explore other classifiers such as screen size estate or resolution level, by which the derived knowledge could be particularly useful to GUI designs of an app. Free Apps and Paid Apps . As mentioned previously, the apps on Wandoujia are all free. Certainly, it would be possible that user behaviors on paid apps are a bit different [20] . Unfortunately, our current dataset inherently cannot address such a limitation. Correlation versus Causation . We made various correlation-analysis studies, such as ratings and the number of downloads/users, the user ratings, the network usage, and the choice of device models. It should be noted that not all of the analysis results can be fully interpreted. In other words, these analyses have only correlation instead of causation . Indeed, comprehensively interpreting causation is often rather difficult for most empirical research studies. However, correlation is the first step ahead of causation, and is very meaningful to cause the focus from relevant stakeholders. As presented previously, one goal of this article is to motivate the related researchers in exploring more opportunities, such as finding the underlying “causation” and even proposing new solutions. 10   Related Work The prevalence of mobile apps significantly changes software development, deployment, delivery, maintenance, and evolution. Supporting mobility becomes a promising trend in software engineering research  [1]. In the past years, various efforts have been made, covering almost all lifecycles of apps, such as requirement analysis [3], [4], [18], code/library analysis  [5], [6], [7], [8], [9], [54], version evolution  [10], and a number of system-level supports  [11], [12], [55]. In a sense, empirical studies of user behaviors can be quite useful to the software engineering research of apps. Understanding user behaviors of mobile apps establishes a foundation for different stakeholders in the research community of mobile computing, e.g., app developers, network providers, app-store operators, and OS vendors. A plethora of empirical studies have been made from different perspectives. Understanding User Behaviors by Field Studies. Given that collecting large-scale user data is hardly feasible for most studies, learning user behaviors by field studies is always a straightforward way. A lot of studies were performed over specific user groups. Rahmati et al. [14] , [56] performed a four-month field study of the adoption and usage of smartphone-based services by 14 novice teenage users. Tossell et al. [15] applied a naturalistic and longitudinal log-based approach to collect real usage data from 24 iPhone users in the wild. Sani et al. [33] collected data from 387 Android users in India, where users pay for cellular data consumed, with little prevalence of unlimited data plans. Falaki et al. [4] found that web browsing contributed over half of the traffic at that time (2010), but currently users can enjoy more Web services via apps. Using detailed traces from 255 volunteer users, Falaki et al. [16] conducted a comprehensive study of smartphone use and found immense diversity of users, by characterizing intentional user activities. Lim et al. [3] made a questionnaire-based study to discover the diverse usages from about 4,800 users across 15 top GDP countries. Yan et al. [57] developed and deployed an app to collect usage logs from over 4,600 users to find their similar interests and explore recommendation systems for smartphone apps. For a study closely related to ours, Xu et al.  [28] presented usage patterns by analyzing IP-level traces of thousands of users from a tier-1 cellular carrier in U.S. They identified traffic from distinct apps based on HTTP signatures and present aggregate results on their spatial and temporal prevalence, locality, and correlation. Some field studies were made on specific apps. Böhmer et al. [58] , [59] made a field study over three popular apps such as Angry Bird, Facebook, and Kindle. Patro et al. [60] deployed a multiplayer RPG app game and an education app, respectively, and collected diverse information to understand various factors affecting app revenues. Mining App Store Data. Some types of app related data like user reviews, star ratings, and like/dislike voting are publicly accessible. Chen et al. [61] presented AR-Miner to extract informative user reviews and group them using topic modeling. Fu et al.  [13] presented WisCom, a system that can analyze tens of millions user ratings and comments in mobile app markets. Petsas et al. [2] monitored and mined four popular third-party Android app stores and showed that the app-popularity distribution deviates from a commonly observed Zipf-like model. User reviews are significant assets in software engineering research. Lorenzo et al. [44] presented CLAP to make release planning based on clustering the meaningful topics from user reviews and aligning these topics to developers’ bug reports. Predicting Apps Usage. Baeza et al. [62] made field studies on the sequence of launching apps, and provided a solution to predict the “next-to-be-used” apps. Shin et al. [63], [64] collected a wide range of smartphone information from 23 users, extracted and analyzed features related to app prediction. Liao et al. [29], [65] proposed a temporal-based app predictor to dynamically predict the apps that are most likely to be used. Montoliu et al. [66] presented a framework to discover places-of-interest from multimodal mobile phone sensory data. Do et al.  [67] presented a framework for predicting where users will go and which app they are to use in the next ten minutes from the contextual information collected by smartphone sensors. Compared to these studies, the major differences of our study include the unique dataset covering millions of users, some unique information such as app installation, uninstallation, and diverse network usage. Although Chinese users take up majority of all users in our dataset, we believe that behavior patterns inferred from millions of users under study should be more comprehensive than those from volunteers. With our dataset, we also validate some results that were reported over a smaller scale of users. For example, a small set of apps account for substantial portion of downloads [2] and unique users  [28], some apps are more likely to be installed together [4], [16], [28], and some functionality-similar apps may vary in terms of performance [33]. However, besides using a different dataset collected from millions of users, our study explores uniquely new findings that were unaddressed previously, but useful for software engineering towards mobility: First, we make multi-dimensional measurement of app popularity from various aspects including downloads, users, and diverse network activities. Such a measurement can present a comprehensive ranking of apps rather than download times only, and can help improve the quality of app recommendation. Second, we explore which apps are likely to be uninstalled and the lifecycle of the abandoned apps. In particular, we report the inconsistency of user ratings and the management activities of apps. The results can help improve the quality assurance of apps. Third, beyond reporting the co-installation of apps, we further explore the possible reasons why these apps are selected together. The results can be helpful to improve recommendation quality and explore new value-added apps. Fourth, we make a fine-granularity analysis of network activities to identify the “network-intensive” apps and “problematic” apps that consume traffic at background. Such results can be very significant to identify possible bugs or problems while reducing the threats to end-users. Finally, we study the impact by the price of device models, and explore how it impacts on user behaviors on apps usage. Such results can help address various requirements of users, and increase the potential revenue of apps. 11   Conclusion We have conducted a systematic descriptive analysis of a large collection of app-store service profile from millions of Android users. Diverse usage patterns are with respect to the aspects such as app popularity, app management, app selection, app abandonment, network usage, and device-specific preferences. Our findings provide implications for various stakeholders in the mobile app ecosystem, and cover a number of issues for app development, deployment, delivery, revenue, evolution, etc. Indeed, this article mainly focuses on the descriptive analysis of the data. However, we believe that this article can make the initiative step for the research on data-driven software engineering of mobile apps. Many findings of the analysis lead to potential research questions or opportunities. In fact, some research topics such as optimizing an app store’s performance, predicting an app’s quality and popularity  [31], [68], and prioritizing the fragmented Android devices for specific apps [51], have already been explored based on our dataset. Along with opening our dataset in this article, we expect that we can contribute a valuable resource for the research community, and promote the development of new research topics to benefit researchers, practitioners, and users. Footnotes 1. Part of our dataset has been released along with the publication of our IMC 2015 paper  [18], and one can find the brief description from http://sei.pku.edu.cn/~liuxzh/appdata/. The dataset can be requested by the researchers who have the IRB approval. 2. http://www.wandoujia.com. 3. Visit its official site via http://www.wandoujia.com. 4. The authentication system of Wandoujia checks whether uploaded apps contain illegal contents and performs basic anti-virus tasks. 5. Apps not belonging to any category are annotated as “MISCS”. 6. http://www.chinatechnews.com/2014/05/07/20496-chinese-android-app-store-inks-deal-with-japanese-messaging-app. 7. In the Wandoujia management app, a pop-up of installation wizard is presented to users when an app is successfully downloaded or updated. So we simply treat “download” and “installation” equally, if not specifically distinguished. 8. In fact, the access to Google Play is currently banned in China. 9. Essentially, these app stores are also native apps that can be monitored by Wandoujia. 10. One co-author serves as a co-founder and the current CTO of Wandoujia. He supervises the process of data collection and de-identification. 11. More specifically, we use the Fit function provided by the powerlaw package. More details can be found at https://pypi.python.org/pypi/powerlaw. 12. On the Android system, some apps can notify the users the release of updates and navigate them to directly download the updates from their websites rather than an app store. Such a behavior cannot be captured by our dataset. 13. We employ the regplot function provided by Python’s seaborn package, where one can control the size of bins by the parameter x_bins . 14. https://itunes.apple.com/us/app/dong-dong-gou-wu-zhu-shou/id868597002?mt=8, is an app for inquiring history price of products on Jd. 15. http://www.xitie.com, is a website for inquiring price history of products on popular e-commerce sites. 16. Spearman Correlation Coefficient. https://en.wikipedia.org/wiki/Spearmanpercent27s_rank_correlation_coefficient. 17. https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.spearmanr.html. 18. https://appthwack.com/ . 19. Testin. http://www.testin.cn. Acknowledgments This work was supported by the National Basic Research Program (973) of China under Grant No. 2014CB347701, the Natural Science Foundation of China (Grant No. 61370020, 61421091, 61572051, 61528201, 61529201). Tao Xie’s work was supported in part by National Science Foundation under grants no. CCF-1409423, CNS-1434582, CNS-1513939, CNS-1564274. Qiaozhu Mei’s work was supported in part by the National Science Foundation under grant no. IIS-1054199 and an MCubed grant at the University of Michigan. Xuanzhe Liu is the corresponding author of this work. References  [1]G. P. Picco, C. Julien, A. L. Murphy, M. Musolesi, and G. Roman, “Software engineering for mobility: Reflecting on the past, peering into the future,” in Proc. Future Softw. Eng., 2014, pp. 13–28. [2]T. Petsas, A. Papadogiannakis, M. Polychronakis, E. P. Markatos, and T. Karagiannis, “Rise of the planet of the apps: A systematic study of the mobile app ecosystem,” in Proc. ACM Internet Meas. Conf., 2013, pp. 277–290. [3]S. L. Lim, P. J. Bentley, N. Kanakam, F. Ishikawa, and S. Honiden, “Investigating country differences in mobile app user study behavior and challenges for software engineering,” IEEE Trans. Softw. Eng., vol. 41, no. 1, pp. 40–64, Jan.2015. [4]H. Falaki, D. Lymberopoulos, R. Mahajan, S. Kandula, and D. Estrin, “A first look at traffic on smartphones,” in Proc. 10th ACM SIGCOMM Conf. Internet Meas., 2010, pp. 281 –287. [5]R. Pandita, X. Xiao, W. Yang, W. Enck, and T. Xie, “WHYPER: Towards automating risk assessment of mobile applications ,” in Proc. 22th USENIX Secur. Symp., 2013, pp. 527–542. [6]J. Huang, X. Zhang, L. Tan, P. Wang, and B. Liang, “AsDroid: Detecting stealthy behaviors in Android applications by user interface and program behavior contradiction,” in Proc. 36th Int. Conf. Softw. Eng., 2014, pp. 1036–1046. [7]A. Gorla, I. Tavecchia, F. Gross, and A. Zeller, “Checking app behavior against app descriptions,” in Proc. 36th Int. Conf. Softw. Eng., 2014 , pp. 1025–1035. [8]K. Thomas, A. K. Bandara, B. A. Price, and B. Nuseibeh, “Distilling privacy requirements for mobile applications,” in Proc. 36th Int. Conf. Softw. Eng., 2014 , pp. 871–882. [9]J. Gui , S. Mcilroy, M. Nagappan, and W. G. J. Halfond, “Truth in advertising: The hidden cost of mobile ads for software developers,” in Proc. 37th IEEE/ACM Int. Conf. Softw. Eng., 2015, pp. 100–110. [10]G. Hecht, B. Omar, R. Rouvoy, N. Moha, and L. Duchien, “Tracking the software quality of Android applications along their evolution,” in Proc. 30th IEEE/ACM Int. Conf. Automated Softw. Eng. , 2015, pp. 236–247. [11]X. Chen, A. Jindal, N. Ding, Y. C. Hu, M. Gupta, and R. Vannithamby, “ Smartphone background activities in the wild: Origin, energy drain, and optimization ,” in Proc. 21st Annu. Int. Conf. Mobile Comput. Netw., 2015, pp. 40–52. [12]J. Crussell, R. Stevens, and H. Chen, “MAdFraud: Investigating ad fraud in Android applications,” in Proc. 12th Annu. Int. Conf. Mobile Syst., Appl., Serv., 2014, pp. 123–134. [13]B. Fu , J. Lin, L. Li, C. Faloutsos , J. I. Hong, and N. M. Sadeh, “ Why people hate your app: Making sense of user feedback in a mobile app store,” in Proc. 19th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2013, pp. 1276–1284. [14]A. Rahmati and L. Zhong, “Studying smartphone usage: Lessons from a four-month field study,” IEEE Trans. Mobile Comput., vol. 12, no. 7, pp. 1417–1427, Jul.2013. [15]C. Tossell, P. T. Kortum, A. Rahmati, C. Shepard, and L. Zhong, “Characterizing web use on smartphones,” in Proc. SIGCHI Conf. Human Factors Comput. Syst., 2012, pp. 2769 –2778. [16]H. Falaki, R. Mahajan, S. Kandula, D. Lymberopoulos, R. Govindan, and D. Estrin, “Diversity in Smartphone usage,” in Proc. 8th Int. Conf. Mobile Syst., Appl., Serv., 2010, pp. 179–194. [17]A. Apaolaza, S. Harper, and C. Jay, “Understanding users in the wild,” in Proc. Int. Cross-Disciplinary Conf. Web Accessibility, 2013, pp. 1–4. [18]H. Li , et al., “Characterizing Smartphone usage patterns from millions of Android users,” in Proc. 2015 Internet Meas. Conf., 2015 , pp. 459–472. [19] Wandoujia, “Wandoujia In-App Search,” 2014. [Online]. Available: http://techcrunch.com/2014/01/12/wandoujia-120m/ [20]M. Nayebi, B. Adams, and G. Ruhe, “Release practices for mobile apps – what do users and developers think?” in Proc. 22nd IEEE Int. Conf. Softw. Anal., Evol., Reengineering, 2016 , pp. 552–562. [21]M. E. J. Newman, “Power laws, pareto distributions and zipf’s law ,” Contemporary Phys., vol. 46, 2005, Art. no. 323. [22]L. A. Adamic and B. A. Huberman, “Power-law distribution of the world wide web,” Sci., vol. 287, no. 5461, pp. 2115–2115, 2000. [23]A.-L. Barabási and R. Albert, “Emergence of scaling in random networks,” Sci. , vol. 286, no. 5439, pp. 509–512, 1999. [24]M. Cha , H. Kwak, P. Rodriguez, Y. Ahn , and S. B. Moon, “I tube, you tube, everybody tubes: Analyzing the world’s largest user generated content video system,” in Proc. 7th ACM SIGCOMM Internet Meas. Conf., 2007, pp. 1–14. [25]A. Clauset, C. R. Shalizi, and M. E. Newman, “ Power-law distributions in empirical data,” SIAM Rev., vol. 51, no. 4, pp. 661–703, 2009. [26]F. Figueiredo, J. M. Almeida, M. A. Gonçalves, and F. Benevenuto, “On the dynamics of social media popularity: A YouTube case study,” ACM Trans. Internet Technol., vol. 14, no. 4, pp. 24:1–24:23, 2014. [27]W. Martin, M. Harman, Y. Jia, F. Sarro, and Y. Zhang, “The app sampling problem for app store mining,” in Proc. 12th IEEE/ACM Work. Conf. Mining Softw. Repositories, 2015 , pp. 123–133. [28]Q. Xu , J. Erman, A. Gerber, Z. M. Mao , J. Pang, and S. Venkataraman, “ Identifying diverse usage behaviors of smartphone apps,” in Proc. 11th ACM SIGCOMM Conf. Internet Meas., 2011, pp. 329–344. [29]Z. Liao, Y. Pan, W. Peng, and P. Lei, “On mining mobile apps usage behavior for predicting apps usage in smartphones,” in Proc. 22nd ACM Int. Conf. Inform. Knowl. Manag., 2013, pp. 609–618. [30]M. Jacomy, T. Venturini, S. Heymann, and M. Bastian, “ForceAtlas2, a continuous graph layout algorithm for handy network visualization designed for the Gephi software,” PloS One , vol. 9, no. 6, 2014, Art. no. e98679. [31]H. Li , et al., “Voting with their feet: Inferring user preferences from app management activities,” in Proc. 25th Int. World Wide Web Conf., 2016, pp. 1351–1361. [32]M. Harman, Y. Jia, and Y. Zhang, “App store mining and analysis: MSR for app stores,” in Proc. IEEE Work. Conf. Mining Softw. Repositories, 2012, pp. 108–111. [33]A. A. Sani, et al., “The wireless data drain of users, apps, & platforms,” Mobile Comput. Commun. Rev., vol. 17, no. 4 , pp. 15–28, 2013. [34]W. Li , H. Li, H. Chen, and Y. Xia , “AdAttester: Secure online mobile advertisement attestation using TrustZone,” in Proc. 13th Annu. Int. Conf. Mobile Syst., Appl., Serv. , 2015, pp. 75–88. [35]M. C. Calzarossa, L. Massari, and D. Tessera, “Workload characterization: A survey revisited,” ACM Comput. Surveys, vol. 48 , no. 3, pp. 1–43, 2016. [36]H. Wang, B. Ding, D. Shi, J. Cao, and A. T. S. Chan, “Auxo: An architecture-centric framework supporting the online tuning of software adaptivity,” Sci. China Inform. Sci., vol. 58, no. 9 , pp. 1–15, 2015. [37]R. Tibshirani, “Regression shrinkage and selection via the lasso: A retrospective ,” J. Roy. Statistical Soc.. Series B (Methodological), vol. 58 , no. 1, pp. 267–288, Jan.1996. [38]L. Breiman, “Random forests,” Mach. Learning , vol. 45, no. 1, pp. 5–32, 2001 . [39]J. H. Friedman, “Stochastic gradient boosting,” Comput. Statist.Data Anal., vol. 38, pp. 367–378, 2002. [40]X. Chen, A. Jindal, N. Ding, Y. C. Hu, M. Gupta, and R. Vannithamby, “ Smartphone background activities in the wild: Origin, energy drain, and optimization ,” in Proc. 21st Annu. Int. Conf. Mobile Comput. Netw., 2015, pp. 40–52. [41]M. Xu , Y. Ma, X. Liu, F. X. Lin , and Y. Liu, “AppHolmes: Detecting and characterizing app collusion among third-party Android markets,” in Proc. 26th Int. World Wide Web Conf., 2017. [42]Z. Fang, Q. Liu, Y. Zhang, K. Wang, Z. Wang , and Q. Wu, “ A static technique for detecting input validation vulnerabilities in Android apps ,” Sci. China Inform. Sci., vol. 60, no. 5, pp. 052 111: 1–052 111:16, 2017. [43] StackOverflow, “Android camera fails,” 2014. [Online]. Available: http://stackoverflow.com/search?q=android+camera+samsung+fail [44]L. Villarroel, G. Bavota, B. Russo, R. Oliveto, and M. D. Penta, “Release planning of mobile apps based on user reviews ,” in Proc. 38th Int. Conf. Softw. Eng., 2016, pp. 14–24. [45]D. Han , C. Zhang, X. Fan, A. Hindle , K. Wong, and E. Stroulia, “ Understanding Android fragmentation with topic analysis of vendor-specific bugs,” in Proc. 19th Work. Conf. Reverse Eng., 2012, pp. 83– 92. [46]X. Zhou, Y. Lee, N. Zhang, M. Naveed, and X. Wang, “The peril of fragmentation: Security hazards in Android device driver customizations,” in Proc. IEEE Symp. Secur. Privacy, 2014, pp. 409–423. [47]H. Khalid, M. Nagappan, E. Shihab, and A. E. Hassan, “Prioritizing the devices to test your app on: A case study of Android game apps,” in Proc. 22nd ACM SIGSOFT Int. Symp. Found. Softw. Eng., 2014, pp. 610–620. [48]J. H. Park, Y. B. Park, and H. K. Ham, “ Fragmentation problem in Android,” in Proc. Int. Conf. Inform. Sci. Appl., 2013, pp. 1–2. [49]“ Android fragementation problem.” (2013). [Online]. Available: http://www.greyheller.com/Blog/androids-fragmentation-problem [50]“ Appbrain,” 2015. [Online]. Available: http://www.appbrain.com/ [51]X. Lu , et al., “PRADA: Prioritizing Android devices for apps by mining large-scale usage data,” in Proc. 38th Int. Conf. Softw. Eng., 2016 , pp. 3–13. [52]J. D. Musa, “Operational profiles in software-reliability engineering ,” IEEE Softw., vol. 10, no. 2 , pp. 14–32, Mar.1993. [53]R. H. Ducoffe, “Advertising value and advertising on the web ,” J. Advertising Res., vol. 36, no. 5, pp. 21 –21, 1996. [54]N. Viennot, E. Garcia, and J. Nieh, “A measurement study of Google Play,” in Proc. ACM SIGMETRICS / Int. Conf. Meas. Model. Comput. Syst. , 2014, pp. 221–233. [55]K. Fukuda, H. Asai, and K. Nagami, “Tracking the evolution and diversity in network usage of smartphones,” in Proc. ACM Internet Meas. Conf., 2015, pp. 253–266. [56]A. Rahmati, C. Tossell, C. Shepard, P. T. Kortum, and L. Zhong, “Exploring iPhone usage: The influence of socioeconomic differences on smartphone adoption, usage and usability,” in Proc. 14th Int. Conf. Human-Comput. Interaction Mobile Devices Serv., 2012, pp. 11–20. [57]B. Yan and G. Chen, “ AppJoy: Personalized mobile application discovery,” in Proc. 9th Int. Conf. Mobile Syst., Appl. Serv., 2011, pp. 113–126 . [58]M. Böhmer, B. Hecht, J. Schöning, A. Krüger, and G. Bauer, “Falling asleep with Angry Birds, Facebook and Kindle: A large scale study on mobile application usage,” in Proc. 13th Conf. Human-Comput. Interaction Mobile Devices Serv., 2011, pp. 47–56. [59]M. Böhmer and A. Krüger, “A study on icon arrangement by smartphone users,” in Proc. ACM SIGCHI Conf. Human Factors Comput. Syst., 2013, pp. 2137 –2146. [60]A. Patro, S. K. Rayanchu, M. Griepentrog, Y. Ma, and S. Banerjee, “Capturing mobile experience in the wild: A tale of two apps ,” in Proc. Conf. Emerging Netw. Experiments Technol., 2013 , pp. 199–210. [61]N. Chen, J. Lin, S. C. H. Hoi, X. Xiao, and B. Zhang, “AR-Miner: Mining informative reviews for developers from mobile app marketplace,” in Proc. 36th Int. Conf. Softw. Eng., 2014 , pp. 767–778. [62]R. A. Baeza-Yates, D. Jiang, F. Silvestri, and B. Harrison, “Predicting the next app that you are going to use,” in Proc. 8th ACM Int. Conf. Web Search Data Mining, 2015, pp. 285–294. [63]C. Shin, J. Hong, and A. K. Dey, “ Understanding and prediction of mobile application usage for smart phones,” in Proc. ACM Conf. Ubiquitous Comput., 2012, pp. 173–182. [64]C. Shin and A. K. Dey, “Automatically detecting problematic use of smartphones,” in Proc. ACM Int. Joint Conf. Pervasive Ubiquitous Comput., 2013, pp. 335– 344. [65]Z. Liao, S. Li, W. Peng, P. S. Yu, and T. Liu, “On the feature discovery for app usage prediction in smartphones ,” in Proc. IEEE 13th Int. Conf. Data Mining, 2013, pp. 1127–1132. [66]R. Montoliu, J. Blom, and D. Gatica-Perez, “ Discovering places of interest in everyday life from smartphone data,” Multimedia Tools Appl., vol. 62, no. 1, pp. 179–207, 2013. [67]T. M. T. Do and D. Gatica-Perez, “Where and what: Using smartphones to predict next locations and applications in daily life,” Pervasive Mobile Comput., vol. 12, pp. 79 –91, 2014. [68]X. Liu , et al., “Derive user preferences of mobile apps from their management activities,” ACM Trans. Inform. Syst.. Xuanzhe Liu is an associate professor in the School of Electronics Engineering and Computer Science, Peking University, Beijing, China. His research interests are in the area of services computing, mobile computing, web-based systems, and big data analytics. He is a member of the IEEE. Huoran Li is working toward the PhD degree in the School of Electronics Engineering and Computer Science of Peking University, Beijing, China. His research interests include mobile computing, software engineering, and human computer interaction. Xuan Lu is working toward the PhD degree in the School of Electronics Engineering and Computer Science of Peking University, Beijing, China. Her research interests include mobile computing and software analytics. Tao Xie is an associate professor and Willett Faculty Scholar in the Department of Computer Science, the University of Illinois, Urbana-Champaign. His research interests are software testing, program analysis, software analytics, software security, and educational software engineering. He is a senior member of the IEEE. Qiaozhu Mei is an associate professor with the University of Michigan School of Information. His major research interests include data mining and information retrieval. Feng Feng is a co-founder of Wandoujia. He is the head architect of the Wandoujia marketplace and management app. He leads the engineering department and has developed dozens of products around Wandoujia. Hong Mei is a full professor of Beijing Institute of Technology and an adjunct professor of Peking University, Beijing, China. His current research interests are in the area of software engineering and operating systems. He is a Member of the Chinese Academy of Sciences, and a Fellow of China Computer Federation (CCF). He is a Fellow of the IEEE.Keywords Androids, Humanoid Robots, Software, Biological System Modeling, Mobile Communication, Electronic Mail, Software Engineering, Mobile Apps, App Store, User Behavior Analysis"
