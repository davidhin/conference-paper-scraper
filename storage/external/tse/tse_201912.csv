title,abstract
A Systematic Evaluation of Static API-Misuse Detectors,"Abstract Application Programming Interfaces (APIs) often have usage constraints, such as restrictions on call order or call conditions. API misuses, i.e., violations of these constraints, may lead to software crashes, bugs, and vulnerabilities. Though researchers developed many API-misuse detectors over the last two decades, recent studies show that API misuses are still prevalent. Therefore, we need to understand the capabilities and limitations of existing detectors in order to advance the state of the art. In this paper, we present the first-ever qualitative and quantitative evaluation that compares static API-misuse detectors along the same dimensions, and with original author validation. To accomplish this, we develop MUC, a classification of API misuses, and MUBENCHPIPE, an automated benchmark for detector comparison, on top of our misuse dataset, MUBENCH. Our results show that the capabilities of existing detectors vary greatly and that existing detectors, though capable of detecting misuses, suffer from extremely low precision and recall. A systematic root-cause analysis reveals that, most importantly, detectors need to go beyond the naive assumption that a deviation from the most-frequent usage corresponds to a misuse and need to obtain additional usage examples to train their models. We present possible directions towards more-powerful API-misuse detectors.Keywords Application Program Interfaces, Failure Analysis, Program Diagnostics, Security Of Data, Static API Misuse Detectors, Misuse Dataset, MUBENCHPIPE Benchmark, Systematic Root Cause Analysis, MUBENCH Dataset, Detectors, Benchmark Testing, Classification, Systematics, Computer Bugs, API Misuse Detection, Survey, Misuse Classification, Benchmark, MU Bench"
Integrative Double Kaizen Loop (IDKL): Towards a Culture of Continuous Learning and Sustainable Improvements for Software Organizations,"Abstract In the past decades, software organizations have been relying on implementing process improvement methods to advance quality, productivity, and predictability of their development and maintenance efforts. However, these methods have proven to be challenging to implement in many situations, and when implemented, their benefits are often not sustained. Commonly, the workforce requires guidance during the initial deployment, but what happens after the guidance stops? Why do not traditional improvement methods deliver the desired results? And, how do we maintain the improvements when they are realized? In response to these questions, we have combined social and organizational learning methods with Lean's continuous improvement philosophy, Kaizen, which has resulted in an IDKL model that has successfully promoted continuous learning and improvement. The IDKL has evolved through a real-life project with an industrial partner; the study employed ethnographic action research with 231 participants and had lasted for almost 3 years. The IDKL requires employees to continuously apply small improvements to the daily routines of the work-procedures. The small improvements by themselves are unobtrusive. However, the IDKL has helped the industrial partner to implant continuous improvement as a daily habit. This has led to realizing sustainable and noticeable improvements. The findings show that on average, Lead Time has dropped by 46 percent, Process Cycle Efficiency has increased by 137 percent, First-Pass Process Yield has increased by 27 percent, and Customer Satisfaction has increased by 25 percent.Keywords Continuous Improvement, Customer Satisfaction, Organisational Aspects, Personnel, Project Management, Software Development Management, Process Improvement Methods, Continuous Improvement Philosophy, First Pass Process Yield, Process Cycle Efficiency, Industrial Partner, IDKL Model, Organizational Learning Methods, Social Learning Methods, Software Organizations, Sustainable Improvements, Continuous Learning, Integrative Double Kaizen Loop, Efficiency 46 0 Percent, Efficiency 137 0 Percent, Efficiency 27 0 Percent, Efficiency 25 0 Percent, Time 3 0 Year, Continuous Improvement, Research And Development, Standards Organizations, Learning Systems, Kaizen, Lean, Organization Learning, Double Loop Learning, Case Study, Empirical Research"
Automating Change-Level Self-Admitted Technical Debt Determination,"Abstract Technical debt (TD) is a metaphor to describe the situation where developers introduce suboptimal solutions during software development to achieve short-term goals that may affect the long-term software quality. Prior studies proposed different techniques to identify TD, such as identifying TD through code smells or by analyzing source code comments. Technical debt identified using comments is known as Self-Admitted Technical Debt (SATD) and refers to TD that is introduced intentionally. Compared with TD identified by code metrics or code smells, SATD is more reliable since it is admitted by developers using comments. Thus far, all of the state-of-the-art approaches identify SATD at the file-level. In essence, they identify whether a file has SATD or not. However, all of the SATD is introduced through software changes. Previous studies that identify SATD at the file-level in isolation cannot describe the TD context related to multiple files. Therefore, it is beneficial to identify the SATD once a change is being made. We refer to this type of TD identification as “Change-level SATD Determination”, which determines whether or not a change introduces SATD. Identifying SATD at the change-level can help to manage and control TD by understanding the TD context through tracing the introducing changes. To build a change-level SATD Determination model, we first identify TD from source code comments in source code files of all versions. Second, we label the changes that first introduce the SATD comments as TD-introducing changes. Third, we build the determination model by extracting 25 features from software changes that are divided into three dimensions, namely diffusion, history and message, respectively. To evaluate the effectiveness of our proposed model, we perform an empirical study on 7 open source projects containing a total of 100,011 software changes. The experimental results show that our model achieves a promising and better performance than four baselines in terms of AUC and cost-effectiveness (i.e., percentage of TD-introducing changes identified when inspecting 20 percent of changed LOC). On average across the 7 experimental projects, our model achieves AUC of 0.82, cost-effectiveness of 0.80, which is a significant improvement over the comparison baselines used. In addition, we found that “Diffusion” is the most discriminative dimension among the three dimensions of features for determining TD-introducing changes.Keywords Project Management, Public Domain Software, Software Maintenance, Software Management, Software Metrics, Software Quality, Long Term Software Quality, Source Code Comments, Source Code Files, Software Changes, SATD Determination Model, Self Admitted Technical Debt Determination, Efficiency 20 0 Percent, Feature Extraction, Java, Labeling, Software Quality, Technical Debt, Software Change, Change Level Determination, Self Admitted Technical Debt"
LEILA: Formal Tool for Identifying Mobile Malicious Behaviour,"Abstract With the increasing diffusion of mobile technologies, nowadays mobile devices represent an irreplaceable tool to perform several operations, from posting a status on a social network to transfer money between bank accounts. As a consequence, mobile devices store a huge amount of private and sensitive information and this is the reason why attackers are developing very sophisticated techniques to extort data and money from our devices. This paper presents the design and the implementation of LEILA (formaL tool for idEntifying mobIle maLicious behAviour), a tool targeted at Android malware families detection. LEILA is based on a novel approach that exploits model checking to analyse and verify the Java Bytecode that is produced when the source code is compiled. After a thorough description of the method used for Android malware families detection, we report the experiments we have conducted using LEILA. The experiments demonstrated that the tool is effective in detecting malicious behaviour and, especially, in localizing the payload within the code: we evaluated real-world malware belonging to several widespread families obtaining an accuracy ranging between 0.97 and 1.Keywords Android Operating System, Formal Verification, Invasive Software, Java, Mobile Computing, LEILA, Android Malware Families Detection, Real World Malware, Formal Tool, Mobile Technologies, Nowadays Mobile Devices, Irreplaceable Tool, Social Network, Bank Accounts, Private Information, Sensitive Information, Sophisticated Techniques, Mobile Malicious Behaviour, Identifying Mobile Malicious Behaviour, Malware, Androids, Humanoid Robots, Payloads, Computer Security, Model Checking, Automata, Security, Malware, Model Checking, Testing, Android"
A Comprehensive Investigation of the Role of Imbalanced Learning for Software Defect Prediction,"Abstract Context: Software defect prediction (SDP) is an important challenge in the field of software engineering, hence much research work has been conducted, most notably through the use of machine learning algorithms. However, class-imbalance typified by few defective components and many non-defective ones is a common occurrence causing difficulties for these methods. Imbalanced learning aims to deal with this problem and has recently been deployed by some researchers, unfortunately with inconsistent results. Objective: We conduct a comprehensive experiment to explore (a) the basic characteristics of this problem; (b) the effect of imbalanced learning and its interactions with (i) data imbalance, (ii) type of classifier, (iii) input metrics and (iv) imbalanced learning method. Method: We systematically evaluate 27 data sets, 7 classifiers, 7 types of input metrics and 17 imbalanced learning methods (including doing nothing) using an experimental design that enables exploration of interactions between these factors and individual imbalanced learning algorithms. This yields 27 × 7 × 7 × 17 = 22491 results. The Matthews correlation coefficient (MCC) is used as an unbiased performance measure (unlike the more widely used F1 and AUC measures). Results: (a) we found a large majority (87 percent) of 106 public domain data sets exhibit moderate or low level of imbalance (imbalance ratio <; 10; median = 3.94); (b) anything other than low levels of imbalance clearly harm the performance of traditional learning for SDP; (c) imbalanced learning is more effective on the data sets with moderate or higher imbalance, however negative results are always possible; (d) type of classifier has most impact on the improvement in classification performance followed by the imbalanced learning method itself. Type of input metrics is not influential. (e) only 52% of the combinations of Imbalanced Learner and Classifier have a significant positive effect. Conclusion: This paper offers two practical guidelines. First, imbalanced learning should only be considered for moderate or highly imbalanced SDP data sets. Second, the appropriate combination of imbalanced method and classifier needs to be carefully chosen to ameliorate the imbalanced learning problem for SDP. In contrast, the indiscriminate application of imbalanced learning can be harmful.Keywords Learning Artificial Intelligence, Pattern Classification, Program Diagnostics, Sampling Methods, Software Engineering, Software Defect Prediction, Machine Learning Algorithms, Input Metrics, Imbalanced Learning Method, Individual Imbalanced Learning Algorithms, Traditional Learning, Moderate Imbalanced SDP Data Sets, Highly Imbalanced SDP Data Sets, Imbalanced Learning Problem, Software Measurement, Boosting, Machine Learning Algorithms, Bagging, Computer Bugs, Software Defect Prediction, Bug Prediction, Imbalanced Learning, Imbalance Ratio, Effect Size"
CHiP: A Configurable Hybrid Parallel Covering Array Constructor,"Abstract We present a configurable, hybrid, and parallel covering array constructor, called CHiP. CHiP is parallel in that it utilizes vast amount of parallelism provided by graphics processing units (GPUs). CHiP is hybrid in that it bundles the bests of two construction approaches for computing covering arrays; a metaheuristic search-based approach for efficiently covering a large portion of the required combinations and a constraint satisfaction-based approach for effectively covering the remaining hard-to-cover-by-chance combinations. CHiP is configurable in that a trade-off between covering array sizes and construction times can be made. We have conducted a series of experiments, in which we compared the efficiency and effectiveness of CHiP to those of a number of existing constructors by using both full factorial designs and well-known benchmarks. In these experiments, we report new upper bounds on covering array sizes, demonstrating the effectiveness of CHiP, and the first results for a higher coverage strength, demonstrating the scalability of CHiP.Keywords Constraint Satisfaction Problems, Graphics Processing Units, Parallel Architectures, Search Problems, Full Factorial Designs, Covering Array Sizes, Hard To Cover By Chance Combinations, GPU, Graphics Processing Units, C Hi P, Constraint Satisfaction Based Approach, Metaheuristic Search Based Approach, Construction Approaches, Configurable Hybrid Parallel Covering Array Constructor, Simulated Annealing, Graphics Processing Units, Parallel Processing, Benchmark Testing, Upper Bound, Scalability, Covering Arrays, Parallel Computing, Graphics Processing Units, CUDA, Metaheuristic Search, Constraint Satisfaction Problem"
