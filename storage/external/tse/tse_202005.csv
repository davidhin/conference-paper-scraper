title,abstract
Extending Abstract Interpretation to Dependency Analysis of Database Applications,"Abstract Dependency information (data- and/or control-dependencies) among program variables and program statements is playing crucial roles in a wide range of software-engineering activities, e.g., program slicing, information flow security analysis, debugging, code-optimization, code-reuse, code-understanding. Most existing dependency analyzers focus on mainstream languages and they do not support database applications embedding queries and data-manipulation commands. The first extension to the languages for relational database management systems, proposed by Willmor et al. in 2004, suffers from the lack of precision in the analysis primarily due to its syntax-based computation and flow insensitivity. Since then no significant contribution is found in this research direction. This paper extends the Abstract Interpretation framework for static dependency analysis of database applications, providing a semantics-based computation tunable with respect to precision. More specifically, we instantiate dependency computation by using various relational and non-relational abstract domains, yielding to a detailed comparative analysis with respect to precision and efficiency. Finally, we present a prototype s e m D D A s semDDA, a semantics-based Database Dependency Analyzer integrated with various abstract domains, and we present experimental evaluation results to establish the effectiveness of our approach. We show an improvement of the precision on an average of 6 percent in the interval, 11 percent in the octagon, 21 percent in the polyhedra and 7 percent in the powerset of intervals abstract domains, as compared to their syntax-based counterpart, for the chosen set of Java Server Page (JSP)-based open-source database-driven web applications as part of the GotoCode project.Keywords Databases, Semantics, Static Analysis, Security, Syntactics, Open Source Software, Debugging, Dependency Graphs, Static Analysis, Relational Databases, Structured Query Languages"
Observation-Enhanced QoS Analysis of Component-Based Systems,"Abstract We present a new method for the accurate analysis of the quality-of-service (QoS) properties of component-based systems. Our method takes as input a QoS property of interest and a high-level continuous-time Markov chain (CTMC) model of the analysed system, and refines this CTMC based on observations of the execution times of the system components. The refined CTMC can then be analysed with existing probabilistic model checkers to accurately predict the value of the QoS property. The paper describes the theoretical foundation underlying this model refinement, the tool we developed to automate it, and two case studies that apply our QoS analysis method to a service-based system implemented using public web services and to an IT support system at a large university, respectively. Our experiments show that traditional CTMC-based QoS analysis can produce highly inaccurate results and may lead to invalid engineering and business decisions. In contrast, our new method reduced QoS analysis errors by 84.4-89.6 percent for the service-based system and by 94.7-97 percent for the IT support system, significantly lowering the risk of such invalid decisions.Keywords Formal Verification, Markov Processes, Object Oriented Programming, Probability, Quality Of Service, Web Services, Observation Enhanced Qo S Analysis, Component Based Systems, Quality Of Service Properties, High Level Continuous Time Markov Chain Model, Probabilistic Model Checkers, Model Refinement, Service Based System, Public Web Services, IT Support System, CTMC Based Qo S Analysis, Quality Of Service, Unified Modeling Language, Analytical Models, Markov Processes, Probabilistic Logic, Component Architectures, Quality Of Service, Component Based Systems, Markov Models, Probabilistic Model Checking"
Chaff from the Wheat: Characterizing and Determining Valid Bug Reports,"Abstract Developers use bug reports to triage and fix bugs. When triaging a bug report, developers must decide whether the bug report is valid (i.e., a real bug). A large amount of bug reports are submitted every day, with many of them end up being invalid reports. Manually determining valid bug report is a difficult and tedious task. Thus, an approach that can automatically analyze the validity of a bug report and determine whether a report is valid can help developers prioritize their triaging tasks and avoid wasting time and effort on invalid bug reports. In this study, motivated by the above needs, we propose an approach which can determine whether a newly submitted bug report is valid. Our approach first extracts 33 features from bug reports. The extracted features are grouped along 5 dimensions, i.e., reporter experience, collaboration network, completeness, readability and text. Based on these features, we use a random forest classifier to identify valid bug reports. To evaluate the effectiveness of our approach, we experiment on large-scale datasets containing a total of 560,697 bug reports from five open source projects (i.e., Eclipse, Netbeans, Mozilla, Firefox and Thunderbird). On average, across the five datasets, our approach achieves an F1-score for valid bug reports and F1-score for invalid ones of 0.74 and 0.67, respectively. Moreover, our approach achieves an average AUC of 0.81. In terms of AUC and F1-scores for valid and invalid bug reports, our approach statistically significantly outperforms two baselines using features that are proposed by Zanetti et al. [104] . We also study the most important features that distinguish valid bug reports from invalid ones. We find that the textual features of a bug report and reporter's experience are the most important factors to distinguish valid bug reports from invalid ones.Keywords Feature Extraction, Program Debugging, Bug Reports, Feature Extraction, AUC, Computer Bugs, Feature Extraction, Collaboration, Forestry, Support Vector Machines, Task Analysis, Software, Bug Report, Feature Generation, Machine Learning"
Analyzing Families of Experiments in SE: A Systematic Mapping Study,"Abstract Context: Families of experiments (i.e., groups of experiments with the same goal) are on the rise in Software Engineering (SE). Selecting unsuitable aggregation techniques to analyze families may undermine their potential to provide in-depth insights from experiments' results. Objectives: Identifying the techniques used to aggregate experiments' results within families in SE. Raising awareness of the importance of applying suitable aggregation techniques to reach reliable conclusions within families. Method: We conduct a systematic mapping study (SMS) to identify the aggregation techniques used to analyze families of experiments in SE. We outline the advantages and disadvantages of each aggregation technique according to mature experimental disciplines such as medicine and pharmacology. We provide preliminary recommendations to analyze and report families of experiments in view of families' common limitations with regard to joint data analysis. Results: Several aggregation techniques have been used to analyze SE families of experiments, including Narrative synthesis, Aggregated Data (AD), Individual Participant Data (IPD) mega-trial or stratified, and Aggregation of p-values. The rationale used to select aggregation techniques is rarely discussed within families. Families of experiments are commonly analyzed with unsuitable aggregation techniques according to the literature of mature experimental disciplines. Conclusion: Data analysis' reporting practices should be improved to increase the reliability and transparency of joint results. AD and IPD stratified appear to be suitable to analyze SE families of experiments.Keywords Data Analysis, Software Engineering, Systematic Mapping Study, Aggregation Technique, SE Families, Software Engineering, SMS, Narrative Synthesis, Aggregated Data, Individual Participant Data, P Values, Systematics, Data Aggregation, Reliability, Data Analysis, Family Of Experiments, Meta Analysis, Narrative Synthesis, IPD, AD"
On Scheduling Constraint Abstraction for Multi-Threaded Program Verification,"Abstract Bounded model checking is among the most efficient techniques for the automated verification of concurrent programs. However, due to the nondeterministic thread interleavings, a large and complex formula is usually required to give an exact encoding of all possible behaviors, which significantly limits the scalability. Observing that the large formula is usually dominated by the exact encoding of the scheduling constraint, this paper proposes a novel scheduling constraint based abstraction refinement method for multi-threaded C program verification. Our method is both efficient in practice and complete in theory, which is challenging for existing techniques. To achieve this, we first proposed an effective and powerful technique which works well for nearly all benchmarks we evaluated. We have proposed the notion of Event Order Graph (EOG), and have devised two graph-based algorithms over EOG for counterexample validation and refinement generation, which can often obtain a small yet effective refinement constraint. Then, to ensure completeness, our method was enhanced with two constraint-based algorithms for counterexample validation and refinement generation. Experimental results on SV-COMP 2017 benchmarks and two real-world server systems indicate that our method is promising and significantly outperforms the state-of-the-art tools.Keywords Graph Theory, Multi Threading, Program Diagnostics, Program Verification, Scheduling, Multithreaded Program Verification, Bounded Model Checking, Automated Verification, Concurrent Programs, Nondeterministic Thread Interleavings, Complex Formula, Exact Encoding, Multithreaded C Program Verification, Event Order Graph, EOG, Graph Based Algorithms, Refinement Generation, Constraint Based Algorithms, Refinement Constraint, Scheduling Constraint Based Abstraction Refinement, Instruction Sets, Electrooculography, Encoding, Concurrent Computing, Programming, Model Checking, Tools, Multi Threaded Program, Bounded Model Checking, Scheduling Constraint, Event Order Graph"
