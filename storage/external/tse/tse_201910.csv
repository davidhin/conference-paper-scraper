title,abstract
Creating Rich and Representative Personas by Discovering Affordances,"Abstract During the last decade, information system designers have used the persona technique to put user needs and preferences at the center of all development decisions. Persona development teams draw on qualitative data, quantitative data or a combination of both to develop personas that are representative of the target users. Despite the benefits of both approaches, qualitative methods are limited by the cognitive capabilities of the experts, whereas quantitative methods lack contextual richness. To gain the advantages of both approaches, this article suggests a mixed qualitative-quantitative approach to create user personas based on the patterns of the affordances they actualize rather than merely the actions they take. It enriches personas by referring to the purposes fulfilled through affordance actualizations, and it grounds personas in readily available objective log data. This study illustrates the practical value of the proposed methodology by empirically creating personas based on real user data. Furthermore, it demonstrates its value by having practitioners compare the suggested method to that of qualitative-only and quantitative-only methods.Keywords Formal Specification, Human Computer Interaction, Information Systems, User Centred Design, Persona Development Teams, Qualitative Data, Quantitative Data, Cognitive Capabilities, User Personas, Information System Designers, Human Computer Interaction, User Centered Design, Software Engineering, Requirements Engineering, Persona Creation, Software, Maintenance Engineering, Task Analysis, Aging, Interviews, Human Computer Interaction, Personas, Affordances, Mixed Qualitative And Quantitative Methods, User Modeling, Interview, Card Sorting, Cluster Analysis, Systems Design And Implementation, Design And Evaluation Of IT Infrastructure, Questionnaire Surveys"
A Screening Test for Disclosed Vulnerabilities in FOSS Components,"Abstract Free and Open Source Software (FOSS) components are ubiquitous in both proprietary and open source applications. Each time a vulnerability is disclosed in a FOSS component, a software vendor using this component in an application must decide whether to update the FOSS component, patch the application itself, or just do nothing as the vulnerability is not applicable to the older version of the FOSS component used. This is particularly challenging for enterprise software vendors that consume thousands of FOSS components and offer more than a decade of support and security fixes for their applications. Moreover, customers expect vendors to react quickly on disclosed vulnerabilities-in case of widely discussed vulnerabilities such as Heartbleed, within hours. To address this challenge, we propose a screening test: a novel, automatic method based on thin slicing, for estimating quickly whether a given vulnerability is present in a consumed FOSS component by looking across its entire repository. We show that our screening test scales to large open source projects (e.g., Apache Tomcat, Spring Framework, Jenkins) that are routinely used by large software vendors, scanning thousands of commits and hundred thousands lines of code in a matter of minutes. Further, we provide insights on the empirical probability that, on the above mentioned projects, a potentially vulnerable component might not actually be vulnerable after all.Keywords DP Industry, Program Slicing, Program Testing, Public Domain Software, Security Of Data, Proprietary Source Applications, Open Source Applications, Screening Test, Disclosed Vulnerabilities, FOSS Component, Free And Open Source Software Component, Thin Slicing, Enterprise Software Vendors, Security, Maintenance Engineering, Tools, Jacobian Matrices, Patents, Open Source Software, Security Maintenance, Security Vulnerabilities, Patch Management, Free And Open Source Software"
Detecting Bugs by Discovering Expectations and Their Violations,"Abstract Code mining has been proven to be a promising approach to inferring implicit programming rules for finding software bugs. However, existing methods may report large numbers of false positives and false negatives. In this paper, we propose a novel approach called EAntMiner to improve the effectiveness of code mining. EAntMiner elaborately reduces noises from statements irrelevant to interesting rules and different implementation forms of the same logic. During preprocessing, we employ program slicing to decompose the original source repository into independent sub-repositories. In each sub-repository, statements irrelevant to critical operations (automatically extracted from source code) are excluded and various semantics-equivalent implementations are normalized into a canonical form as far as possible. Moreover, to tackle the challenge that some bugs are difficult to be detected by mining frequent patterns as rules, we further developed a kNN-based method to identify them. We have implemented EAntMiner and evaluated it on four large-scale C systems. EAntMiner successfully detected 105 previously unknown bugs that have been confirmed by corresponding development communities. A set of comparative evaluations also demonstrate that EAntMiner can effectively improve the precision of code mining.Keywords Data Mining, Nearest Neighbour Methods, Program Debugging, Program Slicing, Program Testing, Code Mining, Implicit Programming Rules, Software Bugs, False Positives, False Negatives, E Ant Miner, Different Implementation Forms, Frequent Pattern Mining, Unknown Bugs, K NN Based Method, Canonical Form, Semantics Equivalent Implementations, Source Code, Sub Repository, Independent Sub Repositories, Original Source Repository, Program Slicing, Computer Bugs, Data Mining, Linux, Programming, Kernel, Semantics, Bug Detection, Code Mining, Program Slicing, Instance Based Learning"
Network-Clustered Multi-Modal Bug Localization,"Abstract Developers often spend much effort and resources to debug a program. To help the developers debug, numerous information retrieval (IR)-based and spectrum-based bug localization techniques have been devised. IR-based techniques process textual information in bug reports, while spectrum-based techniques process program spectra (i.e., a record of which program elements are executed for each test case). While both techniques ultimately generate a ranked list of program elements that likely contain a bug, they only consider one source of information-either bug reports or program spectra-which is not optimal. In light of this deficiency, this paper presents a new approach dubbed Network-clustered Multi-modal Bug Localization (NetML), which utilizes multi-modal information from both bug reports and program spectra to localize bugs. NetML facilitates an effective bug localization by carrying out a joint optimization of bug localization error and clustering of both bug reports and program elements (i.e., methods). The clustering is achieved through the incorporation of network Lasso regularization, which incentivizes the model parameters of similar bug reports and similar program elements to be close together. To estimate the model parameters of both bug reports and methods, NetML employs an adaptive learning procedure based on Newton method that updates the parameters on a per-feature basis. Extensive experiments on 355 real bugs from seven software systems have been conducted to benchmark NetML against various state-of-the-art localization methods. The results show that NetML surpasses the best-performing baseline by 31.82, 22.35, 19.72, and 19.24 percent, in terms of the number of bugs successfully localized when a developer inspects the top 1, 5, and 10 methods and Mean Average Precision (MAP), respectively.Keywords Information Retrieval, Learning Artificial Intelligence, Newton Method, Program Debugging, Network Clustered Multimodal Bug Localization, Net ML, Multimodal Information, Effective Bug Localization, Bug Localization Error, Bug Reports, Information Retrieval Based Bug Localization Techniques, Bug Localization Techniques, IR Based Techniques Process Textual Information, Program Spectra, Spectrum Based Techniques Process, Program Elements, Mean Average Precision, Computer Bugs, Adaptation Models, Optimization, Debugging, Task Analysis, Computational Modeling, Information Retrieval, Bug Localization, Information Retrieval, Program Spectra"
UniDoSA: The Unified Specification and Detection of Service Antipatterns,"Abstract Service-based Systems (SBSs) are developed on top of diverse Service-Oriented Architecture (SOA) technologies or architectural styles. Like any other complex systems, SBSs face both functional and non-functional changes at the design or implementation-level. Such changes may degrade the design quality and quality of service (QoS) of the services in SBSs by introducing poor solutions-service antipatterns. The presence of service antipatterns in SBSs may hinder the future maintenance and evolution of SBSs. Assessing the quality of design and QoS of SBSs through the detection of service antipatterns may ease their maintenance and evolution. However, the current literature lacks a unified approach for modelling and evaluating the design of SBSs in term of design quality and QoS. To address this lack, this paper presents a meta-model unifying the three main service technologies: REST, SCA, and SOAP. Using the meta-model, it describes a unified approach, UniDoSA (Unified Specification and Detection of Service Antipatterns), supported by a framework, SOFA (Service Oriented Framework for Antipatterns), for modelling and evaluating the design quality and QoS of SBSs. We apply and validate UniDoSA on: (1) 18 RESTful APIs, (2) two SCA systems with more than 150 services, and (3) more than 120 SOAP Web services. With a high precision and recall, the detection results provide evidence of the presence of service antipatterns in SBSs, which calls for future studies of their impact on QoS.Keywords Application Program Interfaces, Formal Specification, Quality Of Service, Service Oriented Architecture, Software Maintenance, Software Quality, Web Services, Unified Specification, SOAP Web Services, Service Based Systems, Service Oriented Architecture, SBS, Uni Do SA, Service Antipattern Detection, Service Oriented Framework For Antipatterns, SOFA, SCA, Quality Of Service, Qo S, RES Tful API, Simple Object Access Protocol, Quality Of Service, Service Oriented Architecture, Maintenance Engineering, DSL, Antipatterns, Service Based Systems, REST, SCA, SOAP, Web Services, Specification, Detection, Quality Of Service, Design, Software Maintenance And Evolution"
