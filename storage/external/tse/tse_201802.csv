title,abstract
Automated Test Case Generation as a Many-Objective Optimisation Problem with Dynamic Selection of the Targets,"Abstract The test case generation is intrinsically a multi-objective problem, since the goal is covering multiple test targets (e.g., branches). Existing search-based approaches either consider one target at a time or aggregate all targets into a single fitness function (whole-suite approach). Multi and many-objective optimisation algorithms (MOAs) have never been applied to this problem, because existing algorithms do not scale to the number of coverage objectives that are typically found in real-world software. In addition, the final goal for MOAs is to find alternative trade-off solutions in the objective space, while in test generation the interesting solutions are only those test cases covering one or more uncovered targets. In this paper, we present Dynamic Many-Objective Sorting Algorithm (DynaMOSA), a novel many-objective solver specifically designed to address the test case generation problem in the context of coverage testing. DynaMOSA extends our previous many-objective technique Many-Objective Sorting Algorithm (MOSA) with dynamic selection of the coverage targets based on the control dependency hierarchy. Such extension makes the approach more effective and efficient in case of limited search budget. We carried out an empirical study on 346 Java classes using three coverage criteria (i.e., statement, branch, and strong mutation coverage) to assess the performance of DynaMOSA with respect to the whole-suite approach (WS), its archive-based variant (WSA) and MOSA. The results show that DynaMOSA outperforms WSA in 28 percent of the classes for branch coverage (+8 percent more coverage on average) and in 27 percent of the classes for mutation coverage (+11 percent more killed mutants on average). It outperforms WS in 51 percent of the classes for statement coverage, leading to +11 percent more coverage on average. Moreover, DynaMOSA outperforms its predecessor MOSA for all the three coverage criteria in 19 percent of the classes with +8 percent more code coverage on average.Keywords Optimisation, Program Testing, Search Problems, Sorting, Dynamic Target Selection, Many Objective Sorting Algorithm, Search Based Approaches, Test Sequence, Test Input Data, Branch Coverage, Many Objective Solver, MO As, Many Objective Optimisation Algorithms, Multiple Test Targets, Multiobjective Problem, Many Objective Optimisation Problem, Automated Test Case Generation, Dyna MOSA, Heuristic Algorithms, Optimization, Testing, Software Algorithms, Algorithm Design And Analysis, Sorting, Genetic Algorithms, Evolutionary Testing, Many Objective Optimisation, Automatic Test Case Generation"
A Templating System to Generate Provenance,"Abstract PROV-TEMPLATEIS a declarative approach that enables designers and programmers to design and generate provenance compatible with the PROV standard of the World Wide Web Consortium. Designers specify the topology of the provenance to be generated by composing templates, which are provenance graphs containing variables, acting as placeholders for values. Programmers write programs that log values and package them up in sets of bindings, a data structure associating variables and values. An expansion algorithm generates instantiated provenance from templates and sets of bindings in any of the serialisation formats supported by PROV. A quantitative evaluation shows that sets of bindings have a size that is typically 40 percent of that of expanded provenance templates and that the expansion algorithm is suitably tractable, operating in fractions of milliseconds for the type of templates surveyed in the article. Furthermore, the approach shows four significant software engineering benefits: separation of responsibilities, provenance maintenance, potential runtime checks and static analysis, and provenance consumption. The article gathers quantitative data and qualitative benefits descriptions from four different applications making use of PROV-TEMPLATE. The system is implemented and released in the open-source library ProvToolbox for provenance processing. 1   Introduction Provenance has gained a lot of traction lately in various areas including the Web, legal notices, 1 climate science,2 scientific workflows [1], [2] , [3], computational reproducibility [4] , emergency response [5], medical applications, 3 geospatial domain,4 art and food. The recent standard prov  [6] of the World Wide Web Consortium defines provenance as “as a record that describes the people, institutions, entities, and activities involved in producing, influencing, or delivering a piece of data or a thing.” In an increasing number of applications, provenance has become crucial in making systems accountable, by exposing how information flows in systems, and in helping users decide whether information is to be trusted. Provenance is not restricted to computer systems, it can also be used to describe how objects are transformed and people are involved in a physical system [5]. Applications and use cases for provenance are well documented in the literature  [7], [8], [9], [10]. They include making systems more auditable and accountable [11], reproducing results  [12], deriving trust and classification  [13], asserting attribution and generating acknowledgments [14], supporting predictive analytics [13], and facilitating traceability [15]. To enable such a powerful functionality, however, one needs to adapt or write applications, so that they generate provenance information, which can then be exploited to offer new benefits to their users. A number of approaches have been proposed to generate provenance: run-time, compile-time, and retrospectively. Runtime generation typically requires applications to be instrumented, and provenance generated accordingly  [16], [17], [18]. Instrumenting applications and generating provenance at the same time can be cumbersome from a software engineering perspective. Instead, traditional logging techniques have been combined with provenance generation [19]. Workflow systems are a class of applications generating provenance, which use a mix of instrumentation and logging (cf. Taverna  [1], Vistrails [2], Kepler  [3]). Aspect-oriented approaches have also been used to weave provenance generation instructions into programs [20]. In contrast, compile-time generation uses static analysis, such as dependency analysis [21] and program slicing [22], to produce executables that generate provenance information. In many situations, however, we have to deal with legacy applications, for which we do not have the opportunity to modify the source code to introduce provenance-generating code, or we do not have the opportunity to recompile programs. Thus, in those cases, using application knowledge, provenance can alternatively be reconstructed retrospectively [23], [24]. Regardless of the approach, whether by instrumentation, logging, aspects, static methods, or reconstruction, at some point, a provenance record needs to be constructed. This may involve writing code that generates a provenance record in a well-defined, standard serialization format, such as RDF [25], text [26] or XML [27]. Alternatively, a toolkit can be used to create a memory representation of provenance and to serialize it (e.g., ProvPy [28] and ProvToolbox [29] ). Writing the code for generating provenance and its serialization is error-prone since, to be inter-operable [30], it is required to address all the idiosyncrasies of the model and formats. Whilst using libraries facilitates this task, the programming effort may have to be repeated across the whole of the application's code base. This is particularly challenging since, provenance being still a rather new concept, programmers are generally not familiar with its technical details, and are not the best placed to fine-tune the provenance information to be generated. This problem is particularly compounded in large projects with code developers distributed geographically or across different organisations: specifically, when agile methods are being adopted, the programming overhead makes it very difficult to maintain consistent and timely updates to the provenance generation code, so that the provenance it generates remains aligned with the actual behaviour of the application. To address these challenges, we propose prov-template, a templating approach for generating prov -compliant provenance, with the following original and distinct characteristics: With prov-template, a designer can express the shape of the provenance to be generated; the provenance templates offer a declarative specification of provenance, rather than stating how it has to be generated. Provenance templates are simply provenance documents expressed in a prov-compatible format and containing placeholders, referred to as “variables”, for values. A mechanism allows for values to be injected into those placeholders: prov-template is equipped with an expansion algorithm that, given a template and a set of bindings (associating variables to values), generates a provenance record in one of the standardized prov representations. Given that we have argued against crafting code to generate provenance, we felt strongly that it was not suitable to define yet another specialized language to generate provenance. Hence, the originality of prov-template is that provenance templates are expressed in prov directly. A simple serialization format also exists for sets of bindings. These design constraints have been adopted with a view to minimize the number of specific notations designers and developers have to learn. From a software development viewpoint, we have observed a number of benefits of the prov-template approach: Separation of Responsibilities. Provenance template design and maintenance become the responsibility of “knowledge engineers,” whereas provenance-related coding is reduced to bindings generation that can be embraced by a distributed development team, without prov expertise. Maintenance. It becomes possible to maintain an application-wide library of templates in a single location, allowing for incremental updates of templates, for instance, due to changes to application ontology definitions. Runtime or Static Checks. A number of safety checks can be introduced such as determining whether bindings are compatible with templates; this includes type-compatibility, arity, mandatory/optional nature, potential semantic constraints, and the ability to check whether templates are valid [31]. Provenance Consumption. An application consuming the provenance it generates can directly process sets of bindings, rather than perform graph queries, followed by some query result post-processing. We initially released a specification [32] of prov-template and an implementation in ProvToolbox [29] in 2014. Building on our experience of using the system in various projects, the aim of this article is to present prov-template, with the following original contributions: \text{i)} an overview of the approach and its positioning in the software development cycle; \text{ii)} a description of the template language, the sets of bindings, the expansion algorithm, and core checks; \text{iii)} a quantitative evaluation of the template expansion approach, showing improved performance when manipulating bindings over expanded provenance templates; Z_ $\text{iv)}$_Z a qualitative discussion of our practical experience with prov-template. The article is organized as follows. Section 2 consists of an example of template as well as a brief introduction to prov, and the challenges associated with the programmatic generation of prov. Section 3 then presents the architectural overview of prov-template. Section 4 defines the templates, bindings and expansion algorithm. The quantitative evaluation is then the focus of Section 5, whereas our practical experience with prov-template is discussed in Section 7. We compare our approach to related work in Section 8, before concluding the article in Section 9. 2   Provenance Applications and Example As a way of providing motivations for provenance, we outline four applications exploiting provenance ( Section 5 contains an evaluation that is based on templates and bindings from these applications). One of the applications, EBook (see Section 2.1.3 ), serves as an illustration to convey the intuition of prov and the templating approach. 2.1 Four Provenance-Enabled Applications 2.1.1 Smartshare Smartshare5 [33] is a “car pooling” application that allows drivers and commuters to offer and request rides. Ride offers and requests include details about required travels, timing, locations, capacity, prices, and other details relevant to car sharing. The application automatically matches commuters to available cars. It is fully provenance-enabled, capturing the provenance of any user decision, matching or rating managed by the system. The purpose of provenance in Smartshare is to make the application accountable, in particular, by providing explanations about all decisions made. The \sf {smart} data set in this article consists of all the provenance bindings and associated templates involved in six ride plans leading to two agreed rides between two users. 2.1.2 Food The food application tracks food orders and deliveries in Hampshire schools in England, with a view to develop “due diligence” methods with scientific authorities of the county. The purpose of provenance in this application is to describe the origin of food and develop analytics methods over the food supply chain. The \sf {food} data set in this article consists of the templates that describe orders, deliveries, food specifications and sampling, and associated bindings for six schools in the county over six months. 2.1.3 EBook The EBook project6 released a suite of tools designed to aid in the use and teaching of reproducible statistical analysis techniques with a particular emphasis on their use in social science. It consists of a workflow system capable of logging provenance, and a system to convert provenance back into workflows. The purpose of provenance in EBook is to support the aim of reproducibility of scientific experiments [12]. The \sf {ebook} data set in this article consists of the single template used by the EBook system and sets of bindings generated by the execution of a specific statistical workflow. 2.1.4 PICASO The PICASO application7 (Provenance Interlinking and Collective Authoring for Scientific Objects) is an online platform that crowdsources the links between related scientific objects identified by Unified Resource Identifiers (URI). In PICASO, the purpose of provenance is to form a knowledge graph linking all scientific work; it is published as linked open data to allow for further analyses and research over this kind of information. The \sf {picaso} data set in this article consists of all the templates supported by PICASO and bindings for some 4,000 entries. 2.2 An Example of Template In this section, we present an example of template and sets of bindings and provide a brief overview of the prov data model. For further and normative details, we refer the reader to the prov specifications  [6], [25], [26]. The template depicted in Fig. 1 is a simplification of the template used in the EBook application (see Section 2.1.3). It captures the following provenance pattern: an agent has launched a workflow, consisting of execution steps, referred to as blocks  [34], each consuming some input files, and generating output files. An input or output file is modelled as a prov:Entity (represented by yellow ellipses in Fig. 1, labeled as “consumed” and “produced”, respectively); the execution of the workflow or a workflow step is modelled as a prov:Activity (represented as blue rectangles, labeled as “parent” and “block_instance”, respectively); finally, the user is modelled as a prov:Agent (represented as an orange pentagon labeled “agent”). A few relations interconnect these nodes: the output file is derived from the input file (relation prov:wasDerivedFrom ), the input is used by the block (relation prov:used), whereas the output is generated by it (relation prov:wasGeneratedBy), the execution step was started by the parent execution (relation prov:wasStartedBy), whereas the parent workflow was associated with the user (relation prov:wasAssociatedWith). Further attributes of a block include its type, start and end time, etc. We note that prov relations are expressed using the past tense to highlight that provenance is intended to be a description of something that happened in the past (as opposed to a workflow specification or a program aimed to be executed in the future). Fig. 1. A graphical illustration of a template. It shows an activity block_instance, which used an entity and generated another entity, the latter derived from the former. The activity was started by a parent activity (itself a block or an overarching workflow). An agent is associated with the parent activity. Some properties are associated with the block_instance activity, including its type, its start and end time, a human-readable label and a URI to its full description. Terms appearing in red are variables acting as placeholders for values. Whilst Fig. 1 is purely illustrative, Fig. 2 presents the same template using the normative prov-n notation  [26]. Each node and relation of the graph is expressed by a prov-n statement. The prov-n notation makes it explicit which name is a placeholder for values: all names in the namespace8var are regarded as variables by prov-template (they are marked in red in both Figs. 1 and 2). Note that some of the variables are not displayed in Fig. 1 to preserve its legibility. Fig. 2. Template of Fig. 1 expressed in prov-n. Variables are qualified names with prefix var and appear in red. The template of Fig. 1 describes the execution of an activity block_instance. A typical workflow consists of multiple steps, and the execution of each of these steps has to be described by instantiating the same template, with bindings pertaining to this specific execution. In Fig. 1 the graph is displayed inside a box labeled by variable bundle; likewise, in Fig. 2, we can see that prov terms occur inside a bundle construct, with the same variable. A bundle is a prov construct [6] that allows provenance of provenance to be expressed. Specifically, such bundles are present in templates so that their attribution can be expressed, allowing the process of template expansion to be documentable by provenance itself. Although such an attribution is permitted, it is not included in the example for clarity. For instance, one could express that the template is part of a template library of an application; this also allows details of the template expansion to be captured (such as the version of the library that performed the expansion at a specific date). 2.3 Bindings and Template Expansion An extract from a set of bindings is displayed in Fig. 3: it is a JSON structure that contains a dictionary for the various variables (in red), mapping them to one or more values (in blue). For instance, there are two consumed inputs, so there are two values for the variable “consumed”, expressed as UUIDs denoting each of the two inputs; on the other hand, there is a single output, so there is a single value for the variable “produced”. Fig. 3. An example of set of bindings for the template of Fig. 2. A set of bindings is encoded as a dictionalry associating variables to values. Variable names are shown in red, and values in blue. The expansion of the template of Fig. 2 with the bindings of Fig. 3 is illustrated graphically in Fig. 4, and its prov-n representation is shown in Fig. 5 . Variables have been replaced by values (displayed in blue in Fig. 5 ). We see that two entities are consumed (fe1bf93c-... and 0266d11a-...). Supplementary Material, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/10.1109/TSE.2017.2659745, contains a fuller example of template expansion. Fig. 4. A graphical illustration of the expansion of template of Fig. 1 with bindings of Fig. 3. Fig. 5. The expanded provenance of Fig. 4 expressed in prov-n. Following expansion, variables have been replaced by values shown in blue. 2.4 The Difficulty of Generating Provenance without Template The four applications (Smartshare, Food, EBook, PICASO) introduced in Section 2.1 use templates to generate provenance. Prior to designing prov-template, we developed applications that were creating provenance directly: software engineers had to design, implement and maintain code that generated provenance similar to that of Fig. 5. This presented a number of challenges, which we illustrate, based on our concrete experience with several applications. ProvToolbox [29] is an open-source general-purpose toolkit to manipulate prov-based provenance in Java. It comes with a few examples of Java programs to create a memory representation of provenance and save it to various prov serialization formats. One of the examples is the Provenance Challenge workflow [35], a reference workflow for the provenance community. An implementation can be found on Github9 and is also discussed in more details in Supplementary Material, available online. To a first approximation, each prov term (such as those occurring in Fig. 5) requires a factory method to be called; potentially, a further method call would be required to add each key-value pair occurring in these terms. As a graph structure is being built, each node needs to be identified by a URI or a qualified name (i.e., a URI short form [26]), and likewise, each relation connects two or more nodes, also identified by some URIs. This essentially requires code that manipulates names with a size proportional to that of the provenance graph to be generated. This results in a significant burden on software developers since they would be required to understand the semantics of these constructor methods, and would have to ensure that the graph is constructed correctly, associating resources of types compatible with the edge semantics, and making sure that edges are constructed with the right directions. As an illustration, 200 lines of Java code are required to compose the Provenance Challenge provenance graph, which can be broken down into 50 lines for node constructors, 130 for edge constructors, and 15 for adding attributes. As some nodes have up to 8 incoming edges, there is a significant amount of repetition in node names provided to edge constructors, and therefore of opportunities to code the graph construction incorrectly. ProvToolbox [29] and ProvPy  [28] are libraries that take care of deserializing to, and serializing from, Java and Python representations, respectively. As an illustration, Table 1 provides an approximate line count of this functionality. A developer intending to generate directly a serialization of prov, would be confronted with developing and maintaining a code base whose size is a significant fraction of the ones illustrated in Table 1. TABLE 1 Number of Lines of Code to Support Various Serialization Formats of prov in ProvToolbox and ProvPy  The serialization formats come with their idiosyncrasies, and as specifications are revised, or when incorrect implementations have to be fixed, non-trivial work is required. The type of effort to maintain such libraries is illustrated by the following change to ProvPy. Previously, ProvPy used prov:QualifiedName (implemented by the QualifiedName class) to denote short forms of URIs, and also supported xsd:QName backed by XSDName class, a subclass of QualifiedName . However, to ensure better compliance with prov-n, support for xsd:QName was removed, and instead the new prov:QUALIFIED_NAME with changed capitalization had to be supported. Changes affected some 60 lines.10 In Section 7.3, we revisit this change and show how it affect the PICASO application that is built on ProvPy and templates. CollabMap is a provenance-enabled crowdsourcing application [13]. Provenance is used in CollabMap for auditing the application's behavior and for predicting the quality of data produced by the crowd [13]. CollabMap was being developed as the standardization of prov began, and therefore, relies on a predecessor model of provenance called OPM, the Open Provenance Model; it also predates ProvPy. A significant part of CollabMap code specifically aims at provenance management: about 145 lines are about constructing provenance (out of 400 related to the data model), and some 80 lines are concerned with JSON export, and 50 for RDF export (out of 700 of the view part of the MVC model). Handcrafting the PROV-JSON structure was tedious and error-prone, and changes required code edits. Issues that had to be handled included: lack of default namespace required to resolve qualified names without a prefix, referring to attribute with a string representation instead of a qualified name, upgrading to the prov model as its specification was being agreed. In summary, the set of evidence we have presented point towards the need for a principled method for generating provenance and associated tooling, such as prov-template, which we now describe in details. 3   Architectural Overview Fig. 6 provides an overview of the prov-template architecture. In blue, we show key facets of runtime execution: an application logs values, in the form of bindings, which are used by the template expander to generate provenance documents. The template expander relies on a template for provenance created at design-time; the template may refer to application or domain ontologies. In red, we show the templates and ontologies which are created at design time. Fig. 6. The architecture of the templating approach. The blue and red colors are used to refer to runtime and design time aspects of the approach, respectively. The architecture is agnostic about the mechanism used to create bindings. Applications may log values, and then convert them to bindings. Alternatively, aspects may be weaved into the code to generate such bindings. The architecture is also agnostic about when provenance is generated and which documents are persisted. Template expansions may be interleaved with application execution, possibly persisting provenance graphs; alternatively, bindings may be accumulated, and provenance generated for post-execution analysis. The specific requirements of the application with respect to provenance determines when these operations have to take place. 4   Template, Binding, Template Expansion In this section, we present a conceptualization of prov-template, including the abstract syntax of templates, a definition of sets of bindings, and a description of the expansion algorithm. 4.1 Template Definition Fig. 7 displays the abstract syntax of templates. In this article, the presentation is concerned with the syntactic dimension of the prov data model; for the detailed and normative meaning of these constructs, we refer the reader to the prov data model specification  [6]. Fig. 7. Abstract syntax of the template language. A template, denoted by the meta-variable template is a named set of terms (referred to as “bundle” [6]). Terms can be of five kinds and are denoted by the meta-variables term_i for i=1,\ldots, 5 . Nodes, denoted by term_1 , can be entities, activities, or agents; they are identified by a mandatory name \mu . The remaining terms are relations, connecting such nodes. In the simplest case, relations denoted by term_2 are binary associations, connecting two nodes denoted by their names \alpha and \beta . Qualified relations  [25], expressed by meta-variable Z_ $term_3$_Z , enrich binary relations with a set of attribute-value pairs and an optional identifier \tau . Relations described by term_4 and term_5 further include one or more secondary names \delta , allowing qualified relations to be refined with extra information. When attribute-value pairs are contained in a term, they occur in a set, which allows a given attribute key k_i to be present multiple times with different values; their order is not significant. Constants (\pi \in {\mathcal {P}} ) can be of usual primitive types (for instance, XML Schema built-in datatypes [36]); constants consist of two elements: their external representation as a string, and their type. For example, when we conveniently write ”John Doe”, we mean the constant with external representation “ John Doe” and type xsd:string. Likewise, the conveniently written number 40, in fact, denotes the constant with external representation “40 ” and type xsd:integer. The grammar of templates also allows for a name \gamma to appear in attribute-value position, associated with an attribute key \kappa$ _Z. In that case, there is an implicit conversion of the name Z_$\gamma into a constant in {\mathcal {P}} , consisting of the external representation of \gamma and the reserved type prov:QUALIFIED_NAME. In other words, all constants with this reserved type are considered as names by prov-template. Such constants are abbreviated with the prov-n single quote notation  [26] (for instance, ’var:endtime’ in Fig. 2). Names are said to be qualified [25], consisting of a prefix denoting a namespace URI and a local name. The template language comes with its own namespace, in which some names are defined with a specific meaning.  4.2 Simple Set of Bindings The set of all names {{Names}}(t) in a template t is the set of all names \alpha, \beta, \delta, \gamma, \mu, \tau , for all terms in template t . We distinguish two categories of names, according to their positions in terms. In the definition of a template in Fig. 7, the names \alpha, \beta, \delta, \mu, \tau act as placeholders—or variables—for some concrete names in {\mathcal {N}} . The process of template expansion replaces such placeholders by concrete names. The expansion algorithm expects a template and a set of bindings, which maps names to their concrete instantiation. In contrast, in Fig. 7, an attribute variable name \gamma acts as a placeholder for some constants. Let us consider a binding that maps a name \gamma to a set of constants \pi _0,\pi _1,\ldots . Then, the expansion of an attribute-value pair \kappa =\gamma results in a series of attribute-value pairs \kappa =\pi _0,\kappa =\pi _1,\ldots . We can formally define sets of bindings as follows. Definition 1 (Simple set of bindings). A simple set of bindings \rho for a term term_t (resp. a template t ) is a partial map of names of the term {{Names}}(term_t) (resp. the template {{Names}}(t) ) to some value, whether it is a name in {\mathcal {N}} or a set of constants in {\mathcal {P}} . It is useful for subsequent formalizations to consider a set of bindings as a total map. Given a template t and a simple set of bindings \rho , a total, simple set of bindings \rho _T for t is a total function mapping each name of the template {{Names}}(t) to some value, whether it is a name or a set of constants. For any \nu \in {{Names}}(t) , \begin{equation*} \rho _T(\nu)= \left\lbrace \begin{array}{ll}\rho (\nu) & \text{if} \rho (\nu) \text{is defined}\\ \nu & \text{if} \rho (\nu) \text{is not defined.} \end{array} \right. \end{equation*} 4.3 Simple Name Replacing in Templates Given a simple set of bindings \rho , we can replace a template's names by the values associated with them in Z_$\rho$ _Z. However, there are three ways of doing this, which we refer to as permissive , strict , and prov-aware name replacing, respectively denoted by replace_{perm} , replace_{strict} , and replace_{pa} . These three ways of replacing names correspond to different modes a developer may want to use the prov-template system. Permissive mode, for instance, allows partial instantiation of templates. Strict mode is particularly useful when debugging to ensure that all variables have been instantiated. Finally, prov-aware mode is an intermediate way of operating, taking into account the optional nature of some names. We now define them in turn. Permissive name replacing is the function that substitutes the names found in a set of bindings \rho , and leaves the others untouched. For instance, permissive name replacing of term_3 , written replace_{perm}(term_3,\rho) , is defined as the term term_3^{\prime} \begin{equation*} term_3^{\prime}=rel_3(\tau ^{\prime};\beta ^{\prime},\alpha ^{\prime},\delta ^{\prime},[\kappa _i=\gamma ^{\prime},\ldots, \kappa _j=\pi, \ldots ]), \end{equation*} where each name \nu ^{\prime} is obtained by \nu ^{\prime}=\rho _T(\nu) , where \nu ranges over \alpha, \beta, \delta, \nu, \tau . For a pair k_i=\gamma in term_3 , a name \gamma occurring in attribute-value position is allowed to be bound to multiple values. If \gamma is unbound, or bound to a single name, then we find an attribute key-value pair \kappa _i=\gamma ^{\prime} in term_3^{\prime} , with \gamma ^{\prime}=\rho _T(\gamma) . If \gamma is bound to a set of constants, for each Z_ $\pi \in \rho _T(\gamma)$_Z, there is an attribute key-value pair \kappa _i=\pi$ _Z in Z_$term_3^{\prime} . We note that \rho _T is used, ensuring that names unbound in Z_ $\rho$_Z are kept unchanged (since \rho _T maps them to themselves). Strict name replacing requires all names to be bound by the set of bindings. For instance, strict name replacing of term_3 , written replace_{strict}(term_3,\rho) , is defined as the same term term_3^{\prime} , under the condition that \rho (\nu) is defined for each name \nu ranging over \alpha, \beta, \delta, \gamma, \mu, \tau . If \rho (\nu) is not defined for one of the names Z_$\nu$ _Z, then the strict replacing operation is not defined for the whole term. However, neither permissive nor strict replacing suitably takes the prov semantics into account. The names \alpha and \beta are mandatory, whereas names \tau, \delta$ _Z and names in attribute-value position Z_$\gamma are all optional. Thus, we introduce the provenance-aware replacing strategy , Z_ $replace_{pa}$_Z. If there is no binding for optional names, they are replaced by the distinguished symbol - for \tau, \delta ; likewise, an attribute-value pair \kappa =\gamma is not included, if there is no binding for name \gamma . If a binding is missing for a mandatory name, then the whole replacing operation fails, like in the strict strategy. So, let us define \begin{equation*} \rho _{-}(\nu)= \left\lbrace \begin{array}{ll}\rho (\nu) & \text{if} \rho (\nu) \text{is defined}\\ - & \text{if} \rho (\nu) \text{is not defined.} \end{array} \right. \end{equation*} We define replace_{pa}(term_3,\rho)=term_3^{\prime} , if \alpha ^{\prime}=\rho (\alpha),\beta ^{\prime}=\rho (\beta) are both defined, and \tau ^{\prime}=\rho _-(\tau),\delta ^{\prime}=\rho _-(\delta)$ _Z. For a pair Z_$\kappa _i=\gamma in term_3 , if \gamma is bound to a single name, there is an attribute key-value pair \kappa _i=\gamma ^{\prime} , with \gamma ^{\prime}=\rho (\gamma) . If \gamma is bound to a set of constants, for each Z_ $\pi \in \rho (\gamma)$_Z, there is an attribute key-value pair \kappa _i=\pi$ _Z. If Z_$\rho (\gamma) is not defined, then the pair is not included in term_3^{\prime} . For term_3^{\prime} to remain syntactically correct, each name in \rho must be bound to a name, except for those names occurring only in attribute-value position (Z_$\gamma$ _Z), which are allowed to be bound to any constant. Of course, in the special case when a name \nu occurs in attribute-value position, such as in \kappa =\nu , and also elsewhere in a term, if \rho (\nu)=\nu ^{\prime} , then an implicit conversion takes place into a set of constants Z_$\lbrace \nu ^{\prime}\rbrace$ _Z allowing instantiation into the attribute-value pairs Z_$\kappa =\nu$ _Z. 4.4 Linked Names Let us imagine that we have to design the provenance for a book; attribution could be used to link the book to its author. In the case of multiple authors, we do not want to have to specify a template for each possible number of authors. Instead, we prefer to define a template containing one attribution relation between a book and an author, and provide bindings for multiple authors. For simple binary relations, denoted by term_2 in Fig. 7, it may be interesting to specify their type (one-to-one, one-to-many, many-to-one, many-to-many), so that when a name is given multiple values, it is easy to understand how to expand a template. However, term_3 , term_4 , and term_5 show that we are not just considering binary relations, but generalized n-ary relations over names. So the question is: if names are to be bound to multiple values, which names are expected to be given a similar number of values, and how should the expansion proceed. Against this background, prov-template introduces a notion of linked name. Definition 2 (Linked Name). Two names \nu _1,\nu _2 in a template t are said to be linked if Linked_t(\nu _1,\nu _2) holds, where the relation Linked_t is obtained by the symmetric, transitive closure of the relation Link , iteratively computed as follows for all terms of t : If node(\mu, [\mathrm\sf {{t:linked}}=\gamma, \ldots ]) , then Link(\mu, \gamma) . If rel_i(\tau ;\beta, \alpha, \ldots, [\mathrm\sf {{t:linked}}=\gamma, \ldots ]) for i\in \lbrace 2,3, 4\rbrace , then Link(\tau, \gamma) . By extension, {{Names}}(Linked_t) denotes the set of names related by Linked_t . Example 1. Let us consider the template of Fig. 2. There is no occurence of the \mathrm\sf {{t:linked}} attribute. Therefore, Linked_t=\emptyset and {{Names}}(Linked_t)=\emptyset . Example 2. Let us call “linked-Fig. 2” a variant of template of Fig. 2, in which the variables var:consumed and var:produced are linked, meaning that for each output there is a single input. entity(var:consumed,            [t:linked='var:produced']) In that case, Linked_t=((\mathrm{{\tt var:consumed}}, \mathrm{{\tt var:produced}}), (\mathrm{{\tt var:produced}}, \mathrm{{\tt var:consumed}}))$ _Z and Z_${{Names}}(Linked_t)=\lbrace \mathrm{{\tt var:produced}}, \mathrm{{\tt var:consumed}}\rbrace . Given the Linked_t relation, we can construct a partition of names. Furthermore, given a strict ordering of names, we can construct a unique sequence of names sets, as follows. The following definition relies on the notion of indexing name, which can be found for each term in Fig. 7. Definition 3 (Link-Partition). Let {{I}}(term_t) be the set of indexing names in a term term_t from a template t . Let {{P}}(term_t) be the set of partitionable names, defined as the union of {{I}}(term_t) and \lbrace \gamma \ |\ \gamma \in term_t\ \wedge \ \gamma \in {{Names}}(Linked_t)\rbrace . A link-partition is a sequence N_0, N_1, \ldots, N_{m-1} of sets of names such that \forall k, 0\leq k<m, \forall i,j, 0\leq i<j<m , the following holds: \cup _i N_i={{P}}(term_t) ; N_i\cap N_j=\emptyset ; \forall \nu _m,\nu _n\in N_k, Linked_t(\nu _m,\nu _n) ; \exists \nu ^i\in N_i , such that \forall \nu ^j\in N_j, \nu ^i< \nu ^j . In Definition 3, Clauses 1 and 2 show that N_0, N_1, \ldots, N_{m-1} is a partition. Clause 3 states that each N_k contains linked names. Clause 4 relies on name ordering over {\mathcal {N}} introduced in Fig. 7. We are now equipped with a mechanism that allows us to uniquely enumerate names according to the way they have been linked. Example 3. Let us consider term wasDerivedFrom(var:produced, var:consumed) in Fig. 2. {{I}}(.) for this term is \lbrace \mathrm{{\tt var:produced}}, \mathrm{\tt var:}\tt{consumed}\rbrace$ _Z. So is Z_${{P}}(.) . Its link-partition is N_0=\lbrace \mathrm{\tt var:}\tt{consumed}\rbrace , and N_1=\lbrace \mathrm{{\tt var:produced}}\rbrace , assuming that we used lexicographic order over variable names. Example 4. Let us consider term wasGeneratedBy(var:produced,                                          var:block_instance,                                      [ t:time='var:produced_at',                                        estat:bindingname                                           ='var:produced_name']) in template “linked-Fig. 2” described in Example  2. {{I}}(.) for this term is \lbrace \mathrm{{\tt var:produced}}, \mathrm{{\tt var:block\_instance}}\rbrace . So is {{P}}(.) . Its link-partition is N_0=\lbrace \mathrm{\tt var:block\_}\mathtt{instance}\rbrace , and N_1=\lbrace \mathrm{{\tt var:produced}}\rbrace . Example 5. Instead, if we consider term wasDerivedFrom(var:produced,                                          var:consumed), in template “linked-Fig. 2” described in Example  2. {{I}}(.) for this term is \lbrace \mathrm{{\tt var:produced}}, \mathrm{{\tt var:consumed}}\rbrace$ _Z. So is Z_${{P}}(.) . Its link-partition is N_0=\lbrace \mathrm{{\tt var:produced}}, \tt{var:}\tt{consumed}\rbrace . 4.5 Complex Sets of Bindings and Template Expansion While a simple set of bindings allows for one value (either a name or a set of constants) for each variable, a complex set of bindings allows for multiple values for variables. Such bindings enable cross-products of values to be created by template expansion. Thus, a complex set of bindings for a template is defined as a total function \phi _t that maps each name of the template to a vector of names (from {\mathcal {N}} ) or vectors of sets of values (from {\mathcal {P}} ) \begin{equation*} \phi _t: {{Names}}(t)\rightarrow {Vector}({\mathcal {N}})\cup {Vector}(IP({\mathcal {P}})). \end{equation*} A Link-Partition indicates which partitionable names are linked. When multiple bindings are supported, names in a given partition are regarded as an array of names, being simultaneously assigned values identified in bindings. Therefore, the number of values associated with names in a complex set of bindings must be the same, if those names belong to the same partition; this number provides the number of possible assignments for the names of the partition. This is formalized as follows. Definition 4 (Binding-Partition Compatibility 1). Let us consider {{P}}(term_t) , the set of partitionable names in a term term_t from template t ; its link-partition N_0, N_1, \ldots, N_{m-1} , and a complex set of bindings \phi _t . A complex set of bindings \phi _t is compatible with the link-partition N_0, N_1, \ldots, N_{m-1} , if the following holds: \begin{eqnarray*} {\forall k, 0\leq k< m, \forall \nu _i,\nu _j\in N_k,}&&\\ length(\phi _t(\nu _i))&=&length(\phi _t(\nu _j)), \end{eqnarray*} i.e., the number of possible bindings for two names \nu _i,\nu _j$ _Z from the same partition is the same. Let us denote this number as Z_$size_{\phi _t}^k for N_k . Therefore, an integer \iota in [0,size_{\phi _t}^k-1] can act as an index for a value in the vector \phi _t(\nu) for any \nu \in N_k . Example 6. Building on Example 3, the complex set of bindings \phi _t=\lbrace \mathrm{{\tt var:produced}} \rightarrow \langle p_1, p_2\rangle, \mathrm{{\tt var:consumed}} \rightarrow \langle c_1\rangle \rbrace , which defines two generated entities for one used entity is compatible with the link-partition of Example  3 and thus satisfies Definition  4. Example 7. Following on Example 4, the same complex set of bindings \phi _t=\lbrace \mathrm{{\tt var:produced}} \rightarrow \langle p_1, p_2\rangle, \mathrm{\tt var:}\tt{consumed} \rightarrow \langle c_1\rangle \rbrace is not compatible with Definition 4, because the number of values associated with var:consumed and var:produced is not the same. What about names that are neither indexing nor linked? The purpose of Definition  5 is to set expectations for the number of values to be found associated with those names in a complex set of bindings. Definition 5 (Binding-Partition Compatibility 2). Let us consider {{P}}(term_t) , the set of partitionable names in a term term_t from template t ; its link-partition N_0, N_1, \ldots, N_{m-1} , and a complex set of bindings \phi _t . A complex set of bindings \phi _t is compatible with the link-partition N_0, N_1, \ldots, N_{m-1} , if the following holds: \begin{eqnarray*} \forall \nu, \nu \not\in {{P}}(term_t),\\ length(\phi _t(\nu))&=&\text{0 or} size_{\phi _t}^0\times \ldots size_{\phi _t}^{m-1}, \end{eqnarray*} i.e., the number of bindings for a non-partitionable name \nu is given by the number of possible combinations with all partitionable names. Example 8. Building again on Example 3, \phi _t=\lbrace \mathrm{{\tt var:produced}} \rightarrow \langle p_1, p_2\rangle, \mathrm{{\tt var:consumed}} \rightarrow \langle c_1\rangle, \mathrm{\tt var:}\tt{block\_instance}\rightarrow \langle b_1\rangle, \mathrm{{\tt var:produced\_at}} \rightarrow \langle t_1,t_2\rangle \rbrace satisfies Definition  5, because it has 2 bindings for var:produced_at , since size_{\phi _t}^0=1 and size_{\phi _t}^1=2 . Binding-Partition compatibility requires both Definitions 4 and 5 to be satisfied. Definition 6 (Multi-Index). Let us consider {{P}}(term_t) , the set of partitionable names in a term term_t from template t ; its link-partition N_0, N_1, \ldots, N_{m-1} , and a complex set of bindings \phi _t compatible with the link-partition. A multi-index \omega =\langle \iota _0,\iota _1,\ldots, \iota _{m-1}\rangle \in I N_0\times I N_1\times \ldots I N_{m-1} is a tuple of naturals such that each \iota _k belongs to interval [0,size_{\phi _t}^k[ for 0\leq k<m . Multi-indices \omega _0,\omega _1,\ldots can be ordered lexicographically. Let us call lex the function that returns the position in this lexical sequence for any multi-index. Example 9. Using the bindings \phi _t of Example  8, for which we have link-partition N_0,N_1 of Example 3, we have \omega _0=\langle 0,0\rangle , and \omega _1=\langle 0,1\rangle . The lex funtion is defined as lex(0)=\omega _0$ _Z and Z_$lex(1)=\omega _1 . A multi-index enables us to extract a simple set of bindings from a complex set of bindings, by means of a projection operation defined as follows. Definition 7 (Projection). Let us consider {{P}}(term_t) , the set of partitionable names in a term term_t from template t ; its link-partition N_0, N_1, \ldots, N_{m-1} , a complex set of bindings \phi _t compatible with the link-partition, and a multi-index \omega =\langle \iota _0,\iota _1,\ldots, \iota _{m-1}\rangle . The projection operation project(\phi _t,\omega) is a simple set of bindings \rho , such that: Z_$\forall k\in [0,m-1], \nu \in N_k, \rho (\nu)=\phi _t(\nu)[\iota _k]$ _Z, or \forall \nu \in {{Names}}(term_t), \nu \not\in {{P}}(term_t), \rho (\nu)=\phi _t(\nu)[lex(\omega)] . Example 10. Using the multi-index of of Example 9 and the bindings \phi _t of Example  8, the projection Z_ $project(\phi _t,\omega _1)$_Z is \lbrace \mathrm{{\tt var:produced}} \rightarrow \langle p_2\rangle, \mathrm{{\tt var:consumed}} \rightarrow \langle c_1\rangle, \mathrm{\tt var:block\_}\tt{instance}\rightarrow \langle b_1\rangle, \mathrm{{\tt var:produced\_at}} \rightarrow \langle t_2\rangle \rbrace , in which the values for the second produced entity have been seelcted. We are now ready to define template expansion. We start with the expansion of a term. Definition 8 (Term Expansion). Let us consider {{P}}(term_t) , the set of partitionable names in a term term_t from template t ; its link-partition N_0, N_1, \ldots, N_{m-1} , and a complex set of bindings \phi _t compatible with the link-partition. Let \Omega _{term_t} be the set of all multi-indices for term term_t . The expansion of term_t for \phi _t is the set of terms defined as \begin{align*} expansion& (term_t,\phi _t)=\\ \quad \lbrace s_\omega& \ \ |\ \ \omega \in \Omega, \\ & replace_{pa}(term_t,project(\phi _t,\omega))\ \text{is defined},\\ & s_\omega =replace(term_t,project(\phi _t,\omega))\rbrace, \end{align*} where replace denotes one of the functions replace_{perm} , replace_{strict}, replace_{pa} . ProvToolbox [29], which contains an implementation of prov-template, allows users to choose a replacement function. For instance, the strict mode, which terminates with an error code if some template variables have not been bound, is particularly useful when debugging the application since it allows us to check whether bindings are fully constructed. When an application is deployed, we would use the provenance-aware mode, which drops unbound optional variables. In Supplementary Material, available online, we show an application of the permissive approach, in which templates get partially instantiated to create new templates. Finally, we can define a template expansion as the union of all expansions of terms, as per Definition  8. Definition 9 (Template Expansion). Given a template t=\sf {bundle}\ \mu \ term^* and a complex set of bindings \phi _t \begin{eqnarray*} {expansion(t,\phi _t)}\\ &=& \sf {bundle}\ \mu ^{\prime}\ \lbrace \cup _{term\in term^*} expansion(term,\phi _t)\rbrace, \end{eqnarray*} with \mu ^{\prime}=\phi _t(\mu) . It is assumed here that a single value is provided for the bundle name. 5   Quantitative Evaluation In the prov-template approach, templates specify the topology of the provenance to be generated, whereas bindings contain values required to create the provenance. Given that bindings contain no topological information, the intuition is that bindings are more compact than the expanded templates. The first part of this section contrasts the size of bindings and the size of expanded provenance templates, systematically, across the range of templates supported by the applications discussed in Section 2.1. We observe that the size of bindings is on average 40 percent of the size of expanded templates. This saving is beneficial in terms of both reduced communication cost and reduced storage cost. Indeed, application components only need to submit bindings to a provenance repository, instead of expanded provenance; a more compact representation of bindings reduces communication overheads. Likewise, one may consider provenance repositories that only persist bindings, instead of expanded provenance: they will result in more compact storage. The savings in communication and storage should be understood in the context of the extra cost of expanding templates with some bindings. We show that the cost of expansion itself is very modest. On average, across the applications considered, it takes 0.23 ms to expand a template, which would allow over 4,000 expansions to take place on a single core every second. For the quantitative evaluation, we consider the four applications presented in Section 2.1, which are referred to as \sf {smart} ( Section 2.1.1), Z_$\sf {food}$ _Z ( Section 2.1.2), Z_$\sf {ebook}$ _Z ( Section 2.1.3), and \sf {picaso} (Section 2.1.4). Table 2 summarizes the number of bindings sets and templates per application. TABLE 2 Number of Bindings Sets and Templates per Application  The following evaluation relies on ProvToolbox [29], a Java library to manipulate prov representations, and which includes an implementation of prov-template. The performance evaluation was run on a MacBook Pro OSX 10.1, with an Intel Core i7, 2.7 GHz, and 16 Gb of Memory. We adopted the following procedure for preparing the test data. prov has multiple serializations (RDF, XML, Text) but does not have a canonical representation. Thus, to be able to compare sizes of bindings with sizes of expanded provenance templates, we applied the following transformations. Provenance files are converted to Turtle [37], with all qualified names expanded as URIs, and then converted to prov-n  [26], allowing for new namespace prefixes to be automatically allocated. The conversion to Turtle allows terms to be merged  [31], where appropriate, whereas the conversion to prov-n allows for a compact representation. Bindings are serialized to JSON, as in Fig. 3 but without pretty printing. Having prepared the data, we applied the following method to generate Fig. 8 , which shows the compaction ratio for each template. Apply the expansion algorithm for each set of bindings and template pair. Fig. 8. Compaction ratio, per application, per template. The legend shows the color adopted for each application. Colored horizontal lines are the median compression ratio for each application. Compute the compaction ratio by dividing the size of each set of bindings (as a JSON file), by the size of the expanded provenance templates (as a prov-n file, prepared as above). Create the box plot as per Fig. 8, where the x -axis enumerates the templates, and the Z_$y$ _Z-axis indicates the compaction ratio. The x -axis of Fig. 8 lists the templates, grouped per application (see the legend for the application color coding). The y -axis indicates the compaction ratio. A compaction ratio equal to 1 means that the size of a set of bindings is the same as the size of an expanded template. The smaller the compaction ratio, the more “efficient” the representation of a set of bindings is. For each template, a box plot shows the median compaction ratio, the first quartile and third quartile, and the minimum and maximum compaction ratios. Each of the 6,893 sets of bindings resulted in a compaction ratio less than one. Table 3 provides a numerical summary of Fig. 8 per application. On average, across all the sets of bindings, the compaction ratio is 40 percent. TABLE 3 Summary of Compaction Ratios  Fig. 8 shows some variability in the range of compaction ratios for some templates. For instance, in \sf {ebook} 's block_run template, the bindings leading to the smallest ratio (0.35) are about a block with a large number of outputs (31), whereas, at the other end of the scale, bindings with ratio 0.8 are all for blocks that consume and produce one entity. Likewise, the \sf {food} application's foodspec template was designed to contain general descriptions of food specification. The bindings resulting in the lowest ratio (0.24) contain very little details about the food product, whereas the bindings resulting into the highest (0.73) contain long textual strings overwhelming the topology found in the expanded template. We followed a similar procedure to produce performance data. (A plot is available in Supplementary Material, available online.) For each set of bindings and template pair: Run the expansion algorithm w times Repeat c times: Measure average time over n template expansions Compute average over c measures Normalize measure with respect to length of the sets of bindings. Create a box plot with templates in the x -axis and normalized averages in the y -axis. We ran the process with (w,n,c)=(1,000,40,20) . The value w was selected to introduce a delay before taking measurements, to allow for JVM warm up. Of course, the bigger a template is, the longer its expansion; likewise, the bigger a set of bindings is, the longer the expansion is. For the box plot (available in Supplementary Material, available online), to be able to make meaningful comparisons, we normalized the computed average time with respect to the size of sets of bindings in kilobytes. It is important to note that this experiment only measures expansion time, and does not include the input/output time necessary to read templates and bindings, and write the expanded template. Table 4 summarizes the raw expansion times (without normalization) for the various applications. We see that in average the expansion time is 0.23 ms. This type of performance would allow over 4,000 expansions to be performed on a single core per second. This shows that the approach is entirely tractable. Furthermore, it is to be noted that our implementation follows the definitions of Section 4 and currently does not optimize the expansion process. Section 9 suggests ways of improving the performance of template expansion. TABLE 4 Summary of Expansion Time (in ms)—Mean, Standard Deviation, Median—and Average Bindings Set Size (in Bytes)  We observe that the average expansion time is significantly larger for the \sf {food} application, but likewise, the average size of sets of bindings for this application is larger. This confirms our initial hypothesis that the larger a set of bindings, the longer the expansion time. To validate this, we applied Pearson's correlation test and obtained a Z_ $\rho$_Z -value of 0.8879575 and a p-value 2.2 \times 10^{-16} showing a strong correlation. The sets of bindings in the four applications extensively used the ability to provide multiple values for variables. However, only in a couple of cases did combinatorial explosions occur. Indeed, either the corresponding templates used linked variables to assign values simultaneously, or only one variable was given multiple values, while the others had a single value, resulting in 1-to-n or n-to-1 relations in the expanded template. The most notable case of combinatorial explosion occurred in \sf {ebook} (template block_run ) and involved a binary relation with two values for one variable, and 31 for the other, resulting in 62 instances. With the combinatorial effect on the expanded template, the compression ratio is the lowest (0.319), whereas the absolute expansion time is the largest (1.04 ms), though its normalized expansion time is not an outlier. 6   Bindings Generation The quantitative evaluation of Section 5 demonstrated that bindings are a more compressed representation than that of expanded templates. Bindings representation is more space efficient because it is devoid of topological information: definitions of nodes and edges are to be found in template definitions, whereas the bindings only contain associations between variables and values to instantiate them. This efficient representation also brings some benefits in terms of programming the generation of bindings. First, we discuss some techniques to create bindings easily and efficiently ( Section 6.1). Second, we examine how the development environment can help check whether bindings are well-formed and are aligned with template definitions (Section 6.2). In Supplementary Material, available online, all these techniques are illustrated by examples or code fragments. 6.1 Ease and Performance of Generation We consider four different techniques to generate bindings. Depending on the context of the application, they can be potentially combined together. 6.1.1 Abstract Bindings Creation As discussed in Section 2.4, programming the generation of a provenance graph typically involves a method call for constructing each node and each edge. This requires the programmer to understand the topology of graphs to be generated. Further, these method calls should receive all necessary values to specify the attributes of these nodes and edges. This also typically involves a substantial amount of repetition in the code, since, for each node, there will be incoming and outgoing edges that require the node identifier (or a reference to it) to be passed to the edge constructors. Instead of this, a library to construct bindings enables the programmer simply to identify which variable is associated with which value(s). Thus, no requirement is put on the programmer to understand the graph topology and replicate node identifiers across constructors of its adjacent edges. A library can take care of serializing the in-memory representation of bindings to their concrete serial format, relieving the programmer from knowing the bindings syntax. ProvToolbox provides a reference implementation of such abstract bindings. 6.1.2 Concrete Bindings Creation Given the simple syntax of bindings as a JSON dictionary, it is also easy for the programmer to generate their textual representation directly, or build a dictionary structure (say in Javascript) that serializes directly to JSON. Such technique is particularly useful for programming languages that do not have a library for abstract bindings. It is used in the \sf {smart} application, in which various components logged bindings constructed in their serialization format. 6.1.3 Converting Tabular Values Application data is often available or exportable in tabular format, for instance, in the standardized CSV format [38]. If columns are labeled with the names of variables, each row can be converted into a set of bindings, whether abstract (Section 6.1.1 ) or concrete (Section 6.1.2). Such technique is used in the \sf {food} application, in which food-related data, already existing in a tabular format, is converted to bindings. 6.1.4 Bindings Fragments Let us consider the template of Fig. 1; an activity may run for a long time. If bindings can only be generated at the end of an activity, it means that there may be portions of provenance that may not become available for a long time. This also places an unnecessary burden on the bindings generation code to hold on to values, until the last one becomes available, potentially resulting in memory leaks. In such a case, it may become desirable to create bindings fragments , containing bindings for a subset of the variables of a template. Such technique is used in the \sf {ebook} application, in which bindings fragments are logged asynchronously, and a separate process reconstructs whole bindings out of fragments extracted from the log. The decoupling of bindings generation and provenance generation is critical to preserving the application's performance. 6.2 Support for Checks A potential challenge with the above techniques is that, while the programmer's task is facilitated because there is no requirement to program the topology of provenance graphs, a potential new source of error comes with variables names, and the burden of ensuring that they correspond to the variables occurring in the templates. To address this problem, a constructor of bindings can be generated automatically from a template definition, creating methods such as addConsumed in charge of adding a binding for the variable consumed . If a variable is renamed in a template, then the bindings constructor can be regenerated. At compile-time, it can be detected if the application code refers to the older method name. ProvToolbox provides an implementation of the bindings generator. Another type of check that can be performed on a set of bindings is related to the number of values associated with variables occurring in a specific group, as defined in Section 4. The \sf {picaso} application uses further template metadata to control the user interface, to generate bindings that satisfy constraints on the number of values for variables (for instance, related to minimal or maximal variable cardinality). 7   Practical Experience In this section, first, we discuss the granularity of templates and bindings; second, we revisit the benefits introduced in Section 1, and provide some evidence supporting these, from the four applications that adopted prov-template. 7.1 Templates and Bindings Granularity A question a designer inevitably faces is the granularity with which templates, and to some extent bindings, should be associated with computational modules. We have encountered different cases. In \sf {smart} , a template is typically associated with a method, or a sequence of method invocations, when it is desirable to abstract away from them. In the \sf {food} application, the granularity of templates is dictated by the data the application ingests (invoice, food specification, inspection report): templates are designed to be general so that, for instance, they can accommodate invoices from multiple food suppliers. In \sf {ebook} , a template corresponds to a workflow step. Finally, in \sf {picaso} , each template corresponds to an editable user interface, visualizing the template, allowing values to be dragged on the interface, to create bindings for that template. In general, if a REST application has to be instrumented to generate provenance, templates could be associated with REST operations. For \sf {smart} , \sf {food} , and \sf {picaso} , a full set of bindings is created and submitted, all at once, for template expansion. On the other hand, in \sf {ebook} , as workflow steps can be long, bindings fragment can be submitted independently (See Section 6.1.4 ). 7.2 Benefit 1: Separation of Responsibilities The \sf {smart} application was iteratively developed by four organizations, which contributed various aspects of the design and implementation of three components: user interface (UI), a ride matching service (Orchestrator) and a feedback and rating service (Reputation System). Each component recorded provenance using prov-template exposed as a web service allowing sets of bindings to be submitted for expansion by the components distributed across the Web. The first stage of development required a prov expert to design templates in consultation with each component's lead developer, so as to reflect the component's business logic in the template. prov expertise was required to design the templates in order to support the provenance use cases [39] targeted by the application. In the second phase, the initial integration was completed via pair programming (involving a developer and a prov expert), to ensure that the correct sets of bindings were submitted for the expected template. Since bindings creation can be error-prone in the absence11 of automated checks (see Section 6.2), and the component developers were not familiar with prov and prov-template details, such a type of pair programming was regarded as the most efficient way of minimizing development effort. Over time, as better tools and better training material become available, the needs for a provenance expert will be significantly reduced (see future work in Section 9). In the third phase, further changes to the values of the variables in the bindings were completed by the components’ developers with essentially no support required. 7.3 Benefit 2: Maintenance Templates may need to be changed as applications are redesigned and evolve, potentially due to new requirements or bug fixes. In turn, bindings may be required to change. In this section, we overview broad categories of changes that may be applied to templates, we then review how bindings generated according to techniques of Section 6 may have to be changed. Table 5 summarizes broad types of template changes. Like code, templates may need to be refactored: templates may be renamed (1), while their contents remain unchanged. A new template may be added (2), when a new component is added to an application or a behavior of the application needs to be described by further provenance. On the contrary, components may be decommissioned and corresponding templates dropped (3); or alternatively, templates may be dropped because superseded by more recent ones. Templates can be merged (4) or split (5), depending on the granularity and timing at which provenance needs to be created. Finally, templates may be modified (6) in various ways that we now discuss. Modifications may preserve the graph topology (6.1–6.4) or may alter it (6.5–6.10). Topology-preserving modifications include changing constants, changing ontology terms, and adding or dropping attributes constants. Topology-altering modifications include adding and dropping nodes and relations, and adding and dropping variables. We note that template operations 6.9-6.10 are not typically performed in isolation, but are occurring in conjunction with other changes. In practice, a template modification usually involves multiple of the changes described in Table 5 . TABLE 5 Types of Template Evolution  Table 5 also shows how bindings remain correct (✓) even in the presence of modifications to templates. When a template is added, templates are merged, or a new variable is added, bindings become potentially incomplete (I), resulting in a partially constructed provenance. When a template or a variable is dropped, or when templates are split, some bindings may include associations for variables that become superfluous (S), but are ignored by template expansion. Automatic bindings bean generation ( Section 6.2) allows for a number of those changes to be detected at compile-time (C): a compilation error indicates that the application attempts to create bindings using incorrect names for variables or templates. In some cases, further checks can be performed on bindings at runtime (R), ensuring for instance that all variables have been bound with the required number of values. In the \sf {smart} application, there were several iterations of the templates because of the application's iterative design and distributed development. Table 6 describes how templates in \sf {smart} were refined. Templates were refined by the prov expert, but critically, consisted of changes that did not require the bindings submitted by a component to be altered, and the captured data was still valid; in other words, the changes to the template did not lead to extra development effort in the components and to data conversions. The Orchestrator required the largest number of versions because its provenance was the most complex, with respect to the number of terms in templates, and because of evolving requirements around the targeted provenance use cases. The other components required fewer changes, essentially thanks to their simplicity. TABLE 6 Provenance Templates in Smart and Their Changes  The revisions presented in Table 6 are in fact among the simplest cases of template evolution listed in Table 5. For instance, some of the template changes are due to an adjustment of the project's vocabulary. The technique we presented here complements ontology-oriented software engineering [40], by ensuring that correct URIs are included in programs to refer to the correct Semantic Web concepts. We acknowledge that the maintenance effort was particularly minimal in \sf {smart} because, even though the application was evolving, its broad architecture remained stable, and the use cases for which provenance was captured did not evolve. Thus, bindings that were logged remained correct over the application development cycle. We describe a further situation to illustrate how prov-template helps software maintenance. The application \sf {picaso} underwent a complete change in its templates when a type defined in an ontology had to be replaced by another type belonging to another ontology. Specifically, every occurrence of xsd:QName was replaced by prov:QUALIFIED_NAME across all templates. The reason for this change was to ensure better inter-operability with the prov specifications (see Section 2.4). The application code was left unchanged. The database containing the stored bindings did not need to be changed either. Only the revised templates were required to be expanded again with the same bindings. 7.4 Benefit 3: Runtime and Static Checks Definitions 4 and  5 already specify how to check that a set of bindings is compatible with a template definition. These checks can be performed at expansion time, but could also be executed at binding creation time. For instance, from a template definition, one could generate code that constructs sets of bindings, while ensuring by construction that they remain compatible with the template they are meant to be used with. URIs are used by templates to denote types in external ontologies. Our experience shows that it is fairly frequent to introduce incorrect URIs in the definition of templates. Such a problem can be addressed in part by extracting all URIs from a template definition, and check that they have been defined in a set of imported ontologies (see Fig. 6). Of course, this check is purely syntactic, and can only identify URIs that have not been declared previously. Some form of semantic reasoning would be required to detect if the correct URI has been referenced in a template. The semantics of prov  [31] associates temporal constraints with a core subset. For a set of provenance statements to be meaningful—referred to as “valid” statements—the constraints associated with that set should be satisfiable. A necessary condition for an expanded template to be valid is that the template itself is valid. However, the expansion of a valid template is not guaranteed to generate a valid provenance graph; indeed, some bindings may for instance cause a cycle of derivations to occur in the expanded provenance graph, which would render it invalid. Given that templates are in fact provenance graphs, their validity can be checked at design time using a provenance validator (such as  [41]). Reasoning could also be applied to templates to check that they satisfy some domain-specific constraints. For instance, one may want to check that the types of entities are compatible with the types of the activities that use and generate them. Such a type of reasoning, referred to as semantic validation  [42], relies on ontologies being available and referred to by templates. While automatic methods such as validation and domain-specific reasoning are powerful, the most common automatic operation we have applied to templates is checking whether they are syntactic well-formed. Furthermore, it is important not to dismiss the power of manual methods, since such methods remain practical given the relatively small size of templates, compared to the whole provenance being generated by an application. The most common visual checks that we perform are: detecting whether a template contains disjoint graphs, detecting the presence of loops, or detecting the absence of an edge or some attribute. For instance, in Fig. 1 , we may want to decide that we need to provide a type attribute for the generated entity; likewise, it would have been very easy to point out that an edge is missing, should it have been the case. These tasks would have been more challenging if they had to be performed on the provenance generated by an application. In the applications of Section 2.1, we have not detected examples of invalid provenance templates, because of the continuous manual checks we performed when designing the templates. 7.5 Benefit 4: Provenance Consumption There is a potential software engineering benefit in using prov-template for applications that consume the provenance they have generated. Instead of running graph queries over provenance, applications can instead run queries over the stored bindings. This technique is used by two of our applications. The application \sf {ebook} converts bindings back to workflows that can be executed, whereas \sf {picaso} provides a graphical editor for the expanded templates directly from bindings. 8   Related Work The related work is structured as follows. First, we contrast coarse and fine-grained provenance ( Section 8.1); second, we survey techniques to capture provenance ( Section 8.2); third, we look at an alternative to prov-template and other similar graphs abstractions (Section 8.3). Finally, we position prov-template in the broader context of software engineering (Section 8.4 ). 8.1 Coarse-Grained and Fine-Grained Provenance Some authors [43] distinguish coarse-grained and fine-grained provenance, also commonly referred to as workflow and database provenance, respectively. The context and underpinning assumptions under which these approaches are conceived differ. In a database context, provenance explains which tables, rows, cells may have affected a query result, given a specific query that was run (for a survey of the field, we refer the reader to Cheney et al. [44]). Workflow provenance is regarded as more coarse-grained, because the workflow steps may not necessarily be detailed (e.g., a call to a Fast Fourier Transform) or workflow steps generate files, without the provenance of their contents being detailed. The prov data model is designed to express provenance and exchange it in an interoperable manner. It can be used not only to describe the flow of information, either in workflows or in database systems, but also to describe human participation in activities. When it comes to fine-grained provenance, representing it using prov is possible, although it is unlikely to result in a compact representation that the kind of dedicated database techniques can afford [44]. 8.2 Provenance: Instrumentation, Logging, Reconstructing, Compacting A simple approach to provenance generation is to instrument code, which requires interleaving provenance-generating code in the source code. This operation is not only labor intensive, since it requires fine-tuning of provenance capturing [45] to maintain adequate performance, but it also does require both application and provenance expertise. Cross-cutting concerns of provenance generation can be addressed by aspect-oriented programming, which allows monitoring probes to be weaved into an application  [20]. Instrumenting applications and generating provenance at the same time can be cumbersome from a software engineering perspective. Instead, traditional logging techniques have been combined with provenance reconstruction, in various contexts, with well-known logging tools  [19], system call tracing [16], or even at the level of the operating system kernel, such as PASS [17]. Workflow systems are a class of applications generating provenance, which use a mix of instrumentation and logging (cf. Taverna [1], Vistrails [2] , Kepler [3]). These systems are essentially monolithic “integrated development environments” that allow users to compose workflows and execute them while keeping a trace of execution in the form of a provenance log. Emerging approaches move away from such “integrated development environments” allowing disparate tools to be exploited by users. YesWorkflow [46], [47] allows scientists to annotate their scripts (in Python, R, or Perl) with special comments that reveal the main computational blocks and data flow dependencies, allowing the provenance of scientific results to be constructed and queried. To avoid code instrumentation, YesWorkflow assumes that critical information has been encoded in data product files and directory names, which allows full provenance to be inferred. Such “provenance-friendly” data organization involves URI templates, capturing how inputs and outputs are named and organized. Annotations specify notions of blocks, ports and channels, describing the static topology of the workflow. This approach is completely complementary with the one described in this article. The variables identified in the URI patterns of YesWorkflow can be used to create bindings, used by prov-template to generate prov documents. In some situations, in particular with legacy applications, we do not have the opportunity to modify the source code in order to insert provenance-generating code, or we do not have the possibility of weaving new aspects because we cannot recompile code, or we do not have the possibility of dynamically re-loading and re-linking legacy code. In those cases, data created by such legacy applications can be mined, using application knowledge, with a view of reconstructing provenance [23], [24]. These approaches use a range of techniques to reconstruct provenance. Structural information allows finer grained provenance to be reconstructed, whereas content similarity allows for high-level “information flows” to be described. These approaches typically introduce some uncertainty to indicate the level of confidence associated with the reconstructed provenance relations. Again, the topology of the provenance being reconstructed can also be expressed with templates, to be instantiated with runtime values. Finally, ProvGen is a graph generation technique for provenance [48], which relies on a seed graph, very similar to a template, combined with a set of constraints describing how it can be repeated. We conjecture that the provenance generation component could be completely decoupled from the node generation part, which would then allow prov-template to be used here too. As prov-template bindings are devoid of topology information, bindings have been shown to be more compact than expanded provenance (see Section 5). There has been work investigating techniques to compact provenance, while still maintaining its queryability: Chapman et al. [49] propose factorization techniques, allowing common patterns of provenance information to be identified, and the amount of required storage to be reduced, while still be efficiently queryable. Our experience is that templates tend to specify the shape of provenance for small subgraphs, typically in the close vicinity of an activity. By operating over a whole provenance graph, factorization techniques stand a better chance of compressing provenance. On the other hand, the localized nature of template allows clients, submitting provenance to a provenance repository, to benefit from bindings compact representation. 8.3 Provenance Templates and Views Curcin et al. [50] also propose a notion of template, seen as “a higher-level abstraction of the provenance graph data”. Their templates specify “basic conceptual units that can be recorded in a provenance repository.” Like prov-template, their patterns use a provenance graphical model, where nodes denote concepts rather than instances, but is extended with further constructs to model sub-graph repetition. A key differentiator between their approach and prov-template is that the latter has well-defined interfaces, in terms of templates and bindings in input, and expanded graphs in output, with clear standardized formats, and with a well-defined expansion algorithm. The template language of prov-template has some similarity with the provenance type graphs of Danger et al. [51], and Moreau's provenance summaries [52]. Provenance type graphs are combined with graph transformation techniques, such as removing and inserting nodes, to produce views over provenance graphs that satisfy some access control properties [51]. Alternatively, “user views”, defined as a partition of tasks in a workflow specification [53] , provide the means to selectively identify what aspect of a provenance trace should be exposed to users. prov-template is intended for generating provenance, whereas provenance type graphs, summaries, and views are intended to abstract away from the concrete details of provenance. The provenance type graphs  [51] build on Sun's Typed Provenance Model  [54] by allowing domain specific types to be exposed. In prov-template , such domain specific types can be found in the form of the prov:type attribute supported by the prov data model [6]. The graph transformation technique  [51] is capable of replacing sub-graphs by new nodes, i.e., creating graph abstractions. This is an example of graph rewriting applied to provenance; in contrast, prov-template does not perform full graph rewriting, but instead allows template nodes to be instantiated with one or more instances. Prospective provenance, a term coined by Wilde [55], denotes the “recipe” or procedure used to compute data products. So, prospective provenance is a description of what execution is intended to be; this should be contrasted to prov-template, which is a description of what provenance is to be. prov introduces the notion of prov:Plan, a plan intended by an agent to achieve some goals in the context of this activity, but does not provide any details about the nature of such plans. P-Plan [56] is an approach to prospective provenance, which uses some prov building blocks (such as activity, entity, usage, generation) to describe what an execution is intended to be like. While some relations are provided to link up actual activities and entities found in the provenance to their counterpart in the prospective provenance, P-Plan does not prescribe how provenance is to be shaped. P-Plan, like prov-template, uses a notion of variable to denote actual runtime entities. ProvONE  [57] is a recent community-based extension of prov to support scientific workflow provenance. Like P-Plan, it includes prospective provenance, but replaces variables by the notion of port commonly found in workflow systems [1], [3]. The Workflow Description Ontology, which describes workflow specifications included in Research Objects [58], instead uses a notion of datalink to specify data dependencies between the processes of a workflow. 8.4 Software Engineering and Meta Models PRIME [39], the PRovenance Incorporating MEthodology, is a software engineering methodology to “provenance-enable” applications. It consists of three phases to be applied iteratively. First, provenance use cases need to be elicited to identify the type of functionality that is expected out of provenance information. Second, the application is deconstructed into actors, processes, and information flows. Third, information items are exposed, provenance is then captured, and provenance functionality is implemented. PRIME does not specify how provenance is generated. Again, prov-template has a natural place in this software engineering methodology, since the templates can be expressed by the designer to address some provenance use cases, and can directly be used for provenance generation. More recently, a set of common provenance recipes  [14] has been put forward for provenance patterns commonly encountered in applications. Likewise, ProvErr [59] relies on engineers’ application knowledge to construct a dependency model aimed to support root cause analysis of system faults. Both provenance recipes and dependency models are good candidates for creating provenance templates. Zhu and Bayley [60] propose an algebra of design patterns, from which they derive a notion of equivalence between pattern expressions and a normal form for pattern expressions. It is an open question as to whether a similar algebra can be developed for provenance templates, allowing some reasoning to be made about how templates are composed. prov provenance draws upon the linked data [61] and Semantic Web approaches [62]. In this context, significant attention has been given to the problem of bringing knowledge and software engineering together  [40]. In particular, there has been a growing interest in applying ontologies to the various stages of the software engineering life cycle (for an overview, see [40]). For instance, there are approaches allowing conversion between OWL ontologies and UML models, and vice-versa  [63]. As we have already shown in Fig. 6 , prov-template builds on ontologies by making explicit references to classes and properties defined in ontologies. This presents researchers with opportunities to integrate prov-template in the software engineering process to bring forth its benefits (as discussed in Section 7 ). 9   Conclusion and Future Work Ease of generation remains an adoption hurdle for provenance technology. To address this challenge, we have presented prov-template, a practical approach that facilitates the generation of provenance. It consists of three parts. Templates provide a declarative way of specifying the provenance to be generated, with placeholders (referred to as variables) for values to be filled. Sets of bindings are simple JSON data structures associating variable names to values. An expansion algorithm creates a provenance document from a template, by replacing all variables by values found in a set of bindings. The expansion algorithm is capable of dealing with multiple values for variables. The approach is implemented by the ProvToolbox library, an open source library for manipulating provenance in Java. Our quantitative evaluation shows that exchanging sets of bindings rather than provenance documents incurs a significantly reduced cost in communications and/or storage, as the size of bindings is demonstrated to be on average 40 percent of that of expanded provenance documents. The performance evaluation also shows that the approach is tractable, with only fractions of milliseconds required for expanding typical templates. Our practical experience with prov-template over the course of two years has shown four benefits provided by the approach. It helps with separation of responsibilities, allowing distributed developers to focus on code writing and information logging, whereas a prov expert can focus on the design of provenance templates and their deployment in an application. prov-template facilitates maintenance of provenance since it allows minor revisions of provenance to be supported, without having to modify the application, as long as the templates still rely on the same logged values. prov-template allows for an application-wide library of templates to be assembled, and a series of static and dynamic checks to be supported; these checks help the application log the necessary information to create provenance correctly. Finally, prov-template allows for applications that consume their own provenance to exploit the regular structure of bindings, rather than having to rely on graph queries over provenance. There are a number of opportunities to build upon prov-template in future work. If a designer specifies all the provenance to be generated in an application by means of templates, there is only a need to store sets of bindings and templates. Thus, we could envisage a notion of “provenance repository”  [64], in which prov-compatible provenance is only generated on demand, and is not persisted in that form. Instead, the only information that needs to be captured and stored is templates and sets of bindings. Pushing this approach to its logical end, the idea of a provenance template management system becomes crucial, with key functionality, including editing and storage of templates and their versions, migration of bindings to new templates, and on-the-fly prov-compatible provenance generation. Templates can further be “compiled” into code that generates prov efficiently from a set of bindings. Services for posting bindings can be generated automatically from templates, and perform compatibility checks directly, giving early feedback to developers if something goes wrong. Provenance is typically queried by means of graph queries, expressed in languages such as SPARQL. With the prov-template approach, provenance now consists of templates and bindings. Thus, provenance graph queries can be optimized by developing query plans that take into account the static nature of provenance templates, and by directly querying the bindings, which can also be indexed to improve performance. A theoretical strand of work could investigate the meaning of abstract graphs such as templates. There, nodes no longer represent instances but sets of these. With such a notion, one can also study the set of algebraic operations to process such graphs and the type of reasoning that is possible over such abstract graphs. Footnotes 1. https://www.thegazette.co.uk/ . 2. http://nca2014.globalchange.gov/report. 3. https://www.hl7.org/fhir/provenance.html. 4. http://www.opengeospatial.org/projects/initiatives/ows-10. 5. http://www.smart-society-project.eu/software/smartshare/. 6. http://www.bristol.ac.uk/cmm/research/ebooks/. 7. https://provenance.ecs.soton.ac.uk/picaso/. 8. Fig. 2 also contains names in the namespace t of prov-template. We refer the reader to the ProvToolbox template user's guide [32] for details of how such names are used to control the expansion of variables meant to generate time and string values. Such low-level practical details are not discussed any further in this paper. 9. Provenance Challenge implemented with ProvToolbox is available at  https://github.com/lucmoreau/ProvToolbox/tree/development/tutorial/tutorial5. The difference in size can be attributed to (i) Java's verbosity and (ii) programming styles favouring dynamic typing and reflection in Python and static typing in Java. 10. See the changes at https://github.com/trungdong/prov/commit/c7e21a9cbc551187cb335b7fa28032ef79d695c8 . 11. In that version of Z_$\sf {smart}$ _Z, the expert was involved in programming a library, whose aim was to assemble the bindings’ serial representation ( Section 6.1.2), which was posted to a remote service, expanding them and persisting the expanded provenance. The column marked (N) shows the number of versions for each template. Changes are described according to the classification of Table 5. Acknowledgments This work is funded in part by the EPSRC SOCIAM (EP/J017728/1) and ORCHID (EP/I011587/1) projects, the FP7 SmartSociety (600854) project, and the ESRC eBook (ES/K007246/1) project. The data referred to as \sf {smart} , \sf {ebook} , and \sf {picaso} supporting this study are openly available from the University of Southampton repository at DOI: 10.5258/SOTON/390436. As far as \sf {food} data are concerned, their sets of bindings cannot be made openly available because they contain commercially sensitive data, but the templates and measurements are available. References  [1]P. Alper, K. Belhajjame, C. A. Goble, and P. Karagoz, “Enhancing and abstracting scientific workflow provenance for data publishing,” in Proc. Joint EDBT/ICDT 2013 Workshops , 2013, pp. 313–318. [Online]. Available: http://doi.acm.org/10.1145/2457317.2457370 [2]C. Silva, E. Anderson, E. Santos, and J. Freire, “Using VisTrails and provenance for teaching scientific visualization,” Comput. Graph. Forum, vol. 30 , no. 1, pp. 75–84, Mar.2011. [Online]. Available: http://dx.doi.org/10.1111/j.1467–8659.2010.01830.x [3]I. Altintas, O. Barney, and E. Jaeger-Frank, “ Provenance collection support in the kepler scientific workflow system,” in Proc. Int. Conf. Provenance Annotation Data, 2006, pp. 118–132 . [Online]. Available: http://dx.doi.org/10.1007/11890850_14 [4]F. Chirigati, D. Shasha, and J. Freire, “ReproZip: Using provenance to support computational reproducibility,” in Proc. 5th USENIX Conf. Theory Practice Provenance, 2013, pp. 1–1. [Online]. Available: http://dl.acm.org/citation.cfm?id=2482613.2482614 [5]S. Ramchurn, et al., “HAC-ER: A disaster response system based on human-agent collectives,” Istambul, Turkey, May2015. [Online]. Available: http://eprints.soton.ac.uk/374070/ [6]K. Belhajjame, et al., “PROV-DM: The PROV data model ,” World Wide Web Consortium, W3C Recommendation REC-prov-dm-20130430, L. Moreau, P. Missier Eds., Oct.2013. [Online]. Available: http://www.w3.org/TR/2013/REC-prov-dm-20130430/ [7]R. Bose and J. Frew, “Lineage retrieval for scientific data processing: A survey,” ACM Comput. Surveys, vol. 37, no. 1, pp. 1–28 , Mar.2005. [Online]. Available: http://homepages.inf.ed.ac.uk/rbose/pubs/bose_2005_ACM_CS.pdf [8]S. Miles, P. Groth, M. Branco, and L. Moreau, “The requirements of recording and using provenance in e-science experiments,” J. Grid Comput., vol. 5, no. 1, pp. 1–25, 2007. [Online]. Available: http://eprints.ecs.soton.ac.uk/10269/ [9]L. Moreau, “The foundations for provenance on the Web,” Found. Trends Web Sci., vol. 2, no. 2/3, pp. 99– 241, Nov.2010. [Online]. Available: http://eprints.ecs.soton.ac.uk/21691/ [10]Y. Gil , et al., “Provenance XG final report,” World Wide Web Consortium, 2010. [Online]. Available: http://www.w3.org/2005/Incubator/prov/XGR-prov-20101214/ [11]D. J. Weitzner, H. Abelson, T. Berners-Lee, J. Feigenbaum, J. Hendler, and G. J. Sussman, “Information accountability,” Commun. ACM, vol. 51, no. 6, pp. 81–87, Jun.2008. [Online]. Available: http://hdl.handle.net/1721.1/37600 [12]L. Moreau, “Provenance-based reproducibility in the Semantic Web ,” Web Semantics: Sci. Serv. Agents World Wide Web, vol. 9, pp. 202–221, Feb.2011. [Online]. Available: http://eprints.ecs.soton.ac.uk/21992/ [13]T. D. Huynh, M. Ebden, M. Venanzi, S. Ramchurn, S. Roberts, and L. Moreau, “Interpretation of crowdsourced activities using provenance network analysis ,” in Proc. 1st AAAI Conf. Human Comput. Crowdsourcing, Nov. 2013. [Online]. Available: http://www.aaai.org/ocs/index.php/HCOMP/HCOMP13/paper/view/7388 [14]L. Moreau and P. Groth, Provenance: An Introduction to PROV. San Rafael, CA, USA: Morgan and Claypool, Sep.2013. [15]F. Curbera, Y. Doganata, A. Martens, N. K. Mukhi, and A. Slominski, “Business provenance—a technology to increase traceability of end-to-end operations,” in Proc. OTM Confederated Int. Conf., 2008 , pp. 100–119. [Online]. Available: http://dx.doi.org/10.1007/978–3-540-88871-0_10 [16]J. Frew, D. Metzger, and P. Slaughter, “ Automatic capture and reconstruction of computational provenance,” Concurrency Comput.: Practice Experience, vol. 20, no. 5, pp. 485–496 , 2008. [17]D. A. Holland, M. Seltzer, U. Braun, and K.-K. Muniswamy-Reddy, “Passing the provenance challenge,” Concurrency Comput.: Practice Experience, vol. 20, no. 5, 2008. [Online]. Available: http://www3.interscience.wiley.com/journal/116316566/abstract [18]L. Moreau and P. Groth, “Provenance of publications: A PROV style for latex,” in Proc. 7th USENIX Workshop Theory Practice Provenance, Jul. 2015. [Online]. Available: http://eprints.soton.ac.uk/378019/ [19]D. Ghoshal and B. Plale, “Provenance from log files: A bigdata problem,” in Proc. Joint EDBT/ICDT 2013 Workshops, 2013, pp. 290–297. [Online]. Available: http://doi.acm.org/10.1145/2457317.2457366 [20]P. Brauer, F. Fittkau, and W. Hasselbring, “The aspect-oriented architecture of the CAPS framework for capturing, analyzing and archiving provenance data ,” in Provenance and Annotation of Data and Processes. Berlin, Germany: Springer, 2015, pp. 223– 225. [21]J. Cheney, A. Ahmed, and U. A. Acar, “Provenance as dependency analysis,” Math. Struct. Comput. Sci., vol. 21, no. 6, pp. 1301–1337, 2011. [Online]. Available: http://dx.doi.org/10.1017/S0960129511000211 [22]J. Cheney, “Program slicing and data provenance,” IEEE Data Eng. Bulletin, vol. 30, no. 4, pp. 22–28, Dec.2007. [23]S. Magliacane, “Reconstructing provenance,” in The Semantic Web—ISWC. Berlin, Germany: Springer , 2012, pp. 399–406. [Online]. Available: http://dx.doi.org/10.1007/978-3-642-35173-0_29 [24]T. De Nies, et al., “Towards multi-level provenance reconstruction of information diffusion on social media,” in Proc. 24th ACM Int. Conf. Inf. Knowl. Manag., 2015, pp. 1823–1826. [Online]. Available: https://websci.informatik.uni-freiburg.de/publications/cikm2015-multilevel-provenance [25]K. Behajjame, et al., “PROV-O: The PROV Ontology ,” World Wide Web Consortium, W3C Recommendation REC-prov-o-20130430, T. Lebo, S. Sahoo, and D. McGuinness, Eds., Oct.2013. [Online]. Available: http://www.w3.org/TR/2013/REC-prov-o-20130430/ [26]J. Cheney and S. Soiland-Reyes, “PROV-N: The provenance notation,” World Wide Web Consortium, W3C Recommendation REC-prov-n-20130430, L. Moreau and P. Missier, Eds., Oct.2013. [Online]. Available: http://www.w3.org/TR/2013/REC-prov-n-20130430/ [27]L. Moreau, “PROV-XML: The PROV XML Schema,” World Wide Web Consortium, W3C Working Group Note NOTE-prov-xml-20130430, H. Hua, C. Tilmes, and S. Zednik, Eds., Apr.2013. [Online]. Available: http://www.w3.org/TR/2013/NOTE-prov-xml-20130430/ [28](2014). [Online]. Available: https://pypi.python.org/pypi/prov [29]L. Moreau, “ProvToolbox—Java library to create and convert W3C PROV data model representations,” Apr.2016. [Online]. Available: http://lucmoreau.github.io/ProvToolbox/ [30]T. D. Huynh, P. Groth, and S. Zednik, Eds., “PROV implementation report,” World Wide Web Consortium, W3C Working Group Note NOTE-prov-overview-20130430, Apr.2013. [Online]. Available: http://www.w3.org/TR/2013/NOTE-prov-implementations-20130430/ [31]T. D. Nies, “Constraints of the PROV data model,” World Wide Web Consortium, W3C Recommendation REC-prov-constraints-20130430, J. Cheney, P. Missier, and L. Moreau , Eds., Oct.2013. [Online]. Available: http://www.w3.org/TR/2013/REC-prov-constraints-20130430/ [32]D. Michaelides, T. D. Huynh, and L. Moreau, “ PROV-TEMPLATE: A template system for PROV documents,” Jun.2014. [Online]. Available: https://provenance.ecs.soton.ac.uk/prov-template-2014–06-07/ [33]H. S. Packer, L. Dragan, and L. Moreau, “An auditable reputation service for collective adaptive systems,” in Social Collective Intelligence: Combining the Powers of Humans and Machines to Build a Smarter Society, D. Miorandi, V. Maltese, M. Rovatsos, A. Nijholt, and J. Stewart, Eds.Berlin, Germany: Springer , Aug.2014, pp. 159–184. [Online]. Available: http://eprints.soton.ac.uk/365559/ [34]D. Michaelides, R. Parker, C. Charlton, W. Browne, and L. Moreau, “Intermediate notation for provenance and workflow reproducibility ,” in Proc. 6th Int. Provenance Annotation Workshop, Jun. 2016 , pp. 1–12. [Online]. Available: http://eprints.soton.ac.uk/393117/ [35]L. Moreau, et al., “The first provenance challenge ,” Concurrency Comput.: Practice Experience, vol. 20, no. 5, pp. 409–418, Apr.2008. [Online]. Available: http://www.ecs.soton.ac.uk/~lavm/papers/challenge-editorial.pdf [36]P. V. Biron and A. Malhotra, “XML Schema Part 2: Datatypes,” Oct.2004. [Online]. Available: http://www.w3.org/TR/xmlschema-2/ [37]G. Carothers and E. Prud’hommeaux, “RDF 1.1 Turtle,” World Wide Web Consortium, W3C Recommendation, Feb.2014. [Online]. Available: http://www.w3.org/TR/2014/REC-turtle-20140225/ [38]J. Tennison and G. Kellogg, Eds. , “Model for tabular data and metadata on the web,” World Wide Web Consortium, Recommendation, 2015. [Online]. Available: https://www.w3.org/TR/2015/REC-tabular-data-model-20151217/ [39]S. Miles, P. Groth, S. Munroe, and L. Moreau, “PrIMe: A methodology for developing provenance-aware applications,” ACM Trans. Softw. Eng. Methodology, vol. 20, no. 3, pp. 1–42, Aug.2011 . [Online]. Available: http://eprints.ecs.soton.ac.uk/17450/ [40]H.-J. Happel and S. Seedorf, “Applications of ontologies in software engineering,” in Proc. Workshop Sematic Web Enabled Softw. Eng. ISWC, 2006, pp. 5– 9. [Online]. Available: https://km.aifb.kit.edu/ws/swese2006/final/happel_full.pdf [41]L. Moreau, T. D. Huynh, and D. Michaelides, “An online validator for provenance: Algorithmic design, testing, and API,” in Proc. 17th Int. Conf. Fundam. Approaches Softw. Eng., Apr. 2014, pp. 291– 305. [Online]. Available: http://eprints.soton.ac.uk/361113/ [42]S. Miles, S. C. Wong, W. Fang, P. Groth, K.-P. Zauner, and L. Moreau, “Provenance-based validation of e-science experiments,” Web Semantics: Sci. Serv. Agents World Wide Web, vol. 5, no. 1, pp. 28 –38, 2007. [Online]. Available: http://dx.doi.org/10.1016/j.websem.2006.11.003 [43]W.-C. Tan, “Provenance in databases: Past, current, and future ,” Bulletin Tech. Committee Data Eng., vol. 30, no. 4, pp. 3–12, Dec.2007. [Online]. Available: ftp://ftp.research.microsoft.com/pub/debull/A07dec/wang-chiew.pdf [44]J. Cheney, L. Chiticariu, and W.-C. Tan, “ Provenance in databases: Why, how, and where,” Found. Trends Databases , vol. 1, no. 4, pp. 379–474, Apr. 2009. [Online]. Available: http://dx.doi.org/10.1561/1900000006 [45]P. Groth and L. Moreau, “Recording process documentation for provenance,” IEEE Trans. Parallel Distrib. Syst., vol. 20, no. 9 , pp. 1246–1259, Sep.2009. [Online]. Available: http://www.ecs.soton.ac.uk/ lavm/papers/tpds09.pdf [46]T. M. McPhillips, et al., “YesWorkflow: A user-oriented, language-independent tool for recovering workflow information from scripts,” Int. J. Digit. Curation, vol. 10, no. 1, 2015. [Online]. Available: http://arxiv.org/abs/1502.02403 [47]T. McPhillips, S. Bowers, K. Belhajjame, and B. Ludäscher, “Retrospective provenance without a runtime provenance recorder,” in Proc. 7th USENIX Workshop Theory Practice Provenance, Jul. 2015. [Online]. Available: https://www.usenix.org/conference/tapp15/workshop-program/presentation/mcphillips [48]H. Firth and P. Missier, “ProvGen: Generating synthetic PROV graphs with predictable structure,” in Provenance and Annotation of Data and Processes, B. Ludäscher and B. Plale, Eds.Berlin, Germany : Springer, 2015, pp. 16–27 . [Online]. Available: http://dx.doi.org/10.1007/978-3-319-16462-5_2 [49]A. P. Chapman, H. V. Jagadish, and P. Ramanan, “ Efficient provenance storage,” in Proc. ACM SIGMOD Int. Conf. Manage. Data , 2008, pp. 993–1006. [Online]. Available: http://doi.acm.org/10.1145/1376616.1376715 [50]V. Curcin, E. Fairweather, R. Danger, and D. Corrigan, “Templates as a method for implementing data provenance in decision support systems,” J. Biomed. Informat., vol. 65, pp. 1–21, 2017. [Online]. Available: http://dx.doi.org/10.1016/j.jbi.2016.10.022 [51]R. Danger, V. Curcin, P. Missier, and J. Bryans, “Access control and view generation for provenance graphs,” Future Generation Comput. Syst., vol. 49, pp. 8–27, 2015. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0167739X1500031X [52]L. Moreau, “Aggregation by provenance types: A technique for summarising provenance graphs,” in Proc. Graphs Models, Apr. 2015, pp. 129–144. [Online]. Available: http://eprints.soton.ac.uk/364726/ [53]S. Cohen-Boulakia, O. Biton, S. Cohen, and S. Davidson, “Addressing the provenance challenge using zoom,” Concurrency Comput.: Practice Experience, vol. 20 , no. 5, pp. 497–506, 2008. [Online]. Available: http://dx.doi.org/10.1002/cpe.1232 [54]L. Sun , J. Park, and R. Sandhu, “Engineering access control policies for provenance-aware systems,” in Proc. 3rd ACM Conf. Data Appl. Secur. Privacy , 2013, pp. 285–292. [Online]. Available: http://dx.doi.org/10.1145/2435349.2435390 [55]B. Clifford, I. Foster, J.-S. Voeckler, M. Wilde, and Y. Zhao, “Tracking provenance in a virtual data grid,” Concurrency Comput.: Practice Experience, vol. 20, no. 5, pp. 565–575, Apr.2008. [Online]. Available: http://dx.doi.org/10.1002/cpe.v20:5 [56]D. Garijo and Y. Gil, “Augmenting PROV with plans in P-PLAN: Scientific processes as linked data ,” in Proc. 2nd Int. Workshop Linked Sci.: Tackling Big Data, Held Conjunction Int. Semantic Web Conf. , 2012. [Online]. Available: http://www.isi.edu/gil/papers/garijo-gil-lisc12.pdf [57]V. Cuevas-Vicenttin, et al., “ProvONE: A PROV extension data model for scientific workflow provenance,” DataOne Project, Mar. 2014. [Online]. Available: http://vcvcomputing.com/provone/provone.html [58]K. Belhajjame, et al., “Using a suite of ontologies for preserving workflow-centric research objects,” Web Semantics: Sci. Serv. Agents World Wide Web , vol. 32, pp. 16–42, 2015. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S1570826815000049 [59]P. Chen and B. A. Plale, “ProvErr: System level statistical fault diagnosis using dependency model,” in Proc. 15th IEEE/ACM Int. Symp. Cluster Cloud Grid Comput., 2015 , pp. 525–534. [Online]. Available: http://dx.doi.org/10.1109/CCGrid.2015.86 [60]H. Zhu and I. Bayley, “ An algebra of design patterns,” ACM Trans. Softw. Eng. Methodology , vol. 22, no. 3, pp. 23:1–23:35, Jul. 2013. [Online]. Available: http://doi.acm.org/10.1145/2491509.2491517 [61]T. Heath and C. Bizer, Linked Data: Evolving the Web into a Global Data Space, 1st ed. San Rafael, CA, USA: Morgan & Claypool, 2011. [Online]. Available: http://linkeddatabook.com/ [62]N. Shadbolt, T. Berners-Lee, and W. Hall, “ The Semantic Web revisited,” IEEE Intell. Syst., vol. 21, no. 3, pp. 96–101, May 2006. [Online]. Available: http://dx.doi.org/10.1109/MIS.2006.62 [63]S. Brockmans, et al., “The 5th international Semantic Web conference (ISWC’06),” 2006, pp. 187–200. [Online]. Available: http://dx.doi.org/10.1007/11926078_14 [64]T. D. Huynh and L. Moreau, “ProvStore: A public provenance repository,” in Proc. 5th Int. Provenance Annotation Workshop, Jun. 2014, pp. 275–277 . [Online]. Available: http://eprints.soton.ac.uk/365509/ Luc Moreau is a professor of computer science and head of the Web and Internet Science Group, Department of Electronics and Computer Science, University of Southampton. He is a leading figure in the area of data provenance. He was co-chair of the W3C Provenance Working Group that produced the PROV recommendations. He is co-investigator of the ORCHID, SOCIAM, SmartSociety, and eBook projects. Belfrit Victor Batlajery received the master's degree in business informatics. He is working toward the PhD degree in the Web and Internet Science Group, Department of Electronics and Computer Science, University of Southampton. His interests include provenance and Semantic Web. His undergoing PhD project is about Food Provenance where he investigates the role of provenance to support due diligence in the food industry. Trung Dong Huynh is a researcher in the Web and Internet Science Group, Department of Electronics and Computer Science, University of Southampton. He has extensive experience in the areas of trust, reputation and provenance. He pioneered the provenance network analytics method to classify data based on their provenance. He led the development of PICASO, CollabMap, and ProvStore and is the main author of PROV-JSON and the PROV Python package. Danius Michaelides is a researcher in the Web and Internet Science Group. His research interests include eScience, distributed computing, distributed information management, and Semantic Web technologies. He is currently working on a project funded by the UK's Economic and Social Research Council to build novel tools for training and research in quantitative social science, including the EBook application. Heather Packer received the doctoral degree in computer science for algorithms for the automatic generation of lightweight ontologies for task focused domains from the University of Southampton. She is a research fellow in the Web and Internet Science Group, Department of Electronics and Computer Science, University of Southampton. In the SmartSociety project, she is focusing on provenance, reputation, transparency and accountability, in particular in Smartshare.Keywords Data Structures, Graph Theory, Internet, Software Engineering, Software Maintenance, Data Structure, PROV TEMPLATE, Provenance Generation, Open Source Library Prov Toolbox, Software Engineering, Templating System, Provenance Processing, Quantitative Data, Provenance Consumption, Provenance Maintenance, Expanded Provenance Templates, Expansion Algorithm, Provenance Graphs, World Wide Web Consortium, PROV Standard, Electronic Publishing, Instruments, Standards, Maintenance Engineering, Runtime, Libraries, Automobiles, Provenance, Sc Xmlns Ali Http Www Niso Org Schemas Ali 1 0 Xmlns Mml Http Www W 3 Org 1998 Math Math ML Xmlns Xlink Http Www W 3 Org 1999 Xlink Xmlns Xsi Http Www W 3 Org 2001 XML Schema Instance Prov Sc, Provenance Generation, Template"
Semantic Slicing of Software Version Histories,"Abstract Software developers often need to transfer functionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level, semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a segment of the change history, “inheriting” additional, unwanted functionality. In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and propose techniques to minimize the produced slice. We then instantiate the overall approach, CSlicer, in a specific implementation for Java projects managed in Git and evaluate its correctness and effectiveness on a set of open-source software repositories. We show that it allows to identify subsets of change histories that maintain the functionality of interest but are substantially smaller than the original ones.Keywords History, Semantics, Software, Minimization, Context, Computer Bugs, Java, Software Changes, Version Control, Dependency, Program Analysis"
Measuring the Impact of Code Dependencies on Software Architecture Recovery Techniques,"Abstract Many techniques have been proposed to automatically recover software architectures from software implementations. A thorough comparison among the recovery techniques is needed to understand their effectiveness and applicability. This study improves on previous studies in two ways. First, we study the impact of leveraging accurate symbol dependencies on the accuracy of architecture recovery techniques. In addition, we evaluate other factors of the input dependencies such as the level of granularity and the dynamic-bindings graph construction. Second, we recovered the architecture of a large system, Chromium, that was not available previously. Obtaining the ground-truth architecture of Chromium involved two years of collaboration with its developers. As part of this work, we developed a new submodule-based technique to recover preliminary versions of ground-truth architectures. The results of our evaluation of nine architecture recovery techniques and their variants suggest that (1) using accurate symbol dependencies has a major influence on recovery quality, and (2) more accurate recovery techniques are needed. Our results show that some of the studied architecture recovery techniques scale to very large systems, whereas others do not. 1   Introduction Software architecture is crucial for program comprehension, programmer communication, and software maintenance. Unfortunately, documented software architectures are either nonexistent or outdated for many software projects. While it is important for developers to document software architecture and keep it up-to-date, it is costly and difficult. Even medium-sized projects, of 70 to 280 K source lines of code (SLOC), require an experienced recoverer to expend an average of 100 hours of work to create an accurate “ground-truth” architecture  [1]. In addition, as software grows in size, it is often infeasible for developers to have complete knowledge of the entire system to build an accurate architecture. Many techniques have been proposed to automatically or semi-automatically recover software architectures from software code bases [2], [3], [4], [5], [6], [7]. Such techniques typically leverage code dependencies to determine what implementation-level units (e.g., symbols, files, and modules) form a semantic unit in a software system’s architecture. To understand their effectiveness, thorough comparisons of existing architecture recovery techniques are needed. Among the studies conducted to evaluate different architecture recovery techniques [2], [8], [9], the latest study [10], conducted by a subset of this paper’s authors, compared nine variants of six existing architecture recovery techniques. This study found that, while the accuracy of the recovered architectures varies and some techniques outperform others, their overall accuracy is low. This previous study used include dependencies as inputs to the recovery techniques. These are file-level dependencies established when one file declares that it includes another file. In general, the include dependencies are inaccurate. For example, file foo.c may declare that it includes bar.h, but may not use any functions or variables declared or defined in bar.h. Using include dependencies, one would conclude that foo.c depends on bar.h , while foo.c has no actual code dependency on bar.h. In contrast, symbol dependencies are more accurate. A symbol can be a function or a variable name. For example, consider two files Alpha.c and Beta.c: file Alpha.c contains method A; and file Beta.c contains method B. If method A invokes method B, then method A depends on method B. Based on this information, we can conclude that file Alpha.c depends on file Beta.c. A natural question to ask is, to what extent would the use of symbol dependencies affect the accuracy of architecture recovery techniques? We aim to answer this question empirically, by analyzing a set of real-world systems implemented in Java, C, and C++. Dependencies can be grouped to different levels of granularity, which can affect the manner in which recovery techniques operate. Generally, dependencies are extracted at the file level. For large projects, dependencies can be grouped to the module level, where a module is a semantic unit defined by system build files. Module dependencies can be used to recover architectures even when finer-grained dependencies do not scale. In this paper, we study the extent to which the granularity of dependencies affects the accuracy of architecture recovery techniques. Another key factor affecting the accuracy of a recovery technique is whether dependencies utilized as input to a technique are direct or transitive. Transitive dependencies can be obtained from direct dependencies by using a transitive-closure algorithm, and may add relationships between strongly related components, making it easier for recovery techniques to extract such components from the architecture. However, as the number of dependencies increases, the use of transitive dependencies with some recovery techniques may not scale to large projects. Different symbols can be used (functions, global variables, etc.) to create a symbol dependency graph, but it is unclear which symbols have the most impact on the accuracy of architecture recovery techniques. In this paper, we study the impact of function calls and global variable usage on the quality of architecture recovery techniques. In addition, both C++ and Java offer the possibility of using dynamic-bindings mechanisms. Several techniques exist to build dynamic-bindings graphs [11], [12], [13] and, despite the existence of two early studies  [14], [15] about the impact of call-graph construction algorithms, and the origins of software dependencies on basic architecture recovery, no work has been done to study the effect of dynamic-bindings resolution on recent architecture recovery techniques. The last question we study pertains to the scalability of existing automatic architecture recovery techniques. While large systems have been studied and their architectures analysed in previous work  [16], [17], [18], the largest software system used in the published evaluations of automatic architecture recovery techniques is Mozilla 1.3, comprising 4MSLOC, and it revealed the scalability limits of several recovery techniques [10]—an old version of Linux was also studied, but its size reported in previous evaluations was only 750KSLOC. The size of software is increasing, and many software projects are significantly larger than 4MSLOC. For example, the Chromium open-source browser contains nearly 10 MSLOC. In this paper, we test whether existing automatic architecture recovery techniques can scale to software of such size. To this end, this paper compares the same nine variants of six architecture recovery techniques from the previous study [10], as well as two additional baseline algorithms, using eight different types of dependencies on five software projects to answer the following research questions (RQ): RQ1. Can more accurate dependencies improve the accuracy of existing architecture recovery techniques? RQ2. What is the impact of different input factors, such as the granularity level, the use of transitive dependencies, the use of different symbol dependencies and dynamic-bindings graph construction algorithms, on existing architecture recovery techniques? RQ3. Can existing automatic architecture recovery techniques scale to large projects comprising 10MSLOC or more? This paper makes the following contributions: We compared nine variants of six architecture recovery techniques using eight types of dependencies at different levels of granularity to assess their effects on accuracy. More specifically, we studied the impact of dynamic-bindings resolution algorithms, function calls, and global variable usage on the accuracy of recovery algorithms. We also expand the previous work by studying whether using a higher level of granularity or transitive dependencies improve the accuracy of recovery techniques. This is the first substantial study to the impact of different types of dependencies for architecture recovery. We found that the types of dependencies and the recovery algorithms have a significant effect on recovery accuracy. In general, symbol dependencies produce software architectures with higher accuracy than include dependencies ( RQ1 ). Our results suggest that, apart from the selection of the “right” architecture recovery techniques, other factors to consider for improved recovery accuracy are the dynamic-bindings graph resolution algorithm, the granularity of dependencies, and whether such dependencies are direct or transitive (RQ2 ). Our results show that the accuracy is low for all studied techniques, with only one technique (ACDC) consistently producing better results than k-means, a basic machine learning algorithm. This corroborates past results  [10] but does so on a different set of subject systems, including one significantly larger system, and for a different set of dependency relationships. We recovered the ground-truth architecture of Chromium (svn revision 171,054). This ground-truth architecture was not available previously and we obtained it through two years of regular discussions and meetings with Chromium developers. We also updated the architectures of Bash and ArchStudio that were reported in  [1]. All ground-truth architectures have been certified by the developers of the different projects. We propose a new submodule-based architecture recovery technique that combines directory layout and build configurations. The proposed technique was effective in assisting in the recovery of ground-truth architectures. Compared to FOCUS [19], which is used in previous work  [1], to recover ground-truth architectures, the submodule-based technique is conceptually simple. Since the technique is used for generating a starting point, its simplicity can be beneficial; any issues potentially introduced by the technique itself can later be mitigated by the manual verification step. We found some recovery techniques do, and some do not, scale to the size of Chromium. Working with coarser-grained dependencies and using direct dependencies are two possible solutions to make those techniques scale (RQ2 and RQ3 ). 2   Related Work 2.1 Comparison of Software Architecture Recovery Techniques This paper builds on work that was previously reported in [20]. Novelty with respect to this previous work includes a study of the impact of dynamic-bindings resolution algorithms, function calls, and global variable usage on the accuracy of recovery algorithms. We also expand the previous work by studying whether using a higher level of granularity and transitive dependencies improves the accuracy of recovery techniques. Many architecture recovery techniques have been proposed [2], [3], [4], [5], [6], [7], [21]. The most recent study  [10] collected the ground-truth architectures of eight systems and used them to compare the accuracy of nine variants of six architecture recovery techniques. Two of those recovery techniques—Architecture Recovery using Concerns (ARC) [4] and Algorithm for Comprehension-Driven Clustering (ACDC) [7]—routinely outperformed the others. However, even the accuracy of these techniques showed significant room for improvement. Architecture recovery techniques have been evaluated against one another in many other studies  [2], [5], [8], [9], [10], [22]. The results of the different studies are not always consistent. scaLable InforMation BOttleneck (LIMBO)  [23], a recovery technique leveraging an information loss measure, and ACDC performed similarly in one study [2]; however, in different study, Weighted Combined Algorithm (WCA) [24], a recovery technique based on hierarchical clustering, outperformed Complete Linkage (CL) [24]. In yet another study, CL is shown to be generally better than ACDC [9]. In the most recent study, ARC and ACDC surpass LIMBO and WCA [10]. Wu et al. [9] compared several recovery techniques utilizing three criteria: stability, authoritativeness, and non-extremity. For this study, no recovery technique was consistently superior to others on multiple measures. A possible explanation for the inconsistent results of these studies is their use of different assessment measures. The types of dependencies which serve as input to recovery techniques vary among studies: some recovery techniques leverage control and data dependencies [25], [26], [27]; other techniques use static and dynamic dependency graphs [2]. Previous work [14] examined the effect of different polymorphic call-graph construction algorithms on automatic clustering. Another work [15] studied the impact of source-code versus object-code-based dependencies on software architecture recovery. They found that dependencies obtained directly from source code are more useful than dependencies obtained from object code. While also studying the impact of dependencies on automatic architecture recovery, we focus on the accuracy and the type of the dependencies (e.g., include and symbol dependencies) independently from the way dependencies were extracted, i.e., from object code or source code. None of the papers mentioned above assess the influence of symbol dependencies on recovery techniques when compared to include dependencies. This paper is the first to study (1) the impact of symbol dependencies on the accuracy of recovery techniques and (2) the scalability of recovery techniques to a large project with nearly 10MSLOC. 2.2 Recovery of Ground-Truth Architectures Ground-truth architectures enable the understanding of implemented architectures and the improvement of automated recovery techniques. Several prior studies invested significant time and effort to recover ground-truth architectures for several systems. 2.2.1 Manual Recovery Garcia et al. [1] described a method to recover the ground-truth architectures of four open-source systems. The method involves extensive manual work, and the mean cost of recovering the ground-truth architecture of seven systems ranged from 70KSLOC to 280KSLOC was 107 hours. CacOphoNy  [16] is another approach that uses metamodels to aid in manual recovery of software architectures and had been used to reverse engineer a large software system  [28]. In his work [29], Laine manually recovered the architecture of the X-Window System to illustrate the importance of software architecture for object-oriented development. Grosskurth et al. [30] studied the architecture and evolution of web browsers and provide guidance for obtaining a reference architecture for web browsers. Their work does not address the challenges of recovering an accurate ground-truth architecture in general. In addition, it is not clear if their approach is accurate for modern web browsers such as Chromium, which use new design principles such as a modern threading model for tabbed browsing. Bowman et al. [31] and Xiao et al.  [32] recovered the ground-truth architectures of the Linux kernel 2.0 and Mozilla 1.3 respectively. The Linux kernel and Mozilla are large systems, but the evaluated versions are more than a decade old. The version of the Linux kernel recovered was from 1996 and at that time, it contained only 750KSLOC. Mozilla 1.3 is from 2003 with 4MSLOC. 2.2.2 Tool-Assisted Architecture Recovery Several tools have been created to help developers analyze, visualize, and reconstruct software architectures  [33]. Those tools can be used in different stages of manual architecture recovery. Rigi [34] is a tool that can be used to analyse and visualize software dependencies. While it is possible to generate an architectural view of a project with the help of Rigi  [35], this requires the intervention of a developer with deep knowledge of the project to manually group similar elements (classes, files, etc.) together. Indeed, for large projects, initial views proposed by Rigi are unreadable due to the large number of nodes and dependencies  [36] and manual effort is necessary to recover the architecture of the system. The Portable Bookshelf [37], SHriMP  [38], AOVIS [39], LSEdit [40] are other software visualization and analysis tools that can help manual architecture recovery. Several other tools such as Understand [41], Lattix  [42] and Structure101 [43] have been used to ensure the quality of a given architecture and monitor its evolution. However, none of these tools intend to automatically recover an architecture. 3   Approach Our approach is illustrated in Fig. 1. First, we extract different types of dependencies for each projects. Then, we provide those dependencies as input to six different architecture recovery techniques. We also evaluate three additional techniques that take the project’s source code as input. Finally, we used K-means results and architectures extracted from the directory structure of the project as a baseline. To evaluate the quality of the architecture recovered from different sets of dependencies, we obtain a ground-truth architecture of each project that was certified by each project’s developers or main architect. Then, we measure the quality of the architectures recovered automatically by comparing them to the ground-truth architecture using four different metrics. Fig. 1. Overview of our approach. In the rest of this section, we describe the manner in which we extract the dependencies we study, and elaborate on our approach for obtaining ground-truth architectures. 3.1 Obtaining Dependencies Both symbol dependencies and include dependencies represent relationships between files, but the means by which these dependencies are determined vary. 3.1.1 C/C++ Projects To extract symbol dependencies for C/C++, we use the technique built by our team that scales to software systems comprising millions of lines of code [44]. The technique compiles a project’s source files into LLVM bitcode, analyzes the bitcode to extract the symbol dependencies for all symbols inside the project, and groups dependencies based on the files containing the symbols. At this stage, our extraction process has not considered symbol declarations. As a result, header-file dependencies are often missed because many header files only contain symbol declarations. To ensure we do not miss such dependencies, we augment symbol dependencies by analyzing #include statements in the source code. These symbol dependencies are direct dependencies, which may be used at the file level or grouped at the module level. For large projects such as Chromium1 and ITK,2 many developer teams work independently on different parts of the project. To facilitate this work, developers divided these projects into separated sections (modules) that can be updated independently. To group code-level entities at the module level, we extract module information from the build files of the project provided by the developers (e.g., makefile or equivalent). Transitive dependencies are obtained for all projects using the Floyd-Warshall [45] algorithm on symbol dependencies. Because the Floyd-Warshall algorithm did not scale for Chromium, we also tried to use Crocopat  [46] to obtain transitive dependencies for Chromium and encountered similar scalability issues. To extract include dependencies we use the compiler flag -MM. Include dependencies are similar to the dependencies used in prior work [10]. 3.1.2 Java Projects To extract symbol dependencies for Java, we leverage a tool that operates at the Java bytecode level and extracts high-level information from the bytecode in a structured and human readable format  [47]. This allows for method calls and member access (i.e., relationships between symbols) to be recorded without having to analyze the source code itself. Using this information provides a complete picture of all used and unused parts of classes to be identified. We can identify which file any symbol belongs to, since the Java compiler follows a specific naming convention for inner classes and anonymous classes. With information about usage among symbols and resolving the file location for each symbol, we can build a complete graph of the symbol dependencies for the Java projects. This method accounts only for symbols used in the bytecode and does not account for runtime usage which can vary due to reflective access. We approximate include dependencies for Java by extracting import statements in Java source code by utilizing a script to determine imports and their associated files. To ensure we capture potential build artifacts, the Java projects are compiled before extracting import statements. The script used to extract the dependencies detects all the files in a package. Then for every file, it evaluates each import statement and adds the files mentioned in the import as a dependency. When a wildcard import is evaluated, all classes in the referred package are added as dependencies. The Java projects studied do not contain well-defined modules. In addition, our ground-truth architecture is finer-grained than the package level. For example, Hadoop ground-truth architecture contains 67 clusters when the part of the project we study contains only 52 packages. Therefore, we cannot use Java packages as an equivalent of C++ modules for our module-level evaluation for those specific projects. When studying larger Java projects (e.g., Eclipse), using Java packages could be a good alternative to modules defined in the configuration files used for C++ projects. Maven or Ant build files could also be use as modules for Java projects. 3.1.3 Relative Accuracy of Include and Symbol Dependencies C/C++ include dependencies tend to miss or over-approximate relationships between files, rendering such dependencies inaccurate. Specifically, include dependencies over-approximate relationships in cases where a header file is included but none of the functions or variables defined in the header file are used (recall Section 1). In addition, include dependencies ignore relationships between non-header files (e.g., .cpp to .cpp files), resulting in a significant number of missed dependencies. For example, consider the case where A.c depends on a symbol defined in B.c because A.c invokes a method defined in B.c. Include dependencies will not contain a dependency from A.c to B.c because A.c includes B.h but not B.c. For example, in Bash, we only identified four include dependencies between two non-header files, although there are 1,035 actual dependencies between non-header files based on our symbol results. Include dependencies miss many important dependencies since non-header files are the main semantic components of a project. A recovery technique can treat non-header and header files whose names before their extensions match (e.g., B.c and B.h) as a single unit to alleviate this problem. However, this remedy does not handle cases where such naming conventions are not followed or when the declarations for types are not in a header file. Include dependencies use transitive dependencies for header files. Consider an example of three files A.c , A.h, and B.h, where A.c includes A.h and A.h includes B.h; A.c has an include dependency with B.h because including A.h implicitly includes everything that A.h includes. For Java projects, include dependencies miss relationships between files because they do not account for intra-package dependencies or fully-qualified name usage. At the same time, include dependencies can represent spurious relationships because some imports are unused and wildcard imports are overly inclusive. Include dependencies are therefore significantly less accurate than symbol dependencies. 3.1.4 Overall Accuracy of Symbol Dependencies To ensure the symbol dependencies we extracted are accurate, we randomly sampled 0.05 percent of the symbol dependencies and investigated whether these dependencies are correct. This small sample represents 343 dependencies we manually verified. The sampling was done uniformly across projects and dynamic bindings resolutions (interface-only or class hierarchy analysis). We did not find any incorrectly extracted dependencies in this sample, the margin of error being 5.3 with 95 percent confidence. We did not quantitatively check whether all the existing dependencies were extracted, as it would be extremely time-consuming to do. However, when building the tool used for extracting dependencies  [48], qualitative sanity checks were done to make sure the tool did not miss obvious dependencies. 3.2 Obtaining Ground-Truth Architectures To measure the accuracy of existing software architecture recovery techniques, we need to know the “ground-truth” architecture of a target project. Since it is prohibitively expensive to build architectures manually for large and complex software, such as Chromium, we use a semi-automated approach for ground-truth architecture recovery. We initially showed the architecture recovered using ACDC to a Chromium developer. He explained that most of the ACDC clusters did not make sense and suggested that we start by considering module organization in order to recover the ground truth. In response, we have introduced a simple submodule-based approach to extract automatically a preliminary ground-truth architecture by combining directory layout and build configurations. Starting from this architecture, we worked with developers of the target project to identify and fix mistakes in order to create a ground-truth architecture. The submodule-based approach groups closely related modules, and considers which modules are contained within another module. It consists of three steps. First, we determine the module that each file belongs to by analyzing the configuration files of the project. Second, we determine the submodule relationship between modules. We define a submodule as a module that has all of its files contained within the subdirectory of another module. We first determine a module’s location , which is defined as the common parent directories that contain at least one file belonging to the module. Then we can determine if a particular module has a relation to another module. For example, assume a project has four modules named A, B, C, and D. The file structure of the project is shown in Fig. 2, while the module structure that we generate is shown in Fig. 3. Module A: contains fileA1.cpp and fileA2.cpp. Location is project/folder2. Fig. 2. Example project layout. Fig. 3. Example project submodules. Module B: contains fileB1.cpp and fileB2.cpp. Location is project/folder2/folder2_2. Module C: contains fileC1.cpp. Location is project/folder2/folder2_3. Module D: contains fileD1.cpp and fileD2.cpp. Location is both project/folder1 and project/folder2/folder2_3. Based on the modules’ locations, we determine that module B is a submodule of module A because module B’s location project/folder2/folder2_2 is within module A’s location project/folder2. Similarly, module C is a submodule of module A. The reason module D has two folder locations is because there is no common parent between the two directories. If module D had a file in the project folder, then its location would simply be project. Module D is not a submodule of module A because it has a file located in project/folder1. This preliminary version of the ground-truth architecture does not accurately reflect the “real” architecture of the project and additional manual corrections are necessary. For example, Chromium has two modules webkit_gpu, located in the folder webkit/gpu, and content_gpu , located in the folder content/gpu. The two modules are in completely separate folders and are grouped in different clusters by the submodule approach. However, both are involved with displaying GPU-accelerated content and should be grouped together to indicate their close relationship to the gpu modules. This is an example where the submodule approach based on folder structure may not accurately reflect the semantic structure of modules and needs to be manually corrected. Hundreds of hours of manual work are then required to investigate the source code of the system to verify and fix the relationships obtained. When we are satisfied with our ground-truth version, we send to the developers the list of clusters containing files and modules in the Rigi Standard Format and a visual representation of how the clusters interact with one another for certification. Multiples rounds of verifications, based on developers’ feedback, are necessary to obtain an accurate ground-truth architecture. For the recovery of Chromium, we also had several in-person meetings with a Chromium developer where he explained to us his view of the project’s architecture and updated the parts of our preliminary architectures that were inaccurate. During these meetings, the Chromium developer investigated those clusters to see if they make sense (for example, whether the cluster names match with his understanding of Chromium modules and clusters). Then we showed him which files belongs to each cluster using different visualizations (e.g., the ”spring” model from Graphviz  [49] and a circular view using d3 Javascript library [50]), and he also verified if they were correctly grouped. When he did not agree, we checked if there was some mistakes on our side (i.e., inaccuracy in the submodule technique) or if it was a bug in the Chromium module definition. It took two years of meetings and email exchanges with Chromium developers to obtain the ground truth. The final ground truth we obtain is a nested architecture. Because most of the architecture recovery techniques produce a flat architecture, we flatten our ground-truth architecture by grouping modules that are submodules of one another into a cluster. In the example above, we cluster modules A, B and C into a single cluster and leave module D on its own. Previous work [1], [19] mentioned there might exist different ground-truth architectures for the same project. Despite the fact that our submodule-based approach only recover one ground truth, it is possible to use our approach as a starting point for recovering several ground truths, by having different recoverers and receiving feedback from different developers. Prior work [1], conducted by a subset of this paper’s authors, used a different approach, FOCUS [19], to recover preliminary versions of ground-truth architectures. Compared to FOCUS, the proposed submodule-based technique is conceptually simpler. However, the submodule-based technique uses the same general strategy as FOCUS and can, in fact, be used as one of FOCUS’s pluggable elements. This fact, along with the extensive manual verification step, suggests that the strategy used as the starting point for ground-truth recovery does not impact the resulting architecture (as already observed in [1]). 4   Selected Recovery Techniques We select the same nine variants of six architecture recovery techniques as in previous work  [10] for our evaluation. We also used two baseline clustering algorithms, the K-means algorithm and a directory-based recovery technique. Four of the selected techniques (ACDC, LIMBO, WCA, and Bunch [6]) use dependencies to determine clusters, while the remaining two techniques (ARC and ZBR [3]) use textual information from source code. We include techniques that do not use dependencies to (1) assess the accuracy of finer-grained, accurate dependencies against these information retrieval-based techniques and to (2) determine their scalability. The view that the techniques we evaluate recover are structural views representing components and their configurations. Such views are fundamental and should be as correct as possible before making other architectural decisions. For example, behavioral or deployment views are still highly dependent on accurate component identification and the configurations among components. Algorithm for Comprehension-Driven Clustering  [7]. is a clustering technique for architecture recovery. We included ACDC because it performed well in several previous studies [2], [8], [9], [10]. ACDC aims to achieve three goals. First, to help understand the recovered architecture, the clusters produced should have meaningful names. Second, clusters should not contain an excessive number of entities. Third, the grouping is based on identified patterns that are used when a developer describes the components of a software system. The main pattern used by ACDC is called the “subgraph dominator pattern”. To identify this pattern, ACDC detects a dominator node n_0 and a set of nodes N = \lbrace n_i \mid i \in \mathbb {N} \rbrace that n_0 dominates. A dominator node n_0 dominates another node n_i if any path leading to n_i passes through n_0 . Together, n_0 , N , and their corresponding dependencies form a subgraph. ACDC groups the nodes of such a subgraph together into a cluster. Bunch.  [6], [51] is a technique that transforms the architecture recovery problem into an optimization problem. An optimization function called Modularization Quality (MQ) represents the quality of a recovered architecture. Bunch uses hill-climbing and genetic algorithms to find a partition (i.e., a grouping of software entities into clusters) that maximizes MQ. As in previous work [10], we evaluate two versions of the Bunch hill-climbing algorithms—Nearest and Steepest Ascent Hill Climbing (NAHC and SAHC). Weighted Combined Algorithm  [24] is a hierarchical clustering algorithm that measures the inter-cluster distance between software entities and merges them into clusters based on this distance. The algorithm starts with each entity in its own cluster associated with a feature vector. The inter-cluster distance between all clusters is then calculated, and the two most similar clusters are merged. Finally, the feature vector of the new cluster is recalculated. These steps are repeated until WCA reaches the specific number of clusters defined by the user. Two measures are proposed to measure the inter-cluster distance: Unbiased Ellenberg (UE) and Unbiased Ellenberg-NM (UENM). The main difference between these measures is that UENM integrates more information into the measure and thus might obtain better results. In our recent study  [10], UE and UENM performed differently depending on the systems tested, therefore, we evaluate both. LIMBO  [23] is a hierarchical clustering algorithm that aims to make the Information Bottleneck algorithm scalable for large data sets. The algorithm works in three phases. Clusters of artefacts are summarized in a Distributational Cluster Feature (DCF) tree. Then, the DCF tree leaves are merged using the Information Bottleneck algorithm to produce a specified number of clusters. Finally, the original artefacts are associated with a cluster. The accuracy of this algorithm was evaluated in several studies. It performed well in most of the experiments [2], [8], except in one recent study [10] where LIMBO achieved surprisingly poor results. Architecture Recovery Using Concerns  [4] is a hierarchical clustering algorithm that relies on information retrieval and machine learning to perform a recovery. This technique does not use dependencies and is therefore not used to evaluate the influence of different levels of dependencies. ARC considers a program as a set of textual documents and utilizes a statistical language model, Latent Dirichlet Allocation (LDA) [52], to extract concerns from identifiers and comments of the source code. A concern is as a role, concept or purpose of the system studied. The extracted concerns are used to automatically identify clusters and dependencies. ARC is one of the two best-scoring techniques in our previous evaluation [10] and thus is important to compare against when evaluating for accuracy. Similar to ARC, Zone Based Recovery (ZBR)  [3] is a recovery technique based on natural language semantics of identifiers and comments found in the source code. Each file is represented as a textual document and divided into zones. For each word in a zone, ZBR evaluates the term frequency-inverse document frequency (tf-idf) score. Each zone is weighted using the Expectation-Maximization algorithm. ZBR has multiple methods for weighting zones. The initial weights for each zone can be uniform (ZBR-uni), or set to the ratio of the number of tokens in the zone to the number of tokens in the entire system (ZBR-tok). We chose these two weighting variations to ensure consistency with the previous study  [10]. The last step of ZBR consists of clustering this representation of files by using group-average agglomerative clustering. ZBR demonstrated accuracy in recovering Java package structure  [3] but struggled with memory issues when dealing with larger systems  [10]. Previous techniques are clustering algorithms specifically designed for architecture recovery. To obtain an estimate of the quality of the architectures generated by these algorithms, we used two baselines. For the first baseline, we cluster the files using the K-means algorithm. Each entity (i.e., a file or module) is represented by a feature vector {f_1,f_2 , . . ., f_n }, where n is the number of features. Each of the n features represents a dependency with one of the n entities in the project. For the second baseline, we used the directory structure of the project as an approximation of the architecture of the software. If automatic architecture recovery techniques cannot generate a recovered architecture that is superior to the directory structure of the project, then the recovery technique is not helpful for the specific project. To generate this approximated architecture, we use the same implementation as previous work  [53]. 5   Experimental Setup In this section, we describe our experimental environment, how we obtained the ground-truth architectures for each project, the parameters used in our experiments, and the different metrics used to assess the quality of the recovered architectures. 5.1 Projects and Experimental Environment We conduct our comparative study on five open source projects: Bash, ITK, Chromium, ArchStudio, and Hadoop. Detailed information about these projects can be found in Table 1. We choose those specific version of Bash and Chromium because they were the most recent versions available when we started our ground-truth recovery. For ArchStudio, Hadoop and ITK, we picked those versions because their respective ground-truth architectures were already available. TABLE 1 Evaluated Projects and Architectures  To run our experiments, we leveraged two machines and parallel processing, due to the large size of some projects. We ran ZBR with the two weight variations described in Section 4 on a 3.2 GHz i7-3930K desktop with 12 logical cores, six physical cores, and 48 GB of memory. We ran all the other recovery techniques on a 3.3 GHz E5-1660 server with 12 logical cores, six physical cores, and 32 GB memory. For Bash, Hadoop, and ArchStudio, all techniques take a few seconds to a few minutes to run. For large projects, such as ITK and Chromium, each technique takes several hours to days to run. Running all experiments for Chromium would take more than 20 days of CPU time on a single machine. Consequently, we parallelized our experiments. 5.2 Extracted Dependencies For the C/C++ projects, the number of include dependencies is much larger than the number of symbol dependencies, e.g., 297,530 symbol dependencies versus 1,183,799 include dependencies for Chromium. This is the result of both transitive and over-approximation of dependencies, detailed in Section 3.1.3 . The number of transitive dependencies shown in Table 1 for ITK is strikingly high. We leverage Class Hierarchy Analysis (CHA) [13] to build the dynamic-bindings dependency graph of symbol dependencies which, in turn, is used for extracting dependencies. When using CHA, we consider that each time a method from a specific class is called, all its subclasses are also called. Depending on how the developers use dynamic bindings, this can generate a large number of dependencies. For example, for ITK, more than 75 percent of the dependencies extracted are virtual function calls, as opposed to just 11 percent for Chromium. This high proportion of dynamic bindings also results in an extremely large number of transitive dependencies. For Chromium, the algorithm to obtain transitive dependencies ran out of memory on our 32 GB server. None of the recovery technique scaled for ITK with transitive dependencies. Given that Chromium is around ten times larger than ITK, it is safe to assume that, even if we were able to obtain the transitive dependencies for Chromium, none of the technique would scale to Chromium with transitive dependencies. 5.3 Ground-Truth Architectures To assess the effect of different types of dependencies on recovery techniques, we obtained ground-truth architectures for each selected project. Compared to previous work [10], we do not use Linux 2.0.27 and Mozilla 1.3 because our tool that extracts symbol-level dependencies for C++ projects works with LLVM. Making those two projects compatible with LLVM would require heavy manual work. In place of those medium-sized projects, we included ITK. We also included a very large project, Chromium, for which we recovered the ground truth. Due to issues resolving library dependencies with an older version of OODT, for which a ground-truth architecture is available [1], we were unable to use it for our study. For Chromium, the ground-truth architecture was obtained by manually improving the preliminary architecture extracted using the submodule approach outlined in Section 3.2. After several updates and meetings with a Chromium developer, the ground-truth architecture was certified by one of Chromium’s main developers. ITK was refactored in 2013 and its ground-truth architecture, extracted by ITK’s developers, is available. We contacted one of ITK’s lead developers involved in the refactoring who confirmed that this architecture was still correct for ITK 4.5.2. The version of Bash used in a recent architecture-recovery study [10] was from 1995. Bash has been changed significantly since then (e.g., from 70KSLOC to 115KSLOC). Therefore, we recovered the ground-truth architecture of the latest version of Bash and used it in our study. Our certifier for Bash is one of Bash’s primary developers and its sole maintainer, who also recently authored a chapter on Bash’s idealized architecture [54]. The ground-truth architecture for ArchStudio was updated, from prior work [1] , to be defined at the file level instead of at the class level. Additionally, ArchStudio’s original ground-truth architecture had a number of inconsistencies and missing files, which were verified and corrected by ArchStudio’s primary architect. Hadoop, an open-source Java project used in a recent architecture-recovery study  [10], was the other Java project we evaluated. Its original ground-truth architecture was based on version 0.19.0 and had to be converted from the class level to the file level for our analysis. For our analysis, we focused on the HDFS, Map-Reduce, and core parts of Hadoop. 5.4 Architecture Recovery Software and Parameters To answer the research questions, we compare the clustering results obtained from nine variants of the six architecture recovery techniques, using include and symbol dependencies different types of dependencies. All input dependencies and output recovered architectures are generated in the Rigi Standard Format  [34]. We obtained ACDC and Bunch from their authors’ websites. The K-means-based architecture recovery technique was implemented using the scikit-learn python library  [55]. For the other techniques, we used our implementation from our previous study [10], [53]. Each of those implementations was shared with the original authors of the recovery techniques and confirmed as correct  [10]. Due to the non-determinism of the clustering algorithms used by ACDC and Bunch, we ran each algorithm five times and reported the average results. WCA, LIMBO, ARC and K-means can take varying numbers of clusters as input. We experimented with 20 clusters bellow and above the number of clusters in the ground truth, with an increment of five for all cases. For example, for ArchStudio, we ran these algorithms for 40 to 80 clusters. ARC also takes a varying number of concerns as input. We experimented with 10 to 150 concerns in increments of 10. We report the average results for each technique. 5.5 Accuracy Measures There might be multiple ground-truth architectures for a system [1], [31]; that is, experts might disagree. Therefore, a recovered architecture may be different from a ground-truth architecture used in this paper, but close to another ground-truth architecture of the same project. To mitigate this threat, we selected four different metrics commonly used in other automatic architecture recovery evaluations to measure the impact of the different inputs on the quality of the recovered architectures. One of the metrics—normalized TurboMQ—is independent of any ground-truth architecture, which calculates the quality of the recovered architectures. When we use normalized TurboMQ to compare different recovery techniques, the threat of multiple ground-truth architectures should not apply. The remaining three metrics—MoJoFM, a2a and c2c_{cvg} —calculate the similarity between a recovered architecture and a ground-truth architecture. If one recovery technique consistently performs well according to all metrics, it is less likely due to the bias of one metric or the particular ground-truth architecture. Although using four metrics cannot eliminate the threat of multiple ground-truth architectures entirely, it should give our results more credibility than using MoJoFM alone. We used MoJoFM, a2a and c2c_{cvg} ’s implementations provided by the developers of each technique. For TurboMQ, we used our own implementation based on the technique described by the original authors [56]. MoJoFM  [57] is defined by the following formula, \begin{equation} MoJoFM(M)=\left(1- \frac{mno(A,B)}{max(mno(\forall A,B))} \right) \times 100\%, \end{equation} where mno(A,B) is the minimum number of Move or Join operations needed to transform the recovered architecture A$ _Z into the ground truth Z_$B . This measure allows us to compare the architecture recovered by the different techniques according to their similarity with the ground-truth architecture. A score of 100 percent indicates that the architecture recovered is the same as the ground-truth architecture. A lower score results in greater disparity between A$ _Z and Z_$B . MoJoFM has been shown to be more accurate than other measures and was used in the latest empirical study of architecture recovery techniques  [5], [10]. Architecture-to-Architecture  [58] (a2a) is designed to address some of MoJoFM drawbacks. MoJoFM’s Join operation is excessively cheap for clusters containing a high number of elements. This is particularly visible for large projects. This results in high MoJoFM values for architectures with many small clusters. In addition, we discovered that MoJoFM does not properly handle discrepancy of files between the recovered architecture and the ground truth. This observation corroborates results obtained in recent work [58]. We tried to reduce this problem by adding the missing files to the recovered architecture into a separate cluster before measuring MoJoFM, but this does not entirely solve the issue. In complement of MoJoFM, we use a new metric, a2a, based on architecture adaptation operations identified in previous work [59], [60]. a2a is a distance measure between two architectures     \begin{equation*} a2a (A_i,A_j) =\left(1-\frac{{mto} (A_i,A_j)}{{aco} (A_i) + {aco} (A_j)} \right) \times 100\% \end{equation*} \begin{align*} &{mto} (A_i,A_j) = remC(A_i,A_j) + addC(A_i,A_j) \\ & \quad\quad\quad\quad\quad\;+ remE(A_i,A_j)+ addE(A_i,A_j) + movE(A_i,A_j)\\ &{aco} (A_i) = addC(A_{\emptyset},A_i) + addE(A_{\emptyset},A_i) + movE(A_{\emptyset},A_i), \end{align*} where {mto} (A_i,A_j) is the minimum number of operations needed to transform architecture A_i into A_j ; and {aco} (A_i) is the number of operations needed to construct architecture A_i from a “null” architecture A_{\emptyset} . {mto} and {aco} are used to calculate the total numbers of the five operations used to transform one architecture into another: additions ({addE} ), removals ({remE} ), and moves ({movE} ) of implementation-level entities from one cluster (i.e., component) to another; as well as additions ({addC} ) and removals ({remC} ) of clusters themselves. {mto} (A_i,A_j) is calculated optimally by using the Hungarian algorithm [61] to maximise the weights of a bipartite graph built with the clusters from A_i and A_j . Cluster-to-Cluster Coverage (c2c_{cvg} ) is a metric used in our previous work [62] to assess component-level accuracy. This metric measures the degree of overlap between the implementation-level entities contained in two clusters     \begin{equation*} {c2c} (c_i,c_j) =\frac{\left|{entities(c_i)} \cap {entities(c_j)} \right|}{max(\left|{entities(c_i)} \right|, \left|{entities(c_j)} \right|)} \times 100\%, \end{equation*} where c_i is a technique’s cluster; c_j is a ground-truth cluster; and {entities(c)} is the set of entities in cluster c . The denominator is used to normalize the entity overlap in the numerator by the number of entities in the larger of the two clusters. This ensures that {c2c} provides the most conservative value of similarity between two clusters. To summarize the extent to which clusters of techniques match ground-truth clusters, we leverage architecture coverage (c2c_{cvg} ). c2c_{cvg} is a change metric from our previous work [62] that indicates the extent to which one architecture’s clusters overlap the clusters of another architecture     \begin{equation*} {c2c_{cvg} (A_1,A_2)} =\frac{\left|{simC(A_1,A_2)} \right|}{\left|{A_2.C} \right|} \times 100\% \end{equation*}     \begin{equation*} \begin{array}{rll}{simC(A_1,A_2)} = \lbrace c_i \mid & (c_i \in A_1, \exists c_j \in A_2) \ \wedge \ \\ & ({c2c} (c_i,c_j) > {th_{cvg}}) \rbrace . \end{array} \end{equation*} A_1 is the recovered architecture; A_2 is a ground-truth architecture; and A_2.C are the clusters of A_2 . {th_{cvg}} is a threshold indicating how high the {c2c} value must be for a technique’s cluster and a ground-truth cluster in order to count the latter as covered. Normalized Turbo Modularization Quality (normalized TurboMQ) is the final metric we are using in this paper. Modularization metrics measure the quality of the organization and cohesion of clusters based on the dependencies. They are widely accepted metrics which have been used in several studies  [63], [64], [65]. We implemented the TurboMQ version because it has better performance than BasicMQ  [56]. To compute TurboMQ two elements are required: intra-connectivity, and extra-connectivity. The assumption behind this metric is that architectures with high intra-connectivity are preferable to architectures with a lower intra-connectivity. For each cluster, we calculate a Cluster Factor as followed:     \begin{equation*} {CF_i} =\frac{{\mu _i}}{{\mu}_i + 0.5 \times \sum \nolimits _{j} {\epsilon _{ij} + \epsilon _{ji}}}. \end{equation*} \mu _i is the number of intra-relationships; \epsilon {ij} + \epsilon _{ji} is the number of inter-relationships between cluster i and cluster j . TurboMQ is defined as the sum of all the Cluster Factors \begin{equation*} {TurboMQ} = {\sum \limits _{i=1}^k CF_i} . \end{equation*} We note that TurboMQ by itself is biased toward architectures with a large number of clusters because the sum of CF_i will be very high if the recovered architecture contains numerous clusters. Indeed, we found that for Chromium, the architecture recovered by ACDC contains thousands of clusters. The TurboMQ value for this architecture was 400 times higher than the TurboMQ values of architectures obtained with other recovery techniques. To address this issue, we normalized TurboMQ by the number of clusters in the recovered architecture. 6   Results This section presents the results of our study that answer the three research questions, followed by a comparison of our results and those of prior work. Tables 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, and 21 show the results for all four metrics when applied to a combination of a recovery technique and system; and, if applicable for such a combination, the results for a type of dependency: Inc lude, Sym bol, Funct ion alone, function and global variables (F-GV ), Trans itive, and Mod ule-level dependencies. Symbol dependencies may be resolved by ignoring dynamic bindings (No Vir) or using a class hierarchy analysis of dynamic bindings (S-CHA ) or with interface-only resolution of dynamic bindings (S-Int ). Bash does not contain dynamic bindings because it is implemented in C, and our tool cannot extract function pointers. TABLE 2 MoJoFM Results for Bash  TABLE 3 MoJoFM Results for Chromium  TABLE 4 MoJoFM Results for ITK  TABLE 5 MoJoFM Results for ArchStudio  TABLE 6 MoJoFM Results for Hadoop  TABLE 7 a2a Results for Bash  TABLE 8 a2a Results for ITK  TABLE 9 a2a Results for Chromium  TABLE 10 a2a Results for ArchStudio  TABLE 11 a2a Results for Hadoop  TABLE 12 Normalized TurboMQ Results for Bash  TABLE 13 Normalized TurboMQ Results for Chromium  TABLE 14 Normalized TurboMQ Results for ITK  TABLE 15 Normalized TurboMQ Results for ArchStudio  TABLE 16 Normalized TurboMQ Results for Hadoop  TABLE 17 c2c_{cvg} Results for Majority (50 Percent), Moderate (33 Percent) and Weak (10 Percent) Matches for Bash  TABLE 18 c2c_{cvg} Results for Majority (50 Percent), Moderate (33 Percent) and Weak (10 Percent) Matches for ITK  TABLE 19 c2c_{cvg} Results for Majority (50 Percent), Moderate (33 Percent) and Weak (10 Percent) Matches for Chromium  TABLE 20 c2c_{cvg} Results for Majority (50 Percent), Moderate (33 Percent) and Weak (10 Percent) Matches for ArchStudio  TABLE 21 c2c_{cvg} Results for Majority (50 Percent), Moderate (33 Percent) and Weak (10 Percent) Matches for Hadoop  For certain combinations of recovery techniques and systems, a result may not be attainable due to inapplicable combinations (NA), techniques running out of memory (MEM), or timing out (TO). For example, information retrieval-based techniques such as ARC and ZBR do not rely on dependencies. Therefore, normalized TurboMQ results are not meaningful when studying the impact of the different factors of the dependencies. For this reason, we only report normalized TurboMQ for include and symbol dependencies and mark the other combinations as inapplicable. We do not report results obtained utilizing transitive dependencies for Chromium and ITK because, as discussed above, the use of such dependencies with those projects caused scalability problems. Module-level dependencies are only reported for ITK and Chromium, since they are the only projects that define modules in their documentation or configuration files. 6.1 RQ1: Can Accurate Dependencies Improve the Accuracy of Recovery Techniques? As explained in Section 3.1.3, include dependencies present some issues (e.g., missing relationships between non-header files, etc.) which can be solved by using more accurate dependencies based on symbol interactions. Therefore, to answer this research question, we focus on results obtained using include (Inc) and symbol dependencies (Sym, S-Int, and S-CHA), which are presented in Tables 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, and 21. In these Tables, we reported the average results for each technique and each type of dependency. Three recovery techniques—ARC, ZBR-tok, and ZBR-uni—do not rely on dependencies; however, we include them to assess the accuracy of symbol dependencies against these information retrieval-based techniques. The best score obtained for each recovery technique across all type of dependencies is highlighted in dark gray; the best score between include and symbol dependencies for each technique, when applied to a particular technique, is highlighted in light gray. Our results indicate that symbol dependencies generally improve the accuracy of recovery techniques over include dependencies. According to a2a scores (Tables 7, 8, 9, 10, and 11) relying on both types of symbol dependencies outperforms relying on include dependencies for all of the combinations of techniques and systems which use dependencies. The only exception is in the case of ITK, where relying on include dependencies outperforms interface-only resolution for dynamic bindings. As ITK contains a large number of dynamic-bindings dependencies (more than 75 percent), using interface-only resolution likely results in a significant loss of information, making those dependencies inaccurate. When doing a complete analysis of the dynamic-bindings dependencies of ITK (S-CHA), using symbol dependencies with a class hierarchy analysis of dynamic bindings outperforms using include dependencies for all techniques. On average, using symbol dependencies respectively improves the accuracy by 9 percentage points (percentage point, pp, is the unit for the arithmetic difference between two percentages) according to a2a. For a2a, the technique obtaining the greatest improvement from the use of symbol dependencies, as compared to include dependencies, is K-means, followed by Bunch-SAHC, with an average improvement of, respectively, 12 pp and a 10 pp for a2a. MoJoFM results (Tables 2, 3 , 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, and  16) followed a similar trend, with symbol dependencies generally improving the accuracy of the recovered architecture over include dependencies for five of the projects. However, for Bash, include dependencies produce better results than symbol dependencies for all techniques but ACDC. Tables 17, 18, 19, 20, and 21 show c2c_{cvg} for three different values of {th_{cvg}} , i.e., 50, 33, and 10 percent, (from left to right) for each combination of technique and dependency type. The first value depicts c2c_{cvg} for {th_{cvg}} = 50\% which we refer to as a majority match. We select this threshold to determine the extent to which clusters produced by techniques mostly resemble clusters in the ground truth. The other two Z_ $c2c_{cvg}$_Z scores show the portion of moderate matches (33 percent) and weak matches (10 percent). Dark gray cells show the highest c2c_{cvg} for each recovery technique across all type of dependencies. Light gray cells show the highest Z_ $c2c_{cvg}$_Z between include and symbol dependencies for each technique, when applied to a particular technique for a specific threshold {th_{cvg}} . Several rows do not have any highlighted cells; such rows indicate that c2c_{cvg} is identical for include and symbol dependencies. We observe significant improvement when using symbol dependencies over include dependencies, even for {th_{cvg}} = 50\% . For example, in Table 20, for ACDC on ArchStudio, the c2c_{cvg} for {th_{cvg}} = 50\% for include dependencies is 9 percent, while using symbol dependencies increased it to 56 percent with symbol dependencies and interface-only resolution. Overall, Tables 17, 18, 19, 20, and 21 indicate that (1) the use of symbol dependencies generally produces more accurate clusters (majority matches); and that (2) c2c_{cvg} is low regardless of the types of dependencies used. Tables 12, 13, 14, 15, and  16 presents the normalized TurboMQ results, which measure the organization and cohesion of clusters independent of ground-truth architectures. Both types of symbol dependencies generally obtain higher normalized TurboMQ scores than include dependencies, ACDC and Bunch with CHA resolution being exceptions for ArchStudio and Hadoop. In other words, symbol dependencies help recovery techniques produce architectures with better organization and internal component cohesion than include dependencies. TurboMQ results of the summation of individual scores for each cluster in the architecture make it biased toward architectures with an extremely high number of clusters. For example, ACDC for Chromium, with more than 2,000 clusters, obtains TurboMQ scores with one to two orders of magnitude larger than the other metrics. 6.1.1 Statistical Significance Test We conduct statistical significance tests to verify whether using accurate dependencies improves the quality of recovery techniques. We do not use paired t-tests because our data does not follow a normal distribution. Instead, we use the Wilcoxon signed-rank test, which is a non-parametric test. We also measure the Cliff’s \delta , a non-parametric effect size metric, to quantify the difference among the different types of dependencies. Cliff’s \delta ranges from - 1 to 1. The sign of the \delta indicates whether the first or second sample contains higher values. For example, when looking at results using include versus results obtained using symbol dependencies with CHA resolution for MoJoFM in Table 23, the \delta is positive, indicating that results obtained with symbol dependencies and CHA resolution are generally better than the ones obtained using include dependencies results. In contrast, the \delta for direct versus transitive dependencies is negative, indicating that using direct dependencies generally produces higher results. To interpret the effect size, we use the following magnitude: negligible (|\delta |< 0.147), small (|\delta |< 0.33), medium ( |\delta |< 0.474), and large (0.474 \leq |\delta | )  [66]. To compute these tests, we use average measurements obtained for each type of dependencies and metrics across all projects. This represents 35 paired samples per metric. We report these results in Tables 22 and 23 . TABLE 22 Wilcoxon Signed Rank When Comparing Different Types of Dependencies  TABLE 23 Cliff’s \delta Effect Size Tests  When comparing include dependencies and symbol dependencies with interface-only dynamic-bindings resolution, for 4 out of 6 metrics—a2a, TurboMQ, and two c2c_{cvg} metrics (majority and moderate matches)—we found that the p-values are inferior to 0.001, suggesting our results are statistically significant. For these four metrics, we observed an effect size varying from small (c2c_{cvg} metrics) to large (a2a). When comparing include dependencies and symbol dependencies with CHA, we found that the p-values are inferior to 0.02, for 3 out of 6 metrics. The effect size for these three metrics vary from negligible (c2c_{cvg} metrics) to large (a2a). c2c_{cvg} with weak coverage not being significant indicates that the type of dependency does not matter for producing a “weak” approximation of the ground-truth architecture. However, when attempting to obtain a better architecture (i.e., with a moderate coverage), working with symbol dependencies with interface-only dynamic-bindings resolution is preferable. 6.1.2 Summary of RQ1 The overall conclusion from applying these four metrics is that symbol dependencies allow recovery techniques to increase their accuracy for all systems in almost every case, independently of the metric chosen. Especially, using a2a metrics, we observed a statistically significant improvement coupled with a large effect size in favor of symbol dependencies with interface-only dynamic-bindings. Despite the accuracy improvement of using symbol dependencies over include dependencies, c2c_{cvg} results for majority match are low. This indicates that these techniques’ clusters are significantly different from clusters in the corresponding ground truth. It suggests that improvement is needed for all the evaluated recovery techniques. 6.2 RQ2: What Is the Impact of Different Input Factors, Such as the Granularity Level, the Use of Transitive Dependencies, the Use of Different Symbol Dependencies and Dynamic-Bindings Graph Construction Algorithms, on Existing Architecture Recovery Techniques? There are several types of dependencies that can be used as input to recovery techniques. First, we can break symbol dependencies based on the type of symbol (functions, global variables, etc.). Another important factor to take into consideration concerns the way we resolve dynamic bindings. Finally, we also look at the transitivity and the granularity level of the dependencies. Those different factors are considered as important for other software analyzes and have a significant impact on the quality of the recovered architectures. 6.2.1 Impact of Function Calls and Global Variables Function calls are the most common type of dependencies in a system. The question we study pertains to whether the most common dependencies have the most significant impact on the quality of the recovery techniques. Global variables typically represent a small, but important part of a program. Global variables are a convenient way to share information between different functions of the same program. If two functions use the same global variables, they might be similar and the files they belong to could be clustered together. Two functions may, in fact, be dependent on each other if they both utilize the same global variable. Therefore, adding a global-variable-usage graph to the function-call graph could help connect similar elements, that do not directly interact with one another via function calls. We do not use global variable dependencies alone, because for most of the systems, only a minority of elements accesses global variables. Therefore, by considering global variable dependencies alone, we would miss a large number of elements of the system, making several metrics inaccurate. Instead, we measure the improvement on the quality of the recovered architectures obtained by adding global variable dependencies to functions calls compared to using function calls alone. Static analysis using LLVM can detect global variables for C and C++ projects. For Java projects, we consider variables containing the static keyword as an approximation of C/C++ global variables. Overall, MoJoFM and normalized TurboMQ values for function calls alone and symbol dependencies are highly similar. However, for c2c_{cvg} and a2a, results from all symbol dependencies are significantly better than results from function calls alone. For example, a2a results are, on average 16.8 pp better when using all symbol dependencies available (S-CHA) than when using function calls alone. Despite the fact that function calls have a major impact on the accuracy of architecture recovery techniques, using function calls alone is not sufficient for obtaining accurate recovered architectures. The impact of global variable usage is minor. For example, on average, adding global variable usage to function call dependencies improves the results by 0.1 pp according to a2a. The impact of global variables is reduced because of the small number of global variable accesses in the projects used in our study. For example, Chromium’s C and C++ Style Guide3 discourages the use of global variables. We acknowledge that our results would likely be different for a system relying heavily on global variables, such as the Toyota ETCS system, which contains about 11,000 global variables [67], [68]. We performed statistical tests and confirmed that symbol dependencies are better than functions alone for architecture recovery for 3 out of 6 metrics (p-value < 0.05), with an effect size varying from small to medium in favor of symbol dependencies with interface-only resolution. In addition, we did not observe a statistical difference among architectures recovered from dependencies involving functions alone, and dependencies involving both functions and global variables. It confirms that global variable usage is not a key dependency for accurate architecture recovery for the projects we studied. 6.2.2 Impact of Dynamic-Bindings Resolution Dynamic-bindings resolution is a known problem in software engineering, and several possible strategies for addressing it have been proposed [11], [12], [13]. Due to the high number of dynamic bindings in C++ and Java projects, the type of resolution chosen when extracting symbol dependencies can significantly impact the accuracy of recovery techniques. However, it is unclear which type of dynamic-bindings resolution has the greatest impact on the architecture of a system. To determine which dynamic-bindings resolution to utilize for recovery techniques, we evaluate three different resolution strategies: (1) ignoring dynamic bindings, (2) interface-only resolution, and (3) CHA-based resolution. Ignoring dynamic bindings is the easiest solution to follow. More importantly, including it as a possible resolution strategy allows us to determine whether doing any dynamic bindings analysis improves recovery results. For interface-only resolution, we only consider the interface of the virtual functions as being invoked and discard potential calls to derived functions. This is the simplest resolution that can be performed that does not ignore dynamic bindings. For the third resolution strategy, we use Class Hierarchy Analysis [13] , which is a well-known analysis that is computationally costly to perform. For this type of resolution, we consider all the derived functions as potential calls. This resolution also creates a larger dependency graph than interface-only resolution. The results obtained when ignoring dynamic bindings are shown in column No DyB. The results for symbol dependencies obtained with CHA and Interface-only dynamic-bindings resolution are respectively presented in column S-CHA and S-Int. Bash, written in C, is the only project which does not contain any dynamic bindings. The results obtained when discarding dynamic bindings (column No DyB) are generally not as good as with other symbol dependencies. According to a2a, using only non-dynamic-bindings dependencies reduces the accuracy of the recovery techniques for all projects and all techniques when compared to using dynamic bindings and the average a2a results without dynamic bindings are 11 pp lower than the results with CHA and 6 pp lower than the results with interface-only resolution. There are a few exceptions for Chromium, Hadoop, and ArchStudio with the metrics MoJoFM and normalized TurboMQ. The reason for unexpectedly high results with MoJoFM and normalized TurboMQ is that using partial symbol dependencies is not well handled by those two metrics. Using partial symbol dependencies—in our case, we discard symbol dependencies that are dynamic bindings—results in (1) a significant mismatch of files between the ground-truth architecture and the recovered architectures, and (2) a disconnected dependency graph. The file mismatches create artificially high MoJoFM results, and the disconnected dependency graphs can lead to extremely high or even perfect normalized TurboMQ scores, as it is the case for ArchStudio when using Bunch without dynamic-bindings dependencies. c2c_{cvg} results are not conclusive either way. When looking at interface-only and CHA resolutions, we observe a difference in behavior of the two Java projects and the two C/C++ projects. For Java-based Hadoop and ArchStudio, using an interface-only resolution seems to greatly improve the results over using CHA. Those results are obtained for both projects and for all metrics, with only two exceptions for c2c_{cvg} in Hadoop ( Table 20) where using CHA provides slightly better results. On average, according to normalized TurboMQ, using interface-only resolution improves the results by 20 pp for ArchStudio and Hadoop. However, for C++-based ITK and Chromium, the normalized TurboMQ results are improved by 6 pp when using CHA for dynamic-bindings resolution. There could be several reasons for this difference. First, the two C++ projects are between 10 and 200 times larger than the two Java projects we studied. It is possible that a complete analysis of dynamic bindings only becomes necessary for large projects with many complex virtual objects. Second, in Java, methods are virtual by default, while in C++, methods have to be declared as virtual by using the keyword virtual. C and C++ developers also have the possibility to use function pointers instead of dynamic bindings, which are currently not handled properly by our symbol dependency extractor. Those two elements could also be a reason why we observed different affects of dynamic-bindings resolutions for C++ and Java projects. Our overall results indicate that, to obtain a more accurate recovered architecture, the choice of the dynamic-bindings resolution algorithm depends on the project studied. Specifically, if the project contains a high number of dynamic bindings, CHA is likely to produce better recovery results. Otherwise, interface-only resolution is preferable. Ignoring dynamic bindings is ill-advised in most cases. 6.2.3 Transitive versus Direct Dependencies A transitive dependency can be built from direct dependencies. For example, if A depends on B, and B depends on C, then A transitively depends on C. Recovery techniques can use as input (1) direct dependencies only or (2) transitive dependencies. To compare direct dependencies against transitive dependencies, we run a transitive closure algorithm on the symbol dependencies and study the effect of adding transitive dependencies on the accuracy of architecture recovery. We did not use include dependencies for this study because, as explained in Section 3.1.3, include dependencies for C and C++ projects are not direct dependencies. Furthermore, we did not include Chromium because the algorithm generating transitive dependencies does not scale to that size, even when we tried to use advanced computational techniques, such as Crocopat’s use of binary decision diagrams [46]. For ITK, although we were able to obtain transitive dependencies, none of the architecture recovery techniques scaled to its size. Therefore, we cannot report those results. Results for Bash, Hadoop, and ArchStudio, are reported in the tables corresponding to the different metrics (column Trans). When comparing the results obtained with direct (Sym for Bash and S-Int for Hadoop and ArchStudio) and transitive (Trans) symbol dependencies, we observe that using direct dependencies generally provides similar or better results. Results with MoJoFM, normalized TurboMQ, and c2c_{cvg} tend to favor the use of direct dependencies over transitive dependencies (+15 pp on average for normalized TurboMQ when using direct dependencies, +4.9 pp on average for MoJoFM). According to a2a, using transitive dependencies has a minor impact (Z_$-$ _Z 0.33 pp on average) on the results. a2a gives importance to the discrepancy of files between the recovered architecture and the ground truth. As no files are added or removed when obtaining the transitive dependencies from the direct dependencies, this discrepancy is exactly the same between the direct and transitive dependencies. This is why we do not observe a significant difference between direct and transitive dependencies results when using a2a. When running statistical tests, we found that results from direct dependencies (S-Int) are statistically different from results obtained from transitive dependencies for all metrics, except a2a, confirming our conclusion that direct dependencies have a positive impact on the quality of the recovered architectures. With fewer dependencies, using direct dependencies is more scalable than transitive dependencies. In summary, direct dependencies help generate more accurate architectures than transitive dependencies in most cases. 6.2.4 Impact of the Level of Granularity of the Dependencies Results at the module level are reported for ITK and Chromium under the column Mod. of Tables 3, 4, 8, 9, 13, 14, 18, and 19. Module dependencies are obtained by adding information extracted from the configuration files to group files together. This information is written by the developers and could represent the architecture of the project as it is understood by developers. Given the inherent architectural information in such dependencies, it is expected that they would improve a recovery technique’s accuracy. Because we only have module dependencies for ITK and Chromium, we do not have enough data points to measure statistical significance of our results. However, results obtained from module-level dependencies tend to be much better than from file-level dependencies. For example, on average, compared to using the best file-level dependencies, using module-level information improves the results by 7.5 pp according to a2a. Overall, our results indicate that module information, when available, significantly improves recovery accuracy and scalability of all recovery techniques. As shown in Table 1, the number of module dependencies is almost 70 times lower than the number of file dependencies. Because of this reduction in the number of dependencies, we obtain results from all recovery techniques in a few seconds when working at the module level, as opposed to several hours for each technique when working at the file level. 6.3 RQ3: Can Existing Architecture Recovery Techniques Scale to Large Projects Comprising 10MSLOC or More? 6.3.1 Scalability Overall, ACDC is the most scalable technique. It took only 70-120 minutes to run ACDC on Chromium on our server. The WCA variations and ARC have a similar execution time (8 to 14 hours), with WCA-UENM slightly less scalable than WCA-UE. Bunch-NAHC is the last technique which was able to terminate on Chromium for both kinds of dependencies, taking 20 to 24 hours depending on the kind of dependencies used. LIMBO only terminated for symbol dependencies after running for 4 days on our server. Bunch-SAHC timed out after 24 hours for both include and symbol dependencies. We report here the intermediate architecture recovered at that time. Bunch-SAHC investigates all the neighboring architectures of a current architecture and selects the architecture that improves MQ the most; Bunch-NAHC selects the first neighboring architecture that improves MQ. Bunch-SAHC’s investigation of all neighboring architectures makes it less scalable than Bunch-NAHC. LIMBO failed to terminate for include dependencies after more than 4 days running on our server. Two operations performed by LIMBO, as part of hierarchical clustering, result in scalability issues: construction of new features when clusters are merged and computation of the measure used to compare entities among clusters. Both of these operations are proportional to the size of clusters being compared or merged, which is not the case for other recovery techniques that use hierarchical clustering (e.g., WCA). ZBR needs to store data of the size nzV , where n is the number of files being clustered, z$ _Z is the number of zones, and Z_$V is the number of terms. For large software (i.e., ITK and Chromium), with thousands of files and millions of terms, ZBR ran out of memory after using more than 40 GB of RAM. The use of symbol dependencies improves the recovery techniques’ scalability over include dependencies for large projects (i.e., ITK and Chromium). The main reason for this phenomenon is that include dependencies are less direct than symbol dependencies. As mentioned in the discussion of the previous research question, working at the module-level significantly reduces the number of dependencies and, therefore, greatly improves the scalability of all dependency-based techniques for large projects. Indeed, at the module-level we were able to obtain results in a few seconds, even for techniques that did not scale with file-level dependencies. 6.3.2 Metrics versus Size While some algorithms are scalable for large projects, it does not mean that results obtained for large systems are as relevant as results obtained for smaller systems. We verify if automatic architecture recovery techniques perform equally for software of all sizes by measuring the evolution of the architectures’ quality, when the size of the projects increases. Overall, we can see that results for Chromium (the largest project) are generally less accurate than results for Bash, ArchStudio, or Hadoop (the smallest projects). However, results for ITK are generally worst than for Chromium, despite ITK being ten times smaller. Because we only study five different projects, we cannot draw clear conclusions. Nonetheless, the fact that results for ITK are worst than for Chromium seems to indicate that the size of the project under study is not the only factor affecting the quality of recovered architectures. Other factors such as the programming language, the coding style, and the use of dynamic bindings probably also have an impact that we can’t measure with only five different projects. 6.4 Comparison with Baseline Algorithms To see whether software architecture recovery algorithms are effective, we compare their results with two baseline recovered architecture. For the first baseline, we recovered the architecture based on the directory structure of the project. According to a2a results, all recovery techniques performed better than the baseline for Bash, ITK and Chromium. A similar trend can be observed for TurboMQ and c2c_{cvg} , with a few exception (e.g., WCA and Limbo for Chromium for TurboMQ). This seems to indicate that architecture recovery techniques might be helpful to improve the architecture of these projects. For ArchStudio and Hadoop, the directory structure-based architecture consistently outperforms architectures recovered with other algorithms, except for TurboMQ for which results are less consistent. This seems to indicate that ArchStudio and Hadoop already have a directory relatively similar to their ground truth architecture and that architecture refactoring might not be necessary for these two projects. Our second baseline consists in comparing the algorithms specifically designed for architecture recovery (ACDC, Bunch, WCA, and Limbo) with the results obtained from a basic machine learning algorithm, k-means, used with default parameters. Tables 24 and  25 show the statistical significance and the effect size of the difference between K-means and other algorithms, independently from the dependencies used. ACDC is the only algorithm that produces equivalent or better results than K-means consistently, for all metrics. WCA-UE and Limbo always produce worst results than K-means. Finally, the two Bunch algorithms produce better results than K-means only for some metrics. TABLE 24 Wilcoxon Signed Rank for Each Algorithm When Compared to K-Means  TABLE 25 Cliff’s \delta Effect Size Tests  The three techniques that performed consistently worst than the baseline algorithm are all hierarchical clustering algorithms. It is possible that techniques based on hierarchical clustering are not adapted to recover flat architectures. Results could be different for other projects or ground-truth architectures. 6.5 Summary of Results Overall, we discovered three main findings from our study. First, using accurate symbol dependencies improves the quality of the recovered architectures. Second, using direct dependencies is more scalable and generally improves the quality of the results. Finally, working with high-level dependencies (i.e., module dependencies) is another key factor for scalable and high-quality architecture recovery of large systems. 6.6 Comparison with the Prior Work As previously mentioned, three of our subject systems were also used in our previous study  [10]. It is difficult to compare our results with the prior study because of the differences described in Section 5.3. When using the same type of dependencies (Inc) as in our previous study, we observe minor differences for some algorithms. However, on average, the MoJoFM scores only drop by 0.1 pp for all techniques over the scores reported in  [10]. In the cases of Hadoop and ArchStudio, our previous study used a different level of granularity (class level), which makes comparison with current work irrelevant. 7   Threats to Validity This section describes several secondary results of our research such as issues encountered with the different metrics, extreme architectures, and guidelines concerning the dependencies, the architecture recovery techniques, and the metrics to use in future work. 7.1 Metrics Limitations As mentioned in Section 5.5, some metrics have limitations and can be biased toward specific architectures. In this section, we explain the limitations we encountered with two of the metrics we used. Those limitations appeared because, the metrics in question were neither explicitly intended for nor adapted to specific types of dependencies. The dependencies are often incomplete. For example, include dependencies generally contain fewer files than the ground-truth architecture. The reasons were explained in Section 3.1.3, including the fact that non-header-file to non-header-file dependencies are missing. Unfortunately, one of the most commonly used metrics, MoJoFM, assumes that the two architectures under comparison contain the same elements. Given this limitation, one can create a recovery technique that achieves 100 percent MoJoFM score easily but completely artificially. The technique would simply create a file name that does not exist in a project, and place it in a single-node architecture. The MoJoFM score between the single-node architecture and the ground truth will be 100 percent. By contrast, the a2a metric is specifically designed to compare architectures containing different sets of elements. In addition to the “file mismatch” issue with MoJoFM, we also identified issues with TurboMQ, as discussed in Section 5.5. Replacing TurboMQ by its normalized version yielded an improvement. However, one has to be careful when using normalized TurboMQ. We identified two boundary cases where normalized TurboMQ results are incorrectly high. It is possible to obtain the maximum score for normalized TurboMQ by grouping all the elements of the recovered architecture in a single cluster. As there will be no inter-cluster dependencies, the score will be 100 percent. We manually checked all recovered architectures to make sure this specific case never happened in our evaluation. The second “extreme case” occurs when the dependency graph used as input is not fully connected. This can happen when using only partial symbol dependencies (i.e., global variable usage, non-dynamic-bindings dependency graph, etc.). In this case, some recovery techniques will create architectures in which clusters are not connected to one another. This also results in a normalized TurboMQ score of 100 percent. In our evaluation, this issue occurs when using non-dynamic-bindings dependencies for ArchStudio and Chromium in Tables 13 and  15. This is a limit of normalized TurboMQ when using partial dependencies. Those are specific issues we observed performing our analysis. It is conceivable that biases towards other types of architecture have yet to be discovered. This suggests that a separate, more extensive study on the impact of different architectures on the metrics would be useful in order to obtain a better understanding of those metrics. Such a study has not been performed to date. Metrics are convenient because they quantify the accuracy of an architecture with a score, allowing comparisons between recovery techniques. Our study has included, developed, adapted, and evaluated a larger number of metrics than prior similar studies. However, the value of this score by itself must be treated judiciously. Obviously, the “best” recovered architecture is the one that is the closest to the ground-truth. At the same time, important questions such as “Is the recovered architecture good enough to be used by developers?”, “Can an architecture with an a2a score of 90 percent be used for maintenance tasks?” cannot be answered by solely using metrics. A natural outgrowth of this work, then would be to involve real engineers in performing maintenance tasks in real settings. Then it would be possible to evaluate the extent to which the metrics are indicative of the impact on completing such tasks. We are currently preparing such a study with the help of several of our industrial collaborators. The metrics chosen in this paper measure the similarity and quality of an architecture at different levels—the system level (measured by MoJoFM and a2a), the component level (measured by Z_ $c2c_{cvg}$_Z) and the dependency-cohesion level (measured by normalized TurboMQ). In future work, we intend to measure the accuracy of an architecture from an additional perspective, by analyzing whether the architecture contains undesirable patterns or follows good design principles. 7.2 Selecting Metrics and Recovery Techniques Using only one metric is not enough to assess the quality of architectures. However, some metrics are better than others depending on the context. When working on software evolution, the architectures being compared will likely include a different set of files. In this case, a2a, c2c_{cvg} , and normalized TurboMQ are more appropriate than MoJoFM, which assumes that no files are added or removed across versions. If the architectures being compared contain the same files (e.g., comparing different techniques with the same input), a2a will give results with a small range of variations, making it difficult to differentiate the results of each technique. In this case, MoJoFM results are easier to analyze than the ones obtained with a2a. We do not claim that one recovery technique is better than the others. However, we can provide some guidelines to help practitioners choose the right recovery technique for their specific needs. According to our scalability study, ACDC, ARC, WCA, and Bunch-NAHC are the most adapted to recover large software architectures. When trying to recover the low-level architecture of a system, practitioners should favor ACDC, as it generally produces a high number of small clusters. If a different level of abstraction is needed, WCA, LIMBO, and ARC allow the user to choose the number of clusters of the recovered architecture. Those techniques will be more helpful for developers who already have some knowledge of their project architecture. 7.3 Non-Uniqueness of Ground-Truth Architectures There is not necessarily a unique, correct architecture for a system [1] , [31]. Recovering ground-truth architectures require heavy manual work from experts. Therefore, it is challenging to obtain different certified architectures for the same system. As we are using only one ground-truth architecture per project, there is a threat that our study may not reflect other ground-truth architectures. To reduce this threat, we use four different metrics, including one independent of the ground-truth architecture. Two of the metrics used in this study were developed by some authors of this paper, which might have caused a bias in this study. However, all four metrics, including metrics developed independently, follow the same trend—symbol dependencies are better than include dependencies—which mitigates some of the potential bias. Furthermore, actual developers or architects of the projects aided in the production of our ground-truth architectures, further reducing any bias introduced in those architectures. 7.4 Number of Projects Studied We have evaluated recovery techniques on only five systems, which limits our study’s generalizability to other systems. Adding more projects is challenging. First, manually recovering the ground-truth architecture of a test project is time-consuming and requires the help of an expert with deep knowledge of the project  [1]. Second, the projects studied need to be compatible with the tools used to extract dependencies. For example, the C++ projects evaluated need to be compilable with Clang. To mitigate this threat, we selected systems of different sizes, functionalities, architecture paradigms, and languages. 8   Future Work Dependencies . This paper explores whether the type of dependencies used affects the quality of the architecture recovered, and answers in the affirmative: Each recovery technique improves if more detailed input dependencies are used. The results in this paper show, however, that any attempted evaluation of architecture recovery techniques must be careful about dependencies: For example, if we look at the best architecture recovery technique to recover Bash, MoJoFM would select a different best technique in four out of five cases with different input dependencies; c2c_{cvg} in 3/5 cases; and normalized TurboMQ in 2/5 cases. a2a is more stable and would select Bunch-SAHC in all the cases, but a2a also shows that most of the techniques perform similarly for Bash when using similar dependencies. If we look at the other projects, we also observe that none of the metrics always pick the same best recovery technique when using different dependencies. In this paper, we evaluate architecture recovery techniques using source-code dependencies. Other types of dependencies can alternatively be used. For example, one can look at a developers’ activity (e.g., files modified together) to obtain code dependencies [69] and further work is necessary to evaluate if completely different types of dependencies such as directory structure, historical information or developer’s activity can be use in the context of automatic architecture recovery. In addition, we do not consider weighting dependencies. For example, consider FileA that uses one symbol from FileB, and FileC that uses 20 symbols from FileB. Intuitively, it seems that FileB and FileC are more connected than FileA and FileB. Unfortunately, the current implementations of the architecture recovery techniques do not consider weighted graphs. Using weighted dependencies could also be a way to improve the quality of the recovered architectures. Nested Architectures . The architecture recovery techniques evaluated in this study all recover “flat”, i.e., non-hierarchical architectures. We focus on flat architectures for several reasons. First, for 4/5 systems we only have access to a flat ground-truth architecture. Second, the existing automatic architecture recovery techniques we evaluate only recover flat architectures. In previous work on obtaining ground-truth architectures [1], results indicate that architects do not necessarily agree as to the importance of having a nested or flat architecture. However, when discussing with Google developers during the recovery of Chromium’s ground truth, it appeared that they view their architecture as a nested architecture in which files are clustered into small entities, themselves clustered into larger entities. Some work has been done on improving metrics to compare nested architectures  [70], [71], but little work has been done on proposing and evaluating automatic techniques for recovering nested architectures. A proper treatment of nested architectures, while out of scope of this paper, is an important area for future research. Multiple Ground-Truth Architectures . Our present work relies on several metrics used for evaluation of architecture recovery, some of which require a ground-truth architecture that might not be unique. More empirical work is needed to explore the idea of multiple ground-truth architectures for a given system. One possible direction is to conduct ground-truth extraction with different groups of engineers on the same system. Another direction would be to have system engineers develop ground-truth architectures starting from automatically recovered architectures. Ground-truth architectures are important for quality architecture-recovery evaluation and deserve further examination. 9   Conclusion The paper evaluates the impact of using more accurate symbol dependencies, versus the less accurate include dependencies used in previous studies, on the accuracy of automatic architecture recovery techniques. We also study the effect of different factors on the accuracy and scalability of recovery techniques, such as the type of dynamic bindings resolution, the granularity-level of the dependencies and whether the dependencies are direct or transitive. We studied nine variants of six architecture recovery techniques on five open-source systems. To perform our evaluation, we recovered the ground-truth architecture of Chromium, and updated ArchStudio and Bash architectures. In addition, we proposed a simple but novel submodule-based architecture recovery technique to recover preliminary versions of ground-truth architectures. In general, each recovery technique extracted a better quality architecture when using symbol dependencies instead of the less-detailed include dependencies. Working with direct dependencies at module level also helps with obtaining a more accurate recovered architecture. Finally, it is important to carefully choose the type of dynamic-bindings resolution when working with symbol dependencies, as it can have a significant impact on the quality of the recovered architectures. In some sense this general conclusion that quality of input affects quality of output is not surprising: the principle has been known since the beginning of computer science. Butler et al.  [72] attribute it to Charles Babbage, and note that the acronym “GIGO” was popularized by George Fuechsel in the 1950’s. What is surprising is that this issue has not previously been explored in greater depth in the context of architecture recovery. Our results show that not only does each recovery technique produce better output with better input, but also that the highest scoring technique often changes when the input changes. There are other dimensions of architecture recovery that are worthy of future exploration, such as: recovering nested architectures; evaluating the usefulness of the recovered architecture to do specific maintenance tasks; and resolving function pointers and dynamic bindings. The results presented here clearly demonstrate that there is room for more research both on architecture recovery techniques and on metrics for evaluating them. Footnotes 1. https://www.chromium.org/developers/how-tos/chromium-modularization. 2. https://itk.org/Wiki/ITK/Release_4/Modularization. 3. https://www.chromium.org/developers/coding-style. Acknowledgments We thank Eric Dashofy for his assistance with updating the ground-truth architecture for ArchStudio 4. We thank Tamara Munzner for the help with graph visualization. This work has been supported by the Natural Sciences and Engineering Research Council of Canada, a Google Faculty Research Award, Ontario Ministry of Research and Innovation, the U.S. National Science Foundation under award numbers 1117593, 1218115, and 1321141, and Infosys Technologies, Ltd. Availability: The ground-truth architectures are available at http://asset.uwaterloo.ca/ArchRecovery. References  [1]J. Garcia, I. Krka, C. Mattmann, and N. Medvidovic, “Obtaining ground-truth software architectures,” in Proc. Int. Conf. Softw. Eng., 2013, pp. 901–910. [2]P. Andritsos and V. Tzerpos, “Information-theoretic software clustering,” IEEE Trans. Softw. Eng., vol. 31, no. 2, pp. 150– 165, Feb.2005. [3]A. Corazza, S. Di Martino, V. Maggio, and G. Scanniello, “Investigating the use of lexical information for software system clustering,” in Proc. 15th Eur. Conf. Softw. Maintenance Reengineering, 2011, pp. 35–44. [4]J. Garcia, D. Popescu, C. Mattmann, N. Medvidovic, and Y. Cai, “Enhancing architectural recovery using concerns ,” in Proc. 26th IEEE/ACM Int. Conf. Automat. Softw. Eng., 2011 , pp. 552–555. [5]K. Kobayashi, M. Kamimura, K. Kato, K. Yano, and A. Matsuo, “Feature-gathering dependency-based software clustering using dedication and modularity,” in Proc. IEEE Int. Conf. Softw. Maintenance, 2012, pp. 462–471. [6]S. Mancoridis, B. S. Mitchell, Y.-F. Chen, and E. R. Gansner, “Bunch: A clustering tool for the recovery and maintenance of software system structures,” in Proc. IEEE Int. Conf. Softw. Maintenance, 1999, pp. 50–59 . [7]V. Tzerpos and R. C. Holt, “ACDC : An algorithm for comprehension-driven clustering,” in Proc. 7th Working Conf. Reverse Eng., 2000, pp. 258–267. [8]O. Maqbool and H. Babri, “Hierarchical clustering for software architecture recovery,” IEEE Trans. Softw. Eng., vol. 33, no. 11, pp. 759–780, Nov.2007. [9]J. Wu , A. E. Hassan, and R. C. Holt, “Comparison of clustering algorithms in the context of software evolution,” in Proc. IEEE Int. Conf. Softw. Maintenance, 2005, pp. 525–535 . [10]J. Garcia, I. Ivkovic, and N. Medvidovic, “A comparative analysis of software architecture recovery techniques,” in Proc. 28th IEEE/ACM Int. Conf. Automat. Softw. Eng., 2013, pp. 486–496. [11]D. F. Bacon and P. F. Sweeney , “Fast static analysis of C++ virtual function calls,” ACM SIGPLAN Notices, vol. 31, no. 10, pp. 324–341, 1996. [12]V. Sundaresan, et al., Practical Virtual Method Call Resolution for Java, vol. 35. New York, NY, USA: ACM, 2000, no. 10. [13]J. Dean, D. Grove, and C. Chambers, “ Optimization of object-oriented programs using static class hierarchy analysis,” in Proc. 9th Eur. Conf. Object-Oriented Program., 1995, pp. 77–101 . [14]D. Rayside, S. Reuss, E. Hedges, and K. Kontogiannis, “The effect of call graph construction algorithms for object-oriented programs on automatic clustering,” in Proc. 8th Int. Workshop Program Comprehension, 2000, pp. 191–200 . [15]A. E. Hassan, Z. M. Jiang, and R. C. Holt, “Source versus object code extraction for recovering software architecture,” in Proc. 12th Working Conf. Reverse Eng., 2005, Art. no. 10. [16]J.-M. Favre, “CacOphoNy: Metamodel-driven software architecture reconstruction ,” in Proc. 11th Working Conf. Reverse Eng., 2004, pp. 204–213. [17]Y. Tsuchitoi and H. Sugiura, “10 MLOC in your office copier,” IEEE Softw. , vol. 28, no. 6, pp. 93–95, Nov./Dec.2011. [18]S. Brahler, Analysis of the Android Architecture, vol. 7. Karlsruhe, Germany: Karlsruhe Inst. Technol., 2010. [19]L. Ding and N. Medvidovic , “Focus: A light-weight, incremental approach to software architecture recovery and evolution ,” in Proc. Working IEEE/IFIP Conf. Softw. Archit., 2001, pp. 191–200. [20]T. Lutellier, et al., “Comparing software architecture recovery techniques using accurate dependencies,” in Proc. 37th Int. Conf. Softw. Eng., 2015, pp. 69–78. [21]G. C. Murphy, D. Notkin, and K. Sullivan, “Software reflexion models: Bridging the gap between source and high-level models,” ACM SIGSOFT Softw. Eng. Notes, vol. 20, no. 4, pp. 18–28 , 1995. [22]T. Lethbridge and N. Anquetil, “Comparative study of clustering algorithms and abstract representations for software remodularization,” IEE Proc. Softw., vol. 150, no. 3 , pp. 185–201, Jun. 2003. [23]P. Andritsos, P. Tsaparas, R. J. Miller, and K. C. Sevcik, “LIMBO: Scalable clustering of categorical data,” in Proc. 9th Int. Conf. Extending Database Technol., 2004, pp. 531–532. [24]O. Maqbool and H. A. Babri, “The weighted combined algorithm: A linkage algorithm for software clustering ,” in Proc. 8th Euromicro Working Conf. Softw. Maintenance Reengineering, 2004 , pp. 15–24. [25]R. Fiutem, G. Antoniol, P. Tonella, and E. Merlo, “ART: An architectural reverse engineering environment,” J. Softw. Maintenance, vol. 11, no. 5 , pp. 339–364, 1999. [26]R. Fiutem, P. Tonella, G. Antoniol, and E. Merlo, “A cliche-based environment to support architectural reverse engineering,” in Proc. Int. Conf. Softw. Maintenance, 1996, pp. 319–328. [27]P. Tonella, R. Fiutem, G. Antoniol, and E. Merlo, “Augmenting pattern-based architectural recovery with flow analysis: Mosaic—A case study,” in Proc. Working Conf. Reverse Eng., 1996, pp. 198–207. [28]J.-M. Favre, F. Duclos, J. Estublier, R. Sanlaville, and J.-J. Auffre, “Reverse engineering a large component-based software product ,” in Proc. 5th Eur. Conf. Softw. Maintenance Reengineering, 2001 , pp. 95–104. [29]P. K. Laine, “The role of sw architecture in solving fundamental problems in object-oriented development of large embedded sw systems,” in Proc. Working IEEE/IFIP Conf. Softw. Archit., 2001, pp. 14–23 . [30]A. Grosskurth and M. W. Godfrey , “Architecture and evolution of the modern Web browser,” Preprint submitted to Elsevier Science, Jun.2006. [31]I. T. Bowman, R. C. Holt, and N. V. Brewster, “Linux as a case study: Its extracted software architecture,” in Proc. Int. Conf. Softw. Eng. , 1999, pp. 555–563. [32]C. Xiao and V. Tzerpos, “Software clustering based on dynamic dependencies,” in Proc. 9th Eur. Conf. Softw. Maintenance Reengineering, 2005, pp. 124– 133. [33]L. O’Brien, C. Stoermer, and C. Verhoef, “Software architecture reconstruction: Practice needs and current approaches,” DTIC Document, Fort Belvoir, VA, USA, Tech. Rep. CMU/SEI-2002-TR-024, 2002. [34]M.-A. D. Storey, K. Wong, and H. A. Müller, “ Rigi: A visualization environment for reverse engineering,” in Proc. 19th Int. Conf. Softw. Eng., 1997, pp. 606–607. [35]H. M. Kienle and H. A. Müller, “Rigi—an environment for software reverse engineering, exploration, visualization, and redocumentation,” Sci. Comput. Program., vol. 75 , no. 4, pp. 247–263, 2010. [36]R. Kazman and S. J. Carrière, “Playing detective: Reconstructing software architecture from available evidence ,” Automat. Softw. Eng., vol. 6, no. 2, pp. 107–138, 1999. [37]P. J. Finnigan, et al., “The software bookshelf ,” IBM Syst. J., vol. 36, no. 4, pp. 564– 593, 1997. [38]H. Muller, “Integrating information sources for visualizing Java programs ,” in Proc. IEEE Int. Conf. Softw. Maintenance, 2001, Art. no. 250. [39]J. Koch and K. Cooper, “AOVis: A model-driven multiple-graph approach to program fact extraction for AspectJ/Java source code,” Softw. Eng. Int. J., vol. 1, pp. 60 –71, 2011. [40]N. Synytskyy, R. C. Holt, and I. Davis, “Browsing software architectures with LSEdit,” in Proc. 13th Int. Workshop Program Comprehension , 2005, pp. 176–178. [41]Understand, Scitools.com. [Online]. Available: https://scitools.com, (Accessed on 2016). [42]F. Waldman, “Lattix LDM,” presented at the 8th Int. Des. Structure Matrix Conf., Seattle, WA, USA, Oct.24–26 , 2006. [43]C. Chedgey, P. Hickey, P. O’Reilly, and R. McNamara, Structure101. [Online]. Available: https://structure101.com/products/#products=0, (Accessed on 2016). [44]P. Wang, J. Yang, L. Tan, R. Kroeger, and D. Morgenthaler, “Generating precise dependencies for large software ,” in Proc. 4th Int. Workshop Manag. Techn. Debt, May 2013, pp. 47 –50. [45]R. W. Floyd, “Algorithm 97: Shortest path,” Commun. ACM, vol. 5, no. 6, 1962, Art. no. 345. [46]D. Beyer, “Relational programming with CrocoPat,” in Proc. 28th Int. Conf. Softw. Eng., 2006, pp. 807– 810. [47]D. Rayside and K. Kontogiannis, “Extracting Java library subsets for deployment on embedded systems ,” Sci. Comput. Program., vol. 45, no. 2/3, pp. 245 –270, Nov.2002. [48]P. Wang, “Generating accurate dependencies for large software ,” (Master's thesis). (2013). [Online]. Available: http://hdl.handle.net/10012/7875 [49]E. R. Gansner and S. C. North, “An open graph visualization system and its applications to software engineering ,” Softw.: Practice Experience, vol. 30, no. 11, pp. 1203 –1233, 2000. [50]S. Teller, Data Visualization with D3.Js. Birmingham, U.K. : Packt Publishing, 2013. [51]S. Mancoridis, B. S. Mitchell, and C. Rorres, “ Using automatic clustering to produce high-level system organizations of source code ,” in Proc. 6th Int. Workshop Program Comprehension, 1998, pp. 45 –53. [52]D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent Dirichlet allocation,” J. Mach. Learn. Res., vol. 3, pp. 993–1022, Mar.2003. [53]D. M. Le, P. Behnamghader, J. Garcia, D. Link, A. Shahbazian, and N. Medvidovic, “An empirical study of architectural change in open-source software systems ,” in Proc. 12th Working Conf. Mining Softw. Repositories, 2015 , pp. 235–245. [54]A. Brown and G. Wilson, The Architecture of Open Source Applications. Raleigh, NC, USA: Lulu, 2011. [55]F. Pedregosa, et al., “Scikit-learn: Machine learning in Python ,” J. Mach. Learn. Res., vol. 12, pp. 2825 –2830, 2011. [56]B. S. Mitchell, “A heuristic approach to solving the software clustering problem ,” in Proc. Int. Conf. Softw. Maintenance, 2003, pp. 285–288. [57]Z. Wen and V. Tzerpos, “ An effectiveness measure for software clustering algorithms,” in Proc. 12th IEEE Int. Workshop Program Comprehension, 2004, pp. 194–203. [58]D. Le , P. Behnamghader, J. Garcia, D. Link , A. Shahbazian, and N. Medvidovic, “ An empirical study of architectural change in open-source software systems,” in Proc. 12th Working Conf. Mining Softw. Repositories, 2015, pp. 235 –245. [59]P. Oreizy, N. Medvidovic, and R. N. Taylor, “ Architecture-based runtime software evolution,” in Proc. 20th Int. Conf. Softw. Eng., 1998, pp. 177–186. [60]N. Medvidovic, “ADLs and dynamic architecture changes,” in Proc. Joint Proc. 2nd Int. Softw. Archit. Workshop Int. Workshop Multiple Perspectives Softw. Develop. SIGSOFT Workshops, 1996, pp. 24–27. [61]J. Munkres, “Algorithms for the assignment and transportation problems ,” J. Soc. Ind. Appl. Math., vol. 5, no. 1, pp. 32–38, 1957. [62]J. Garcia, D. Le, D. Link, A. S. Pooyan Behnamghader, E. F. Ortiz , and N. Medvidovic, “An empirical study of architectural change and decay in open-source software systems,” Center Syst. Softw. Eng., Univ. Southern California, Los Angeles, CA, USA, Tech. Rep. USC-CSSE-2014–509, 2014. [63]M. Arzoky, S. Swift, A. Tucker, and J. Cain, “Munch: An efficient modularisation strategy to assess the degree of refactoring on sequential source code checkings,” in Proc. IEEE 4th Int. Conf. Softw. Testing Verification Validation Workshops, 2011, pp. 422–429. [64]A. S. Mamaghani and M. R. Meybodi , “Clustering of software systems using new hybrid algorithms,” in Proc. 9th IEEE Int. Conf. Comput. Inf. Technol., 2009, pp. 20–25. [65]B. S. Mitchell and S. Mancoridis , “On the automatic modularization of software systems using the bunch tool ,” IEEE Trans. Softw. Eng., vol. 32, no. 3, pp. 193–208, Mar.2006. [66]J. Romano, J. D. Kromrey, J. Coraggio, and J. Skowronek, “Appropriate statistics for ordinal level data: Should we really be using t-test and Cohen’s Z_$\delta$ _Z for evaluating group differences on the NSSE and other surveys?” in Proc. Annu. Meeting Florida Assoc. Institutional Res., 2006, pp. 1–3. [67]P. Koopman, “A case study of Toyota unintended acceleration and software safety ,” presented at the 9th TSP Symposium, Pittsburgh, PA, USA, pp. 3 –6, Nov.2014. [68]M. Kirsch, et al., “Technical support to the National Highway Traffic Safety Administration (NHTSA) on the reported Toyota Motor Corporation (TMC) unintended acceleration (UA) investigation,” NASA Eng. Safety Center Tech., Assessment Rep. #TI-10-00618, Hampton, VA, USA, 2011. [69]M. Konôpka and M. Bieliková, “Software developer activity as a source for identifying hidden source code dependencies,” in SOFSEM 2015: Theory and Practice of Computer Science. Berlin, Germany: Springer, 2015, pp. 449–462. [70]M. Shtern and V. Tzerpos, “Lossless comparison of nested software decompositions,” in Proc. 14th Working Conf. Reverse Eng., 2007, pp. 249–258 . [71]M. Shtern and V. Tzerpos, “A framework for the comparison of nested software decompositions,” in Proc. 11th Working Conf. Reverse Eng., 2004, pp. 284– 292. [72]J. Butler, W. Lidwell, and K. Holden, Universal Principles of Design, 2nd ed. Gloucester, MA, USA: Rockport Publishers, 2010. Thibaud Lutellier received the diplôme d’Ingénieur from Télécom Saint-Etienne and the MASc degree in computer engineering from the University of Waterloo. He is currently working toward the PhD degree at the University of Waterloo. His research interests include software architecture, software testing, and vulnerability prediction. Devin Chollak received the BS degree in software engineering from the University of Calgary and the MMath degree in computer science from the University of Waterloo. He is currently a software developer with Curve Dental, Inc., Calgary. He also does IT support for a number of clients and works as an independent game developer on the side. Joshua Garcia received the BS degree in computer engineering and computer science, the MS in computer science, and the PhD in computer science from the University of Southern California. He is an associate project scientist in the Institute for Software Research, which is located with the University of California, Irvine. He conducts research in software engineering with a focus on software security, software testing, and software architecture. Lin Tan received the PhD degree from the University of Illinois, Urbana-Champaign. She is an associate professor in the Department of Electrical and Computer Engineering, University of Waterloo. She is on the editorial board of the Springer Empirical Software Engineering Journal (2015-present). She was the program co-chairs of MSR 2017, ICSE-NIER 2017, and ICSME-ERA 2015. She received the an NSERC Discovery Accelerator Supplements Award, an Ontario Early Researcher Award, an Ontario Professional Engineers Awards—Engineering Medal for Young Engineer, the University of Waterloo Outstanding Performance Award, two Google Faculty Research Awards, and an IBM CAS Research Project of the Year Award. Her co-authored papers have received an ACM SIGSOFT Distinguished Paper Award at FSE in 2016 and IEEE Micro’s Top Picks in 2006. Derek Rayside received the BASc degree in systems design engineering, in 1999, the MASc degree in computer engineering, in 2001, both from the University of Waterloo, and the PhD degree in computer science from the Massachusetts Institute of Technology (MIT), in 2010. He is an associate professor in the Electrical & Computer Engineering Department, University of Waterloo. When a student, he had several internships at IBM Toronto in compilers. He is a member of the Association of Computing Machinery (ACM) SIGSOFT and SIGPLAN. He is licensed with Professional Engineers Ontario (PEO). He is a member of the IEEE Computer Society Technical Council on Software Engineering since 2013. Nenad Medvidović is a professor in the Computer Science Department and in the informatics program with the University of Southern California. He is the founding director of the SoftArch Laboratory, USC. He has previously served as director of the USC Center for Systems and Software Engineering (2009-2013), associate chair for PhD Affairs in USC’s CS Department (2011-2015), and chair of the Steering Committe for the International Conference on Software Engineering (2013-2015). He was the program co-chair of ICSE 2011. He is currently serving as chair of ACM SIGSOFT. He has served or is currently serving as an associate editor of the two flagship software engineering journals: the IEEE Transactions on Software Engineering (2010-2014) and the ACM Transactions on Software Engineering and Methodology (2014-present). He received the National Science Foundation CAREER award, the Okawa Foundation Research Grant, the IBM Real-Time Innovation Award, and the USC Mellon Mentoring Award. He is a co-author of the ICSE 1998 paper titled Architecture-Based Runtime Software Evolution, which was recognized as that conference's Most Influential Paper. He is an ACM distinguished scientist and a fellow of the IEEE. Robert Kroeger received the BSc degree in computer science from the University of Ottawa and the MMath and PhD degrees in computer science from the University of Waterloo. He is currently works with Google, where he helps lead converting the Chrome compositing stack to a service-based architecture. Previously, he’s worked on the ChromeOS display code, Chrome touch support, v.io, GMail, hardware performance modelling and cache-coherent interconnects.Keywords Software Architecture, Software Quality, System Recovery, Symbol Dependencies, Accurate Recovery Techniques, Recovery Quality, Submodule Based Technique, Ground Truth Architecture, Input Dependencies, Software Implementations, Software Architectures, Software Architecture Recovery Techniques, Code Dependencies, Computer Architecture, Software Architecture, Software, Heuristic Algorithms, Chromium, Software Algorithms, Manuals, Software Architecture, Empirical Software Engineering, Maintenance And Evolution, Program Comprehension"
