title,abstract
Automatic Loop Summarization via Path Dependency Analysis,"Abstract Analyzing loops is very important for various software engineering tasks such as bug detection, test case generation and program optimization. However, loops are very challenging structures for program analysis, especially when (nested) loops contain multiple paths that have complex interleaving relationships. In this paper, we propose the path dependency automaton (PDA) to capture the dependencies among the multiple paths in a loop. Based on the PDA, we first propose a loop classification to understand the complexity of loop summarization. Then, we propose a loop analysis framework, named Proteus, which takes a loop program and a set of variables of interest as inputs and summarizes path-sensitive loop effects (i.e., disjunctive loop summary) on the variables of interest. An algorithm is proposed to traverse the PDA to summarize the effect for all possible executions in the loop. We have evaluated Proteus using loops from five open-source projects and two well-known benchmarks and applying the disjunctive loop summary to three applications: loop bound analysis, program verification and test case generation. The evaluation results have demonstrated that Proteus can compute a more precise bound than the existing loop bound analysis techniques; Proteus can significantly outperform the state-of-the-art tools for loop program verification; and Proteus can help generate test cases for deep loops within one second, while symbolic execution tools KLEE and Pex either need much more time or fail.Keywords Program Debugging, Program Diagnostics, Program Testing, Program Verification, Public Domain Software, Automatic Loop Summarization, Test Case Generation, Program Optimization, Program Analysis, Path Dependency Automaton, PDA, Loop Classification, Loop Analysis Framework, Disjunctive Loop Summary, Proteus, Loop Program Verification, Loop Bound Analysis, Path Sensitive Loop Effects, Debugging, Automata, Benchmark Testing, Public Domain Software, Disjunctive Loop Summary, Path Dependency Automaton, Path Interleaving"
ConPredictor: Concurrency Defect Prediction in Real-World Applications,"Keywords Concurrency Control, Program Diagnostics, Program Testing, Public Domain Software, Software Fault Tolerance, Software Metrics, Software Quality, Real World Programs, Software Defect Prediction, Predicting Models, Sequential Programs, Concurrent Programs, Program Characteristics, Con Predictor, Static Program Metrics, Dynamic Program Metrics, Within Project Defect Prediction, Cross Project Defect Prediction, Concurrency Defect Prediction, Static Code Metrics, Mutation Analysis, Open Source Projects, Concurrent Computing, Predictive Models, Software, Programming, Testing, Synchronization, Concurrency, Defect Prediction, Software Quality, Software Metrics"
makeSense: Simplifying the Integration of Wireless Sensor Networks into Business Processes,"Abstract A wide gap exists between the state of the art in developing Wireless Sensor Network (WSN) software and current practices concerning the design, execution, and maintenance of business processes. WSN software is most often developed based on low-level OS abstractions, whereas business process development leverages high-level languages and tools. This state of affairs places WSNs at the fringe of industry. The makeSense system addresses this problem by simplifying the integration of WSNs into business processes. Developers use BPMN models extended with WSN-specific constructs to specify the application behavior across both traditional business process execution environments and the WSN itself, which is to be equipped with application-specific software. We compile these models into a high-level intermediate language-also directly usable by WSN developers-and then into OS-specific deployment-ready binaries. Key to this process is the notion of meta-abstraction, which we define to capture fundamental patterns of interaction with and within the WSN. The concrete realization of meta-abstractions is application-specific; developers tailor the system configuration by selecting concrete abstractions out of the existing codebase or by providing their own. Our evaluation of makeSense shows that i) users perceive our approach as a significant advance over the state of the art, providing evidence of the increased developer productivity when using makeSense; ii) in large-scale simulations, our prototype exhibits an acceptable system overhead and good scaling properties, demonstrating the general applicability of makeSense; and, iii) our prototype-including the complete tool-chain and underlying system support-sustains a real-world deployment where estimates by domain specialists indicate the potential for drastic reductions in the total cost of ownership compared to wired and conventional WSN-based solutions.Keywords Business Data Processing, Operating Systems Computers, Wireless Sensor Networks, Business Processes, WSN Software, Low Level OS Abstractions, Make Sense System, WSN Specific Constructs, Application Specific Software, OS Specific Deployment Ready Binaries, Meta Abstraction, Concrete Abstractions, Complete Tool Chain, Wired WSN Based Solutions, High Level Intermediate Language, Developer Productivity, WSN Developers, Business Process Execution Environments, Business Process Development, High Level Languages, Wireless Sensor Networks, Business, Programming, Ventilation, Software, Concrete, Business Processes, Wireless Sensor Networks, Embedded Software, Internet Of Things"
“Sampling” as a Baseline Optimizer for Search-Based Software Engineering,"Abstract Increasingly, Software Engineering (SE) researchers use search-based optimization techniques to solve SE problems with multiple conflicting objectives. These techniques often apply CPU-intensive evolutionary algorithms to explore generations of mutations to a population of candidate solutions. An alternative approach, proposed in this paper, is to start with a very large population and sample down to just the better solutions. We call this method “Sway”, short for “the sampling way”. This paper compares Sway versus state-of-the-art search-based SE tools using seven models: five software product line models; and two other software process control models (concerned with project management, effort estimation, and selection of requirements) during incremental agile development. For these models, the experiments of this paper show that Sway is competitive with corresponding state-of-the-art evolutionary algorithms while requiring orders of magnitude fewer evaluations. Considering the simplicity and effectiveness of Sway, we, therefore, propose this approach as a baseline method for search-based software engineering models, especially for models that are very slow to execute.Keywords Evolutionary Computation, Process Control, Search Problems, Software Prototyping, Baseline Optimizer, CPU Intensive Evolutionary Algorithms, Software Product Line Models, Software Process Control Models, Baseline Method, Search Based Software Engineering Models, Search Based Optimization, Evolutionary Algorithms, SWAY, The Sampling Way, Agile Development, Software Engineering, Optimization, Software, Numerical Models, Evolutionary Computation, Estimation, Sociology, Search Based SE, Sampling, Evolutionary Algorithms"
Text Filtering and Ranking for Security Bug Report Prediction,"Abstract Security bug reports can describe security critical vulnerabilities in software products. Bug tracking systems may contain thousands of bug reports, where relatively few of them are security related. Therefore finding unlabelled security bugs among them can be challenging. To help security engineers identify these reports quickly and accurately, text-based prediction models have been proposed. These can often mislabel security bug reports due to a number of reasons such as class imbalance, where the ratio of non-security to security bug reports is very high. More critically, we have observed that the presence of security related keywords in both security and non-security bug reports can lead to the mislabelling of security bug reports. This paper proposes FARSEC, a framework for filtering and ranking bug reports for reducing the presence of security related keywords. Before building prediction models, our framework identifies and removes non-security bug reports with security related keywords. We demonstrate that FARSEC improves the performance of text-based prediction models for security bug reports in 90 percent of cases. Specifically, we evaluate it with 45,940 bug reports from Chromium and four Apache projects. With our framework, we mitigate the class imbalance issue and reduce the number of mislabelled security bug reports by 38 percent.Keywords Information Filtering, Program Debugging, Public Domain Software, Security Of Data, Text Analysis, Security Bug Report Prediction, Security Critical Vulnerabilities, Text Based Prediction Models, Text Filtering, Software Products, FARSEC, Security, Computer Bugs, Predictive Models, Software, Data Models, Measurement, Buildings, Security Cross Words, Security Related Keywords, Security Bug Reports, Text Filtering, Ranking, Prediction Models, Transfer Learning"
Correction of “A Comparative Study to Benchmark Cross-Project Defect Prediction Approaches”,"Abstract Unfortunately, the article “A Comparative Study to Benchmark Cross-project Defect Prediction Approaches” has a problem in the statistical analysis which was pointed out almost immediately after the pre-print of the article appeared online. While the problem does not negate the contribution of the the article and all key findings remain the same, it does alter some rankings of approaches used in the study. Within this correction, we will explain the problem, how we resolved it, and present the updated results. 1   Introduction Unfortunately, the article “A Comparative Study to Benchmark Cross-project Defect Prediction Approaches” [1] has a problem in the statistical analysis performed to rank Cross-Project Defect Prediction (CPDP) approaches. Prof. Yuming Zhou from Nanjing University pointed out an inconsistency in Table 8 of the the article. He noted that in some cases the r a n k s c o r e s r are worse even if the mean values for performance metrics are better. While this is possible in theory, with the Friedman test [2] with post-hoc Nemenyi test [3], such inconsistencies are unlikely. Therefore, we immediately proceeded to check our results. These checks revealed that the inconsistencies are due to a problem with our statistical analysis for the Research Question 1 (RQ1) “Which CPDP approaches perform best in terms of F-measure, G-measure, AUC, and MCC?”. None of the raw results of the benchmark, nor any of the other research questions are affected by the problem. We will describe the problem and how we solved in Section 2. Then, we will show the updated results regarding RQ1 and discuss the changes in Section 3. Afterwards, we analyze the reasons for the changes in Section 4 to determine if all changes due to the correction are plausible and the correction resolves the inconsistencies reported by Y. Zhou. We performed an additional sanity check using a different statistical test, which we describe in Section 5. In Section 6, we describe how we updated our replication kit as part of this correction. Finally, we will conclude in Section 7. Please note, that we assume that readers have read to the original article and are familiar with the terminology used. We do not re-introduce any of the terminology in this correction. 2   Problem with the Nemenyi Test Implementation On July 15th 2017, Y. Zhou imformed us that he found an inconsistency between the results of CV-NET and CamargoCruz09-DT for the RELINK data for the performance metric AUC. He noted that the mean value for CV-NET was higher than for CamargoCruz09-DT, but the r a n k s c o r e r was lower. He went to the raw data provided as part of the replication kit [4] and confirmed that the mean values were correct, and the AUC for CV-NET was higher for all three products of the RELINK data. Based on this observervation, we re-checked our statistical analysis of the results. We found the problem in our implementation of the Nemenyi post-hoc test. 2.1 Summary of the Friedman and Nemenyi Tests To understand the problem, we briefly recap how the Friedman test with post-hoc Nemenyi test works. The Friedman test determines if there are stastical significant differences between populations. This is done using pair-wise comparisons between the rankings of populations. If the Friedman test determines significant differences, the Nemenyi post-hoc test compares the populations to each other to determine the statistically significantly different ones. The analysis with the Nemenyi test is based on two parameters: the Critical Distance (CD) and the average ranks of the populations in the pair-wise comparisons between all populations on each data set. Following the description by Demšar [5], CD is defined as C D = q α √ k ( k + 1 ) 6 N , (1) (1)CD=qαk(k+1)6N, where q α = q t u k e y ( α , N , inf ) √ 2 q is the studentized range distribution with infinite degrees of freedom divided1 by √ 2 2 , α α the significance level, k k the number of populations compared and N N the number of data sets. We can thus rewrite CD as C D = q t u k e y ( α , N , inf ) √ 2 √ k ( k + 1 ) 6 N = q t u k e y ( α , N , inf ) √ k ( k + 1 ) 12 N . (2) (2)CD=qtukey(α,N,inf)2k(k+1)6N=qtukey(α,N,inf)k(k+1)12N. If we now assume that R i , R j R are the average ranks of population i , j ∈ { 1 , … , N } i , the two populations are stastically significantly different if | R i − R j | > C D . (3) (3)|Ri−Rj|>CD. In case a control population is available, it is possible to use a procedure like Bonferroni correction [6]. In this case, all populations are compared to the control classifier instead of to each other. This greatly reduces the number of pair-wise comparisons and can make the test more powerful. In this case, for each pair a z z -value is computed as z = ( R i − R j ) / √ k ( k + 1 ) 6 N . (4) (4)z=(Ri−Rj)/k(k+1)6N. The z z -values are then used to rank classifiers. Since we do not have a control classifier and have to do pair-wise comparisons with the CD and cannot make use of the z z -values. However, the z z -values play an important role when it comes to the problem with our analysis. 2.2 z-Values Instead of Ranks Now that the concepts of the statistical tests are introduced, we can discuss the actual problem in our implementation. We used the posthoc.friedman.nemenyi.test function of the PMCMR package [7] to implement the test. As part of the return values, the function returns a matrix called PSTAT. Without checking directly in the source code, we assumed these were the average ranks for each population, based on the documention of the package. However, these are actually the absolute z z -values multiplied with √ 2 2 , i.e., z ′ = | R i − R j | / √ k ( k + 1 ) 6 N ⋅ √ 2 . (5) (5)z′=|Ri−Rj|/k(k+1)6N⋅2. Thus, when we compared ranks, we did not actually compare the average ranks, but the mean z ′ z -values. This led to a wrong determination of ranks, which explain the inconsistencies found by Y. Zhou. 2.3 The Solution To resolve the problem, we adopted the code from the PMCMR package that determines the average ranks. We cross-checked our code with another implementation of the Nemenyi test [8], to ensure that the new code solves the problem.2 We then used the mean averages from that code, instead of the z z -values that were returned from the PMCMR package. As a result, the Nemenyi-test became much more sensitive, because the scale of the average ranks is different than the scale of the z z -values. Let us consider the scales for our experiments with the JURECZKO data. Here, we have N = 62 N data sets and k = 135 k populations, i.e., CPDP approach and classifier combinations. The best possible average rank is 135 (always wins), the worst possible is 1 (always loses). Thus, the average ranks are on a scale from 1 to 135. In comparison, the highest possible z ′ z -value is z = ( 135 − 1 ) / √ 135 ⋅ 136 6 ⋅ 62 ⋅ √ 2 ≈ 26.97 , (6) (6)z=(135−1)/135⋅1366⋅62⋅2≈26.97, i.e., the scale is just from 0 (no difference in average ranks) and 26.97. Thus, the scale of the z ′ z -values has only about a fifth of the range the scale of the average ranks has. Basically, with z ′ z -values, 135 populations are fit into the scale 0 to 26.97, with rankings in the scale 1 to 135. This means that the average distance between approaches is 0.2 with z ′ z -values and 1 in case of average ranks. Considering that we have a C D ≈ 1.26 C for α = 0.95 α in this example, this makes a huge difference. With z ′ z -values, it is unlikely that two subsequently ranked approaches to have a distance greater than CD, because C D C was more than 6.3 times higher than average distance expected on the scale. This changes if the real scale with ranks is used. If you have 135 cases with an average distance of 1, it is quite likely that a few of these distances will be greater than 1.26, i.e., the CD. We discuss this change in scales in this detail, because it requires a small change in the ranking of approaches based on the Nemenyi test. Before, we considered three distinct ranks for the calculation of the rankscore to achieve non-overlapping ranks: The populations that are within the CD of the best average ranking population (top rank 1). The populations that are within the CD of the worst average ranking population (bottom rank 3). The populations that are neither (middle rank 2). This was the only way to deal with the small differences that resulted from using the z ′ z -values. However, this approach breaks on the larger scale, because the distances now become larger, meaning fewer results are within the CD from the best/worst ranking. For example, for the JURECZKO data and the performance metric AUC, only two approaches would be on the first rank, i.e., only one approach is within the CD of the best approach. Similarly, only six approaches would be on the third rank, i.e., only five approaches are within the CD of the worst approach. This would leave us with 127 approaches on the middle rank. This ranking would be to coarse, and not show actual differences between approaches anymore. To deal with this larger scale of ranks, we use a simple and more fine-grained grouping strategy to create the ranks. We sort all approaches by their average ranking. Each time that the difference between two subsequently ranked approaches is larger than the CD, we increase the rank. Because the rank is only increased if the difference is larger than the CD, we ensure that each group only contains approaches that are statistically significantly different from the other groups. Afterwards, we calculate the normalized rankscore as before. Algorithm 1 formalizes this strategy. This change in ranking increases the sensitivity of the test and makes the results more fine-grained in comparison to our original ranking procedure. Algorithm 1.   Ranking Algorithm Input: Sorted mean ranks R i R such that ∀  i , j ∈ { 1 , … , N } : i < j , R i ≥ R j ∀  Output: r a n k s c o r e i r for all ranks. r a n k _ t m p 1 = 1 r  c u r r e n t _ r a n k ← 0 c  for i = 2 , … , N i do  ▹ ▹ If difference is larger than CD increase rank  if R i − 1 − R i > C D R then  c u r r e n t _ r a n k ← c u r r e n t _ r a n k + 1 c   end if  r a n k _ t m p i ← c u r r e n t _ r a n k r  end for r a n k _ m a x ← c u r r e n t _ r a n k r   ▹ ▹ Determine rankscores for i = 1 , … , N i do  r a n k s c o r e i ← 1 − r a n k _ t m p i r a n k _ m a x r  end for 3   Results We now show the corrected results for RQ1. We will directly compare the changes in the results with the originally published results. Fig. 1 shows the mean rankscore averaged over the four performance metrics F-Measure, G-Measure, AUC, and MCC and the five data sets JURECZKO, MDP, AEEEM, NETGENE, and RELINK. Table 1 shows detailed results including the mean values and rankscores for each performance metrics and each data set. Fig. 1 is the correction of Fig. 3 and Table 1 the correction of Table 8 from the original publication. Table 1 and Fig. 1 only report the results for the best classifier for each approach. In case these changed between the original results and our correction, you will not find the exact same rows. For example, for CamargoCruz09, we reported DT as best classifier in the original analysis, and now NB. This is because with the problem in the statistical analysis DT was ranked best for CamargoCruz09, but in the corrected version NB performs better. The reasons for these, and other changes are explained in Section 4 Fig. 1. Mean rank score over all data sets for the metrics AUC , F-measure , G-measure , and MCC . In case multiple classifiers were used, we list only the result achieved with the best classifier. TABLE 1 Mean Results Over All Products with Rankscores in Brackets  The most important finding remains the same: the approach CamargoCruz09 still provides the best-ranking classification model with a mean rankscore of 0.917 for CamargoCruz09-NB. However, the rankscore is not a perfect 1.0 anymore. We attribute this to the more sensitive ranking due to the correction of the Nemenyi test. The differences to the next-ranking approaches are still rather small, though the group of approaches that is within 10 percent of the best ranking approach now only consists of CV-RF, Amasaki15, Peters15, and Ma12. The bottom of the ranking is nearly unaffected by the changes as well. The last seven ranked approaches are still the same. Additionally, our findings regarding the comparison of using ALL data, versus transfer learning approaches has not changed: ALL is still in the upper mid-field of approaches. With the corrected and more finegrained ranking, only six of the cross-project approaches actually outperform this baseline, whereas seventeen are actually worse. With respect to CV versus CPDP, we still observe that CPDP can outperform CV in case multiple performance criteria are considered because CV-RF is outperformed by CamargoCruz09-NB. Thus, we still note that this is possible, but far less conclusively than before, where CV was actually only in the mid-field of the approaches and not a close second. Due to these small overall small differences, we change our answer to RQ1 slightly: Answer RQ1: CamargoCruz09-NB performs best among the compared CPDP approaches and even outperforms cross-validation. However, the differences to other approaches are small. The baseline ALL-NB is ranked higher than seventeen of the CPDP approaches. 4   Reasons for Changes We checked our raw results for the reasons for all changes in rankings. The problem with the statistical analysis actually led to two reasons for ranking changes: first, the z z -values already consider differences in ranks. Thus, if the rank was very high, this could would lead to larger z z -values, which would negatively impact the ranking. Second, because differences were downscaled with the z z -values in comparison to differences in mean ranks, too many approaches were grouped together as not statistically significantly different. For approaches that are now better ranked than before, this means that they were often among the best performing approaches within a group. For those that are now ranking worse, they were at often near the bottom of their groups. For example, CV was often among the best approaches on the middle rank. Now, it is clearly distinguished from the others there, leading to the strong rise in the ranking. Others that were affected the same way, though to a lesser extend are Amasaki15, Peters15, YZhang15, and Herbold13. On the other hand Menzies11 and Watanabe08 were often at the bottom of their groups, leading to the big losses in rankings for both approaches. Another change in our results is that NB is often the best performing classifier, whereas before DT and LR were most often the best performing classifiers. We previously already noted in our discussion that “for many approaches the differences between the classifiers were rather small” [1]. Together with the reasons for ranking changes explained above, theses changes are not unexpected. Overall, all changes in the result are plausible. Moreover, our comparison of the results of the statistical analysis with both mean values, as well as the raw results of the benchmark did not reveal any inconsistencies of the type that Y. Zhou reported to us. Therefore, we believe that the problem was correctly resolved. 5   Sanity Check Using the Scott-Knott Test In addition to fixing the problem with our statistical evaluation procedure based on the Friedman test and the post-hoc Nemenyi test, we performed another sanity check of our results. The Scott-Knott test [9] is popolar in the recent defect prediction literature (e.g., [10],[11]) because it provides a clear ranking of results and does not have overlapping rankings as is possible with the Nemenyi test. The Scott-Knott test is based on ANOVA [12]. Since our data does not fulfill the assumptions of ANOVA, i.e., the data is not normally distributed and not homoscedastic, we ruled out the Scott-Knott test as evaluation procedure. However, we may still use it as sanity check here, because while we cannot guarantee that the results are statistically valid if we use Scott-Knott, due to the broken assumptions, they should still be similar to the results we achieve with the post-hoc Nemenyi test. Thus, in case the results with Scott-Knott are vastly different, this would be an indicator of another problem with our statistical procedure, while similar results would serve as validation of our correction. Fig. 2 shows the results of the ranking with the Scott-Knott test. We observe a high similarity between the results. At the top, this is only broken by Nam13-NB. However, this is because Scott-Knott ranks nearly all approaches perfectly on the RELINK data, i.e., the only data set on which we could evaluate Nam13. If we ignore this data point, the mean difference in ranks between using the Scott-Knott test and the Nemenyi test is 1.88 and the median is 1. Considering that we rank 27 different approaches (excluding Nam13), these differences are very small. Moreover, CamargoCruz09 is still the top-ranking approach. This indicates that our results are robust against a switch in statistical tests as well as that there are no further problems with the implementation of the Nemenyi test. Fig. 2. Ranking of approaches based on the Scott-Knott test. In case multiple classifiers were used, we list only the result achieved with the best classifier. We would like to stress that we only report on the results of the Scott-Knott test to provide a further sanity check. In comparison to the results with the Nemenyi test, we cannot guarantee that the outcome of the test is always valid due to broken assumptions of the tests. Therefore, we recommend to use the simpler tests only as sanity checks if assumptions are broken and still use the more complex non-parametric tests, to ensure statistically valid results. 6   Update of the Replication Kit We updated the replication kit archived at Zenodo [13]. The changes two the replication kit are two-fold. We corrected the problem with the statistical analysis in the generate_results.R script. We updated the provided CD diagrams due to the changes in the Nemenyi test. The changes can be reviewed in detail in the commit to the GitHub archive of the replication kit.3 7   CONCLUSION A problem with the implementation of the Nemenyi post-hoc test led to incorrect results being published in our benchmark paper on cross-project defect prediction. The mistake only affected research question RQ1, the other three research questions were not affected. Within this correction paper, we explained the problem in the statistical test, how this problem affected our results, presented the corrected, and explained the changes that occoured. The major findings regarding RQ1 are not changed, including the best performing approach, the result that the naïve baseline of using all data outperforms most proposed transfer learning approaches, as well as that cross-validation can be outperformed by CPDP. Thus, the contributions of the article are still valid. Still, the correction leads to differences in the rankings which are properly corrected and discussed here. We apologize for this mistake and hope that this timely correction mitigates the potential negative impact the wrong results may have. Footnotes 1. For simplicity, we refer to the studentized range distribution as q t u k e y ( α , N ) q , following the name of the related method in R. 2. Both implementations of the test do not return the raw pair-wise comparisons and can, therefore, not be used directly. 3. https://goo.gl/AbvSRj. Acknowledgments We want to thank Yuming Zhou from Nanjing University for pointing out the inconsistencies in the results to us so fast, as well as the editors of this journal who helped to determine how we should communicate this problem to the community within days. References  [1]S. Herbold, A. Trautsch, and J. Grabowski, “A comparative study to benchmark cross-project defect prediction approaches,” IEEE Trans. Softw. Eng., 2017, doi: 10.1109/TSE.2017.2724538. [2]M. Friedman, “A comparison of alternative tests of significance for the problem of m rankings,” Ann. Math. Statist., vol. 11, no. 1, pp. 86–92, 1940. [Online]. Available: http://www.jstor.org/stable/2235971 [3]P. Nemenyi, “Distribution-free multiple comparison,” Ph.D. dissertation, Princeton University, Princeton, NJ, USA, 1963. [4]S. Herbold, “sherbold/replication-kit-tse-2017-benchmark: Release of the replication kit,” May2017. [Online]. Available: https://doi.org/10.5281/zenodo.581178 [5]J. Demšar, “Statistical comparisons of classifiers over multiple data sets,” J. Mach. Learn. Res., vol. 7, pp. 1–30, Dec.2006. [Online]. Available: http://dl.acm.org/citation.cfm?id=1248547.1248548 [6]O. J. Dunn, “Multiple comparisons among means,” J. Amer. Statist. Assoc., vol. 56, no. 293, pp. 52–64, 1961. [Online]. Available: http://www.tandfonline.com/doi/abs/10.1080/01621459.1961.10482090 [7]T. Pohlert, The Pairwise Multiple Comparison Mean Ranks Package, 2014. [Online]. Available: http://CRAN.R-project.org/package=PMCMR [8]I. Svetunkov and N. Kourentzes, “Tstools,” 2017. [Online]. Available: https://github.com/trnnick/TStools [9]A. J. Scott and M. Knott, “A cluster analysis method for grouping means in the analysis of variance,” Biometrics, vol. 30, no. 3, pp. 507–512, 1974. [10]B. Ghotra, S. McIntosh, and A. E. Hassan, “Revisiting the impact of classification techniques on the performance of defect prediction models,” in Proc. 37th IEEE Int. Conf. Softw. Eng., May 2015, vol. 1, pp. 789–800. [11]S. Herbold, A. Trautsch, and J. Grabowski, “Global versus local models for cross-project defect prediction,” Empirical Softw. Eng., pp. 1–37, 2016. [Online]. Available: http://dx.doi.org/10.1007/s10664-016-9468-y [12]R. A. Fisher, “The correlation between relatives on the supposition of mendelian inheritance,” Philosoph. Trans. Roy. Soc. Edinburgh, vol. 52, pp. 399–433, 1918. [13]Steffen Herbold, “sherbold/replication-kit-tse-2017-benchmark: Correction of the replication kit,” Zenodo, Jul.2017, https://doi.org/10.5281/zenodo.834847Keywords Sociology, Measurement, Benchmark Testing, Statistical Analysis, Ranking Statistics, Terminology, Cross Project Defect Prediction, Benchmark, Comparison, Replication, Correction"
