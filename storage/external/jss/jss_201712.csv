title,abstract
Research article - Stochastic modeling for performance and availability evaluation of hybrid storage systems,"AbstractImprovements in computational systems may be constrained by the efficiency of storage drives. Therefore, replacing hard disk drives (HDD) with solid-state drives (SSD) may also be an effective way to improve system performance, but the higher cost per gigabyte and reduced lifetime of SSDs hinder a thorough replacement. To mitigate these issues, several architectures have been conceived based on hybrid storage systems, but performance and availability models have not been proposed to better assess such different architectures. This paper presents an approach based on stochastic models (i.e., stochastic Petri nets and reliability block diagrams) for performance and availability evaluation of hybrid storage systems. The proposed models can represent write and read operations, and they may estimate response time, throughput and availability. A case study based on OpenStack Swift platform is adopted to demonstrate the feasibility of the proposed approach."
Research article - Self-organizing multi-agent systems for the control of complex systems,"AbstractBecause of the law of requisite variety, designing a controller for complex systems implies designing a complex system. In software engineering, usual top-down approaches become inadequate to design such systems. The Adaptive Multi-Agent Systems (AMAS) approach relies on the cooperative self-organization of autonomous micro-level agents to tackle macro-level complexity. This bottom-up approach provides adaptive, scalable, and robust systems. This paper presents a complex system controller that has been designed following this approach, and shows results obtained with the automatic tuning of a real internal combustion engine."
Research article - Towards collaborative storage scheduling using alternating direction method of multipliers for mobile edge cloud,"AbstractPerformance of cloud computing would be much improved by extending storage capabilities to devices at the edge of network. Unfortunately, the commonly employed algorithms fail to be adaptive to the new storage pattern on mobile edge cloud. To address this issue, we propose a collaborative storage architecture model and an alternating-direction-method-of-multipliers-based collaborative storage scheduling algorithm called ACMES (Algorithm of Collaborative Mobile Edge Storage), in which heterogeneous information of nodes in mobile edge cloud is considered and integrated to make decisions. Besides, feasible solutions for storage will be acquired after iterations of computing. By formulating the collaborative storage scheduling problem in the mobile edge cloud and designing the collaborative decision-making process with the theory of Alternating Direction Method of Multipliers (ADMM), the proposed ACMES is able to minimize power usage and the risk of node withdrawal without reducing the reliability of node storage, and meanwhile make storage scheduling decisions at the edge environment directly and work in a distributed and parallel way. The convergence analysis shows that ACMES has the ability to solve complicated mobile edge cloud storage problems in reality. Extensive experiments validate its effectiveness as well as its superiority to three existing strategies (ADM, RDM and ERASURE) in total cost, reliability, power usage and withdrawal risks."
Research article - Striving for balance: A look at gameplay requirements of massively multiplayer online role-playing games,"AbstractEngineering gameplay requirements is the most important task for game development organizations. Game industry discourse is concerned with continuous redesign of gameplay to enhance players' experience and boost game's appeal. However, accounts of gameplay requirements practices are rare. In responding to calls for more research into gameplay requirements engineering, we performed an exploratory study in the context of massively multiplayer online role-playing games (MMORPGs), from the perspective of practitioners involved in the field. Sixteen practitioners from three leading MMORPG-producing companies were interviewed and their gameplay requirements documents were reviewed. Interviewing and qualitative data analysis occurred in a cyclical process with results at each stage of the study informing decisions about data collection and analysis in the next.The analysis revealed a process of striving to reach a balance among three perspectives of gameplay requirements: a process perspective, an artifact perspective and a player-designer relationship perspective. This balance-driven process is co-created by game developers and players, is endless within the MMORPG, and is happening both in-game and off-game. It heavily relies on 'paper-prototyping' and play-testing for the purpose of gameplay requirements validation. The study concludes with discussion on validity threats and on implications for requirements engineering research, practice and education."
Research article - Understanding the interplay between the logical and structural coupling of software classes,"AbstractDuring the lifetime of object-Oriented (OO) software systems, new classes are added to increase functionality, also increasing the inter-dependencies between classes. Logical coupling depicts the change dependencies between classes, while structural coupling measures source code dependencies induced via the system architecture. The relationship or dependency between logical and structural coupling have been debated in the past, but no large study has confirmed yet their interplay.In this study, we have analysed 79 open-source software projects of different sizes to investigate the interplay between the two types of coupling. First, we quantified the overlapping or intersection of structural and logical class dependencies. Second, we statistically computed the correlation between the strengths of logical and structural dependencies. Third, we propose a simple technique to determine the stability of OO software systems, by clustering the pairs of classes as “stable” or “unstable”, based on their co-change pattern.The results from our statistical analysis show that although there is no strong evidence of a linear correlation between the strengths of the coupling types, there is substantial evidence to conclude that structurally coupled class pairs usually include logical dependencies. However, not all co-changed class pairs are also linked by structural dependencies. Finally, we identified that only a low proportion of structural coupling shows excessive instability in the studied OSS projects."
Research article - Multi-cloud service composition using Formal Concept Analysis,"AbstractRecent years have witnessed a rapid growth in exploiting Cloud environments to deliver various types of resources as services. To improve the efficiency of software development, service reuse and composition is viewed as a powerful means. However, effectively composing services from multiple clouds has not been solved yet. Indeed, existing solutions assume that the services participating to a composition come from a single cloud. This approach is unrealistic since the other existing clouds may host more suitable services. In order to deliver high quality service compositions, the user request must be checked against the services in the multi-cloud environment (MCE) or at least clouds in the availability zone of the user. In this paper, we propose a multi-cloud service composition (MCSC) approach based on Formal Concept Analysis (FCA). We use FCA to represent and combine information of multiple clouds. FCA is based on the concept lattice which is a powerful mean to classify clouds and services information. We first model the multi-cloud environment as a set of formal contexts. Then, we extract and combine candidate clouds from formal concepts. Finally, the optimal cloud combination is selected and the MCSC is transformed into a classical service composition problem. Conducted experiments proved the effectiveness and the ability of FCA based method to regroup and find cloud combinations with a minimal number of clouds and a low communication cost. Also, the comparison with two well-known combinatorial optimization approaches showed that the adopted top-down strategy allowed to rapidly select services hosted on the same and closest clouds, which directly reduced the inter-cloud communication cost, compared to existing approaches."
Research article - Efficient exact Boolean schedulability tests for fixed priority preemption threshold scheduling,"AbstractFixed priority preemption threshold scheduling (PTS) is effective in scalable real-time system design, which requires system tuning processes, where the performance of schedulability tests for PTS matters much. This paper proposes five methods for efficient exact Boolean schedulability tests for PTS, which returns the Boolean result that tells whether the given task is schedulable or not. We regard the sufficient test for fully preemptive fixed priority scheduling (FPS) and the early exit in the response time calculations as the conventional approach.We propose (1) a new sufficient test, (2) new initial values for the start/finish times, (3) pre-calculation of the interference time within the start time, (4) incremental start/finish time calculation, and (5) early exits in start/finish time calculations. These are based on some previous work for FPS. The new initial start time, pre-calculation, and the incremental calculations also can be used for the exact response time analysis for PTS. Our empirical results show that the overall proposed methods reduce the iteration count/run time of the conventional test by about 60%/40%, regardless of the number of tasks and the total utilization."
Research article - Task Scheduling in Big Data Platforms: A Systematic Literature Review,"AbstractContext: Hadoop, Spark, Storm, and Mesos are very well known frameworks in both research and industrial communities that allow expressing and processing distributed computations on massive amounts of data. Multiple scheduling algorithms have been proposed to ensure that short interactive jobs, large batch jobs, and guaranteed-capacity production jobs running on these frameworks can deliver results quickly while maintaining a high throughput. However, only a few works have examined the effectiveness of these algorithms.Objective: The Evidence-based Software Engineering (EBSE) paradigm and its core tool, i.e., the Systematic Literature Review (SLR), have been introduced to the Software Engineering community in 2004 to help researchers systematically and objectively gather and aggregate research evidences about different topics. In this paper, we conduct a SLR of task scheduling algorithms that have been proposed for big data platforms.Method: We analyse the design decisions of different scheduling models proposed in the literature for Hadoop, Spark, Storm, and Mesos over the period between 2005 and 2016. We provide a research taxonomy for succinct classification of these scheduling models. We also compare the algorithms in terms of performance, resources utilization, and failure recovery mechanisms.Results: Our searches identifies 586 studies from journals, conferences and workshops having the highest quality in this field. This SLR reports about different types of scheduling models (dynamic, constrained, and adaptive) and the main motivations behind them (including data locality, workload balancing, resources utilization, and energy efficiency). A discussion of some open issues and future challenges pertaining to improving the current studies is provided."
Research article - An industrial case study on an architectural assumption documentation framework,"AbstractAs an important type of architectural knowledge, documenting architectural assumptions (AAs) is critical to the success of projects. In this work, we proposed and validated an Architectural Assumption Documentation Framework (AADF), which is composed of four viewpoints (i.e., the Detail, Relationship, Tracing, and Evolution viewpoint), to document AAs in projects. One case study with two cases was conducted at two companies from different domains and countries. The main findings are: (1) AADF can be understood by architects in a short time (i.e., a half day workshop); (2) the AA Evolution view requires the least time to create, followed by the AA Detail view and the AA Relationship view; (3) AADF can help stakeholders to identify risks and understand AAs documented by other stakeholders; and (4) understanding and applying AADF is related to various factors, including factors regarding the framework per se (e.g., tutorial, examples, concepts, and terms), personal experience, resources (e.g., time), tool support, and project context (e.g., project size and number of AAs). Adjusting these factors in an appropriate way can facilitate the usage of AADF and further benefit the projects."
Research article - Minimum-cost deployment of adjustable readers to provide complete coverage of tags in RFID systems,"AbstractIn Internet of things (IoT), radio frequency identification (RFID) plays an important role to help people rapidly obtain information of objects associated with tags. Passive tags are cheap and require no batteries to operate, so they are widely used in RFID applications. Readers, on the other hand, have to provide power to activate passive tags to get their data. However, collision occurs when two readers send signals to a tag at the same time. Therefore, it is critical to decide the locations of readers, namely reader deployment, to avoid collision. This paper considers adjustable readers, whose transmitted power is configurable to provide different communication range, and proposes a minimum-cost RFID reader deployment (MR2D) problem. Given the positions of tags, it determines how to deploy readers and adjust their transmitted power to cover all tags, such that we can use the minimum number of readers and save their energy. To facilitate data transmission and reduce hardware cost, we restrict the number of tags that each reader can cover and allow readers to have few overlapped tags in their communication range. Then, we develop an efficient solution to the MR2D problem by clustering tags into groups and placing a reader to cover each group to meet the above conditions. Simulation results show that our proposed solution not only saves the number of RFID readers but also reduces their energy consumption, as compared with existing methods."
Research article - Time series forecasting for dynamic quality of web services: An empirical study,"AbstractWeb Services (WSs) constitute a critical component of modern software development. Knowing the dynamic qualities of WSs is mandatory during use, and these qualities vary continuously over time. However, in most cases, the quality information furnished by service providers is static and does not consider dynamic variations in quality over time. Thus, it is necessary to determine a method for acquiring accurate quality values for WSs. The motivation for this research is that the most suitable time-series method for dynamic quality prediction of WSs remains unknown because no comprehensive empirical comparison of the representative time-series methods has been performed. Therefore, in this paper, we implement all the representative time-series methods and compare their dynamic quality predictions for WSs using a real-world quality dataset. For empirical comparison, we have ensured that our study is reproducible and referenceable by providing diverse specifics and evaluating their validity in detail. The experimental results and diverse discussions presented in this paper may act as a valuable reference to both academic researchers and WS consumers and providers in industry because they can depend on the results when selecting the most suitable time-series method for direct use or as a starting point for further modifications. Based on our experimental results, among the included time-series forecasting approaches, genetic programming (GP) can achieve the highest quality of service (QoS) forecasting accuracy (in our experiments, the average mean absolute error and mean absolute percentage error are 1258 and 20%, respectively); however, this approach also requires the longest time to produce a QoS predictor (67.7 s on average). Though auto-regressive integrated moving average (ARIMA, with average error measures of 1343 and 25.4%) and exponential smoothing (ES, with average error measures of 1354 and 25.7%) present slightly worse accuracy than GP, ARIMA and ES require much less time to generate a predictor than GP (on average, 0.1612 and 0.1519 s, respectively); thus, these approaches might represent a compromise between forecasting accuracy and cost."
Research article - Measuring social networks when forming information system project teams,"AbstractDespite the advances in the project management field, little is known about how creating performing software teams in a systematical and repeatable way. The technical dimension is not enough to achieve this. Software development is a complex collaborative process where people and interpersonal relationships – i.e., how people interact, behave and organize – significantly influence the project success. In this paper, we define a framework to assign people to projects from this socio-technical perspective. Social networks characterizing the interplay between teammates are built and analyzed to predict productive collaborations and identify adequate team-members depending on the organization needs and the kind of project. A noteworthy novelty of these social networks is that they estimate compatibility between coworkers according to previous collaborations, but also according to individuals’ social skills. This allows analyzing the compatibility among people who have not worked together before. We present results of using the proposed framework in a multinational corporation during a more-than-two-year period. Our in-company experiments emphasize that we can significantly improve the expected outcomes characterizing and measuring the social interaction among coworkers. Social aspects discussed may be highly relevant in the context of distributed software engineering, since it implies new challenges in the interplay among coworkers."
Research article - An empirical investigation of the influence of persona with personality traits on conceptual design,"AbstractPersona, an archetypical user, is increasingly becoming a popular tool for Software Engineers to design and communicate with stakeholders. A persona is a representative of a class of end users of a product or service. However, the majority of personas presented in the literature do not take into consideration that the personality of users affects the way they interact with a product or service. This study empirically explores variations in conceptual design based on the personality of a persona. We carried out two studies in Australia and one study in Denmark. We presented four personas with different personalities to 91 participants who collectively completed 218 design artifacts. The results from the studies indicate that the participants’ views and prioritization of the needs and system requirements were influenced by the personality traits of the provided personas. For an introverted and emotionally unstable personality, inclusion of confidence building and socializer design features had a higher priority compared with the identified requirements for an extravert and emotionally stable personality. The findings support the proposition that personas with personality traits can aid software engineers to produce conceptual designs tailored to the needs of specific personalities."
Research article - QuickFuzz testing for fun and profit,"AbstractFuzzing is a popular technique to find flaws in programs using invalid or erroneous inputs but not without its drawbacks. At one hand, mutational fuzzers require a set of valid inputs as a starting point, in which modifications are then introduced. On the other hand, generational fuzzing allows to synthesize somehow valid inputs according to a specification. Unfortunately, this requires to have a deep knowledge of the file formats under test to write specifications of them to guide the test case generation process.In this paper we introduce an extended and improved version of QuickFuzz, a tool written in Haskell designed for testing unexpected inputs of common file formats on third-party software, taking advantage of off-the-self well known fuzzers.Unlike other generational fuzzers, QuickFuzz does not require to write specifications for the file formats in question since it relies on existing file-format-handling libraries available on the Haskell code repository. It supports almost 40 different complex file-types including images, documents, source code and digital certificates.In particular, we found QuickFuzz useful enough to discover many previously unknown vulnerabilities on real-world implementations of web browsers and image processing libraries among others."
Research article - Enhancing developer recommendation with supplementary information via mining historical commits,"AbstractGiven a software issue request, one important activity is to recommend suitable developers to resolve it. A number of approaches have been proposed on developer recommendation. These developer recommendation techniques tend to recommend experienced developers, i.e., the more experienced a developer is, the more possible he/she is recommended. However, if the experienced developers are hectic, the junior developers may be employed to finish the incoming issue. But they may have difficulty in these tasks for lack of development experience. In this article, we propose an approach, EDR_SI, to enhance developer recommendation by considering their expertise and developing habits. Furthermore, EDR_SI also provides the personalized supplementary information for developers to use, such as personalized source code files, developer network and source-code change history. An empirical study on five open source subjects is conducted to evaluate the effectiveness of EDR_SI. In our study, EDR_SI is also compared with the state-of-art developer recommendation techniques, iMacPro, Location and ABA-Time-tf-idf, to evaluate the effectiveness of developer recommendation, and the results show that EDR_SI can not only improve the accuracy of developer recommendation, but also effectively provide useful supplementary information for them to use when they implement the incoming issue requests."
Research article - The Bayesian Network based program dependence graph and its application to fault localization,"AbstractFault localization is an important and expensive task in software debugging. Some probabilistic graphical models such as probabilistic program dependence graph (PPDG) have been used in fault localization. However, PPDG is insufficient to reason across nonadjacent nodes and only support making inference about local anomaly. In this paper, we propose a novel probabilistic graphical model called Bayesian Network based Program Dependence Graph (BNPDG) that has the excellent inference capability for reasoning across nonadjacent nodes. We focus on applying the BNPDG to fault localization. Compared with the PPDG, our BNPDG-based fault localization approach overcomes the reasoning limitation across nonadjacent nodes and provides more precise fault localization by taking its output nodes as the common conditions to calculate the conditional probability of each non-output node. The experimental results show that our BNPDG-based fault localization approach can significantly improve the effectiveness of fault localization."
Research article - Improving feature location in long-living model-based product families designed with sustainability goals,"AbstractThe benefits of Software Product Lines (SPL) are very appealing: software development becomes better, faster, and cheaper. Unfortunately, these benefits come at the expense of a migration from a family of products to a SPL. Feature Location could be useful in achieving the transition to SPLs. This work presents our FeLLaCaM approach for Feature Location. Our approach calculates similarity to a description of the feature to locate, occurrences where the candidate features remain unchanged, and changes performed to the candidate features throughout the retrospective of the product family. We evaluated our approach in two long-living industrial domains: a model-based family of firmwares for induction hobs that was developed over more than 15 years, and a model-based family of PLC software to control trains that was developed over more than 25 years. In our evaluation, we compare our FeLLaCaM approach with two other approaches for Feature Location: (1) FLL (Feature Location through Latent Semantic Analysis) and (2) FLC (Feature Location through Comparisons). We measure the performance of FeLLaCaM, FLL, and FLC in terms of recall, precision, Matthews Correlation Coefficient, and Area Under the Receiver Operating Characteristics curve. The results show that FeLLaCaM outperforms FLL and FLC."
Research article - Predicting change consistency in a clone group,"AbstractCode cloning has been accepted as one of the general code reuse methods in software development, thanks to the increasing demand in rapid software production. The introduction of clone groups and clone genealogies enable software developers to be aware of the presence of and changes to clones as a collective group; they also allow developers to understand how clone groups evolve throughout software life cycle. Due to similarity in codes within a clone group, a change in one piece of the code may require developers to make consistent change to other clones in the group. Failure in making such consistent change to a clone group when necessary is commonly known as “clone consistency-defect”, which can adversely impact software reusability.In this work, we propose an approach to predict the need for making consistent change in clones within a clone group at the time when changes have been made to one of its clones. We build a variant of clone genealogies to collect all consistent/inconsistent changes to clone groups, and extract three attribute sets from clone groups as input for predicting the need for consistent clone change. These three attribute sets are code attributes, context attributes and evolution attributes respectively. Together, they provide a holistic view about clone changes. We conduct experiments on four open source projects. Our experiments show that our approach has reasonable precision and recall in predicting whether a clone group requires (or is free of) consistent change. This holistic approach can aid developers in maintaining clone changes, and avoid potential consistency-defect, which can improve software quality and reusability."
Research article - Reusability of open source software across domains: A case study,"AbstractExploiting the enormous amount of open source software (OSS) as a vehicle for reuse is a promising opportunity for software engineers. However, this task is far from trivial, since such projects are sometimes not easy to understand and adapt to target systems, whereas at the same time the reusable assets are not obvious to identify. In this study, we assess open source software projects, with respect to their reusability, i.e., the easiness to adapt them in a new system. By taking into account that domain-specific reuse is more beneficial than domain-agnostic; we focus this study on identifying the application domains that contain the most reusable software projects. To achieve this goal, we compared the reusability of approximately 600 OSS projects from ten application domains through a case study. The results of the study suggested that in every aspect of reusability, there are different dominant application domains. However, Science and Engineering Applications and Software Development Tools, have proven to be the ones that are the most reuse-friendly. Based on this observation, we suggest software engineers, who are focusing on the specific application domains, to consider reusing assets from open source software projects."
Research article - A semi-automatic maintenance and co-evolution of OCL constraints with (meta)model evolution,"AbstractMetamodels are core components of modeling languages to define structural aspects of a business domain. As a complement, OCL constraints are used to specify detailed aspects of the business domain, e.g. more than 750 constraints come with the UML metamodel. As the metamodel evolves, its OCL constraints may need to be co-evolved too. Our systematic analysis shows that semantically different resolutions can be applied depending not only on the metamodel changes, but also on the user intent and on the structure of the impacted constraints. In this paper, we first investigate the syntactical reasons that lead to apply different resolutions. We then propose a co-evolution approach that offers alternative resolutions while allowing the user to choose the best applicable one. We evaluated our approach on six case studies of metamodel evolution and their OCL constraints co-evolution. The results show the usefulness of alternative resolutions along with user decision to cope with real co-evolution scenarios. Within our six case studies our approach led to an average of 92% (syntactically) and 93% (semantically) matching co-evolution w.r.t. the user intent."
Research article - A theory of power in emerging software ecosystems formed by small-to-medium enterprises,"AbstractIn a software ecosystem, partner companies rely on each other to succeed and survive. This scenario of mutual dependence entails a flow of power among companies. Power is an intrinsic property of their relationship and an asset to be exercised with a degree of intentionality. This paper presents a substantive theory to explain how power and dependence manifest in partnerships among small-to-medium enterprises (SMEs) building a software ecosystem. We performed exploratory case studies of two emerging software ecosystems formed by SMEs. We interpreted the results in light of a theoretical framework underpinned by French and Raven's power taxonomy. Finally, we performed a cross-case analysis to evaluate our findings and build the theory. The proposed theory highlights the interactions among different forms of power and corresponding sources of power employed by companies. It provides a better understanding on how power and dependence influence the behaviour and coordination of companies within a software ecosystem. The theory is a useful lens for researchers to explore ecosystem partnerships by understanding the structure and impact of power relationships between partners. In addition, it is a valuable tool for companies to analyse power distribution and define sustainable strategies for software ecosystem governance."
