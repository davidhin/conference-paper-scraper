title,abstract
Research article - Efficient modeling and optimizing of checkpointing in concurrent component-based software systems,"AbstractA common mechanism to improve availability and performance is checkpointing and rollback. When it is time to checkpoint, a system stores a job’s state to nonvolatile memory, and, when a failure occurs, it rolls back to the latest stored state instead of restarting the job from the beginning, thus improving performance in the presence of failures. Too frequent checkpointing reduces the amount of work to be redone in case of failures but generates excessive overhead, degrading performance. This paper presents a novel and very efficient queuing network model that addresses software component contention for hardware resources and shows how it can be used to model checkpointing in heterogeneous component-based software systems. We validated this model against a previous model, developed by the authors, that used Markov Chains. Our new model is orders of magnitude faster than the previous one and can be used to plan for checkpointing at run-time. As an additional contribution of this paper, we present an optimizer to find, for each software component, the optimal checkpointing interval that minimizes execution time, maximizes availability, or minimizes checkpointing overhead."
Research article - Using reliability risk analysis to prioritize test cases,"AbstractIn this paper, we present a risk-based test case prioritization (Ri-TCP) algorithm based on the transmission of information flows among software components. Most of the existing approaches rely on the historical code changes or test case execution data, few of them effectively use the system topology information covered by test cases when scheduling the execution of test cases. From the perspective of code structure, the proposed algorithm firstly maps software into an information flow-based directed network model. Then, functional paths covered by each test case are represented by a set of barbell motifs. Finally, combining with probabilistic risk analysis (PRA) and fault tree model, we assign a priority to each test case by calculating the sum of risk indexes of all the barbells covered by it. Experimental results demonstrate that Ri-TCP technique has a higher detection rate of faults with serious risk indicators and performs stably in different systems, compared with the other state-of-the-art algorithms."
Research article - Requirements engineering: A systematic mapping study in agile software development,"AbstractContextRequirements engineering in agile software development is a relatively recent software engineering topic and it is not completely explored and understood. The understanding of how this process works on agile world needs a deeper analysis.ObjectiveThe goal of this paper is to map the subject area of requirements engineering in agile context to identify the main topics that have been researched and to identify gaps to develop future researches. It is also intended to identify the obstacles that practitioners face when using agile requirements engineering.MethodA systematic mapping study was conducted and as a result 2171 papers were initially identified and further narrowed to 104 by applying exclusion criteria and analysis.ConclusionAfter completing the classification and the analysis of the selected studies it was possible to identify 15 areas (13 based on SWEBOK) where researches were developed. Five of such areas points to the need of future researches, among them are requirements elicitation, change management, measuring requirements, software requirements tools and comparative studies between traditional and agile requirements. In this research, some obstacles that practitioners face dealing with requirements engineering in agile context were also identified. They are related to environment, people and resources."
Research article - Towards efficiently supporting database as a service with QoS guarantees,"AbstractQuality of Service (QoS) is at the core of the vision of Database as a Service (DBaaS), which provides guarantees to database users on the usability of database services, even when the underlying database infrastructure is shared by multiple users. When QoS guarantees are necessitated, traditional approaches in DBaaS often have to reserve computation resources (e.g. CPU and memory) for tenants according to their performance Service Level Objectives (SLOs), so that the database engine always possesses sufficient resources to accomplish the expected workloads under any circumstance. Such resource reservation schemes inevitably result in poor resource utilization, as the actual workloads of tenants are usually below their maximal workload expectation described in their SLOs.In this paper, we propose a novel scheme called FrugalDB to further improve resource utilization and thus reduce operational cost for DBaaS systems with QoS guarantees. FrugalDB accommodates two independent database engines, an in-memory engine for heavy workloads with tight SLOs, and a disk-based engine for light workloads with loose SLOs. The in-memory database is leveraged to migrate temporary heavy workloads from the disk-based database, when the latter itself does not suffice to handle all active tenants’ workloads, and thus it could be relieved of the tremendous data serving pressure yielded by those heavy workloads, and focus on processing massive numbers of light workloads. When the heavy workloads fade out, FrugalDB reversely migrates a tenant’s workload from the in-memory database into the disk-based database, so that the occupied memory resources could be recollected for subsequent workload migrations. By an effective workload estimation method and an efficient migration schedule algorithm, FrugalDB tries to minimize workload migration cost incurred in moving workloads between the two engines. By allocating each tenant’s workload to an appropriate engine via workload migration, this dual-engine scheme can substantially save computation resources, and thus consolidate more tenants on a single database server. We evaluate FrugalDB with extensive experiments, which show that it has a higher tenant consolidation rate with performance SLO guarantees and fewer performance SLO violations than the existing systems, as well as with acceptable response latency."
Research article - A resource efficient framework to run automotive embedded software on multi-core ECUs,"AbstractThe increasing functionality and complexity of automotive applications requires not only the use of more powerful hardware, e.g., multi-core processors, but also efficient methods and tools to support design decisions. Component-based software engineering proved to be a promising solution for managing software complexity and allowing for reuse. However, there are several challenges inherent in the intersection of resource efficiency and predictability of multi-core processors when it comes to running component-based embedded software. In this paper, we present a software design framework addressing these challenges. The framework includes both mapping of software components onto executable tasks, and the partitioning of the generated task set onto the cores of a multi-core processor. This paper aims at enhancing resource efficiency by optimizing the software design with respect to: 1) the inter-software-components communication cost, 2) the cost of synchronization among dependent transactions of software components, and 3) the interaction of software components with the basic software services. An engine management system, one of the most complex automotive sub-systems, is considered as a use case, and the experimental results show a reduction of up to 11.2% total CPU usage on a quad-core processor, in comparison with the common framework in the literature."
Research article - Cloud service evaluation method-based Multi-Criteria Decision-Making: A systematic literature review,"AbstractA substantial effort has been made to solve the cloud-service evaluation problem. Different Cloud Service Evaluation Methods (CSEMs) have been developed to address the problem. Cloud services are evaluated against multiple criteria, which leads to a Multi-Criteria Decision-Making (MCDM) problem. Yet, studies that assess, analyse, and summarize the unresolved problems and shortcomings of current CSEM-based MCDM are limited. In the existing review studies, only individual parts of CSEMs, rarely the full solution, are reviewed and examined. To investigate CSEMs comprehensively, we present a systematic literature review based on Evaluation Theory, a theory that generalizes six evaluation components, target, criteria, yardstick, data gathering techniques, synthesis techniques, and evaluation process. These six evaluation components and the CSEMs validation approach are the seven dimensions used to assess and analyse 77 papers published from 2006 to 2016. Sixteen research deficiencies were identified. The results confirm that the majority of the studies of the proposed CSEMs were either incomplete or lacked sufficient evidence. This research not only provides the relative strengths and weaknesses of the different CSEMs but also offers a basis for researchers and decision makers to develop improved CSEMs."
Research article - Fault-aware management protocols for multi-component applications,"AbstractNowadays, applications are composed by multiple heterogeneous components, whose management must be suitably coordinated by taking into account inter-component dependencies and potential failures. In this paper, we first present fault-aware management protocols, which allow to model the management behaviour of application components, and we then illustrate how such protocols can be composed to analyse and automate the overall management of a multi-component application. We also show how to recover applications that got stuck because a fault was not handled properly, or because a component is behaving differently than expected. To illustrate the feasibility of our approach, we present Barrel, a proof-of-concept application that permits editing and analysing fault-aware management protocols in multi-component applications. We also discuss the usefulness of Barrel by showing how it was fruitfully exploited it in a concrete case study and in a controlled experiment."
Research article - Change impact analysis for evolving configuration decisions in product line use case models,"AbstractProduct Line Engineering is becoming a key practice in many software development environments where complex systems are developed for multiple customers with varying needs. In many business contexts, use cases are the main artifacts for communicating requirements among stakeholders. In such contexts, Product Line (PL) use cases capture variable and common requirements while use case-driven configuration generates Product Specific (PS) use cases for each new customer in a product family. In this paper, we propose, apply, and assess a change impact analysis approach for evolving configuration decisions in PL use case models. Our approach includes: (1) automated support to identify the impact of decision changes on prior and subsequent decisions in PL use case diagrams and (2) automated incremental regeneration of PS use case models from PL use case models and evolving configuration decisions. Our tool support is integrated with IBM Doors. Our approach has been evaluated in an industrial case study, which provides evidence that it is practical and beneficial to analyze the impact of decision changes and to incrementally regenerate PS use case models in industrial settings."
Research article - Localizing multiple software faults based on evolution algorithm,"AbstractDuring software debugging, a significant amount of effort is required for programmers to identify the root cause of manifested failures. Various spectrum-based fault localization techniques have been proposed to automate the procedure. However, most of the existing fault localization approaches do not consider the fact that programs tend to have multiple faults. Considering faults in isolation results in less accurate analysis. In this paper, we propose a flexible framework called FSMFL for localizing multiple faults simultaneously based on genetic algorithms with simulated annealing. FSMFL can be easily extended by different fitness functions for the purpose of localizing multiple faults simultaneously. We have implemented a prototype and conducted extensive experiments to compare FSMFL against existing spectrum based fault localization approaches. The experimental results show that FSMFL is competitive in single-fault localization and superior in multi-fault localization."
Research article - Automated generation of (F)LTL oracles for testing and debugging,"AbstractFor being able to draw on automated reasoning that helps us in improving the quality of some software artifact or cyber-physical system, we have to express desired system traits in precise formal requirements. Verifying that a system adheres to these requirements allows us then to gain the crucial level of confidence in its capabilities and quality. Complementing related methods like model checking or runtime monitors, for testing and most importantly debugging recognized problems, we would certainly be interested in automated oracles. These oracles would allow us to judge whether observed (test) data really adhere to desired properties, and also to derive program spectra that have been shown to be an effective reasoning basis for debugging purposes. In this paper, we show how to automatically derive such an oracle as a dedicated satisfiability encoding that is specifically tuned to the considered test data at hand. In particular, we instantiate a dedicated SAT problem in conjunctive normal form directly from the requirements and a test case’s execution data. Our corresponding experiments illustrate that our approach shows attractive performance and can be fully automated."
Research article - Anomaly detection and diagnosis for cloud services: Practical experiments and lessons learned,"AbstractThe dependability of cloud computing services is a major concern of cloud providers. In particular, anomaly detection techniques are crucial to detect anomalous service behaviors that may lead to the violation of service level agreements (SLAs) drawn with users. This paper describes an anomaly detection system (ADS) designed to detect errors related to the erroneous behavior of the service, and SLA violations in cloud services. One major objective is to help providers to diagnose the anomalous virtual machines (VMs) on which a service is deployed as well as the type of error associated to the anomaly. Our ADS includes a system monitoring entity that collects software counters characterizing the cloud service, as well as a detection entity based on machine learning models. Additionally, a fault injection entity is integrated into the ADS for the training the machine learning models. This entity is also used to validate the ADS and to assess its anomaly detection and diagnosis performance. We validated our ADS with two case studies deployments: a NoSQL database, and a virtual IP Multimedia Subsystem developed implementing a virtual network function. Experimental results show that our ADS can achieve a high detection and diagnosis performance."
Research article - A framework for semi-automated co-evolution of security knowledge and system models,"AbstractSecurity is an important and challenging quality aspect of software-intensive systems, becoming even more demanding regarding long-living systems. Novel attacks and changing laws lead to security issues that did not necessarily rise from a flawed initial design, but also when the system fails to keep up with a changing environment. Thus, security requires maintenance throughout the operation phase. Ongoing adaptations in response to changed security knowledge are inevitable. A necessary prerequisite for such adaptations is a good understanding of the security-relevant parts of the system and the security knowledge.We present a model-based framework for supporting the maintenance of security during the long-term evolution of a software system. It uses ontologies to manage the system-specific and the security knowledge. With model queries, graph transformation and differencing techniques, knowledge changes are analyzed and the system model is adapted. We introduce the novel concept of Security Maintenance Rules to couple the evolution of security knowledge with co-evolutions of the system model.As evaluation, community knowledge about vulnerabilities is used (Common Weakness Enumeration database). We show the applicability of the framework to the iTrust system from the medical care domain and hence show the benefits of supporting co-evolution for maintaining secure systems."
