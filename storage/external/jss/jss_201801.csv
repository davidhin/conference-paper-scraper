title,abstract
Research article - Managing architectural technical debt: A unified model and systematic literature review,"AbstractLarge Software Companies need to support the continuous and fast delivery of customer value in both the short and long term. However, this can be impeded if the evolution and maintenance of existing systems is hampered by what has been recently termed Technical Debt (TD). Specifically, Architectural TD has received increased attention in the last few years due to its significant impact on system success and, left unchecked, it can cause expensive repercussions. It is therefore important to understand the underlying factors of architectural TD. With this as background, there is a need for a descriptive model to illustrate and explain different architectural TD issues. The aim of this study is to synthesize and compile research efforts with the goal of creating new knowledge with a specific interest in the architectural TD field. The contribution of this paper is the presentation of a novel descriptive model, providing a comprehensive interpretation of the architectural TD phenomenon. This model categorizes the main characteristics of architectural TD and reveals their relations. The results show that, by using this model, different stakeholders could increase the system's success rate, and lower the rate of negative consequences, by raising awareness about architectural TD."
Research article - On the value of a prioritization scheme for resolving Self-admitted technical debt,"AbstractProgrammers tend to leave incomplete, temporary workarounds and buggy codes that require rework in software development and such pitfall is referred to as Self-admitted Technical Debt (SATD). Previous studies have shown that SATD negatively affects software project and incurs high maintenance overheads. In this study, we introduce a prioritization scheme comprising mainly of identification, examination and rework effort estimation of prioritized tasks in order to make a final decision prior to software release. Using the proposed prioritization scheme, we perform an exploratory analysis on four open source projects to investigate how SATD can be minimized. Four prominent causes of SATD are identified, namely code smells (23.2%), complicated and complex tasks (22.0%), inadequate code testing (21.2%) and unexpected code performance (17.4%). Results show that, among all the types of SATD, design debts on average are highly prone to software bugs across the four projects analysed. Our findings show that a rework effort of approximately 10 to 25 commented LOC per SATD source file is needed to address the highly prioritized SATD (vital few) tasks. The proposed prioritization scheme is a novel technique that will aid in decision making prior to software release in an attempt to minimize high maintenance overheads."
Research article - Quantum genetic algorithm based scheduler for batch of precedence constrained jobs on heterogeneous computing systems,AbstractDistributed systems are efficient means of realizing High-Performance Computing (HPC). They are used in meeting the demand of executing large-scale high-performance computational jobs. Scheduling the tasks on such computational resources is one of the prime concerns in the heterogeneous distributed systems. Scheduling requires optimizing either single objective or multiple objectives. Load Imbalance (LIB) and Load balancing Cost Ratio (LCR) are two such objectives. LIB and LCR are used in deciding the distribution of load over available resources for utilization and the computational cost respectively. Scheduling jobs with precedence constraints are NP-complete in nature. Scheduling requires either heuristic or metaheuristic approach for sub-optimal but acceptable solutions. Quantum computing is one such metaheuristic approach. It has proven to be promising in addressing the multi-objective scheduling problems with an effective exploration of the search space. This work proposes a dual-objective Quantum-inspired Genetic Algorithm based Load Balancing Strategy (QGLBS) for workflow application with the objective of optimizing both LIB and LCR. The DAG representation of the batch of jobs helps in effective exploitation of the parallelism available at the job level as well as the sub-job level with the modules at the same level of the batch executing at the same time. The strategy ensures that the communication cost and the critical path are considered in making scheduling decisions. QGLBS exploits the features of Quantum Computing and Genetic Algorithm to establish the Pareto fronts using non-dominated sorting based on NSGA-II. Hypervolume measures are used to compare the results of the quality assessment of Pareto fronts. Simulation study on Gridsim ensures that the QGLBS approach works effectively for real life scenario with various workflow applications.
Research article - Sacbe: A building block approach for constructing efficient and flexible end-to-end cloud storage,"AbstractEnd-to-end solutions enable users to protect their data, before sending them to the cloud, from confidentiality violations, service outages and vendor lock-in incidents. These solutions however require the integration and orchestration of multiple applications that affect the end-user service experience. This paper presents Sacbe, an approach for building efficient and flexible end-to-end cloud storage based on building blocks (BB), which are logical representations of independent applications encapsulated into containers. The developers can build structures such as pipelines, stacks and/or clusters of applications by chaining BBs through I/O interfaces. These structures enable users to move/process data/metadata in continuous dataflows from their devices to the cloud and enables organizations to build cloud storage services. We implemented a complete realization of an end-to-end cloud storage solution, which includes pipelines of BBs running on client-side for end-users to ensure in-house the confidentiality and reliability of data as well as stacks and clusters of BBs to build authentication, sharing, and storage services in a private cloud. This prototype was evaluated through controlled experimentation and a case study based on a satellite imagery, which revealed the feasibility of end-to-end solutions built with Sacbe as the end-user service experience was significantly improved in comparison with other solutions."
Research article - Large universe attribute based access control with efficient decryption in cloud storage system,"AbstractCiphertext Policy Attribute Based Encryption scheme is a promising technique for access control in the cloud storage, since it allows the data owner to define access policy over the outsourced data. However, the existing attribute based access control mechanism in the cloud storage is based on small universe construction, where the attribute set is defined at setup, and the size of the public parameters scales with the number of attributes. A large number of new attributes need to be added to the system over time, small universe attribute based access control is no longer suitable for cloud storage, whereas large universe attribute based encryption where any string can be employed as an attribute and attributes are not required to be enumerated at system setup meets this requirement. Unfortunately, one of the main efficiency drawbacks of existing large universe attribute based encryption is that ciphertext size and decryption time scale with the complexity of the access structure. In this work, we propose large universe attribute based access control scheme with efficient decryption. The user provides the cloud computing server with a transformation key with which the cloud computing server transforms the ciphertext associated with the access structure satisfied by the attributes associated with the private key into a simple and short ciphertext; thus it significantly reduces the time for the user to decrypt the ciphertext without the cloud computing server knowing the underlying plaintext; the user can check whether the transformation done by the cloud computing server is correct to verify transformation correctness. Security analysis and performance evaluation show our scheme is secure and efficient."
Research article - CHAIN: Developing model-driven contextual help for adaptive user interfaces,"AbstractAdaptive user interfaces (UIs) change their presentation at runtime to remain usable in different contexts-of-use. Such changes can cause discrepancies between the UI and static help materials, e.g., videos and screenshots, thereby negatively impacting the latter's usefulness (usability and utility). This paper presents Contextual Help for Adaptive INterfaces (CHAIN), which is an approach for developing model-driven contextual help that maintains its usefulness across UI adaptations. This trait is achieved by interpreting the help models at runtime and overlaying instructions on the running adapted UI. A language called Contextual Help for Adaptive INterfaces eXtensible Markup Language (CHAINXML) and a visual notation were developed for expressing and depicting help models. A technique was also devised for presenting CHAIN help models over legacy applications, whether or not their source-code is available. A supporting tool was developed as an extension to Cedar Studio. This work was empirically evaluated in two studies. The first study performed a preliminary evaluation of CHAIN's visual notation. The second study evaluated CHAIN's strengths and shortcomings after using it to develop help for real-life adaptive UIs. The results gave a positive indication about CHAIN's technical qualities and provided insights that could inform future work."
Research article - Increasing the capturing angle in print-cam robust watermarking,"AbstractIt is nowadays more probable that a print media is captured and shared with a mobile phone than with a scanner. The reasons for photographing the print range from intention of copying the image to simply sharing an interesting add with friends. Watermarking offers a solution for carrying side information in the images, and if the watermarking method being used is robust to the print-cam process, the information can be read with a mobile phone camera. In this paper, we present a print-cam robust watermarking method that is also implemented on a mobile phone and evaluated with user tests. Especially, the lens focusing problem when the picture is captured in a wide angle with respect to the printout is addressed. The results show that the method is highly robust to capturing the watermark without errors in angles up to 60° with processing times that are acceptable for real-life applications."
Research article - R-SHT: A state history tree with R-Tree properties for analysis and visualization of highly parallel system traces,"AbstractUnderstanding the behaviour of distributed computer systems with many threads and resources is a challenging task. Dynamic analysis tools such as tracers have been developed to assist programmers in debugging and optimizing the performance of such systems. However, complex systems can generate huge traces, with billions of events, which are hard to analyze manually. Trace visualization and analysis programs aim to solve this problem. Such software needs fast access to data, which a linear search through the trace cannot provide. Several programs have resorted to stateful analysis to rearrange data into more query friendly structures.In previous work, we suggested modifications to the State History Tree (SHT) data structure to correct its disk and memory usage. While the improved structure, eSHT, made near optimal disk usage and had reduced memory usage, we found that query performance, while twice as fast, exhibited scaling limitations.In this paper, we proposed a new structure using R-Tree techniques to improve query performance. We explain the hybrid scheme and algorithms used to optimize the structure to model the expected behaviour. Finally, we benchmark the data structure on highly parallel traces and on a demanding trace visualization use case.Our results show that the hybrid R-SHT structure retains the eSHT’s optimal disk usage properties while providing several orders of magnitude speed up to queries on highly parallel traces."
Research article - Test case prioritization for object-oriented software: An adaptive random sequence approach based on clustering,"AbstractTest case prioritization (TCP) attempts to improve fault detection effectiveness by scheduling the important test cases to be executed earlier, where the importance is determined by some criteria or strategies. Adaptive random sequences (ARSs) can be used to improve the effectiveness of TCP based on white-box information (such as code coverage information) or black-box information (such as test input information). To improve the testing effectiveness for object-oriented software in regression testing, in this paper, we present an ARS approach based on clustering techniques using black-box information. We use two clustering methods: (1) clustering test cases according to the number of objects and methods, using the K-means and K-medoids clustering algorithms; and (2) clustered based on an object and method invocation sequence similarity metric using the K-medoids clustering algorithm. Our approach can construct ARSs that attempt to make their neighboring test cases as diverse as possible. Experimental studies were also conducted to verify the proposed approach, with the results showing both enhanced probability of earlier fault detection, and higher effectiveness than random prioritization and method coverage TCP technique."
Research article - Fault localisation for WS-BPEL programs based on predicate switching and program slicing,"AbstractService-Oriented Architecture (SOA) enables the coordination of multiple loosely coupled services. This allows users to choose any service provided by the SOA without knowing implementation details, thus making coding easier and more flexible. Web services are basic units of SOA. However, the functionality of a single Web service is limited, and usually cannot completely satisfy the actual demand. Hence, it is necessary to coordinate multiple independent Web services to achieve complex business processes. Business Process Execution Language for Web Services (WS-BPEL) makes the coordination possible, by helping the integration of multiple Web services and providing an interface for users to invoke. When coordinating these services, however, illegal or faulty operations may be encountered, but current tools are not yet powerful enough to support the localisation and removal of these problems. In this paper, we propose a fault localisation technique for WS-BPEL programs based on predicate switching and program slicing, allowing developers to more precisely locate the suspicious faulty code. Case studies were conducted to investigate the effectiveness of the proposed technique, which was compared with predicate switching only, slicing only, and one existing fault localisation technique, namely Tarantula. The experimental results show that the proposed technique has a higher fault localisation effectiveness and precision than the baseline techniques."
Research article - Motivating the contributions: An Open Innovation perspective on what to share as Open Source Software,"AbstractOpen Source Software (OSS) ecosystems have reshaped the ways how software-intensive firms develop products and deliver value to customers. However, firms still need support for strategic product planning in terms of what to develop internally and what to share as OSS. Existing models accurately capture commoditization in software business, but lack operational support to decide what contribution strategy to employ in terms of what and when to contribute. This study proposes a Contribution Acceptance Process (CAP) model from which firms can adopt contribution strategies that align with product strategies and planning. In a design science influenced case study executed at Sony Mobile, the CAP model was iteratively developed in close collaboration with the firm’s practitioners. The CAP model helps classify artifacts according to business impact and control complexity so firms may estimate and plan whether an artifact should be contributed or not. Further, an information meta-model is proposed that helps operationalize the CAP model at the organization. The CAP model provides an operational OI perspective on what firms involved in OSS ecosystems should share, by helping them motivate contributions through the creation of contribution strategies. The goal is to help maximize return on investment and sustain needed influence in OSS ecosystems."
Research article - Lean Internal Startups for Software Product Innovation in Large Companies: Enablers and Inhibitors,"AbstractContext: Startups are disrupting traditional markets and replacing well-established actors with their innovative products.To compete in this age of disruption, large and established companies cannot rely on traditional ways of advancement, which focus on cost efficiency, lead time reduction and quality improvement. Corporate management is now looking for possibilities to innovate like startups. Along with it, the awareness and the use of the Lean startup approach have grown rapidly amongst the software startup community and large companies in recent years.Objective: The aim of this study is to investigate how Lean internal startup facilitates software product innovation in large companies. This study also identifies the enablers and inhibitors for Lean internal startups.Method: A multiple case study approach is followed in the investigation. Two software product innovation projects from two different large companies are examined, using a conceptual framework that is based on the method-in-action framework and extended with the previously developed Lean-Internal Corporate Venture model. Seven face-to-face in-depth interviews of the employees with different roles and responsibilities are conducted. The collected data is analysed through a careful coding process. Within-case analysis and cross-case comparison are applied to draw the findings from the two cases.Results: A generic process flow summarises the common key processes of Lean internal startups in the context of large companies. The findings suggest that an internal startup can be initiated top-down by management, or bottom-up by employees, which faces different challenges. A list of enablers and inhibitors of applying Lean startup in large companies are identified, including top management support and cross-functional team as key enablers. Both cases face different inhibitors due to the different process of inception, objective of the team and type of the product.Conclusions: The contribution of this study for research is threefold. First, this study is one of the first attempt to investigate the use of Lean startup approach in the context of large companies empirically. Second, the study shows the potential of the method-in-action framework to investigate the Lean startup approach in non-startup context. The third contribution is a general process of Lean internal startup and the evidence of the enablers and inhibitors of implementing it, which are both theory-informed and empirically grounded. Future studies could extend our study by addressing the limitations of the research approach undertaken in this study."
Research article - A decision-making process-line for selection of software asset origins and components,"AbstractSelecting sourcing options for software assets and components is an important process that helps companies to gain and keep their competitive advantage. The sourcing options include: in-house, COTS, open source and outsourcing. The objective of this paper is to further refine, extend and validate a solution presented in our previous work. The refinement includes a set of decision-making activities, which are described in the form of a process-line that can be used by decision-makers to build their specific decision-making process. We conducted five case studies in three companies to validate the coverage of the set of decision-making activities. The solution in our previous work was validated in two cases in the first two companies. In the validation, it was observed that no activity in the proposed set was perceived to be missing, although not all activities were conducted and the activities that were conducted were not executed in a specific order. Therefore, the refinement of the solution into a process-line approach increases the flexibility and hence it is better in capturing the differences in the decision-making processes observed in the case studies. The applicability of the process-line was then validated in three case studies in a third company."
