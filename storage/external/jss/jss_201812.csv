title,abstract
Research article - How does docker affect energy consumption? Evaluating workloads in and out of Docker containers,"AbstractContext: Virtual machines provide isolation of services at the cost of hypervisors and more resource usage. This spurred the growth of systems like Docker that enable single hosts to isolate several applications, similar to VMs, within a low-overhead abstraction called containers.Motivation: Although containers tout low overhead performance, how much do they increase energy use?Methodology: This work statistically compares the energy consumption of three application workloads in Docker and on bare-metal Linux.Results: In all cases, there was a statistically significant (t-test and Wilcoxon p < .05) increase in energy consumption when running tests in Docker, mostly due to the performance of I/O system calls. Developers worried about I/O overhead could consider baremetal deployments over Docker container deployments."
Research article - Hybrid charging scheduling schemes for three-dimensional underwater wireless rechargeable sensor networks,"AbstractRecent breakthrough in wireless power transfer provides a new paradigm for enabling wireless energy replenishment for wireless rechargeable sensor networks, especially in the underwater environment. In this paper, we first propose the concept of UWRSNs (underwater wireless rechargeable sensor networks) and then develop a series of 3D charging schemes for enhancing charging efficiency, using underwater charging robot mules in three-dimensional charging scenarios. Through constructing the architecture of UWRSNs, we develop a basic charging scheme SCS (Shortest-path Charging Scheme), which minimizes the traveling cost for the charging mules in the 3D underwater environment. Then, ECS (Emergency Charging Scheme) is proposed, which concentrates on serving emergency nodes. After that, a charging algorithm that combines ECS and SCS to collaboratively solve the charging problem, namely, HOCS (Hybrid Optimal Charging Scheme) is developed. At last, experimental simulations are conducted to show the outperformed merits of the proposed scheme. Experimental results demonstrate that our schemes not only save energy and time, but also ensure effective utilization of resources."
Research article - Requirements traceability technologies and technology transfer decision support: A systematic review,"AbstractRequirements traceability (RT) is a core activity in Requirements Engineering. Various types of RT technologies have been extensively studied for decades. In this paper, we present a systematic literature review from 114 papers between 2006 and 2016 on RT techniques. We summarized 10 major challenges in current RT activities, and categorized existing RT techniques into 6 groups and 25 sub-groups. Moreover, we built mapping relations between these challenges and techniques, and identified 7 potential future research directions. Based on 83 empirical studies, the evaluations for technology transfer are conducted. The main conclusions are: (1) The “trustworthy” and “automated” challenges are the most widely investigated ones, while “scalable”, “coordinated”, “dynamic” and “lightweight” challenges receive much less attention; (2) “Trace link generation”, especially information retrieval-based (IR-based) methods, are the most studied techniques; (3) IR-based methods have the most potential to be adopted by industry, as they have been validated from multiple viewpoints; (4) Seven promising future research directions are identified, which include developing scalable, dynamic and lightweight tracing techniques, introducing new approaches in other disciplines to meet the RT challenges, improving the express ability of trace links, promoting the industry adoption of RT technologies and developing new techniques to support developers’ coordination."
Research article - Test prioritization in continuous integration environments,"AbstractTwo heuristics namely diversity-based (DBTP) and history-based test prioritization (HBTP) have been separately proposed in the literature. Yet, their combination has not been widely studied in continuous integration (CI) environments. The objective of this study is to catch regression faults earlier, allowing developers to integrate and verify their changes more frequently and continuously. To achieve this, we investigated six open-source projects, each of which included several builds over a large time period. Findings indicate that previous failure knowledge seems to have strong predictive power in CI environments and can be used to effectively prioritize tests. HBTP does not necessarily need to have large data, and its effectiveness improves to a certain degree with larger history interval. DBTP can be used effectively during the early stages, when no historical data is available, and also combined with HBTP to improve its effectiveness. Among the investigated techniques, we found that history-based diversity using NCD Multiset is superior in terms of effectiveness but comes with relatively higher overhead in terms of method execution time. Test prioritization in CI environments can be effectively performed with negligible investment using previous failure knowledge, and its effectiveness can be further improved by considering dissimilarities among the tests."
Research article - Performance and programming effort trade-offs of android persistence frameworks,"AbstractA fundamental building block of a mobile application is the ability to persist program data between different invocations. Referred to as persistence, this functionality is commonly implemented by means of persistence frameworks. Without a clear understanding of the energy consumption, execution time, and programming effort of popular Android persistence frameworks, mobile developers lack guidelines for selecting frameworks for their applications. To bridge this knowledge gap, we report on the results of a systematic study of the performance and programming effort trade-offs of eight Android persistence frameworks, and provide practical recommendations for mobile application developers."
Research article - What’s in a GitHub Star? Understanding Repository Starring Practices in a Social Coding Platform,"AbstractBesides a git-based version control system, GitHub integrates several social coding features. Particularly, GitHub users can star a repository, presumably to manifest interest or satisfaction with an open source project. However, the real and practical meaning of starring a project was never the subject of an in-depth and well-founded empirical investigation. Therefore, we provide in this paper a throughout study on the meaning, characteristics, and dynamic growth of GitHub stars. First, by surveying 791 developers, we report that three out of four developers consider the number of stars before using or contributing to a GitHub project. Then, we report a quantitative analysis on the characteristics of the top-5,000 most starred GitHub repositories. We propose four patterns to describe stars growth, which are derived after clustering the time series representing the number of stars of the studied repositories; we also reveal the perception of 115 developers about these growth patterns. To conclude, we provide a list of recommendations to open source project managers (e.g., on the importance of social media promotion) and to GitHub users and Software Engineering researchers (e.g., on the risks faced when selecting projects by GitHub stars)."
Research article - An anatomy of requirements engineering in software startups using multi-vocal literature and case survey,"AbstractContext: Software startups aim to develop innovative products, grow rapidly, and thus become important in the development of economy and jobs. Requirements engineering (RE) is a key process area in software development, but its effects on software startups are unclear.Objective: The main objective of this study was to explore how RE (elicitation, documentation, prioritization and validation) is used in software startups.Method: A multi-vocal literature review (MLR) was used to find scientific and gray literature. In addition, a case survey was employed to gather empirical data to reach this study’s objective.Results: In the MLR, 36 primary articles were selected out of 28,643 articles. In the case survey, 80 respondents provided information about software startup cases across the globe. Data analysis revealed that during RE processes, internal sources (e.g., for source), analyses of similar products (e.g., elicitation), uses of informal notes (e.g., for documentation), values to customers, products and stakeholders (e.g., for prioritization) and internal reviews/prototypes (e.g., for validation) were the most used techniques.Conclusion: After an analysis of primary literature, it was concluded that research on this topic is still in early stages and more systematic research is needed. Furthermore, few topics were suggested for future research."
Research article - A multi-target approach to estimate software vulnerability characteristics and severity scores,"AbstractSoftware vulnerabilities constitute a great risk for the IT community. The specification of the vulnerability characteristics is a crucial procedure, since the characteristics are used as input for a plethora of vulnerability scoring systems. Currently, the determination of the specific characteristics -that represent each vulnerability- is a process that is performed manually by the IT security experts. However, the vulnerability description can be very informative and useful to predict vulnerability characteristics. The primary goal of this research is the enhancement, the acceleration and the support of the manual procedure of the vulnerability characteristic assignment. To achieve this goal, a model, which combines texts analysis and multi-target classification techniques was developed. This model estimates the vulnerability characteristics and subsequently, calculates the vulnerability severity scores from the predicted characteristics. To perform the present research, a dataset that contains 99,091 records from a large -publicly available- vulnerability database was used. The results are encouraging, since they show accuracy in the prediction of the vulnerability characteristics and scores."
Research article - Architecture-based change impact analysis in cross-disciplinary automated production systems,"AbstractMaintaining an automated production system is a challenging task as it comprises artifacts from multiple disciplines – namely mechanical, electrical, and software engineering. As the artifacts mutually affect each other, even small modifications may cause extensive side effects. Consequently, estimating the maintenance effort for modifications in an automated production system precisely is time consuming and often nearly as complicated as implementing the modifications. In this paper, we present the KAMP4aPS approach for architecture-based change impact analysis in production automation. We propose metamodels to specify the various artifacts of the system and modifications to them, as well as algorithms and rules for change propagation analysis based on the models. We evaluate KAMP4aPS for three different change scenarios based on the established xPPU community case study on production automation. In the case study, we investigate different configurations of metamodels and change propagation rules. Evaluation results indicate the accuracy of change propagation for applying KAMP4aPS to the specific metamodel and rules."
Research article - Parallel construction of interprocedural memory SSA form,"AbstractInterprocedural memory SSA form, which provides a sparse data-flow representation for indirect memory operations, paves the way for many advanced program analyses. Any performance improvement for memory SSA construction benefits for a wide range of clients (e.g., bug detection and compiler optimisations). However, its construction is much more expensive than that for scalar-based SSA form. The memory objects distinguished at a pointer dereference significantly increases the number of variables that need to be put on SSA form, resulting in considerable analysis overhead when analyzing large programs (e.g., millions of lines of code).This paper presents ParSSA, a fully parameterised approach for parallel construction of interprocedural memory SSA form by utilising multi-core computing resources. ParSSA partitions whole-program memory objects into uniquely identified memory regions. The indirect memory accesses in a function are fully parameterised using partitioned memory regions, so that the memory SSA construction of a parameterised function is readily parallelised. We implemented ParSSA in LLVM using Intel Threading Building Block (TBB) for creating parallel tasks. We evaluated ParSSA using 15 large applications. ParSSA achieves up to 6.9 ×  speedup against the sequential version on an 8-core machine."
Research article - Genetic algorithm for energy-efficient clustering and routing in wireless sensor networks,"AbstractWireless sensor networks have been employed widely in various fields, including military, health care, and manufacturing applications. However, the sensor nodes are limited in terms of their energy supply, storage capability, and computational power. Thus, in order to improve the energy efficiency and prolong the network life cycle, we present a genetic algorithm-based energy-efficient clustering and routing approach GECR. We add the optimal solution obtained in the previous network round to the initial population for the current round, thereby improving the search efficiency. In addition, the clustering and routing scheme are combined into a single chromosome to calculate the total energy consumption. We construct the fitness function directly based on the total energy consumption thereby improving the energy efficiency. Moreover, load balancing is considered when constructing the fitness function. Thus, the energy consumption among the nodes can be balanced. The experimental results demonstrated that the GECR performed better than other five methods. The GECR achieved the best load balancing with the lowest variances in the loads on the cluster heads under different scenarios. In addition, the GECR was the most energy-efficient with the lowest average energy consumed by the cluster heads and the lowest energy consumed by all the nodes."
Research article - The pains and gains of microservices: A Systematic grey literature review,"AbstractThe design, development, and operation of microservices are picking up more and more momentum in the IT industry. At the same time, academic work on the topic is at an early stage, and still on the way to distilling the actual “Pains & Gains” of microservices as an architectural style. Having witnessed this gap, we set forth to systematically analyze the industrial grey literature on microservices, to identify the technical/operational pains and gains of the microservice-based architectural style. We conclude by discussing research directions stemming out from our analysis."
Research article - Efficient cloud service discovery approach based on LDA topic modeling,"AbstractWith the rapid development of Cloud-based services, the necessity of a Cloud service discovery engine becomes a fundamental requirement. A semantic focused crawler is one of the most key components of Cloud service discovery engines. However, the huge size and varied functionalities of Cloud services on the Web have a great effect on crawlers to provide effective Cloud services. It is a challenge for semantic crawlers to search only for URLs that offer Cloud services from this explosion of information. To solve these issues, this paper proposes a self-adaptive semantic focused crawler based on Latent Dirichlet Allocation (LDA) for efficient Cloud service discovery. In this paper, we present a Cloud Service Ontology (CSOnt) that defines Cloud service categories. CSOnt contains a set of concepts, allowing the crawler to automatically collect and categorize Cloud services. Moreover, our proposed crawler adopts URLs priority techniques to maintain the order of URLs to be parsed for efficient retrieval of the relevant Cloud services. Additionally, we create a self-adaptive semantic focused crawler, which has an ontology-learning function to automatically improve the proposed Cloud Service Ontology and maintain the crawler’s performance."
"Research article - Agile values or plan-driven aspects: Which factor contributes more toward the success of data warehousing, business intelligence, and analytics project development?","AbstractPractically all organizations are developing data warehousing, business intelligence, and analytics (DW/BIA) projects for achieving customer value. A DW/BIA development project may be characterized by both agile and plan-driven aspects. The reported study investigated two research questions: (1) Which factor, agile values or plan-driven aspects, contributes more toward the success of DW/BIA? (2) What are the significant antecedents of agile values and plan-driven aspects? 124 respondents engaged in DW/BIA development filled a 30-item questionnaire on seven constructs. The partial least squares structural equation modeling (PLS-SEM) method was used to determine the strength of the relationships among the following factors: technological capability, shared understanding, top management commitment, and complexity as antecedents; agile values and plan-driven aspects as mediating; and project success as the dependent construct. Based on a prediction-oriented segmentation (PLS-POS) analysis, the findings indicate that there are two groups, agile-plan balanced and agile-heavy, which represent different approaches to DW/BIA development. Top management commitment and shared understanding emerge as strong antecedents to agile values and plan-driven aspects. Overall, the factor agile values contributes more toward the success of DW/BIA development."
Research article - A prediction-Based VM consolidation approach in IaaS Cloud Data Centers,"AbstractRecent years have witnessed a rapid growth in exploiting Cloud environments to host and deliver various types of virtualized resources as on-demand services. In order to optimally use Cloud resources, the arrangement of virtual machines (VMs) in physical machines (PMs) must be performed strategically, because the placement of VMs in accordance with the available resources can reduce energy consumption, improve resource utilization and, consequently, can increase companies benefits. However, VMs could have time varying workloads, which leads to degradation of performance and power consumption. Thus, re-configuring the VMs placement is essential. Virtual machine consolidation aims to optimally use the available resources by allocating several virtual machines on a set of physical ones (PMs). To determine the PMs capacities to reallocate VMs, it is important to predict their states based on resource utilization history within each VM, and the past VMs migration traffic. However, a common limitation between existing VM consolidation approaches is the lack of information about the history of (and the future) VM migration traffic. Through this paper, we aim to propose a virtual machine consolidation approach based on the estimation of requested resources and the future VM migration traffic. We exploit the strength of Kernel Density Estimation technique (KDE) as a powerful mean to forecast the future resource usage of each VM, and AKKA toolkit as an actor-based model that allows exchanging useful information about the host’s states. We adopt a weighted-graph representation to model the history of migration traffic between PMs and to design the actor-based topology of the data center. The obtained results show the effectiveness of our approach in terms of total number of migrations and energy consumption."
Research article - Developing and using checklists to improve software effort estimation: A multi-case study,"AbstractExpert judgment based effort estimation techniques are widely used for estimating software effort. In the absence of process support, experts may overlook important factors during estimation, leading to inconsistent estimates. This might cause underestimation, which is a common problem in software projects. This multi-case study aims to improve expert estimation of software development effort. Our goal is two-fold: 1) to propose a process to develop and evolve estimation checklists for agile teams, and 2) to evaluate the usefulness of the checklists in improving expert estimation processes. The use of checklists improved the accuracy of the estimates in two case companies. In particular, the underestimation bias was reduced to a large extent. For the third case, we could not perform a similar analysis, due to the unavailability of historical data. However, when checklist was used in two sprints, the estimates were quite accurate (median Balanced Relative Error (BRE) bias of -0.05). The study participants from the case companies observed several benefits of using the checklists during estimation, such as increased confidence in estimates, improved consistency due to help in recalling relevant factors, more objectivity in the process, improved understanding of the tasks being estimated, and reduced chances of missing tasks."
Research article - Past and future of software architectures for context-aware systems: A systematic mapping study,"AbstractThere is a growing interest on context-aware systems in recent years. Context-aware systems are able to change their behaviour depending on new conditions regarding the user, the platform and the environment. These systems are evolving towards interacting with the user in a transparent and ubiquitous manner, especially by means of different types of sensors, which can gather a wide range of data from the user, the platform the user is interacting with, and the environment where such interaction is taking place. It is worth noting that the software architecture of a system is a key artefact during its development and its adaptation process. Hence, the definition of the software architecture becomes essential while developing context-aware systems since it should reflect how the context is tackled for adaptation purposes. With the aim of studying this issue, we have designed and conducted a systematic mapping study to provide an overview about the different architectural approaches used in context-aware systems. One of the main findings of this study is that there are not many software architecture proposals that deal with context-awareness in an explicit way during the adaptation process. It was also detected that there are Human Computer Interaction (HCI) works that focus on context-aware adaptations but neglect partially or completely any possible change in the system architecture during the adaptation process. Due to this, we perceived a need to analyse what research works highlight the use of context and its relationship to the software architecture in existing context-aware systems. Therefore, this mapping study attempts to bridge the gap between Software Architecture and HCI in order to align the adaptation at the architectural level (changes in the configuration of architectural components) and at the HCI level (changes in the interaction modality or the user interface in general)."
Research article - Automated composition and optimization of services for variability-intensive domains,"AbstractThe growth in the number of publicly available services on the Web has encouraged developers to rely more heavily on such services to deliver products in a faster, cheaper and more reliable fashion. Many developers are now using a collection of these services in tandem to build their applications. While there has been much attention to the area of service composition, there are few works that examine the possibility of automatically generating service compositions for variability-intensive application domains. High variability in a domain is often captured through an organized feature space, which has the potential for developing many different application instantiations. The focus of our work is to develop an end-to-end technique that would enable the automatic generation of composite services based on a specific configuration of the feature space that would be directly executable and presented in WS-BPEL format. To this end, we adopt concepts from software product line engineering and AI planning to deliver the automated composition of online services. We will further benefit from such notions as safeness and threat from AI planning to optimize the generated service compositions by introducing parallelism where possible. Furthermore, we show how the specification of the generated service composition can be translated into executable WS-BPEL code. More specifically, the core contributions of our work are: (1) we show how AI planning techniques can be used to generate a workflow based on a feature model configuration; (2) we propose a method for optimizing a workflow generated based on AI planning techniques; and (3) we demonstrate that the optimized workflow can be directly translated into WS-BPEL code. We evaluate our work from two perspectives: (i) we will first formally prove that the methods that we have proposed are sound and complete from a theoretical perspective, and (ii) we will show through experimentation that our proposed work is usable from a practical point of view."
Research article - Designing and implementing an environment for software start-up education: Patterns and anti-patterns,"AbstractToday’s students are prospective entrepreneurs, as well as potential employees in modern, start-up-like intrapreneurship environments within established companies. In these settings, software development projects face extreme requirements in terms of innovation and attractiveness of the end-product. They also suffer severe consequences of failure such as termination of the development effort and bankruptcy. As the abilities needed in start-ups are not among those traditionally taught in universities, new knowledge and skills are required to prepare students for the volatile environment that new market entrants face. This article reports experiences gained during seven years of teaching start-up knowledge and skills in a higher-education institution. Using a design-based research approach, we have developed the Software Factory, an educational environment for experiential, project-based learning. We offer a collection of patterns and anti-patterns that help educational institutions to design, implement and operate physical environments, curricula and teaching materials, and to plan interventions that may be required for project-based start-up education."
Research article - ESPRET: A tool for execution time estimation of manual test cases,"AbstractManual testing is still a predominant and an important approach for validation of computer systems, particularly in certain domains such as safety-critical systems. Knowing the execution time of test cases is important to perform test scheduling, prioritization and progress monitoring. In this work, we present, apply and evaluate ESPRET (EStimation and PRediction of Execution Time) as our tool for estimating and predicting the execution time of manual test cases based on their test specifications. Our approach works by extracting timing information for various steps in manual test specification. This information is then used to estimate the maximum time for test steps that have not previously been executed, but for which textual specifications exist. As part of our approach, natural language parsing of the specifications is performed to identify word combinations to check whether existing timing information on various test steps is already available or not. Since executing test cases on the several machines may take different time, we predict the actual execution time for test cases by a set of regression models. Finally, an empirical evaluation of the approach and tool has been performed on a railway use case at Bombardier Transportation (BT) in Sweden."
