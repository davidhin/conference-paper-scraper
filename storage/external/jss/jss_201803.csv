title,abstract
Research article - A Systematic Literature Review of iStar extensions,"AbstractiStar is a general-purpose goal-based modelling language used to model requirements at early and late phases of software development. It has been used in industrial and academic projects. Often the language is extended to incorporate new constructs related to an application area. The language is currently undergoing standardisation, so several studies have focused on the analysis of iStar variations to identify the similarities and defining a core iStar. However, we believe it will continue to be extended and it is important to understand how iStar is extended. This paper contributes to this purpose through the identification and analysis of the existing extensions and its constructs. A Systematic Literature Review was conducted to guide identification and analysis. The results point to 96 papers and 307 constructs proposed. The extensions and constructs were analysed according to well-defined questions in three dimensions: a general analysis; model-based analysis (to characterise the extensions from semantic and syntactic definitions); and a third dimension related to semiotic clarity. The application area targeted by the iStar extensions and their evolutions are presented as results of our analysis. The results point to the need for more complete, consistent and careful development of iStar extensions. The paper concludes with some discussions and future directions for this research field."
Research article - Multi-level contention-free policy for real-time multiprocessor scheduling,"AbstractThe contention-free policy has received attention in real-time multiprocessor scheduling owing to its wide applicability and significant improvement in offline schedulability guarantees. Utilizing the notion of contention-free slots in which the number of active jobs is smaller than or equal to the number of processors, the policy improves the schedulability by offloading executions in contending time slots to contention-free ones. In this paper, we propose the multi-level contention-free policy by exploiting a new, generalized notion of multi-level contention-free slots. In a case study, we present how the multi-level contention-free policy is applied to EDF (Earliest Deadline First) scheduling and develop a schedulability test for EDF that adopts the new policy. Our evaluation results demonstrate that the multi-level contention-free policy significantly improves the schedulability by up to 4188% and 127%, compared to vanilla EDF and EDF adopting the existing contention-free policy, respectively."
Research article - Software engineering problems and their relationship to perceived learning and customer satisfaction on a software capstone project,"AbstractIn educational projects, having students encounter problems is desirable, if it increases learning. However, in capstone projects with industrial customers, negative effects problems can have on customer satisfaction must be considered. We conducted a survey in a capstone project course in order to study problems, learning and customer satisfaction related to eleven software engineering topics. On the average, students working in the managerial roles learned quite a lot about each topic, and the developers learned moderately, but the degree of learning varied a lot among the teams, and among the team members. The most extensively encountered problems were related to testing, task management, effort estimation and technology skills. The developers contributed quite a lot to solving problems with technology skills, but only moderately or less with other topics, whereas the managers contributed quite a lot with most of the topics. Contributing to solving problems increased learning moderately for most of the topics. The increases were highest with maintaining motivation and technology skills. Encountering problems with task management, customer expectations and customer communication affected customer satisfaction very negatively. When considering both learning and customer satisfaction, the best topics to encounter problems in were effort estimation, testing, and technology skills."
Research article - New deep learning method to detect code injection attacks on hybrid applications,"AbstractMobile phones are becoming increasingly pervasive. Among them, HTML5-based hybrid applications are more and more popular because of their portability on different systems. However these applications suffer from code injection attacks. In this paper, we construct a novel deep learning network, Hybrid Deep Learning Network (HDLN), and use it to detect these attacks. At first, based on our previous work, we extract more features from Abstract Syntax Tree (AST) of JavaScript and employ three methods to select key features. Then we get the feature vectors and train HDLN to distinguish vulnerable applications from normal ones. Finally thorough experiments are done to validate our methods. The results show our detection approach with HDLN achieves 97.55% in accuracy and 97.60% in AUC, which outperforms those with other traditional classifiers and gets higher average precision than other detection methods."
Research article - Developing an integrated framework for using data mining techniques and ontology concepts for process improvement,"AbstractProcess, as an important knowledge resource, must be effectively managed and improved. The main problems are the large number of processes, their specific features, and the complicated relationships between them, which all lead to the increase in complexity and create a high-dimensionality problem. Traditional process management systems are unable to manage and improve processes with a high volume of data. Data mining techniques, however, can be employed to identify valuable patterns. With the aid of these patterns, suggestions for process improvement can be presented. Further, process ontology can be applied to share the process patterns between people, facilitate the process understanding, and develop the reusability of the extracted patterns for process improvement.This study presents a combined three-part, five-stage framework of data mining, process improvement, and process ontology. To evaluate the applicability and effectiveness of the proposed framework, a real process dataset is applied. Two clustering and classification techniques are used to discover valuable patterns as the process ontology. The output of these two techniques can be considered as the recommendations for improving the processes. The proposed framework can be exploited to support process improvement methodologies in organizations."
Research article - Kanban in software engineering: A systematic mapping study,"AbstractFollowing a well-established track record of success in other domains such as manufacturing, Kanban is increasingly used to achieve continuous development and delivery of value in the software industry. However, while research on Kanban in software is growing, these articles are largely descriptive, and there is limited rigorous research on its application and with little cohesive building of cumulative knowledge. As a result, it is extremely difficult to determine the true value of Kanban in software engineering. This study investigates the scientific evidence to date regarding Kanban by conducting a systematic mapping of Kanban literature in software engineering between 2006 and 2016. The search strategy resulted in 382 studies, of which 23 were identified as primary papers relevant to this research. This study is unique as it compares the findings of these primary papers with insights from a review of 23 Kanban experience reports during the same period. This study makes four important contributions, (i) a state-of-the-art of Kanban research is provided, (ii) the reported benefits and challenges are identified in both the primary papers and experience reports, (iii) recommended practices from both the primary papers and experience reports are listed and (iv) opportunities for future Kanban research are identified."
Research article - A categorization scheme for software engineering conference papers and its application,"AbstractBackgroundIn Software Engineering (SE), conference publications have high importance both in effective communication and in academic careers. Researchers actively discuss how a paper should be organized to be accepted in mainstream conferences.AimingThis work tackles the problem of generalizing and characterizing the type of papers accepted at SE conferences.MethodThe paper offers a new perspective in the analysis of SE literature: a categorization scheme for SE papers is obtained by merging, extending and revising related proposals from a few existing studies. The categorization scheme is used to classify the papers accepted at three top-tier SE conferences during five years (2012–2016).ResultsWhile a broader experience is certainly needed for validation and fine-tuning, preliminary outcomes can be observed relative to what problems and topics are addressed, what types of contributions are presented and how they are validated.ConclusionsThe results provide insights to paper writers, paper reviewers and conference organizers in focusing their future efforts, without any intent to provide judgments or authoritative guidelines."
Research article - Scalable code clone detection and search based on adaptive prefix filtering,"AbstractCode clone detection is a well-known software engineering problem that aims to detect all the groups of code blocks or code fragments that are functionally equivalent in a code base. It has numerous and wide ranging important uses in areas such as software metrics, plagiarism detection, aspect mining, copyright infringement investigation, code compaction, virus detection, and detecting bugs. A scalable code clone detection technique, able to process large source code repositories, is crucial in the context of multi-project or Internet-scale code clone detection scenarios. In this paper, we focus on improving the scalability of code clone detection, relative to current state of the art techniques. Our adaptive prefix filtering technique improves the performance of code clone detection for many common execution parameters, when tested on common benchmarks. The experimental results exhibit improvements for commonly used similarity thresholds of between 40% and 80%, in the best case decreasing the execution time up to 11% and increasing the number of filtered candidates up to 63%."
Research article - Crowdsourcing user reviews to support the evolution of mobile apps,"AbstractIn recent software development and distribution scenarios, app stores are playing a major role, especially for mobile apps. On one hand, app stores allow continuous releases of app updates. On the other hand, they have become the premier point of interaction between app providers and users. After installing/updating apps, users can post reviews and provide ratings, expressing their level of satisfaction with apps, and possibly pointing out bugs or desired features. In this paper we empirically investigate—by performing a study on the evolution of 100 open source Android apps and by surveying 73 developers—to what extent app developers take user reviews into account, and whether addressing them contributes to apps’ success in terms of ratings. In order to perform the study, as well as to provide a monitoring mechanism for developers and project managers, we devised an approach, named CRISTAL, for tracing informative crowd reviews onto source code changes, and for monitoring the extent to which developers accommodate crowd requests and follow-up user reactions as reflected in their ratings. The results of our study indicate that (i) on average, half of the informative reviews are addressed, and over 75% of the interviewed developers claimed to take them into account often or very often, and that (ii) developers implementing user reviews are rewarded in terms of significantly increased user ratings."
Research article - A Metrics Suite for code annotation assessment,"AbstractCode annotation is a language feature that enables the introduction of custom metadata on programming elements. In Java, this feature was introduced on version 5, and today it is widely used by main enterprise application frameworks and APIs. Although this language feature potentially simplifies metadata configuration, its abuse and misuse can reduce source code readability and complicate its maintenance. The goal of this paper is to propose software metrics regarding annotations in the source code and analyze their distribution in real-world projects. We have defined a suite of metrics to assess characteristics of the usage of source code annotations in a code base. Our study collected data from 24947 classes extracted from open source projects to analyze the distribution of the proposed metrics. We developed a tool to automatically extract the metrics and provide a full report on annotations usage. Based on the analysis of the distribution, we defined an appropriate approach for the calculation of thresholds to interpret the metric values. The results allow the assessment of annotated code characteristics. Using the thresholds values, we proposed a way to interpret the use of annotations, which can reveal potential problems in the source code."
Research article - An effective approach for software project effort and duration estimation with machine learning algorithms,"AbstractDuring the last two decades, there has been substantial research performed in the field of software estimation using machine learning algorithms that aimed to tackle deficiencies of traditional and parametric estimation techniques, increase project success rates and align with modern development and project management approaches. Nevertheless, mostly due to inconclusive results and vague model building approaches, there are few or none deployments in practice.The purpose of this article is to narrow the gap between up-to-date research results and implementations within organisations by proposing effective and practical machine learning deployment and maintenance approaches by utilization of research findings and industry best practices. This was achieved by applying ISBSG dataset, smart data preparation, an ensemble averaging of three machine learning algorithms (Support Vector Machines, Neural Networks and Generalized Linear Models) and cross validation. The obtained models for effort and duration estimation are intended to provide a decision support tool for organisations that develop or implement software systems."
Research article - Adaptive generation of challenging scenarios for testing and evaluation of autonomous vehicles,"AbstractIn this paper we propose a new method for generating test scenarios for black-box autonomous systems that demonstrate critical transitions in performance modes. This method provides a test engineer with key insights into the software’s decision-making engine and how those decisions affect transitions between performance modes. We achieve this via adaptive, simulation-based testing of the autonomous system where each sample represents a simulated scenario. The test scenario, i.e the system input, represents a given configuration of environmental or mission parameters and the resulting outputs are the system’s performance based on high-level success criteria. For realistic testing scenarios, the dimensionality of the configuration space and the computational expense of high-fidelity simulations precludes exhaustive or uniform sampling. Thus, we have developed specialized adaptive search algorithms designed to discover performance boundaries of the autonomy using a minimal number of samples. Further, unsupervised clustering techniques are presented that can group test scenarios by the resulting performance modes and sort them by those which are most effective at diagnosing changes in the autonomous system’s behavior. The result is a testing framework that gives the test engineer a set of diverse scenarios that exercises the decision boundaries of the autonomous system under test."
Research article - Compositional execution semantics for business process verification,"AbstractService compositions are programmed as executable business processes in languages like WS-BPEL (or BPEL in short). In such programs, activities are nested within concurrency, isolation, compensation and event handling constructs that cause an overwhelming number of execution paths. Program correctness has to be verified based on a formal definition of the language semantics. For BPEL , previous works have proposed execution semantics in formal languages amenable to model checking. Most of the times the service composition structure is not preserved in the formal model, which impedes tracing the verification findings in the original program. Here, we propose a compositional semantics and a structure-preserving translator of BPEL programs onto the BIP component framework. In addition, we verify essential correctness properties that affect process responsiveness, and the compliance with partner services. The scalability of the proposed translation and analysis is demonstrated on BPEL programs of various sizes. Our compositional translation approach can be also applied to other executable languages with nesting syntax."
Research article - Formal semantics of OMG’s Interaction Flow Modeling Language (IFML) for mobile and rich-client application model driven development,"AbstractModel Driven Engineering relies on the availability of software models and of development tools supporting the transition from models to code. The generation of code from models requires the unambiguous interpretation of the semantics of the modeling languages used to specify the application. This paper presents the formalization of the semantics of the Interaction Flow Modeling Language (IFML), a recent OMG MDA standard conceived for the specification of the front-end part of interactive applications. IFML constructs are mapped to equivalent structures of Place Chart Nets (PCN), which allows their precise interpretation. The defined semantic mapping is implemented in an online Model-driven development environment that enables the creation and inspection of PCNs from IFML, the analysis of the behavior of IFML specifications via PCN simulation, and the generation of code for mobile and web-based architectures."
Research article - On the use of replacement messages in API deprecation: An empirical study,"AbstractLibraries are commonly used to support code reuse and increase productivity. As any other system, they evolve over time, and so do their APIs. Consequently, client applications should be updated to benefit from better APIs. To facilitate this task, API elements should always be deprecated with replacement messages. However, in practice, there are evidences that API elements are deprecated without these messages. In this paper, we study questions regarding the adoption of deprecation messages. Our goal is twofold: to measure the real usage of deprecation messages and to investigate whether a tool is needed to recommend them. We assess (i) the frequency of deprecated elements with replacement messages, (ii) the impact of software evolution on this frequency, and (iii) the characteristics of systems that deprecate API elements in a correct way. Our analysis on 622 Java and 229 C# systems shows that: (i) on the median, 66.7% and 77.8% of the API elements are deprecated with replacement messages per project, (ii) there is no major effort to improve deprecation messages, and (iii) systems that deprecated API elements with messages are different in terms of size and community. As a result, we provide the basis for creating a tool to support clients detecting missing deprecation messages."
Research article - Combinatorial double auction-based resource allocation mechanism in cloud computing market,"AbstractThe cloud computing environment may be considered as market for computing and storage resources. Providers rent their available resources in the form of Virtual Machines (VM) and charge the users accordingly. One of the challenges in this market is providing a mechanism for the allocation of resources and their pricing, such that the proper benefit of both users and providers are guaranteed. In this paper, a combinatorial double auction-based market is studied in which a broker performs the allocation of the providers’ VMs according to the users’ requests. The proposed allocation problem is formulated as an integer linear programming model aiming at maximizing the total profit of users and providers. It is proved that the proposed model satisfies the desirable properties including: truthfulness, fairness, economic efficiency and allocation efficiency. Furthermore, due to the high complexity of the proposed model, a heuristic resource allocation algorithm with a quasi linear time complexity is presented. The results of evaluations confirm the good agreement of the heuristic algorithm with the optimization model in terms of allocation performance. Moreover, simulation results using CloudSim indicate that, compared to the previous works in literature, the proposed algorithm increases the profit of providers and users and reduces the resource wastage."
Research article - Heuristic-based approaches for speeding up incremental record linkage,"AbstractRecord Linkage is the task of processing a dataset in order to identify which records refer to the same real world entity. The intrinsic complexity of this task brings many challenges to traditional or naive approaches, especially in contexts such as Big Data, unstructured data and frequent data increments over the dataset. To deal with these contexts, especially the latter, an incremental record linkage approach may be employed in order to avoid (re)processing the entire dataset to update the deduplication results. For doing so, different classification techniques can be employed to identify duplicate entities. Recently, many algorithms have been proposed to combine collective classification, which employs clustering algorithms, together with the incremental principle. In this article, we propose new metrics for incremental record linkage using collective classification and new heuristics (which combine clustering, coverage component filters and a greedy approach) to speed up even more a solution to incremental record linkage. These heuristics have been evaluated using three different scale datasets and the results were analyzed and discussed based on both classical and the newly proposed metrics. The experiments present different trade-offs, regarding efficacy and efficiency results, which are generated by the considered heuristics. Also, the results indicate that, for large and frequent data increments, it is possible to slightly reduce efficacy results by employing a coverage filter-based heuristic that is reasonably faster than the current state-of-the-art approach. In turn, it is also possible to employ single-pass clustering algorithms, which are able to execute significantly faster than the state-of-the-art approach at the cost of sacrificing precision results."
Research article - The impact of tailoring criteria on agile practices adoption: A survey with novice agile practitioners in Brazil,"AbstractThe software development industry adopts agile methods in different ways by considering contextual requirements. To fulfill organizational needs, adoption strategies consider agile methods tailoring. However, tailoring according to the context of the organization remains a problem to be solved. Literature on criteria for adopting software development methods exists, but not specifically for agile methods. Given this scenario, the following research question arises: what is the impact of software method tailoring criteria on the adoption of agile practices? To answer this question, we conducted a survey among agile practitioners in Brazil to gather data about importance of the tailoring criteria and agile practices adopted. A model for agile practices adoption based on the tailoring criteria is proposed using the results of the survey with a majority of novice agile practitioners. The proposed model was validated using PLS-SEM (partial least squares structural equation modeling) and the survey data. Results show that adoption of agile practices was influenced by criteria such as external environment, previous knowledge and internal environment. Results also indicate that organizations tend to use hybrid/custom software methods and select agile practices according to their needs."
Research article - Secure multi-keyword ranked search over encrypted cloud data for multiple data owners,"AbstractSecure multi-keyword ranked search over outsourced cloud data has become a hot research field. Most existing works follow the model of “Single Owner”, which just supports searching on the outsourced data belong to only one data owner. But the more realistic scenario is “Multiple Owners”, users could search on all datasets outsourced by different data owners. However, directly extending “Single Owner” schemes into “Multiple Owners” scenario still face the major two challenges: (1) the inconvenient key management and the resulting high communication cost; (2) due to the different authorities of owners, the qualities of documents are also different even if they are about the similar topic, but current rank functions in this area cannot rank documents based on their qualities. In this paper, we propose a secure multi-keyword ranked search scheme for multiple data owners. A trusted third party is imported to solve the problem of key management. We exploit the vector space model for generating index and query, and our new-designed KDO algorithm is utilized for providing keyword weight, so that the rank function not only considers about the relevance between query and document, but also takes into account the document quality. In order to protect privacy for both owners and users, the Asymmetric Scalar-product Preserving Encryption approach is utilized for encrypting weighted index and query. Besides, we construct the Grouped Balanced Binary tree index, which could further improve the search efficiency by Greedy Depth-first search algorithm. Extensive experiments demonstrate that our proposed scheme is secure, accurate and efficient."
Research article - Omniscient debugging for executable DSLs,"AbstractOmniscient debugging is a promising technique that relies on execution traces to enable free traversal of the states reached by a model (or program) during an execution. While a few General-Purpose Languages (GPLs) already have support for omniscient debugging, developing such a complex tool for any executable Domain Specific Language (DSL) remains a challenging and error prone task. A generic solution must: support a wide range of executable DSLs independently of the metaprogramming approaches used for implementing their semantics; be efficient for good responsiveness. Our contribution relies on a generic omniscient debugger supported by efficient generic trace management facilities. To support a wide range of executable DSLs, the debugger provides a common set of debugging facilities, and is based on a pattern to define runtime services independently of metaprogramming approaches. Results show that our debugger can be used with various executable DSLs implemented with different metaprogramming approaches. As compared to a solution that copies the model at each step, it is on average sixtimes more efficient in memory, and at least 2.2 faster when exploring past execution states, while only slowing down the execution 1.6 times on average."
Research article - Characterizing the contribution of quality requirements to software sustainability,"AbstractBackgroundSince sustainability became a challenge in software engineering, researchers mainly from requirements engineering and software architecture communities have contributed to defining the basis of the notion of sustainability-aware software.ProblemDespite these valuable efforts, the assessment and design based on the notion of sustainability as a software quality is still poorly understood. There is no consensus on which sustainability requirements should be considered.Aim and MethodTo fill this gap, a survey was designed with a double objective: i) determine to which extent quality requirements contribute to the sustainability of software-intensive systems; and ii) identify direct dependencies among the sustainability dimensions. The survey involved different target audiences (e.g. software architects, ICT practitioners with expertise in Sustainability). We evaluated the perceived importance/relevance of each sustainability dimension, and the perceived usefulness of exploiting a sustainability model in different software engineering activities.ResultsMost respondents considered modifiability as relevant for addressing both technical and environmental sustainability. Functional correctness, availability, modifiability, interoperability and recoverability favor positively the endurability of software systems. This study has also identified security, satisfaction, and freedom from risk as very good contributors to social sustainability. Satisfaction was also considered by the respondents as a good contributor to economic sustainability."
Research article - Reliability over consecutive releases of a semiconductor Optical Endpoint Detection software system developed in a small company,"AbstractDemonstrating software reliability across multiple software releases has become essential in making informed decisions of upgrading software releases without impacting significantly end users’ characterized processes and software quality standards. Standard defect and workload data normally collected in a typical small software development organization can be used for this purpose. Most of these organizations are normally under aggressive schedules with limited resources and data availability that are significantly different from large commercial software organizations where software reliability engineering has been successfully applied. Trend test, input domain reliability models (IDRM), and software reliability growth models (SRGM) were used in this paper on a semiconductor Optical Endpoint Detection (OED) software system to examine its overall trend and stability, to assess the system’s operational reliability, and to track its reliability growth over multiple releases. These techniques also provided evidence that continuous defect fixes increased software reliability substantially over time for this software system."
Editorial - Introduction to the special issue on software reliability engineering,AbstractThis Special Issue brings together novel research results in the Software Reliability Engineering area. This is the result of a collective effort from authors and reviewers and includes 23 manuscripts selected from 66 high quality submissions from 27 different countries.
Research article - Characterizing and diagnosing out of memory errors in MapReduce applications,"AbstractOut of memory (OOM) errors are common and serious in MapReduce applications. Since MapReduce framework hides the details of distributed execution, it is challenging for users to pinpoint the OOM root causes. Current memory analyzers and memory leak detectors can only figure out what objects are (unnecessarily) persisted in memory but cannot figure out where the objects come from and why the objects become so large. Thus, they cannot identify the OOM root causes.Our empirical study on 56 OOM errors in real-world MapReduce applications found that the OOM root causes are improper job configurations, data skew, and memory-consuming user code. To identify the root causes of OOM errors in MapReduce applications, we design a memory profiling tool Mprof. Mprof can automatically profile and quantify the correlation between a MapReduce application’s runtime memory usage and its static information (input data, configurations, user code). Mprof achieves this through modeling and profiling the application’s dataflow, the memory usage of user code, and performing correlation analysis on them. Based on this correlation, Mprof uses quantitative rules to trace OOM errors back to the problematic user code, data, and configurations.We evaluated Mprof through diagnosing 28 real-world OOM errors in diverse MapReduce applications. Our evaluation shows that Mprof can accurately identify the root causes of 23 OOM errors, and partly identify the root causes of the other 5 OOM errors."
Research article - Combining heterogeneous anomaly detectors for improved software security,"AbstractHost-based Anomaly Detection Systems (ADSs) monitor for significant deviations from normal software behavior. Several techniques have been investigated for detecting anomalies in system call sequences. Among these, Sequence Time-Delay Embedding (STIDE), Hidden Markov Model (HMM), and One-Class Support Vector Machine (OCSVM) have shown a high level of anomaly detection accuracy. Although ADSs can detect novel attacks, they generate a large number of false alarms due to the difficulty in obtaining complete descriptions of normal software behavior. This paper presents a multiple-detector ADS that efficiently combines the decisions from heterogeneous detectors (e.g., STIDE, HMM, and OCSVM), using Boolean combination in the Receiver Operating Characteristics (ROC) space, to reduce the false alarms. Results on two modern and large system call datasets generated from Linux and Windows operating systems show that the proposed ADS consistently outperforms an ADS based on a single best detector and on an ensemble of homogeneous detectors. At an operating point of zero percent alarm rate, the proposed multiple-detector ADS increased the true positive rate by 500% on the Linux dataset and by 25% on the Window dataset. Furthermore, the combinations of decisions from multiple heterogeneous detectors make the ADS more reliable and resilient against evasion and adversarial attacks."
Research article - On early detection of application-level resource exhaustion and starvation,"AbstractSoftware systems are often engineered and tested for functionality under normal rather than worst-case conditions. This makes the systems vulnerable to denial-of-service attacks, where attackers engineer conditions that result in overconsumption of resources or starvation and stalling of execution. While the security community is well familiar with volumetric resource exhaustion attacks at the network and transport layers, application-specific attacks pose a challenging threat. In this paper, we present Radmin, a novel system for early detection of application-level resource exhaustion and starvation attacks. Radmin works directly on compiled binaries. It learns and executes multiple probabilistic finite automata from benign runs of target programs. Radmin confines the resource usage of target programs to the learned automata and detects resource usage anomalies at their early stages. We demonstrate the effectiveness of Radmin by testing it using a variety of synthetic and in-the-wild attacks. We provide a theoretical analysis of the attacker’s knowledge of Radmin and provide a metric for the degree of vulnerability of a program that is protected by Radmin. Finally, we also compare the accuracy and effectiveness of two different architectures, Radmin which works in both the user and kernel spaces, and URadmin which works solely in user space."
Research article - JSTrace: Fast reproducing web application errors,"AbstractJavaScript has become the most popular language for client-side web applications. Due to JavaScript's highly-dynamic and event-driven features, it is challenging to diagnose web application errors. Record-replay techniques are used to reproduce errors in web applications. After a long run, these techniques will record a long event trace that triggers an error. Although the error-related events are few, they are interleaved with other massive error-irrelevant events. It is time-consuming to diagnose errors with long event traces.In this article, we present JSTrace, which effectively removes error-irrelevant events from the long event trace, and further facilitates error diagnosis. Based on fine-grained dependences of JavaScript and DOM instructions, we develop a novel dynamic slicing technique that can remove events irrelevant to the error. We further present rules to remove irrelevant events, which cannot be removed by dynamic slicing. In this process, many events and related instructions are removed without losing the error reproducing accuracy. Our evaluation on 13 real-world web application errors shows that the reduced event traces can faithfully reproduce errors with an average reduction rate of 97%. We further performed case studies on 4 real-world errors, and the result shows that JSTrace is useful to diagnose web application errors."
Research article - Automated and reliable resource release in device drivers based on dynamic analysis,"AbstractIn a modern operating system, device drivers acquire system resources to work. The acquired resources should be explicitly released by the drivers, because the operating system never reclaims them. Moreover, improper resource release can cause system crashes or hangs. Thus resource release is very important to driver reliability. However, according to our study on Linux driver mailing lists, many applied patches involve the modifications of resource release. Thus current resource management in drivers is not reliable enough.In this paper, we propose a novel approach named AutoRR, which can automatically and reliably release resources based on dynamic analysis. To identify resource handling operations, we use the dynamic specification-mining technique to mine resource acquiring and releasing functions. During execution, we maintain a resource-state list by intercepting the mined functions. If the driver fails to release acquired resources, AutoRR will report bugs and call corresponding releasing functions to safely release the resources. Dynamic analyses of resource dependency, allocation hierarchy, error handling form and releasing time are performed to avoid introducing new bugs when releasing resources. The evaluation on 12 Linux drivers shows that 40 detected bugs are all successfully and safely tolerated, and the overhead is only 7.84%."
"Research article - The impacts of techniques, programs and tests on automated program repair: An empirical study","AbstractManual program repair is notoriously tedious, error-prone, and costly, especially for the modern large-scale projects. Automated program repair can automatically find program patches without much human intervention, greatly reducing the burden of developers as well as accelerating software delivery. Therefore, much research effort has been dedicated to design powerful program repair techniques. To date, although various program repair techniques have been proposed, to our knowledge, there lacks extensive study on the impacts of repair techniques, subject programs, and test suites on the repair effectiveness and efficiency. In this paper, we perform such an extensive study on repairing 180 seeded and real faults from 17 small to large sized programs. We study the impacts of five representative automated program repair techniques, including GenProg, RSRepair, Brute-force-based technique, AE and Kali, on the repair results. We further investigate the impacts of different subject programs and test suites on effectiveness and efficiency of program repair techniques. Our study demonstrates a number of interesting findings: Brute-force-based technique generates the maximum number of patches but is also the most costly technique, while Kali is the most efficient and has medium effectiveness among the studied techniques; techniques that work well with small programs become too costly or ineffective when applied to large sized programs; since tool-reported patches may overfit the selected test cases, we calculate the false positive rates and find that the influence of failed test cases is much larger than that of passed test cases; finally, surprisingly, all the studied techniques except RSRepair can find more than 80% of successful patches within the first 50% of search space."
Research article - The impact of Software Testing education on code reliability: An empirical assessment,"AbstractSoftware Testing (ST) is an indispensable part of software development. Proper testing education is thus of paramount importance. Indeed, the mere exposition to ST knowledge might have an impact on programming skills. In particular, it can encourage the production of more correct - and thus reliable - code. Although this is intuitive, to the best of our knowledge, there are no studies about such effects. Concerned with this, we have conducted two investigations related to ST education: (1) a large experiment with students to evaluate the possible impact of ST knowledge on the production of reliable code; and (2) a survey with professors that teach introductory programming courses to evaluate their level of ST knowledge. Our study involved 60 senior-level computer science students, 8 auxiliary functions with 92 test cases, a total of 248 implementations, and 53 professors of diverse subfields that completed our survey. The investigation with students shows that ST knowledge can improve code reliability in terms of correctness in as much as 20%, on average. On the other hand, the survey with professors reveals that, in general, university instructors tend to lack the same knowledge that would help students increase their programming skills toward more reliable code."
Research article - Failure patterns in operating systems: An exploratory and observational study,"AbstractSophisticated critical computer applications need to run on top of operating system (OS) software. Given the natural intrinsic dependency of user applications on the OS software, OS failures can severely impact even the most reliable applications. Thus, it is essential to understand how OS failures occur in order to improve software reliability. In this paper, we present an exploratory and observational study on OS failure patterns. We analyze 7007 real OS failures collected from 566 computers used in different workplaces. We start with a general characterization of the failure dataset examined in this study, where interesting findings are presented, e.g., the most frequent failure types per period of a day and per different workplaces. Next, we investigate the existence of failure patterns. For this purpose, we introduce an OS failure pattern discovery protocol that identifies failure patterns exhibiting consistency across different computers used in the same as well as different workplaces. In total, we discovered 45 failure patterns with 153,511 occurrences. Based on these patterns, we found that the most prevalent failures were related to the software updates of the OS components. The main causes of these failures involved infrastructural and environmental factors such as disk-space unavailability and concurrent execution of OS services. Empirical evidence of time-correlated failures of these OS components is also discussed in this paper. Other findings include the OS components that contributed more to create the discovered failure patterns and the most prevalent combination of failure events and their temporal order. This study aims at to contribute to a better understanding of the mechanisms behind OS failures."
Research article - Metric selection and anomaly detection for cloud operations using log and metric correlation analysis,"AbstractCloud computing systems provide the facilities to make application services resilient against failures of individual computing resources. However, resiliency is typically limited by a cloud consumer’s use and operation of cloud resources. In particular, system operations have been reported as one of the leading causes of system-wide outages. This applies specifically to DevOps operations, such as backup, redeployment, upgrade, customized scaling, and migration – which are executed at much higher frequencies now than a decade ago. We address this problem by proposing a novel approach to detect errors in the execution of these kinds of operations, in particular for rolling upgrade operations. Our regression-based approach leverages the correlation between operations’ activity logs and the effect of operation activities on cloud resources. First, we present a metric selection approach based on regression analysis. Second, the output of a regression model of selected metrics is used to derive assertion specifications, which can be used for runtime verification of running operations. We have conducted a set of experiments with different configurations of an upgrade operation on Amazon Web Services, with and without randomly injected faults to demonstrate the utility of our new approach."
Research article - An empirical study of collaborative model and its security risk in Android,"AbstractAndroid provides a framework for the development of collaborative applications, which is considered as one of the reasons behind its success. Collaborative model provides flexibility to an application in utilizing services offered by other applications. This approach offers several advantages to developers, such as allowing them to dedicate all of their resources in developing only core functionalities of an application while leveraging services offered by other applications for its auxiliary functionalities. However, the collaborative model also has some disadvantages, such as opening of attack surfaces in an application during exposure of some of its components as it offers its services. Malicious actions can be performed through the exposed components of the application. Android provides permission-based security to protect the exposed components. However, developers must implement the security correctly. In this paper, we empirically evaluate the scale of the collaborative model adopted by Android applications. We also investigate various methods to achieve collaboration among applications. Furthermore, we evaluate the scale of security risk instigated by the collaborative model and perform several other empirical studies on 13,944 Android applications."
Research article - A configurable V&V framework using formal behavioral patterns for OSEK/VDX operating systems,"AbstractVerification and Validation (V&V) of small-scale embedded software must consider the operating system. Unlike general-purpose systems, the underlying operating system is closely coupled with the application logic, generating potentially an infinite number of different control programs depending on the application configuration and application logic. Verifying this software individually is time-consuming and costly, especially when the objective is rigorous verification.To assist in rigorous V&V activities for such embedded software, the proposed work suggests a pattern-based framework that can be used to generate configurable formal OS and test models. At the core of the framework, lies a set of predefined behavioral patterns and constraint patterns that can be composed for the auto-generation of formal models for variously configured operating systems. These configurable formal models form the basis of formal validation and verification activities such as model checking safety properties, model-based test generation, and formal application simulation. We have implemented a prototype tool, specially designed for embedded control software based on the OSEK/VDX international standard, to demonstrate the benefits of the framework in task simulation, test generation, and formal verification. A series of experiments and analysis demonstrate that the suggested pattern-based framework is more efficient in test sequence generation and more effective in identifying problems compared to existing approaches."
Research article - Architecture-level hazard analysis using AADL,"AbstractSoftware systems are becoming increasingly important in safety-critical areas. Designing safe software requires a significant emphasis on hazards in the early design phase of software development. In this paper, we propose a hazard analysis approach based on Architecture Analysis and Design Language (AADL). First, to make up the deficiencies of Error Model Annex (EMV2), we create Hazard Model Annex (HMA) to specify the hazard sources, hazards, hazard trigger mechanisms, and mishaps. By using HMA, a safety model can be built by annotating an architecture model with the error model and hazard model. Then, an architecture-level hazard analysis approach is proposed to automatically generate the hazard analysis table. The approach contains the model transformation from a safety model to a Deterministic Stochastic Petri Nets (DSPNs) model for calculating the occurrence probability of hazards and mishaps. In addition, we present the formal semantics for each constituent part of the safety model, define the model mapping rules, and verify the semantic preservation of the transformation. Finally, HMA is implemented to build safety models and two Eclipse plug-ins of our methodology are also implemented. A case study on a flight control software system has been employed to demonstrate the feasibility of our proposed technique."
Research article - Analyzing inconsistencies in software product lines using an ontological rule-based approach,"AbstractSoftware product line engineering (SPLE) is an evolving technical paradigm for generating software products. Feature model (FM) represents commonality and variability of a group of software products that appears within a specific domain. The quality of FMs is one of the factors that impacts the correctness of software product line (SPL). Developing FMs might also incorporate inaccurate relationships among features which cause numerous defects in models. Inconsistency is one of such defect that decreases the benefits of SPL. Existing approaches have focused in identifying inconsistencies in FMs however, only a few of these approaches are able to provide their causes. In this paper FM is formalized from an ontological view by converting model into a predicate-based ontology and defining a set of first-order logic based rules for identifying FM inconsistencies along with their causes in natural language in order to assist developers with solutions to fix defects. A FM available in software product lines online tools repository has been used to explain the presented approach and validated using 24 FMs of varied sizes up to 22,035 features. Evaluation results demonstrate that our approach is effective and accurate for the FMs scalable up to thousands of features and thus, improves SPL."
Research article - Efficient detection and validation of atomicity violations in concurrent programs,"AbstractAtomicity violations are a major source of bugs in concurrent programs. Empirical studies have shown that the majority of atomicity violations are instances of the three-access pattern, where two accesses to a shared variable by a thread are interleaved by an access to the same variable by another thread. This article describes two advancements in atomicity violation detection. First, we describe a new technique that directs the execution of a dynamic analysis tool towards three-access pattern (TAP) instances. The directed search is based on constraint solving and concolic execution. We implemented this technique in a tool called AtomChase. Using 27 benchmarks comprising 5.4 million lines of Java, we compared AtomChase to five other tools. AtomChase found 20% more TAP instances than all five tools combined. Second, we show that not all TAP instances are atomicity violations and present a formally grounded approach to validating the non-atomicity of TAP instances. This approach, called HyperCV, prevents the inclusion of false positives in results presented to users. HyperCV uses a set of provably sufficient conditions for non-atomicity to efficiently validate TAP instances. Using the same benchmarks, HyperCV validated 79% of TAP instances in linear rather than exponential execution time."
Research article - An approach for optimized feature selection in large-scale software product lines,"AbstractContext: Feature selection in product line engineering is an essential step for individual product customization, in which the multiple objectives, that are often competing and conflicting, have to be taken into consideration. These objectives always need to be balanced during selection, leading to a process of multi-objective optimization. What’s more, the massive complex dependency and constraint relationships between features present another huge challenge for optimization.Objective: In this work, we propose a multi-objective optimization algorithm, IVEA-II, to automatically search through configurations to obtain an optimal balance between various objectives. Additionally, all the relationships between features must be conformed to by the optimal feature solutions.Method: Firstly, a two-dimensional fitness function in our previous work is reserved. Secondly, to prevent the negative impact of this 2D fitness on the diversity of final Pareto Fronts, the crowding distance is introduced into each fitness-based selection. Lastly, a new mutation operator is designed to improve the scalability of IVEA-II.Results: A series of experiments were conducted to verify the effectiveness of IVEA-II on five large-scale feature models with five optimization goals.Conclusion: Experiments showed that IVEA-II can generate more valid solutions over a set period of time, with optimal solutions also having better diversity and convergence."
Research article - PreX: A predictive model to prevent exceptions,"AbstractThe exception handling mechanism has been one of the most used reliability tools in programming languages in the last decades. However, this model has not changed much with time, in spite of advances in programming languages, which include concurrent programming and a shift towards more reactive paradigms, the basic principle remains the same - an exception occurs, and the mechanism reacts. We propose a new paradigm, inspired by Online Failure Prediction (OFP), to predict exceptions and possibly avert them by triggering the execution of preventive actions. The proposed model - PreX - is, thus, proactive, operating in a much finer-grained level than any other form of online failure prediction. OFP has shown promising results in predicting failures at a higher level, but has never been available to the developer, being mainly a system level technique. Thus, PreX will offer developers a new range of revitalization strategies. In this work, we describe the model and evaluate its performance by applying it to a real e-commerce solution, demonstrating how it is capable of predicting and preventing exceptions at run-time. Furthermore, we also show that PreX increases the overall availability and performance of the system under the same conditions."
Research article - Hora: Architecture-aware online failure prediction,"AbstractComplex software systems experience failures at runtime even though a lot of effort is put into the development and operation. Reactive approaches detect these failures after they have occurred and already caused serious consequences. In order to execute proactive actions, the goal of online failure prediction is to detect these failures in advance by monitoring the quality of service or the system events. Current failure prediction approaches look at the system or individual components as a monolith without considering the architecture of the system. They disregard the fact that the failure in one component can propagate through the system and cause problems in other components. In this paper, we propose a hierarchical online failure prediction approach, called Hora, which combines component failure predictors with architectural knowledge. The failure propagation is modeled using Bayesian networks which incorporate both prediction results and component dependencies extracted from the architectural models. Our approach is evaluated using Netflix’s server-side distributed RSS reader application to predict failures caused by three representative types of faults: memory leak, system overload, and sudden node crash. We compare Hora to a monolithic approach and the results show that our approach can improve the area under the ROC curve by 9.9%."
Research article - Effective fault prediction model developed using Least Square Support Vector Machine (LSSVM),"AbstractSoftware developers and project teams spend considerable amount of time in identifying and fixing faults reported by testers and users. Predicting defects and identifying regions in the source code containing faults before it is discovered or invoked by users can be valuable in terms of saving maintenance resources, user satisfaction and preventing major system failures post deployment. Fault prediction can also improve the effectiveness of software quality assurance activities by guiding the test team to focus efforts on fault prone components. The work presented in this paper involves building an effective fault prediction tool by identifying and investigating the predictive power of several well-known and widely used software metrics for fault prediction. We apply ten different feature selection techniques to choose the best set of metrics from a set of twenty source code metrics. We build the fault prediction model using Least Squares Support Vector Machine (LSSVM) learning method associated with linear, polynomial and radial basis function kernel functions. We perform experiments on 30 Open Source Java projects. Experimental results reveals that our prediction model is best suitable for projects with faulty classes less than the threshold value depending on fault identification efficiency (low- 52.139%, median- 46.206%, and high- 32.080%)."
Research article - A survey on reliable distributed communication,"AbstractFrom entertainment to personal communication, and from business to safety-critical applications, the world increasingly relies on distributed systems. Despite looking simple, distributed systems hide a major source of complexity: tolerating faults and component crashes is very difficult, due to the incompleteness of (remote) knowledge. The need to overcome this problem, and provide different guarantees to applications, sparked a huge research effort and resulted in a large body of communication protocols, and middleware. Thus, it is worthwhile to survey the state of the art in distributed systems, with a particular emphasis on reliable communication. We discuss key concepts in reliable communication, such as interaction patterns (e.g., one-way vs. request-response, synchronous vs. asynchronous), reliability semantics (e.g., at-least-once, at-most-once), and reliability targets (e.g., message, conversation), and we analyze a wide set of current communication solutions, which map to the different concepts. Building on the concepts, we analyze applications that have different reliable communication needs. As a result, we observe that, in most cases, elaborate communication solutions offering superior guarantees are purely academic efforts that cannot compete with the popularity and maturity of established, albeit poorer solutions. Based on our analysis, we identify and discuss open research topics in this area."
Research article - Time-space efficient regression testing for configurable systems,"AbstractConfigurable systems are those that can be adapted from a set of input options, reflected in code in form of variations. Testing these systems is challenging because of the vast array of configuration possibilities where bugs can hide. In the context of evolution, testing becomes even more challenging — not only code but also the set of plausible configurations can change across versions.This paper proposes EvoSPLat, a regression testing technique for configurable systems that explores all dynamically reachable configurations from a test. EvoSPLat supports two important application scenarios of regression testing. In the RCS scenario EvoSPLat prunes configurations (not tests) that are not impacted by changes. In the RTS scenario EvoSPLat prunes tests (not configurations) which are not impacted by changes.To evaluate EvoSPLat under the RCS scenario we used a selection of configurable Java programs. Results indicate that EvoSPLat reduced time by  ∼22% and reduced the number of configurations tested by  ∼45%. To evaluate EvoSPLat under the RTS scenario we used GCC. Results indicate that EvoSPLat reduced time to run tests by  ∼35%. Overall, results suggest that EvoSPLat is a promising technique to test configurable systems in the prevalent scenario of evolution."
Research article - Domain model slicing and constraint classification for local validation on rich clients,"AbstractWeb-based rich client applications have emerged as a solid and popular approach in both web and native applications. Their capability to manage their own domain model and locally verify constraints provides a more responsive and robust user experience. This local model is often a subset of the application's global domain model (GDM) that is managed on the server. Both ends should always manage their entities, relationships and constraints consistently between them. Designing such client model manually implies identifying the GDM domain elements and constraints that should also be present on the client and adapting each one of them if needed. This is a complex and error-prone task, and any additional modification to the server model requires reviewing the client side. In our opinion, all the information needed for automating the client model generation can be derived from the GDM and the set of entities involved in the client functionality. This work includes a formal description of a method that, from that initial information, combines model slicing and constraint analysis techniques to create the client domain model, and classifies the constraints according to their server independency."
Research article - Security slicing for auditing common injection vulnerabilities,"AbstractCross-site scripting and injection vulnerabilities are among the most common and serious security issues for Web applications. Although existing static analysis approaches can detect potential vulnerabilities in source code, they generate many false warnings and source-sink traces with irrelevant information, making their adoption impractical for security auditing.One suitable approach to support security auditing is to compute a program slice for each sink, which contains all the information required for security auditing. However, such slices are likely to contain a large amount of information that is irrelevant to security, thus raising scalability issues for security audits.In this paper, we propose an approach to assist security auditors by defining and experimenting with pruning techniques to reduce original program slices to what we refer to as security slices, which contain sound and precise information.To evaluate the proposed approach, we compared our security slices to the slices generated by a state-of-the-art program slicing tool, based on a number of open-source benchmarks. On average, our security slices are 76% smaller than the original slices. More importantly, with security slicing, one needs to audit approximately 1% of the total code to fix all the vulnerabilities, thus suggesting significant reduction in auditing costs."
Research article - IoT–TEG: Test event generator system,"AbstractInternet of Things (IoT) has been paid increasingly attention by the government, academe and industry all over the world. One of the main drawbacks of the IoT systems is the amount of information they have to handle. This information arrives as events that need to be processed in real time in order to make correct decisions. Given that processing the data is crucial, testing the IoT systems that will manage that information is required. In order to test IoT systems, it is necessary to generate a huge number of events with specific structures and values to test the functionalities required by these systems. As this task is very hard and very prone to error if done by hand, this paper addresses the automated generation of appropriate events for testing. For this purpose, a general specification to define event types and its representation are proposed and an event generator is developed based on this definition. Thanks to the adaptability of the proposed specification, the event generator can generate events of an event type, or events which combine the relevant attributes of several event types. Results from experiments and real-world tests show that the developed system meets the demanded requirements."
