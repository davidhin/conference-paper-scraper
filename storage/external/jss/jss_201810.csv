title,abstract
Research article - Code smells and their collocations: A large-scale experiment on open-source systems,"AbstractCode smells indicate possible flaws in software design, that could negatively affect system’s maintainability. Interactions among smells located in the same classes (i.e., collocated smells) have even more detrimental effect on quality. Extracted frequent patterns of collocated smells could help to understand practical consequences of collocations. In this paper we identify and empirically validate frequent collocations of 14 code smells detected in 92 Java systems, using three approaches: pairwise correlation analysis, PCA and associative rules. To cross-validate the results, we used up to 6 detectors for each smell. Additionally, we examine and compare techniques used to extract the relationships. The contribution is three-fold: (1) we identify and empirically validate relationships among the examined code smells on a large dataset that we made publicly available, (2) we discuss how the choice of code smell detectors affects results, and (3) we analyze the impact of software domain on existence of the smell collocations. Additionally, we found that analytical methods we used to discover collocations, are complementary. Smells collocations display recurring patterns that could help prioritizing the classes affected by code smells to be refactored and developing or enhancing detectors exploiting information about collocations. They can also help the developers focusing on classes deserving more maintenance effort."
Research article - Improving software design reasoning–A reminder card approach,"AbstractSoftware designers have been known to think naturalistically. This means that there may be inadequate rational thinking during software design. In the past two decades, many research works suggested that designers need to produce design rationale. However, design rationale can be produced to retrofit naturalistic decisions, which means that design decisions may still not be well reasoned. Through a controlled experiment, we studied design reasoning and design rationale by asking participants to carry out a group design. As treatment, we provided 6 out of 12 student teams with a set of reasoning reminder cards to see how they compare with teams without the reminder cards. Additionally, we performed the same experiment with 2 teams of professionals who used the reminder cards, and compared the results with 3 teams of professionals. The experimental results show that both professionals and students who were equipped with the reasoning reminder cards reasoned more with their design. Second, the more a team discusses design reasoning, the more design rationale they find."
Research article - Bug-proneness and late propagation tendency of code clones: A Comparative study on different clone types,"AbstractCode clones are defined to be the exactly or nearly similar code fragments in a software system’s code-base. The existing clone related studies reveal that code clones are likely to introduce bugs and inconsistencies in the code-base. However, although there are different types of clones, it is still unknown which types of clones have a higher likeliness of introducing bugs to the software systems and so, should be considered more important for managing with techniques such as refactoring or tracking. With this focus, we performed an empirical study that compared the bug-proneness of the major clone-types: Type 1, Type 2, and Type 3.According to our experimental results on thousands of revisions of nine diverse subject systems, Type 3 clones exhibit the highest bug-proneness among the three clone-types. The bug-proneness of Type 1 clones is the lowest. Also, Type 3 clones have the highest likeliness of being co-changed consistently while experiencing bug-fixing changes. Moreover, the Type 3 clones that experience bug-fixes have a higher possibility of evolving following a Similarity Preserving Change Pattern (SPCP) compared to the bug-fix clones of the other two clone-types. From the experimental results it is clear that Type 3 clones should be given a higher priority than the other two clone-types when making clone management decisions. Our investigation on the relatedness between bug-proneness and late propagation in code clones implies that bug-proneness of code clones is not primarily related with late propagation. The possibility that a bug-fix experienced by a clone fragment will be related with late propagation is only 1.4%. Moreover, for only 10.76% of the cases, a late propagation experienced by clone fragments can be related with a bug. Thus, late propagation contributes to a very little proportion of the bugs in code clones. We believe that our study provides useful implications for ranking clones for management such as refactoring and tracking."
Research article - An exploratory case study on reusing architecture decisions in software-intensive system projects,"AbstractReusing architecture decisions from previous projects promises to support architects when taking decisions. However, little is known about the state of art of decision-reuse and the benefits and challenges associated with reusing decisions. Therefore, we study how software architects reuse architecture decisions, the stakeholders and their concerns related to decision-reuse, and how architects perceive the ideal future state of decision-reuse. We conducted a qualitative explorative case study in the software-intensive systems industry. The study has shown that architects frequently reuse decisions but are confined to decisions they already know or have heard about. The results also suggest that architects reuse decisions in an ad-hoc manner. Moreover this study presents a conceptual model of decision-reuse and lists stakeholder concerns with regards to decision-reuse. The results of this study indicate that improving the documentation and discoverability of decisions holds a large potential to increase reuse of decisions and that decision documentation is not only important for system understanding or in the context of architecture reviews but also to support architects in upcoming projects."
Research article - Investigating faults missed by test suites achieving high code coverage,"AbstractCode coverage criteria are commonly used to determine the adequacy of a test suite. However, studies investigating code coverage and fault-finding capabilities have mixed results. Some studies have shown that creating test suites to satisfy coverage criteria has a positive effect on finding faults, while other studies do not. In order to improve the fault-finding capabilities of test suites, it is essential to understand what is causing these mixed results. In this study, we investigated one possible source of variation in the results observed: fault type. Specifically, we studied 45 different types of faults and evaluated how effectively human-created test suites with high coverage percentages were able to detect each type of fault. Our results showed, with statistical significance, there were specific types of faults found less frequently than others. However, improvements in the formulation and selection of test oracles could overcome these weaknesses. Based on our results and the types of faults that were missed, we suggest focusing on the strength of test oracles along with code coverage to improve the effectiveness of test suites."
Research article - Automatic assignment of integrators to pull requests: The importance of selecting appropriate attributes,"AbstractIn open-source projects that adopt the pull-based development workflow, a core developer needs to analyze the contribution received via pull requests and decide on integrating it or not in the repository. However, this process is time-consuming, leading to an increasing number of pull requests left to be analyzed. Consequently, the assignment of suitable integrators to pull requests becomes an important step in the pull-based development workflow. Classification methods have already been used to recommend integrators, based on different sets of predictive attributes. The main contribution of this paper is to identify a set of attributes that can improve the performance of the integrator prediction task reported in the literature. To do so, we first evaluate different sets of attributes used by previous studies with different classification algorithms. Besides, we explore attribute selection strategies on an extended set of attributes composed not only by the attributes already used in the literature but also new attributes we consider relevant to the problem. Experiments with 32 open-source projects evidenced that after applying attribute selection strategies and, consequently, identifying a more suitable set of attributes, the recommendation has achieved normalized improvements 54% higher than the state-of-the-art."
Research article - A new Prefetching-aware Data Replication to decrease access latency in cloud environment,"AbstractData replication is an effective technique that decreases retrieval time, thus reducing energy consumption in Cloud. When necessary files aren't locally available, they will be fetched from remote locations that is very high-time consuming process. Therefore, it is superior to pre-replicate the popular files. Even though few previous works considered prediction-based replication strategy, the prediction is not precise at many situations and occupies the storage. To address these challenges, a new dynamic replication strategy called Prefetching-aware Data Replication (PDR) is proposed, which determines the correlation of the data files using the file access history and pre-fetches the most popular files. So, the next time that this site requires a file, it will be locally available. In addition, due to the storage space restriction, replica replacement strategy plays a vital role. PDR strategy can ascertain the importance of valuable replicas based on the fuzzy inference system with four input parameters (i.e., number of accesses, cost of replica, the last time the replica was accessed, and data availability). Extensive experiments with CloudSim show that PDR achieves high data availability, high hit ratio, low storage and bandwidth consumption. On average PDR reduces over 35% of response time when compared to the other algorithms."
Research article - Analytical metadata modeling for next generation BI systems,"AbstractBusiness Intelligence (BI) systems are extensively used as in-house solutions to support decision-making in organizations. Next generation BI 2.0 systems claim for expanding the use of BI solutions to external data sources and assisting the user in conducting data analysis. In this context, the Analytical Metadata (AM) framework defines the metadata artifacts (e.g., schema and queries) that are exploited for user assistance purposes. As such artifacts are typically handled in ad-hoc and system specific manners, BI 2.0 argues for a flexible solution supporting metadata exploration across different systems.In this paper, we focus on the AM modeling. We propose SM4AM, an RDF-based Semantic Metamodel for AM. On the one hand, we claim for ontological metamodeling as the proper solution, instead of a fixed universal model, due to (meta)data models heterogeneity in BI 2.0. On the other hand, RDF provides means for facilitating defining and sharing flexible metadata representations. Furthermore, we provide a method to instantiate our metamodel. Finally, we present a real-world case study and discuss how SM4AM, specially the schema and query artifacts, can help traversing different models instantiating our metamodel and enabling innovative means to explore external repositories in what we call metamodel-driven (meta)data exploration."
Research article - Software startup engineering: A systematic mapping study,"AbstractSoftware startups have long been a significant driver in economic growth and innovation. The on-going failure of the major number of startups calls for a better understanding of state-of-the-practice of startup activities. Objective With a focus on engineering perspective, this study aims at identifying the change in focus of research area and thematic concepts operating startup research.A systematic mapping study on 74 primary papers (in which 27 papers are newly selected) from 1994 to 2017 was conducted with a comparison with findings from previous mapping studies. A classification schema was developed, and the primary studies were ranked according to their rigour.We discovered that most research has been conducted within the SWEBOK knowledge areas software engineering process, management, construction, design, and requirements, with the shift of focus towards process and management areas. We also provide an alternative classification for future startup research. We find that the rigour of the primary papers was assessed to be higher between 2013–2017 than that of 1994–2013. We also find an inconsistency of characterizing startups.Future work can focus on certain research themes, such as startup evolution models and human aspects, and consolidate the thematic concepts describing software startups."
Research article - Threat analysis of software systems: A systematic literature review,"AbstractArchitectural threat analysis has become an important cornerstone for organizations concerned with developing secure software. Due to the large number of existing techniques it is becoming more challenging for practitioners to select an appropriate threat analysis technique. Therefore, we conducted a systematic literature review (SLR) of the existing techniques for threat analysis. In our study we compare 26 methodologies for what concerns their applicability, characteristics of the required input for analysis, characteristics of analysis procedure, characteristics of analysis outcomes and ease of adoption. We also provide insight into the obstacles for adopting the existing approaches and discuss the current state of their adoption in software engineering trends (e.g. Agile, DevOps, etc.). As a summary of our findings we have observed that: the analysis procedure is not precisely defined, there is a lack of quality assurance of analysis outcomes and tool support and validation are limited."
"Research article - Introducing TRAILS: A tool supporting traceability, integration and visualisation of engineering knowledge for product service systems development","AbstractDeveloping state of the art product service systems (PSS) requires the intense collaboration of different engineering domains, such as mechanical, software and service engineering. This can be a challenging task, since each engineering domain uses their own specification artefacts, software tools and data formats. However, to be able to seamlessly integrate the various components that constitute a PSS and also being able to provide comprehensive traceability throughout the entire solution life cycle it is essential to have a common representation of engineering data.To address this issue, we present TRAILS, a novel software tool that joins the heterogeneous artefacts, such as process models, requirements specifications or diagrams of the systems structure. For this purpose, our tool uses a semantic model integration ontology onto which various source formats can be mapped. Overall, our tool provides a wide range of features that supports engineers in ensuring traceability, avoiding system inconsistencies and putting collaborative engineering into practice. Subsequently, we show the practical implementation of our approach using the case study of a bike sharing system and discuss limitations as well as possibilities for future enhancement of TRAILS."
Research article - The next 700 CPU power models,"AbstractSoftware power estimation of CPUs is a central concern for energy efficiency and resource management in data centers. Over the last few years, a dozen of ad hoc power models have been proposed to cope with the wide diversity and the growing complexity of modern CPU architectures. However, most of these CPU power models rely on a thorough expertise of the targeted architectures, thus leading to the design of hardware-specific solutions that can hardly be ported beyond the initial settings. In this article, we rather propose a novel toolkit that uses a configurable/interchangeable learning technique to automatically learn the power model of a CPU, independently of the features and the complexity it exhibits. In particular, our learning approach automatically explores the space of hardware performance counters made available by a given CPU to isolate the ones that are best correlated to the power consumption of the host, and then infers a power model from the selected counters. Based on a middleware toolkit devoted to the implementation of software-defined power meters, we implement the proposed approach to generate CPU power models for a wide diversity of CPU architectures (including Intel, ARM, and AMD processors), and using a large variety of both CPU and memory-intensive workloads. We show that the CPU power models generated by our middleware toolkit estimate the power consumption of the whole CPU or individual processes with an accuracy of 98.5% on average, thus competing with the state-of-the-art power models."
Research article - A comparative study of workflow customization strategies: Quality implications for multi-tenant SaaS,"AbstractMulti-tenant Software-as-a-Service (SaaS) applications share a single runtime instance among multiple customer organizations (tenants). To account for differences in tenant requirements, they have to support run-time customization. Run-time customization involves a wide range of software artifacts such as user interfaces, databases, web-services and business process or workflow definitions.This paper analyzes and compares the quality implications of different business process customization strategies for multi-tenant SaaS applications. The customization strategies are selected from an existing survey and the comparison criteria are derived from two essential characteristics of SaaS: it is (i) a business model aiming at “economies of scale”, and (ii) a software delivery model with specific automation requirements.The comparative study shows that there is no single best strategy, and provides SaaS architects with support for making appropriate trade-off decisions when adopting a workflow customization strategy. As a by-product of this study, a number of points for future improvement and innovations in existing workflow technology are identified, which are exclusively relevant for multi-tenant SaaS applications."
Research article - A Systematic Mapping Study driven by the margin of error,"AbstractUntil recently, many Systematic Literature Reviews (SLRs) and Systematic Mapping Studies (SMSs) have been proposed. However, when SMS is performed on a broad topic with a large amount of primary studies, the cost of assessment of all primary studies requires unjustified resources. In this paper, a new approach is introduced for performing SMSs, called SMS driven by the margin of error. The main objective of the described work was to decrease the assessment cost of primary studies by stopping the process of classification of primary studies when enough evidence has been collected. We introduced a statistical approach with random sampling and a margin of error into the design of SMSs when a topic under discussion is broad with a large number of primary studies. In this paper, SMS driven by the margin of error was applied on three different use cases: SMS on Domain-Specific Languages, SMS on Template-based Code Generation, and SMS on Software Reliability Modeling, where it was shown that the proposed approach reduced the cost of assessing primary studies and quantified the reliability of SMS."
Research article - A systematic review on the code smell effect,"AbstractContext: Code smell is a term commonly used to describe potential problems in the design of software. The concept is well accepted by the software engineering community. However, some studies have presented divergent findings about the usefulness of the smell concept as a tool to support software development tasks. The reasons of these divergences have not been considered because the studies are presented independently. Objective: To synthesize current knowledge related to the usefulness of the smell concept. We focused on empirical studies investigating how smells impact the software development, the code smell effect. Method: A systematic review about the smell effect is carried out. We grouped the primary studies findings in a thematic map. Result: The smell concept does not support the evaluation of quality design in practice activities of software development. There is no strong evidence correlating smells and some important software development attributes, such as effort in maintenance. Moreover, the studies point out that human agreement on smell detection is low. Conclusion: In order to improve analysis on the subject, the area needs to better outline: (i) factors affecting human evaluation of smells; and (ii) a classification of types of smells, grouping them according to relevant characteristics."
Research article - A systematic literature review on the semi-automatic configuration of extended product lines,"AbstractProduct line engineering has become essential in mass customisation given its ability to reduce production costs and time to market, and to improve product quality and customer satisfaction. In product line literature, mass customisation is known as product configuration. Currently, there are multiple heterogeneous contributions in the product line configuration domain. However, a secondary study that shows an overview of the progress, trends, and gaps faced by researchers in this domain is still missing. In this context, we provide a comprehensive systematic literature review to discover which approaches exist to support the configuration process of extended product lines and how these approaches perform in practice. Extend product lines consider non-functional properties in the product line modelling. We compare and classify a total of 66 primary studies from 2000 to 2016. Mainly, we give an in-depth view of techniques used by each work, how these techniques are evaluated and their main shortcomings. As main results, our review identified (i) the need to improve the quality of the evaluation of existing approaches, (ii) a lack of hybrid solutions to support multiple configuration constraints, and (iii) a need to improve scalability and performance conditions."
Research article - A systematic mapping study on text analysis techniques in software architecture,"AbstractContextInformation from artifacts in each phase of the software development life cycle can potentially be mined to enhance architectural knowledge. Many text analysis techniques have been proposed for mining such artifacts. However, there is no comprehensive understanding of what artifacts these text analysis techniques analyze, what information they are able to extract or how they enhance architecting activities.ObjectiveThis systematic mapping study aims to study text analysis techniques for mining architecture-related artifacts and how these techniques have been used, and to identify the benefits and limitations of these techniques and tools with respect to enhancing architecting activities.MethodWe conducted a systematic mapping study and defined five research questions. We analyzed the results using descriptive statistics and qualitative analysis methods.ResultsFifty-five studies were finally selected with the following results: (1) Current text analysis research emphasizes on architectural understanding and recovery. (2) A spectrum of text analysis techniques have been used in textual architecture information analysis. (3) Five categories of benefits and three categories of limitations were identified.ConclusionsThis study shows a steady interest in textual architecture information analysis. The results give clues for future research directions on improving architecture practice through using these text analysis techniques."
Research article - Pragmatic cyber physical systems design based on parametric models,"AbstractThe adaptive nature of cyber physical systems (CPS) comes from the fact that they are deeply immersed in the physical environments that are inherently dynamic. CPS also have stringent requirements on real-time operation and safety that are fulfilled by rigorous model design and verification. In the real-time literature, adaptation is mostly limited to off-line modeling of well known and predicted transitions; but this is not appropriate for cyber physical systems as each transition can have unique and unknown characteristics. In the adaptive systems literature, adaptation solutions are silent about timely execution and about the underlying hardware possibilities that can potentially speed up execution. This paper presents a solution for designing adaptive cyber physical systems by using parametric models that are verified during the system execution (i.e., online), so that adaptation decisions are made based on the timing requirements of each particular adaptation event. Our approach allows the system to undergo timely adaptations that exploit the potential parallelism of the software and its execution over multicore processors. We exemplify the approach on a specific use case with autonomous vehicles communication, showing its applicability for situations that require time-bounded online adaptations."
Research article - Specifying uncertainty in use case models,"AbstractContextLatent uncertainty in the context of software-intensive systems (e.g., Cyber-Physical Systems (CPSs)) demands explicit attention right from the start of development. Use case modeling—a commonly used method for specifying requirements in practice, should also be extended for explicitly specifying uncertainty.ObjectiveSince uncertainty is a common phenomenon in requirements engineering, it is best to address it explicitly by identifying, qualifying, and, where possible, quantifying uncertainty at the beginning stage. The ultimate aim, though not within the scope of this paper, was to use these use cases as the starting point to create test-ready models to support automated testing of CPSs under uncertainty.MethodWe extend the Restricted Use Case Modeling (RUCM) methodology and its supporting tool to specify uncertainty as part of system requirements. Such uncertainties include those caused by insufficient domain expertise of stakeholders, disagreements among them, and known uncertainties about assumptions about the environment of the system. The extended RUCM, called U-RUCM, inherits the features of RUCM, such as automated analyses and generation of models, to mention but a few. Consequently, U-RUCM provides all the key benefits offered by RUCM (i.e., reducing ambiguities in requirements), but also, it allows specification of uncertainties with the possibilities of reasoning and refining existing ones and even uncovering unknown ones.ResultsWe evaluated U-RUCM with two industrial CPS case studies. After refining RUCM models (specifying initial requirements), by applying the U-RUCM methodology, we successfully identified and specified additional 306% and 512% (previously unknown) uncertainty requirements, as compared to the initial requirements specified in RUCM. This showed that, with U-RUCM, we were able to get a significantly better and more precise characterization of uncertainties in requirement engineering.ConclusionEvaluation results show that U-RUCM is an effective methodology (with tool support) for dealing with uncertainty in requirements engineering. We present our experience, lessons learned, and future challenges, based on the two industrial case studies."
Research article - On a pursuit for perfecting an undergraduate requirements engineering course,"AbstractRequirements Engineering (RE) is an essential component of any software development cycle. Understanding and satisfying stakeholder needs and wants is the difference between the success and failure of a product. However, RE is often perceived as a “soft” skill by students and is often ignored by students who prioritize the learning of coding, testing, and algorithmic thinking. This view contrasts with the industry, where “soft” skills are instead valued equal to any other engineering ability. A key challenge in teaching RE is that students who are accustomed to technical work have a hard time relating to something that is non-technical. Furthermore, students are rarely afforded the opportunity to practice requirements elicitation and management skills in a meaningful way while learning the RE concepts as an adjunct to other content. At Rose-Hulman, several project-based approaches have been experimented with in teaching RE, and these have evolved over time. In this paper, the progress of teaching methodologies is documented to capture the pros and cons of these varied approaches, and to reflect on what worked and what did not in teaching RE to undergraduate engineering students."
Research article - Applying a maturity model during a software engineering course – How planning and task-solving processes influence the course performance,"AbstractIn industry, the benefit of maturity models is uncontested, and models like CMMI are normally taught in at least advanced software engineering courses. However, when not being part of real-world projects, the added values are difficult to be experienced on first hand by our students. In this paper we report on a study and teaching approach where, in three successive semesters and at two different institutions, we started rating the process-maturity of students solving tasks in our software engineering courses and transparently related the maturity levels to the task performances. It turned out that not only the quality of the students’ task preparation plays a crucial role, but that there is also a non-negligible correlation between the individual process maturity and performances. Considering this finding, the approach might yield to students’ process-improvement steps during our courses, help in fostering the understanding of the term process maturity, and finally, also might help in improving the overall students’ performances."
Research article - Collaborative and teamwork software development in an undergraduate software engineering course,"AbstractTwo key elements of modern software development are collaboration and teamwork. Current methodologies (e.g., agile) and platforms are based on these key elements. This paper describes our experience in stimulating collaboration and teamwork activities of students in the context of a software engineering course at the third year of an undergraduate program in computer science at the University of Milano-Bicocca in Italy. The students were asked to develop a software project in teams of 3 to 5 students for the final exam of the course. The students used GitHub as a collaborative software development platform. In addition, they analyzed the quality of the developed software through SonarQube. The students were also asked to perform project management tasks (e.g., the Gantt) using the Microsoft Project tool. At the end of the course, we gathered the student feedback through a questionnaire on their collaboration and teamwork experience (through GitHub and Microsoft Project tools) and on the use of a software analysis assessment tool, i.e., SonarQube. From their feedback, the students were enthusiastic about working in teams for their project development and about learning how to use tools which are exploited not only in the academic world but also in industry."
Research article - Adapting agile practices in university contexts,"AbstractTeaching agile practices has found its place in software engineering curricula in many universities across the globe. As a result, educators and students have embraced different ways to apply agile practices during their courses through lectures, games, projects, workshops and more for effective theoretical and practical learning. Practicing agile in university contexts comes with challenges for students and to counter these challenges, they perform some adaptations to standard agile practices making them effective and easier to use in university contexts. This study describes the constraints the students faced while applying agile practices in a university course taught at the University of Auckland, including difficulty in setting up common time for all team members to work together, limited availability of customer due to busy schedule and the modifications the students introduced to adapt agile practices to suit the university context, such as daily stand-ups with reduced frequency, combining sprint meetings, and rotating scrum master from team. In addition, it summarizes the effectiveness of these modifications based on reflection of the students. Recommendations for educators and students are also provided. Our findings and recommendations will help educators and students better coordinate and apply agile practices on industry-based projects in university contexts."
Research article - Bridging the gap between awareness and trust in globally distributed software teams,"AbstractTrust remains a key challenge for globally distributed teams despite decades of research. Awareness, a key component of collaboration, has even more research around it. However, detailed accounts of the interrelationship of awareness and trust are still lacking in the literature, particularly in the setting of software teams. The gap we seek to fill with this article is to examine how software tool support for awareness can engender trust among globally distributed software developers. We highlight qualitative results from a previous and extensive field study that shows how trust is still a problem in contemporary teams. These results motivate a specific examination of how developers form attributions of one another. We describe a collection of visualization widgets designed to address the specific issues found in the field. To evaluate their effectiveness, we performed a controlled laboratory study with 28 students and 12 professional software developers who used these visualizations collected into a tool environment called Theseus. The results show that in general, participants using the visualizations make more accurate attributions, and their perceived trustworthiness of their remote teammates more accurately reflects actual circumstances. We conclude with a discussion of the implications of our results for theory and practice."
Research article - Self-adaptation of service compositions through product line reconfiguration,"AbstractThe large number of published services has motivated the development of tools for creating customized composite services known as service compositions. While service compositions provide high agility and development flexibility, they can also pose challenges when it comes to delivering guaranteed functional and non-functional requirements. This is primarily due to the highly dynamic environment in which services operate. In this paper, we propose adaptation mechanisms that are able to effectively maintain functional and non-functional quality requirements in service compositions derived from software product lines. Unlike many existing work, the proposed adaptation mechanism does not require explicit user-defined adaptation strategies. We adopt concepts from the software product line engineering paradigm where service compositions are viewed as a collection of features and adaptation happens through product line reconfiguration. We have practically implemented the proposed mechanism in ourMagus tool suite and performed extensive experiments, which show that our work is both practical and efficient for automatically adapting service compositions once violations of functional or non-functional requirements are observed."
Research article - Runtime management and quantitative evaluation of changing system goals in complex autonomous systems,"AbstractA key challenge in cyber-physical systems (CPS) design is their highly dynamic nature including runtime changes of system goals. Additional safety regulations or changed priorities may apply, e.g. (temporarily) focusing on safety goals after some incident occurred. Goal-aware CPS continuously evaluate goal achievement and autonomously perform adaptations for re-achievement at runtime. For complex system goals capturing dependencies, priorities, and conflicts, efficient goal evaluation techniques are required. To enable a fine-grained balancing of the cost-benefit ratio of autonomous decisions at runtime, a qualitative evaluation of goals is not sufficient. We provide an algorithm that efficiently calculates the quantitative “distance” between a system state and the system goals. We organise various goal types, their parent-children-relationships, context-dependent importances, and dependency relations in a hierarchical goal model. Due to its modular structure, goals can easily be added, removed, and changed at runtime. We illustrate our approach with an exemplary autonomous air drone delivery system and discuss it based on illustrative example scenarios. We argue that our approach enables a) the design of complex context-dependent quantitative goal models for autonomous goal-aware systems, b) the measurement of the impact of autonomous decisions at runtime, and c) the efficient runtime management of changing system goals."
Research article - Strategies for managing power relationships in software ecosystems,"AbstractBuilding a software ecosystem provides companies with business benefits as well as share risks and costs with a network of partners. The ability to establish successful partnerships with other companies can influence the success or failure of the ecosystem. Companies use power to build alliances and strengthen their position in the ecosystem. However, the inappropriate use of power may create tensions that threaten partnerships. To explore the dynamics of power and dependence in software ecosystems, we conducted three case studies of ecosystems formed by small-to-medium enterprises. As a result, we present a set of hypotheses that explain the effects of power on software ecosystems. As theoretical contribution, we present a meta-model that integrates concepts from software ecosystems literature with constructs from classical power theories. Our practical contribution is a set of strategies that companies can employ to manage power relationships with partners, so that their ecosystems can evolve in a healthy and prosperous manner. By obtaining an understanding of the occurrence of power and dependence, companies can recognise how to exercise power and deal with the power from partners in order to leverage their relationships."
Research article - A systematic identification of consistency rules for UML diagrams,"AbstractUML diagrams describe different views of one software. These diagrams strongly depend on each other and must therefore be consistent with one another, since inconsistencies between diagrams may be a source of faults during software development activities that rely on these diagrams. It is therefore paramount that consistency rules be defined and that inconsistencies be detected, analyzed and fixed. The relevant literature shows that authors typically define their own consistency rules, sometimes defining the same rules and sometimes defining rules that are already in the UML standard. The reason might be that no consolidated set of rules that are relevant by authors can be found to date. The aim of our research is to provide an up to date, consolidated set of UML consistency rules and obtain a detailed overview of the current research in this area. We therefore followed a systematic procedure in order to collect from literature up to March 2017 and analyze UML consistency rules. We then consolidated a set of 119 UML consistency rules (avoiding redundant definitions or definitions already in the UML standard), which can be used as an important reference for UML-based software development activities, for teaching UML-based software development, and for further research."
Research article - Artifact-based vs. human-perceived understandability and modifiability of refactored business processes: An experiment,"AbstractBusiness processes modeling has proven to be effective and reverse engineering techniques with which to recover business process models when they are missing or outmoded have therefore emerged. Regrettably, these techniques often lead to models with quality flaws and consequently to models with low levels of understandability and modifiability. Refactoring has been widely used to deal with such flaws, altering the internal structure of models while preserving their semantics. There are several studies concerning how understandability and modifiability are affected by refactoring in terms of several artifact-based measures. However, there is little evidence regarding how refactoring affects quality in terms of human-perceived measures. The goals of this paper are, therefore: to collect further empirical evidence about the influence of refactoring on understandability and modifiability of business process models and to investigate the correlation between artifact-based understandability and modifiability and human-perceived ones. The obtained results are not trivial and show that business process obtained by means of reverse engineering has recurrent quality flaws, and the understandability and modifiability of business process models cannot be assessed by using artifact-based measures only. Human-perceived measures need to be taken in to consideration in order to have a more accurate evaluation."
Research article - A systematic literature review of software visualization evaluation,"AbstractContext:Software visualizations can help developers to analyze multiple aspects of complex software systems, but their effectiveness is often uncertain due to the lack of evaluation guidelines.Objective: We identify common problems in the evaluation of software visualizations with the goal of formulating guidelines to improve future evaluations.Method:We review the complete literature body of 387 full papers published in the SOFTVIS/VISSOFT conferences, and study 181 of those from which we could extract evaluation strategies, data collection methods, and other aspects of the evaluation.Results:Of the proposed software visualization approaches, 62% lack a strong evaluation. We argue that an effective software visualization should not only boost time and correctness but also recollection, usability, engagement, and other emotions.Conclusion:We call on researchers proposing new software visualizations to provide evidence of their effectiveness by conducting thorough (i) case studies for approaches that must be studied in situ, and when variables can be controlled, (ii) experiments with randomly selected participants of the target audience and real-world open source software systems to promote reproducibility and replicability. We present guidelines to increase the evidence of the effectiveness of software visualization approaches, thus improving their adoption rate."
Research article - Early software defect prediction: A systematic map and review,"AbstractContextSoftware defect prediction is a trending research topic, and a wide variety of the published papers focus on coding phase or after. A limited number of papers, however, includes the prior (early) phases of the software development lifecycle (SDLC).ObjectiveThe goal of this study is to obtain a general view of the characteristics and usefulness of Early Software Defect Prediction (ESDP) models reported in scientific literature.MethodA systematic mapping and systematic literature review study has been conducted. We searched for the studies reported between 2000 and 2016. We reviewed 52 studies and analyzed the trend and demographics, maturity of state-of-research, in-depth characteristics, success and benefits of ESDP models.ResultsWe found that categorical models that rely on requirement and design phase metrics, and few continuous models including metrics from requirements phase are very successful. We also found that most studies reported qualitative benefits of using ESDP models.ConclusionWe have highlighted the most preferred prediction methods, metrics, datasets and performance evaluation methods, as well as the addressed SDLC phases. We expect the results will be useful for software teams by guiding them to use early predictors effectively in practice, and for researchers in directing their future efforts."
Research article - Supporting end users to control their smart home: design implications from a literature review and an empirical investigation,"AbstractDesigning tools that allow end users to easily control and manage a smart home is a critical issue that researchers in Ambient Intelligence and Internet of Things have to address. Because of the variety of available solutions, with their advantages and limitations, it is not straightforward to understand which are the requirements that must be satisfied to effectively support end users. This paper aims to contribute to this topic through a systematic and rigorous activity based on two main pillars of the empirical research in software engineering: i) a literature review addressing design and evaluation of tools for smart home control oriented to end users, and ii) an experimental study in which three tools, that emerged from the literature review as the most suitable and widespread, were compared in order to identify the interaction mechanisms that end users appreciate most. On the basis of the obtained results, a set of design implications that may drive the development of future tools for smart home control and management are presented."
Research article - Increasing test efficiency by risk-driven model-based testing,"AbstractWe introduce an approach and a tool, RIMA, for adapting test models used for model-based testing to augment information regarding failure risk. We represent test models in the form of Markov chains. These models comprise a set of states and a set of state transitions that are annotated with probability values. These values steer the test case generation process, which aims at covering the most probable paths. RIMA refines these models in 3 steps. First, it updates transition probabilities based on a collected usage profile. Second, it updates the resulting models based on fault likelihood at each state, which is estimated based on static code analysis. Third, it performs updates based on error likelihood at each state, which is estimated with dynamic analysis. The approach is evaluated with two industrial case studies for testing digital TVs and smart phones. Results show that the approach increases test efficiency by revealing more faults in less testing time."
