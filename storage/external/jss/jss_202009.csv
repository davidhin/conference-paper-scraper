title,abstract
Research article - The effect of multiple developers on structural attributes: A Study based on java software,"AbstractContextLong-term software projects employ different software developers who collaborate on shared artifacts. The accumulation of changes pushed by different developers leave traces on the underlying code, that have an effect on its future maintainability, and even reuse.ObjectiveThis study focuses on the how the changes by different developers might have an impact on the code: we investigate whether the work of multiple developers, and their experience, have a visible effect on the structural metrics of the underlying code.MethodWe consider nine object-oriented (OO) attributes and we measure them in a GitHub sample containing the top 200 ‘forked’ projects. For each of their classes, we evaluated the number of distinct developers contributing to its source code, and their experience in the project.ResultsWe show that the presence of multiple developers working on the same class has a visible effect on the chosen OO metrics, and often in the opposite direction to what the guidelines for each attribute suggest. We also show how the relative experience of developers in a project plays an important role in the distribution of those metrics, and the future maintenance of the Java classes.ConclusionsOur results show how distributed development has an effect on the structural attributes of a software system and how the experience of developers plays a fundamental role in that effect. We also discover workarounds and best practices in 4 applied case studies."
Research article - Achieving agility and quality in product development - an empirical study of hardware startups,"AbstractContext: Startups aim at scaling their business, often by developing innovative products with limited human and financial resources. The development of software products in the startup context is known as opportunistic, agility-driven, and with high tolerance for technical debt. The special context of hardware startups calls for a better understanding of state-of-the-practice of hardware startups’ activities. Objective: This study aimed to identify whether and how startups can achieve product quality while maintaining focus on agility. Method: We conducted an exploratory study with 13 hardware startups, collecting data through semi-structured interviews and analysis of documentation. We proposed an integrative model of agility and quality in hardware startups. Results: Agility in hardware startups is complex and not achieved through adoption of fast-paced development practices alone. Hardware startups follow a quality-driven approach for development of core components, where frequent user testing is a measure for early debt management. Hardware startups often lack mindset and strategies for achieving long-term quality in early stages. Conclusions: Hardware startups need attention to hardware quality to allow for evolutionary prototyping and speed. Future research should focus on defining quality-driven practices that contribute to agility, and strategies and mindsets to support long-term quality in the hardware startup context."
Research article - Multilevel analysis of the java virtual machine based on kernel and userspace traces,"AbstractPerformance analysis of Java applications requires a deep understanding of the Java virtual machine and the system on which it is running. An unexpected latency can be caused by a bug in the source code, a misconfiguration or an external factor like CPU or disk contention. Existing tools have difficulties finding the root cause of some latencies because they do not efficiently collect performance data from the different layers of the system. In this paper, we propose a multilevel analysis framework that uses Kernel and userspace tracing to help developers understand and evaluate the performance of their applications. Kernel tracing is used to gather information about thread scheduling, system calls, I/O operations, etc. and userspace tracing is used to monitor the internal components of the JVM such as the garbage collectors and the JIT compilers. By bridging the gap between kernel and userspace traces, our tool provides full visibility to developers and helps them diagnose difficult performance issues. We show the usefulness of our approach by using it to detect problems in different Java applications."
Research article - Integrating GitLab metrics into coursework consultation sessions in a software engineering course,"AbstractSoftware developers use version control systems for collaborative coding. These systems are integrated into several software development platforms (including GitLab and GitHub) which support additional software engineering functionalities. Using these platforms in an educational context allows students to gain skills relevant to industry, whilst providing a means of keeping track of their activities. In this paper, we investigate the effect of presenting teams of students with GitLab metrics about their performance at coursework consultation sessions (checkpoint sessions), with a particular focus on the number of issues assigned and completed, and the number of commits made to the repository. A comparative analysis of project marks in two consecutive academic years indicates that these checkpoint sessions may lead to better student outcomes. An interview study with students and teaching assistants identified viewing the GitLab metrics in the checkpoints as an opportunity to see the relative contributions of team members and address resulting issues, and as a catalyst for improving engagement with the team project. The study also identified drawbacks of using the metrics too simplistically, and suggested that it was important to consider the quality and amount of written code, as well as the number of times someone committed to the repository."
"Research article - General framework, opportunities and challenges for crowdsourcing techniques: A Comprehensive survey","AbstractCrowdsourcing, a distributed human problem-solving paradigm is an active research area which has attracted significant attention in the fields of computer science, business, and information systems. Crowdsourcing holds novelty with advantages like open innovation, scalability, and cost-efficiency. Although considerable research work is performed, however, a survey on the crowdsourcing process-technology has not been divulged yet. In this paper, we present a systematic survey of crowdsourcing in focussing emerging techniques and approaches for improving conventional and developing future crowdsourcing systems. We first present a simplified definition of crowdsourcing. Then, we propose a framework based on three major components, synthesize a wide spectrum of existing studies for various dimensions of the framework. According to the framework, we first introduce the initialization step, including task design, task settings, and incentive mechanisms. Next, in the implementation step, we look into task decomposition, crowd and platform selection, and task assignment. In the last step, we discuss different answer aggregation techniques, validation methods and reward tactics, and reputation management. Finally, we identify open issues and suggest possible research directions for the future."
Research article - The influence of Technical Debt on software developer morale,"AbstractContextPrevious research in the Technical Debt (TD) field has mainly focused on the technical and economic aspects, while its human aspect has received minimal attention.ObjectiveThis paper aims to understand how software developers’ morale is influenced by TD and how their morale is influenced by TD management activities. Furthermore, this study correlates the morale with the amount of wastage of time due to TD.MethodFirstly, we conducted 15 interviews with professionals, and, secondly, these data were complemented with a survey. Thirdly, we collected 473 data points from 43 developers reporting their amount of wasted time. The collected data were analyzed using both quantitative and qualitative techniques, including thematic and statistical analysis.ResultsOur results show that the occurrence of TD is associated with a lack of progress and waste of time. This might have a negative influence on developers’ morale. Further, management of TD seems to have a positive influence on developers’ morale.ConclusionsThe results highlight the effects TD has on practitioners’ software work. This study presents results indicating that software suffering from TD reduces developers’ morale and thereby also their productivity. However, our results also indicate that TD management increases developers’ morale and developer productivity."
Research article - Code smells and refactoring: A tertiary systematic review of challenges and observations,"AbstractRefactoring and smells have been well researched by the software-engineering research community these past decades. Several secondary studies have been published on code smells, discussing their implications on software quality, their impact on maintenance and evolution, and existing tools for their detection. Other secondary studies addressed refactoring, discussing refactoring techniques, opportunities for refactoring, impact on quality, and tools support.In this paper, we present a tertiary systematic literature review of previous surveys, secondary systematic literature reviews, and systematic mappings. We identify the main observations (what we know) and challenges (what we do not know) on code smells and refactoring. We perform this tertiary review using eight scientific databases, based on a set of five research questions, identifying 40 secondary studies between 1992 and 2018.We organize the main observations and challenges about code smell and their refactoring into: smells definitions, most common code-smell detection approaches, code-smell detection tools, most common refactoring, and refactoring tools. We show that code smells and refactoring have a strong relationship with quality attributes, i.e., with understandability, maintainability, testability, complexity, functionality, and reusability. We argue that code smells and refactoring could be considered as the two faces of a same coin. Besides, we identify how refactoring affects quality attributes, more than code smells. We also discuss the implications of this work for practitioners, researchers, and instructors. We identify 13 open issues that could guide future research work.Thus, we want to highlight the gap between code smells and refactoring in the current state of software-engineering research. We wish that this work could help the software-engineering research community in collaborating on future work on code smells and refactoring."
Research article - A refinement checking based strategy for component-based systems evolution,"AbstractWe propose inheritance and refinement relations for a CSP-based component model (BRIC), which supports a constructive design based on composition rules that preserve classical concurrency properties such as deadlock freedom. The proposed relations allow extension of functionality, whilst preserving behavioural properties. A notion of extensibility is defined on top of a behavioural relation called convergence, which distinguishes inputs from outputs and the context where they are communicated, allowing extensions to reuse existing events with different purposes. We mechanise the strategy for extensibility verification using the FDR4 tool, and illustrate our results with an autonomous healthcare robot case study."
Research article - Model-based testing of software product lines: Mapping study and research roadmap,"AbstractModel-Based Testing (MBT) has been successfully applied to Software Product Lines (SPL). This paper provides a panorama of state-of-the-art on MBT of SPLs. We performed a systematic mapping for answering questions related with domains, approaches, solution types, variability, test case automation, artifacts, and evaluation. We built a roadmap from 44 selected studies. Main obtained results are: Software and Automotive domains are most considered; Black-box testing is widely performed; most studies have fully-automated support; variability is considered in most studies; Finite State Machines is the most used model to test SPLs; Behavioral-based and Scenario-based are the most used models; Case Studies and Experiments are used to evaluate MBT solutions and the majority is performed in industrial environments; traceability is not widely explored for MBT solutions. Furthermore, we provide a roadmap synthesizing studies based on used models, more formal artifacts, supporting tools, variability management, (semi-)automation, and traceability. The roadmap contributes to identify related primary studies based on given artifacts, variability management, tools, automation, and traceability techniques and to identify, from a given primary study, which artifacts, tools, variability management, automation and traceability techniques are related. Therefore, the roadmap serves as a guide to researchers and practitioners on how to model-based test SPLs."
Research article - When to update systematic literature reviews in software engineering,"Abstract[Context] Systematic Literature Reviews (SLRs) have been adopted by the Software Engineering (SE) community for approximately 15 years to provide meaningful summaries of evidence on several topics. Many of these SLRs are now potentially outdated, and there are no systematic proposals on when to update SLRs in SE. [Objective] The goal of this paper is to provide recommendations on when to update SLRs in SE. [Method] We evaluated, using a three-step approach, a third-party decision framework (3PDF) employed in other fields, to decide whether SLRs need updating. First, we conducted a literature review of SLR updates in SE and contacted the authors to obtain their feedback relating to the usefulness of the 3PDF within the context of SLR updates in SE. Second, we used these authors’ feedback to see whether the framework needed any adaptation; none was suggested. Third, we applied the 3PDF to the SLR updates identified in our literature review. [Results] The 3PDF showed that 14 of the 20 SLRs did not need updating. This supports the use of a decision support mechanism (such as the 3PDF) to help the SE community decide when to update SLRs. [Conclusions] We put forward that the 3PDF should be adopted by the SE community to keep relevant evidence up to date and to avoid wasting effort with unnecessary updates."
Research article - CODE reuse in practice: Benefiting or harming technical debt,"AbstractDuring the last years the TD community is striving to offer methods and tools for reducing the amount of TD, but also understand the underlying concepts. One popular practice that still has not been investigated in the context of TD, is software reuse. The aim of this paper is to investigate the relation between white-box code reuse and TD principal and interest. In particular, we target at unveiling if the reuse of code can lead to software with better levels of TD. To achieve this goal, we performed a case study on approximately 400 OSS systems, comprised of 897 thousand classes, and compare the levels of TD for reused and natively-written classes. The results of the study suggest that reused code usually has less TD interest; however, the amount of principal in them is higher. A synthesized view of the aforementioned results suggest that software engineers shall opt to reuse code when necessary, since apart from the established reuse benefits (i.e., cost savings, increased productivity, etc.) are also getting benefits in terms of maintenance. Apart from understanding the phenomenon per se, the results of this study provide various implications to research and practice."
Research article - WARDER: Towards effective spreadsheet defect detection by validity-based cell cluster refinements,"AbstractNowadays spreadsheets are very popular and being widely used. However, they can be prone to various defects and cause severe consequences when end users poorly maintain them. Our research communities have proposed various techniques for automated detection of spreadsheet defects, but they commonly fall short of effectiveness, either due to their limited scope or relying on strict patterns. In this article, we discuss and improve one state-of-the-art technique, CUSTODES, which exploits spreadsheet cell clustering and defect detection to extend its scope and make its detection patterns adaptive to varying spreadsheet styles. Still, CUSTODES can be prone to problematic clustering when accidentally involving irrelevant cells, leading to a largely reduced detection precision. Regarding this, we present WARDER to refine CUSTODES’s spreadsheet cell clustering based on three extensible validity-based properties. Experimental results show that WARDER could improve the precision by 19.1% on spreadsheet cell clustering, which contributed to a precision improvement of 23.3 ~ 24.3% for spreadsheet defect detection, as compared to CUSTODES (F-measure increased from 0.71 to 0.79 ~ 0.82). WARDER also exhibited satisfactory results on another practical large-scale spreadsheet corpus VEnron2, improving the defect detection precision by 10.7 ~ 21.2% over CUSTODES."
Research article - An extensive study of class-level and method-level test case selection for continuous integration,"AbstractContinuous Integration (CI) is an important practice in agile development. With the growth of integration system, running all tests to verify the quality of submitted code, is clearly uneconomical. This paper aims at selecting a proper test subset for continuous integration so as to reduce test cost as much as possible without sacrificing quality. We first propose a static test case selection framework Sapient which aims at selecting a precise test subset towards fully covering all changed code and affected code. There are four major steps: locate changes, build dependency graphs, identify related tests by searching dependency graphs, and extend tests. Based on Sapient, we then develop a class-level test case selection approach FEST and a method-level approach MEST. FEST captures the class-level dependencies, especially the hidden references, which facilitates the modeling of full dependency relations at the class level. MEST combines two dynamic execution rules (i.e., dynamic invocation in reflection and dynamic binding in inheritance) with static dependencies to support building the precise method-level dependencies. Evaluation is conducted on 18 open source projects with 261 continuous integration versions from Eclipse and Apache communities. Results show that both FEST and MEST can detect almost all faults of two baselines (i.e., actual CI testing and ClassSRTS), and find new faults compared with actual CI testing (in 25% and 26% versions) and ClassSRTS (in 18% and 27% versions) respectively. Furthermore, MEST outperforms FEST in reduced test size, fault detection efficiency and test cost, indicating method-level test case selection can be more effective than class-level selection. This study can further speed up the feedback and improve the fault detection efficiency of CI testing, and has good application prospects for the CI testing in large-scale and complex systems."
Research article - An automatic software vulnerability classification framework using term frequency-inverse gravity moment and feature selection,"AbstractVulnerability classification is an important activity in software development and software quality maintenance. A typical vulnerability classification model usually involves a stage of term selection, in which the relevant terms are identified via feature selection. It also involves a stage of term-weighting, in which the document weights for the selected terms are computed, and a stage for classifier learning. Generally, the term frequency-inverse document frequency (TF-IDF) model is the most widely used term-weighting metric for vulnerability classification. However, several issues hinder the effectiveness of the TF-IDF model for document classification. To address this problem, we propose and evaluate a general framework for vulnerability severity classification using the term frequency-inverse gravity moment (TF-IGM). Specifically, we extensively compare the term frequency-inverse gravity moment, term frequency-inverse document frequency, and information gain feature selection using five machine learning algorithms on ten vulnerable software applications containing a total number of 27,248 security vulnerabilities. The experimental result shows that: (i) the TF-IGM model is a promising term weighting metric for vulnerability classification compared to the classical term-weighting metric, (ii) the effectiveness of feature selection on vulnerability classification varies significantly across the studied datasets and (iii) feature selection improves vulnerability classification."
Research article - ASPLe: A methodology to develop self-adaptive software systems with systematic reuse,"AbstractMore than two decades of research have demonstrated an increasing need for software systems to be self-adaptive. Self-adaptation manages runtime dynamics, which are difficult to predict before deployment. A vast body of knowledge to develop Self-Adaptive Software Systems (SASS) has been established. However, we discovered a lack of process support to develop self-adaptive systems with reuse. The lack of process support may hinder knowledge transfer and quality design. To that end, we propose a domain-engineering based methodology, Autonomic Software Product Lines engineering (ASPLe), which provides step-by-step guidelines for developing families of SASS with systematic reuse. The evaluation results from a case study show positive effects on quality and reuse for self-adaptive systems designed using the ASPLe compared to state-of-the-art engineering practices."
Research article - Identifying vulnerabilities of SSL/TLS certificate verification in Android apps with static and dynamic analysis,"AbstractMany Android developers fail to properly implement SSL/TLS during the development of an app, which may result in Man-In-The-Middle (MITM) attacks or phishing attacks. In this work, we design and implement a tool called DCDroid to detect these vulnerabilities with the combination of static and dynamic analysis. In static analysis, we focus on four types of vulnerable schema and locate the potential vulnerable code snippets in apps. In dynamic analysis, we prioritize the triggering of User Interface (UI) components based on the results obtained with static analysis to confirm the misuse of SSL/TLS. With DCDroid we analyze 2213 apps from Google Play and 360app. The experimental results show that 457 (20.65%) apps contain potential vulnerable code. We run apps with DCDroid on two Android smart phones and confirm that 245 (11.07%) of 2213 apps are truly vulnerable to MITM and phishing attacks. We propose several strategies to reduce the number of crashes and shorten the execution time in dynamic analysis. Comparing with our previous work, DCDroid decreases 57.18% of the number of apps’ crash and 32.47% of the execution time on average. It also outperforms other three tools, namely, AndroBugs, kingkong and appscan, in terms of detection accuracy."
Research article - Uncertainty modeling and runtime verification for autonomous vehicles driving control: A machine learning-based approach,"AbstractIntelligent Transportation Systems (ITS) are attracting much attention from the industry, academia, and government in staging the new generation of transportation. In the coming years, the human-driven vehicles and autonomous vehicles would co-exist for a long time in uncertain environments. How to efficiently control the autonomous vehicle and improve the interaction accuracy as well as the human drivers’ safety is a hot topic for the autonomous industry. The safety-critical nature of the ITSs demands the system designers to provide provably correct guarantees about the actions, models, control, and performance. To model and recognize the drivers’ behavior, we use machine learning classification algorithms based on the data we get from the uncertain environments. We define a parameterized modeling language stohChart(p) (parameterized stochastic hybrid statecharts) to describe the interactions of agents in ITSs. The learning result of the driver behavior classification is transferred to stohChart(p) as the parameters timely. Then we propose a mapping algorithm to transform stohChart(p) to NPTA (Networks of Probabilistic Timed Automata) and use the statistical model checker UPPAAL-SMC to verify the quantitative properties. So the run-time verification method can help autonomous vehicles make “more intelligent” decisions at run-time. We illustrate our approach by modeling and analyzing a scenario of the autonomous vehicle try to change to a lane occupied by a human-driven car."
Research article - Transformed k-nearest neighborhood output distance minimization for predicting the defect density of software projects,"AbstractBackgroundSoftware defect prediction is one of the most important research topics in software engineering. An important product measure to determine the effectiveness of software processes is the defect density (DD). Cased-based reasoning (CBR) has been the prediction technique most widely applied in the software prediction field. The CBR involves k-nearest neighborhood for finding the number (k) of similar software projects selected to be involved in the prediction process.ObjectiveTo propose the application of a transformed k-nearest neighborhood output distance minimization (TkDM) algorithm to predict the DD of software projects to compare its prediction accuracy with those obtained from statistical regression, support vector regression, and neural networks.MethodData sets were obtained from the ISBSG release 2018. A leave-one-out cross validation method was performed. Absolute residual was used as the prediction accuracy criterion for models.ResultsStatistical significance tests among models showed that the TkDM had the best prediction accuracy than those ones from statistical regression, support vector regression, and neural networks.ConclusionsA TkDM can be used for predicting the DD of new and enhanced software projects developed and coded in specific platforms and programming languages types."
