title,abstract
Research article - Are unit and integration test definitions still valid for modern Java projects? An empirical study on open-source projects,"AbstractContextUnit and integration testing are popular testing techniques. However, while the software development context evolved over time, the definitions remained unchanged. There is no empirical evidence, if these commonly used definitions still fit to modern software development.ObjectiveWe analyze, if the existing standard definitions of unit and integration tests are still valid in modern software development contexts. Hence, we analyze if unit and integration tests detect different types of defects, as expected from the standard literature.MethodWe classify 38,782 test cases into unit and integration tests according to the definition of the IEEE and use mutation testing to assess their defect detection capabilities. All integrated mutations are classified into five different defect types. Afterwards, we evaluate if there are any statistically significant differences in the results between unit and integration tests.ResultsWe could not find any evidence that one test type is more capable of detecting certain defect types than the other one. Our results suggest that the currently used definitions do not fit modern software development contexts.ConclusionsThis finding implies that we need to reconsider the definitions of unit and integration tests and suggest that the current property-based definitions may be exchanged with usage-based definitions."
Research article - HSP: A hybrid selection and prioritisation of regression test cases based on information retrieval and code coverage applied on an industrial case study,"AbstractThe usual way to guarantee quality of software products is via testing. This paper presents a novel strategy for selection and prioritisation of Test Cases (TC) for Regression testing. In the lack of code artifacts from where to derive Test Plans, this work uses information conveyed by textual documents maintained by Industry, such as Change Requests. The proposed process is based on Information Retrieval techniques combined with indirect code coverage measures to select and prioritise TCs. The aim is to provide a high coverage Test Plan which would maximise the number of bugs found. This process was implemented as a prototype tool which was used in a case study with our industrial partner (Motorola Mobility). Experiments results revealed that the combined strategy provides better results than the use of information retrieval and code coverage independently. Yet, it is worth mentioning that any of these automated options performed better than the previous manual process deployed by our industrial partner to create test plans."
Research article - Automatic method change suggestion to complement multi-entity edits,"AbstractWhen maintaining software, developers sometimes change multiple program entities (i.e., classes, methods, and fields) to fulfill one maintenance task. We call such complex changes multi-entity edits. Consistently and completely applying multi-entity edits can be challenging, because (1) the changes scatter in different entities and (2) the incorrectly edited code may not trigger any compilation or runtime error. This paper introduces CMSuggester, an approach to suggest complementary changes for multi-entity edits. Given a multi-entity edit that (i) adds a new field or method and (ii) modifies one or more methods to access the field or invoke the method, CMSuggester suggests other methods to co-change for the new field access or method invocation. The design of CMSuggester is motivated by our preliminary study, which reveals that co-changed methods usually access existing fields or invoke existing methods in common.Our evaluation shows that based on common field accesses, CMSuggester recommended method changes in 463 of 685 tasks with 70% suggestion accuracy; based on common method invocations, CMSuggester handled 557 of 692 tasks with 70% accuracy. Compared with prior work ROSE, TARMAQ, and Transitive Association Rules (TAR), CMSuggester recommended more method changes with higher accuracy. Our research can help developers correctly apply multi-entity edits."
"Research article - Exploring onboarding success, organizational fit, and turnover intention of software professionals","AbstractThe IT sector struggles with talent acquisition and low retention rates. While several field studies have explored onboarding of software developers, the software engineering literature lacks studies that develop and evaluate theoretical models. This study seeks to explore the link between onboarding of new hires and turnover intention of these professionals. In particular, we develop a theoretical model that identifies a number of onboarding activities, and link these to onboarding success. We then look at what we have termed “organizational fit,” which we define as two aspects of software professionals, namely job satisfaction and the quality of their relationships on the workfloor, and investigate how these mediate the relation between short-term onboarding success and a longer-term intention to leave (or stay with) an organization. We test our model with a sample of 102 software professionals using PLS-SEM. The findings suggest that providing support to new hires plays a major role in onboarding success, but that training is less important. Further, we found that job satisfaction mediates the relationship between onboarding success and turnover intention, but workplace relationship quality does not. Based on the findings, we discuss a number of implications for practice and suggestions for future research."
Research article - Effects of contextual information on maintenance effort: A controlled experiment,"AbstractThere has been an increased focus on context-aware tools in software engineering. Within this area, an important challenge is to define and model the context for software-development projects and software development in general. This article reports a controlled experiment that compares the effort to implement changes, the correctness and the maintainability of an existing application between two projects; one that uses qualitative dashboards depicting contextual information, and one that does not. The results of this controlled experiment suggest that the usage of qualitative dashboards improves the correctness during the software maintenance activities and reduces the effort to implement these activities."
Research article - Effective testing of Android apps using extended IFML models,"AbstractThe last decade has seen a vast proliferation of mobile apps. To improve the reliability of such apps, various techniques have been developed to automatically generate tests for them. While such techniques have been proven to be useful in producing test suites that achieve significant levels of code coverage, there is still enormous demand for techniques that effectively generate tests to exercise more code and detect more bugs of apps.We propose in this paper the Adamant approach to automated Android app testing. Adamant utilizes models that incorporate valuable human knowledge about the behaviours of the app under consideration to guide effective test generation, and the models are encoded in an extended version of the Interaction Flow Modeling Language (IFML).In an experimental evaluation on 10 open source Android apps, Adamant generated over 130 test actions per minute, achieved around 68% code coverage, and exposed 8 real bugs, significantly outperforming other test generation tools like Monkey, AndroidRipper, and Gator in terms of code covered and bugs detected."
Research article - A survey on clone refactoring and tracking,"AbstractCode clones, identical or nearly similar code fragments in a software system’s code-base, have mixed impacts on software evolution and maintenance. Focusing on the issues of clones researchers suggest managing them through refactoring, and tracking. In this paper we present a survey on the state-of-the-art of clone refactoring and tracking techniques, and identify future research possibilities in these areas. We define the quality assessment features for the clone refactoring and tracking tools, and make a comparison among these tools considering these features. To the best of our knowledge, our survey is the first comprehensive study on clone refactoring and tracking. According to our survey on clone refactoring we realize that automatic refactoring cannot eradicate the necessity of manual effort regarding finding refactoring opportunities, and post refactoring testing of system behaviour. Post refactoring testing can require a significant amount of time and effort from the quality assurance engineers. There is a marked lack of research on the effect of clone refactoring on system performance. Future investigations in this direction will add much value to clone refactoring research. We also feel the necessity of future research towards real-time detection, and tracking of code clones in a big-data environment."
Research article - StaDART: Addressing the problem of dynamic code updates in the security analysis of android applications,"AbstractDynamic code update techniques (Android Studio – support for dynamic delivery), such as dynamic class loading and reflection, enable Android apps to extend their functionality at runtime. At the same time, these techniques are misused by malware developers to transform a seemingly benign app into a malware, once installed on a real device. Among the corpus of evasive techniques used in modern real-world malware, evasive usage of dynamic code updates plays a key role.First, we demonstrate the ineffectiveness of existing tools to analyze apps in the presence of dynamic code updates using our test apps, i.e., Reflection-Bench and InboxArchiver. Second, we present StaDART, combining static and dynamic analysis of Android apps to reveal the concealed behavior of malware. StaDART performs dynamic code interposition using a vtable tampering technique for API hooking to avoid modifications to the Android framework. Furthermore, we integrate it with a triggering solution, DroidBot, to make it more scalable and fully automated. We present our evaluation results with a dataset of 2000 real world apps; containing 1000 legitimate apps and 1000 malware samples. The evaluation results with this dataset and Reflection-Bench show that StaDART reveals suspicious behavior that is otherwise hidden to static analysis tools."
Research article - Affect Recognition in Code Review: An In-situ Biometric Study of Reviewer’s Affect,"AbstractCode reviews are an important practice in software development that increases team productivity and improves product quality. They are also examples of remote, computer-mediated asynchronous communications which are prone to the loss of affective information. Prior research has focused on sentiment analysis in source codes, as positive affect has been linked to developer productivity. Although methods of sentiment analysis have advanced, challenges remain due to numerous domain-specific expressions, subtle nuance, and indications of sentiment. In this paper, we uncover the potential for 1) nonverbal behavioral signals such as conventional typing, and 2) indirect physiological measures (eye gaze, GSR, touch pressure) to reveal genuine affective states in in situ code review in a large software company.Nonverbal behavioral signals of 33 professional software developers were recorded unobtrusively while they worked on their daily code reviews. After analyzing these signals using Linear Mixed Effect Models, we observe that affect presented in the written comments is associated with prolonged typing duration. Using physiological features, a trained Random Forest classifier can predict post-task valence with 90.0% accuracy (F1-score = 0.937) and arousal with 83.9% accuracy (F1-score = 0.856). The results show promise for the creation of intelligent affect-aware interfaces for code review."
Research article - A data replication strategy with tenant performance and provider economic profit guarantees in Cloud data centers,"AbstractMeeting tenant performance requirements through data replication while ensuring an economic profit is very challenging for cloud providers. For this purpose, we propose a data Replication Strategy that satisfies Performance tenant objective and provider profit in Cloud data centers (RSPC). Before the execution of each tenant query Q, data replication is considered only if: (i) the estimated Response Time of Q (RTQ) exceeds a critical RT threshold (per-query replication), or (ii) more often, if RTQ exceeds another (lower) RT threshold for a given number of times (replication per set of queries). Then, a new replica is really created only if a suitable replica placement is heuristically found so that the RT requirement is satisfied again while ensuring an economic profit for the provider. Both the provider's revenues and expenditures are also estimated while penalties and replication costs are taken into account. Furthermore, the replica factor is dynamically adjusted in order to reduce the resource consumption. Compared to four other strategies, RSPC best satisfies the RT requirement under high loads, complex queries and strict RT thresholds. Moreover, penalty and data transfer costs are significantly reduced, which impacts the provider profit."
Research article - Sequence effects in the estimation of software development effort,"AbstractCurrently, little is known about how much the sequence in which software development tasks or projects are estimated affects judgment-based effort estimates. To gain more knowledge, we examined estimation sequence effects in two experiments. In the first experiment, 362 software professionals estimated the effort of three large tasks of similar sizes, whereas in the second experiment 104 software professionals estimated the effort of four large and five small tasks. The sequence of the tasks was randomised in both experiments. The first experiment, with tasks of similar size, showed a mean increase of 10% from the first to the second and a 3% increase from the second to the third estimate. The second experiment showed that estimating a larger task after a smaller one led to a mean decrease in the estimate of 24%, and that estimating a smaller task after a larger one led to a mean increase of 25%. There was no statistically significant reduction in the sequence effect with higher competence. We conclude that more awareness about how the estimation sequence affects the estimates may reduce potentially harmful estimation biases. In particular, it may reduce the likelihood of a bias towards too low effort estimates."
Research article - Using Orthogonal Defect Classification to characterize NoSQL database defects,"AbstractNoSQL databases are increasingly used for storing and managing data in business-critical Big Data systems. The presence of software defects (i.e., bugs) in these databases can bring in severe consequences to the NoSQL services being offered, such as data loss or service unavailability. Thus, it is essential to understand the types of defects that frequently affect these databases, allowing developers take action in an informed manner (e.g., redirect testing efforts). In this paper, we use Orthogonal Defect Classification (ODC) to classify a total of 4096 software defects from three of the most popular NoSQL databases: MongoDB, Cassandra, and HBase. The results show great similarity for the defects across the three different NoSQL systems and, at the same time, show the differences and heterogeneity regarding research carried out in other domains and types of applications, emphasizing the need for possessing such information. Our results expose the defect distributions in NoSQL databases, provide a foundation for selecting representative defects for NoSQL systems, and, overall, can be useful for developers for verifying and building more reliable NoSQL database systems."
Research article - Maintaining interoperability in open source software: A case study of the Apache PDFBox project,"AbstractSoftware interoperability is commonly achieved through the implementation of standards for communication protocols or data representation formats. Standards documents are often complex, difficult to interpret, and may contain errors and inconsistencies, which can lead to differing interpretations and implementations that inhibit interoperability. Through a case study of two years of activity in the Apache PDFBox project we examine day-to-day decisions made concerning implementation of the PDF specifications and standards in a community open source software (OSS) project. Thematic analysis is used to identify semantic themes describing the context of observed decisions concerning interoperability. Fundamental decision types are identified including emulation of the behaviour of dominant implementations and the extent to which to implement the PDF standards. Many factors influencing the decisions are related to the sustainability of the project itself, while other influences result from decisions made by external actors, including the developers of dependencies of PDFBox. This article contributes a fine grained perspective of decision-making about software interoperability by contributors to a community OSS project. The study identifies how decisions made support the continuing technical relevance of the software, and factors that motivate and constrain project activity."
Research article - A complete run-time overhead-aware schedulability analysis for MrsP under nested resources,"AbstractMultiprocessor Resource Sharing Protocol (MrsP) is a hard real-time multiprocessor resource sharing protocol for fully partitioned fixed-priority systems, and adopts a novel helping mechanism to allow task migrations during resource accessing. Previous research focusing on analysing MrsP systems have delivered two forms of timing analysis which effectively bound response time and migration cost of tasks under MrsP, and have demonstrated advantages of this protocol. An adjustable non-preemptive section is also introduced that effectively reduces the number of migrations needed during each resource access. However, these analysis methods are only applicable if a non-nested resource accessing model is assumed. In addition, there is no clear approach towards the configuration of the non-preemptive section length, and the computation cost for applying the analysis remains unknown.In this paper, we extend the MrsP analysis for systems with nested resources. Major run-time costs incurred by MrsP tasks are also taken into account to form a complete run-time cost-aware schedulability analysis. In addition, recommendations towards non-preemptive section configuration are given from both analytic and empiric perspectives. Finally, a set of evaluations are conducted to investigate schedulability of MrsP under nested resources and the cost for applying the proposed analysis. As a result of this paper, the schedulability test for MrsP is complete and the computation costs of its use are now understood."
Research article - A survey on the use of access permission-based specifications for program verification,"AbstractVerifying the correctness and reliability of imperative and object-oriented programs is one of the grand challenges in computer science. In imperative programming models, programmers introduce concurrency manually by using explicit concurrency constructs such as multi-threading. Multi-threaded programs are prone to synchronization problems such as data races and dead-locks, and verifying API protocols in object-oriented programs is a non-trivial task due to improper and unexpected state transition at run time. This is in part due to the unexpected sharing of program states in such programs. With these considerations in mind, access permissions have been investigated as a means to reasoning about the correctness of such programs. Access permissions are abstract capabilities that characterize the way a shared resource can be accessed by multiple references.This paper provides a comprehensive survey of existing access permission-based verification approaches. We describe different categories of permissions and permission-based contracts. We elaborate how permission-based specifications have been used to ensure compliance of API protocols and to avoid synchronization problems in concurrent programs. We compare existing approaches based on permission usage, analysis performed, language and/or tool supported, and properties being verified. Finally, we provide insight into the research challenges posed by existing approaches and suggest future directions."
Research article - Finding help with programming errors: An exploratory study of novice software engineers’ focus in stack overflow posts,"AbstractMonthly, 50 million users visit Stack Overflow, a popular Q&A forum used by software developers, to share and gather knowledge and help with coding problems. Although Q&A forums serve as a good resource for seeking help from developers beyond the local team, the abundance of information can cause developers, especially novice software engineers, to spend considerable time in identifying relevant answers and suitable suggested fixes.This exploratory study aims to understand how novice software engineers direct their efforts and what kinds of information they focus on within a post selected from the results returned in response to a search query on Stack Overflow. The results can be leveraged to improve the Q&A forum interface, guide tools for mining forums, and potentially improve granularity of traceability mappings involving forum posts. We qualitatively analyze the novice software engineers’ perceptions from a survey as well as their annotations of a set of Stack Overflow posts. Our results indicate that novice software engineers pay attention to only 27% of code and 15–21% of text in a Stack Overflow post to understand and determine how to apply the relevant information to their context. Our results also discern the kinds of information prominent in that focus."
Research article - Detection of intermittent faults in software programs through identification of suspicious shared variable access patterns,"AbstractIntermittent faults are a very common problem in the software world, while difficult to be debugged. Most of the existing approaches though assume that suitable instrumentation has been provided in the program, typically in the form of assertions that dictate which program states are considered to be erroneous. In this paper we propose a method that can be used to detect probable sources of intermittent faults within a program. Our method proposes certain points in the code, whose data interdependencies combined with their execution interweaving indicate that they could be the cause of intermittent faults. It is the responsibility of the user to accept or reject these proposals. An advantage of this method is that it removes the need for having predefined assertion points in the code, being able to detect potential sources of intermittent faults in the whole bulk of the code, with no instrumentation requirements on the side of the programmer. The proposed approach exploits information from the dynamic behavior of the program. In comparison with parser-based approaches which analyze only the program structure, our approach is immutable to language term changes and in general is not depending on any user-provided assertions or configuration."
Review article - Game-theoretic analysis of development practices: Challenges and opportunities,"AbstractDevelopers continuously invent new practices, usually grounded in hard-won experience, not theory. Game theory studies cooperation and conflict; its use will speed the development of effective processes. A survey of game theory in software engineering finds highly idealised models that are rarely based on process data. This is because software processes are hard to analyse using traditional game theory since they generate huge game models. We are the first to show how to use game abstractions, developed in artificial intelligence, to produce tractable game-theoretic models of software practices. We present Game-Theoretic Process Improvement (GTPI), built on top of empirical game-theoretic analysis. Some teams fall into the habit of preferring “quick-and-dirty” code to slow-to-write, careful code, incurring technical debt. We showcase GTPI’s ability to diagnose and improve such a development process. Using GTPI, we discover a lightweight intervention that incentivises developers to write careful code: add a singlecode reviewer who needs to catch only 25% of kludges. This 25% accuracy is key; it means that a reviewer does not need to examine each commit in depth, making this process intervention cost-effective."
Research article - hW-inference: A heuristic approach to retrieve models through black box testing,"AbstractWe present an efficient approach to retrieve behavioural models from reactive software systems in the form of Finite State Machines by testing them. The system is accessed in black box mode; thus, no source or binary code is needed. The novelty of the approach is that it does not require to reset the system between tests (queries) and does not require any knowledge of the system apart from its input domain. Experiments have shown that it can scale up to systems that may have thousands of states."
Research article - Graph-based root cause analysis for service-oriented and microservice architectures,"AbstractService-oriented architectures and microservices define two ways of designing software with the aim of dividing an application into loosely-coupled services that communicate among each other. This translates into rapid development, where each service is developed and deployed by small teams, enabling continuous shipping of new features and fast-evolving applications. However, the underlying complexity of this type of architecture can hinder observability and maintenance by the user. In particular, identifying the root cause of an anomaly detected in the application can be a difficult and time-consuming task, considering the numerous services and connections to be examined. In this work, we present a root cause analysis framework, based on graph representations of these architectures. The graphs can be used to compare any anomalous situation that happens in the system with a library of anomalous graphs that serves as a knowledge base for the user troubleshooting those anomalies. We use the Grid’5000 testbed to deploy three different architectures and inject a set of anomalies. The results show how our graph-based approach is 19.41% more effective than a machine learning method that does not take into account the relationship between elements."
Research article - Run-time evaluation of architectures: A case study of diversification in IoT,"AbstractRun-time properties of modern software system environments, such as Internet of Things (IoT), are a challenge for existing software architecture evaluation methods. Such systems are largely data-driven, characterized by their dynamism, unpredictability in operation, hyper-connectivity, and scale. Properties, such as performance, delayed delivery, and scalability, are acknowledged to pose great risk and are difficult to evaluate at design-time. Run-time evaluation could potentially be used to complement design-time evaluation, enabling significant deviations from the expected performance values to be captured. However, there are no systematic software architecture evaluation methods that intertwine and interleave design-time and run-time evaluation. This paper addresses this gap by proposing a novel run-time architecture evaluation method suited for systems that exhibit uncertainty and dynamism in their operation. Our method uses machine learning and cost-benefit analysis at run-time to continuously profile the architecture decisions made, to assess their added value. We demonstrate the applicability and effectiveness of this approach in the context of an IoT system architecture, where some architecture design decisions were diversified to meet Quality of Service (QoS) requirements. Our approach provides run-time assessment for these decisions which can inform deployment, refinement, and/or phasing-out decisions."
