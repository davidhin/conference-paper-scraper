title,abstract
Research article - Characterizing testing methods for context-aware software systems: Results from a quasi-systematic literature review,"AbstractContext-Aware Software Systems (CASS) use environmental information to provide better service to the systems’ actors to fulfill their goals. Testing of ubiquitous software systems can be challenging since it is unlikely that, while designing the test cases, the tester can identify all possible context variations. A quasi-Systematic Literature Review has been undertaken to characterize the methods usually used for testing CASS. The analysis and generation of knowledge in this work rely on classifying the extracted information. Established taxonomies of software testing and context-aware were used to characterize and interpret the findings. The results show that, although it is possible to observe the utilization of some software testing methods, few empirical studies are evaluating such methods when testing CASS. The selected technical literature conveys a lack of consensus on the understanding of context and CASS, and on the meaning of software testing. Furthermore, context variation in CASS has only been partially addressed by the identified approaches. They either rely on simulating context or in fixing the values of context variables during testing. We argue that the tests of context-aware software systems need to deal with the diversity of context instead of mitigating their effects."
Research article - Chord: Checkpoint-based scheduling using hybrid waiting list in shared clusters,"AbstractCloud platforms supported by shared clusters are getting increasingly effective. Numerous tasks are submitted into clusters by a variety of users. Cloud platforms usually assign tasks with different priorities based on different Quality of Services (QoS) chosen by users. High-priority tasks can be executed primarily. As a consequence, preemption frequently occurs in almost all the commercial cloud platforms, such as Google and Amazon cluster. Although kill-based preemption is adopted as an optimal solution for high-priority tasks, it severely harms low-priority tasks. Especially, during the peak time, some low-priority tasks may be preempted and restarted repeatedly resulting in consuming much more precious resources including CPU cores, RAM and hard drives. Thanks to the checkpoint technology that provides an efficient solution to addressing the preemption issue. However, using checkpoint blindly will cause more resource waste. To address this issue, in this paper, we propose a concept of hybrid waiting list that holds all unfinished tasks and makes the resumption of tasks regularly. We leverage the checkpoint technology and design a novel approach based on the hybrid waiting list named Chord (Checkpoint with hybrid scheduling method) which effectively improves the performance of shared clusters. Specifically, by checking the occupancy of resources periodically and making checkpoints for certain tasks, our approach can effectively reduce unnecessary checkpoints and improve the performance of the whole cluster, especially for low-priority tasks. Extensive simulation experiments injecting tasks from the Google cloud trace logs were conducted to validate the superiority of our approach. Compared with the ordinary priority scheduling methods adopt by several commercial clouds, the improvement of response time gained by our Chord can reach 18.94%."
Research article - Modularity and architecture of PLC-based software for automated production Systems: An analysis in industrial companies,"AbstractAdaptive and flexible production systems require modular and reusable software especially considering their long-term life cycle of up to 50 years. SWMAT4aPS, an approach to measure Software Maturity for automated Production Systems is introduced. The approach identifies weaknesses and strengths of various companies’ solutions for modularity of software in the design of automated Production Systems (aPS). At first, a self-assessed questionnaire is used to evaluate a large number of companies concerning their software maturity. Secondly, we analyze PLC code, architectural levels, workflows and abilities to configure code automatically out of engineering information in four selected companies. In this paper, the questionnaire results from 16 German world-leading companies in machine and plant manufacturing and four case studies validating the results from the detailed analyses are introduced to prove the applicability of the approach and give a survey of the state of the art in industry."
Research article - Rendex: A method for automated reviews of textual requirements,"AbstractConducting requirements reviews before the start of software design is one of the central goals in requirements management. Fast and accurate reviews promise to facilitate software development process and mitigate technical risks of late design modifications. In large software development companies, however, it is difficult to conduct reviews as fast as needed, because the number of regularly incoming requirements is typically several thousand. Manually reviewing thousands of requirements is a time-consuming task and disrupts the process of continuous software development. As a consequence, software engineers review requirements in parallel with designing the software, thus partially accepting the technical risks. In this paper we present a measurement-based method for automating requirements reviews in large software development companies. The method, Rendex, is developed in an action research project in a large software development organization and evaluated in four large companies. The evaluation shows that the assessment results of Rendex have 73%-80% agreement with the manual assessment results of software engineers. Succeeding the evaluation, Rendex was integrated with the requirements management environment in two of the collaborating companies and is regularly used for proactive reviews of requirements."
Research article - Embedding statecharts into Teleo-Reactive programs to model interactions between agents,"AbstractContextThe Teleo-Reactive (TR) approach offers many possibilities for goal-oriented modeling of reactive systems, but it also has drawbacks when the number of interactions among agents is high, leading to barely legible specifications and losing the original benefits of the approach.ObjectiveThis work combines the TR paradigm with statecharts and provides advantages for modeling reactive systems and removing the shortcomings detected.MethodA basic example is adopted to reveal the problem that appears when agents are modeled only with the TR approach and have frequent interactions with others. This paper proposes an extension to the TR approach that integrates the modeling using statecharts. A transformation procedure from statecharts to TR programs makes it possible to continue using the infrastructure of existing execution platforms such as TeleoR. The approach has been validated for a particular domain by considering a more complex case study in which traditionally there have been no results on the application of the TR paradigm. A survey was carried out on students to verify the benefits of the approach.ResultsA method to consider statecharts when modeling TR programs.ConclusionsStatecharts can facilitate the adoption of the TR approach."
Research article - A training process for improving the quality of software projects developed by a practitioner,"AbstractBackgroundThe quality of a software product depends on the quality of the software process followed in developing the product. Therefore, many higher education institutions (HEI) and software organizations have implemented software process improvement (SPI) training courses to improve the software quality.ObjectiveBecause the duration of a course is a concern for HEI and software organizations, we investigate whether the quality of software projects will be improved by reorganizing the activities of the ten assignments of the original personal software process (PSP) course into a modified PSP having fewer assignments (i.e., seven assignments).MethodThe assignments were developed by following a modified PSP with fewer assignments but including the phases, forms, standards, and logs suggested in the original PSP. The measurement of the quality of the software assignments was based on defect density.ResultsWhen the activities in the original PSP were reordered into fewer assignments, as practitioners progress through the PSP training, the defect density improved with statistical significance.ConclusionsOur modified PSP could be applied in academy and industrial environments which are concerned in the sense of reducing the PSP training time."
Research article - Software product lines adoption in small organizations,"AbstractContextAn increasing number of studies has demonstrated improvements in product quality, and time-to-market reductions when Software Product Line (SPL) engineering is introduced. However, despite the amount of successful stories about the use of SPL engineering, there is a lack of guidelines to support its adoption, especially to small-sized software organizations.ObjectiveThe aim of this study is to investigate SPL adoption in small organizations and to improve the generalization of evidence through the use of a multi-method approach.MethodThis paper reports on a multi-method study, where results from a mapping study, industrial case study and also expert opinion survey were considered to identify a set of findings.ResultsThe study provides a better understanding of SPL adoption in the context of small to medium-sized organizations, by documenting evidence observed during the transition from single-system development to an SPL approach. This evidence is strengthened by the use of different research methods, which results in 22 findings regarding to the SPL adoption.ConclusionThis research has synthesized the available evidence in SPL adoption and identifies gaps between required strategies, organizational structures, maturity level and existing adoption barriers. These findings are an important step to establish guidelines for SPL adoption."
Research article - Dataclay: A distributed data store for effective inter-player data sharing,"AbstractIn the Big Data era, both the academic community and industry agree that a crucial point to obtain the maximum benefits from the explosive data growth is integrating information from different sources, and also combining methodologies to analyze and process it . For this reason, sharing data so that third parties can build new applications or services based on it is nowadays a trend . Although most data sharing initiatives are based on public data, the ability to reuse data generated by private companies is starting to gain importance as some of them (such as Google, Twitter, BBC or New York Times) are providing access to part of their data. However, current solutions for sharing data with third parties are not fully convenient to either or both data owners and data consumers. Therefore we present dataClay, a distributed data store designed to share data with external players in a secure and flexible way based on the concepts of identity and encapsulation. We also prove that dataClay is comparable in terms of performance with trendy NoSQL technologies while providing extra functionality, and resolves impedance mismatch issues based on the Object Oriented paradigm for data representation."
Research article - A case study of black box fail-safe testing in web applications,"AbstractExternal failures like network changes can affect system operation negatively. Mitigation requirements try to prevent or reduce their effects. This paper presents a black box testing approach that tests fail-safe behavior in web applications. Failure mitigation tests are built based on a functional test suite. Mitigation requirements are used to build mitigation models and mitigation tests paths through them. A genetic algorithm is used to generate failure scenarios (failure mitigation test requirements). Weaving rules describe how mitigation test paths are combined with behavioral test paths to create failure mitigation test paths. These are then transformed into an executable test suite. The paper also evaluates the genetic algorithm used to generate test requirements with respect to efficiency and effectiveness. A large case study, a commercial mortgage lending system, is used to explore applicability, scalability, and effectiveness of the approach."
Research article - Energy-efficient heterogeneous resource management for wireless monitoring systems,"AbstractVarious energy-saving designs have been proposed for reducing the power consumption of processors through dynamic voltage and frequency scaling (DVFS). When dynamic random access memory (DRAM) or peripheral power consumption is high, dynamic power management (DPM) can be adopted to dynamically activate or deactivate devices or to switch them into energy-saving states during idle periods. This paper proposes a heterogeneous resource management mechanism to manage device scheduling for multiple tasks and task scheduling in a processor. A wireless network monitoring system was analyzed as a case study, wherein a resource sharing mechanism was developed for managing the scheduling of multiple wireless adapters, and the concept of instantaneous utilization was leveraged to enable chain-based task scheduling. This paper explores DVFS and DPM energy saving techniques for peripherals and a processor by considering both the required device time and processor time for each task without violating performance requirements under constraints of buffer size. The proposed algorithms were then implemented on a wireless network monitoring system and real traces were collected from a laboratory and downloaded from the UMass Trace Repository for use as inputs. A series of experiments was conducted to evaluate the quality of our algorithms for energy saving within the constraints of system performance requirements and hardware resources."
Research article - Deadlock detection in complex software systems specified through graph transformation using Bayesian optimization algorithm,"AbstractWhile developing concurrent systems, one of the important properties to be checked is deadlock freedom. Model checking is an accurate technique to detect errors, such as deadlocks. However, the problem of model checking in complex software systems is state space explosion in which all reachable states cannot be generated due to exponential memory usage. When a state space is too large to be explored exhaustively, using meta-heuristic and evolutionary approaches seems a proper solution to address this problem. Recently, a few methods using genetic algorithm, particle swarm optimization and similar approaches have been proposed to handle this problem. Even though the results of recent approaches are promising, the accuracy and convergence speed may still be a problem. In this paper, a novel method is proposed using Bayesian Optimization Algorithm (BOA) to detect deadlocks in systems specified formally through graph transformations. BOA is an Estimation of Distribution Algorithm in which a Bayesian network (as a probabilistic model) is learned from the population and then sampled to generate new solutions. Three different structures are considered for the Bayesian network to investigate deadlocks in the benchmark problems. To evaluate the efficiency of the proposed approach, it is implemented in GROOVE, an open source toolset for designing and model checking graph transformation systems. Experimental results show that the proposed approach is faster and more accurate than existing algorithms in discovering deadlock states in the most of case studies with large state spaces."
Research article - MeshFS: A distributed file system for cloud-based wireless mesh network,"AbstractWireless mesh networks have attracted considerable interest in recent years in both the academic and industrial communities. As wireless mesh routers can be interconnected through wireless links, wireless mesh networks provide greater flexibility and better cost-effectiveness. In particular, due to their ease of installation and maintenance, they can be used in different environments, especially where cable installation is difficult. Apart from providing routing service, wireless mesh networks can also be employed to provide other value-added services. Inspired by cloud computing and other distributed file systems, this paper presents the design and implementation of MeshFS, a distributed file system specifically for wireless mesh networks. A key technical challenge is to develop a lightweight software system that can be implemented over memory-limited wireless mesh network environments. With the aim of providing a lightweight distributed file system, allowing limited resources to be utilized more effectively, MeshFS integrates scattered storage resources from wireless mesh routers to provide a mountable file system interface to Unix/Linux file system with fault-tolerant capabilities and cloud computing-like storage functions."
Research article - Obscured by the cloud: A resource allocation framework to model cloud outage events,"AbstractAs Small Medium Enterprises (SMEs) adopt Cloud technologies to provide high value customer offerings, uptime is considered important. Cloud outages represent a challenge to SMEs and micro teams to maintain a services platform. If a Cloud platform suffers from downtime this can have a negative effect on business revenue. Additionally, outages can divert resources from product development/delivery tasks to reactive remediation. These challenges are immediate for SMEs or micro teams with a small levels of resources. In this paper we present a framework that can model the arrival of Cloud outage events. This framework can be used by DevOps teams to manage their scarce pool of resources to resolve outages, thereby minimising impact to service delivery. We analysed over 300 Cloud outage events from an enterprise data set. We modelled the inter-arrival and service times of each outage event and found a Pareto and a lognormal distribution to be a suitable fit. We used this result to produce a special case of the G/G/1 queue system to predict busy times of DevOps personnel. We also investigated dependence between overlapping outage events. Our predictive queuing model compared favourably with observed data, 72% precision was achieved using one million simulations."
Research article - No silver brick: Opportunities and limitations of teaching Scrum with Lego workshops,"AbstractEducation in Software Engineering has to both teach technical content such as databases and programming but also organisational skills such as team work and project management. While the former can be evaluated from a product perspective, the latter are usually embedded in a Software Engineering process and need to be assessed and adapted throughout their implementation. The in-action property of processes puts a strain on teachers since we cannot be present throughout the students’ work. To address this challenge we have adopted workshops to teach Scrum by building a Lego city in short sprints to focus on the methodological content. In this way we can be present throughout the process and coach the students. We have applied the exercise in six different courses, across five different educational programmes and observed more than 450 participating students. In this paper, we report on our experiences with this approach, based on quantitative data from the students and qualitative data from both students and teachers. We give recommendations for learning opportunities and best practices and discuss the limitations of these workshops in a classroom setting. We also report on how the students transferred their methodological knowledge to software development projects in an academic setting."
Research article - Examining decision characteristics & challenges for agile software development,"AbstractAlthough agile software development is often associated with improved decision making, existing studies tend to focus on narrow aspects of decision making in such environments. There is a lack of clarity on how teams make and evaluate a myriad of decisions from software feature inception to product delivery and refinement. Indeed there is relatively little known about a) the decision characteristics related to agile values, and b) the challenges they present for decision making on agile teams. We present an in-depth exploratory case study based on a pluralistic approach comprising semi-structured interviews, focus groups, team meeting observations, and document analysis. The study identifies failings of decision making in an agile setting. Explicitly considering the decision process, information intelligence used in decision making, and decision quality, the key contribution of this paper is the development of an over-arching framework of agile decision making, which identifies particular decision characteristics across 4 key agile values and the related challenges for agile team decision making. It provides a framework for researchers and practitioners to evaluate the decision challenges of an agile software development team and to improve decision quality."
Research article - A framework for automatically ensuring the conformance of agent designs,"AbstractMulti-agent systems are increasingly being used in complex applications due to features such as autonomy, pro-activity, flexibility, robustness and social ability. These very features also make verifying multi-agent systems a challenging task. In this article, we propose a mechanism, including automated tool support, for early phase defect detection by comparing the plan structures of a Belief-Desire-Intention agent design against the requirements models and interaction protocols. The basic intuition of our approach is to extract sets of possible behaviour runs from the agents’ behaviour models and to verify whether these runs conform to the specifications of the system-to-be or not. This approach is applicable at design time, not requiring source code, thus enabling detection and removal of some defects at an early phase of the software development lifecycle. We followed an experimental approach for evaluating the proposed verification framework. Our evaluation shows that even simple system’s specifications developed by relatively experienced developers are prone to defects, and our approach is successful in uncovering most of these defects. In addition, we conducted a scalability analysis on the approach, and the outcomes show that our approach can scale when designs grow in size."
Research article - Analyzing software evolution and quality by extracting Asynchrony change patterns,"AbstractChange patterns describe two or more files were often changed together during the development or the maintenance of software systems. Several studies have been presented to detect change patterns and to analyze their types and their impact on software quality. In this context, we introduced the Asynchrony change pattern to describes a set of files that always change together in the same change periods, regardless developers who maintained them. In this paper, we investigate the impact of Asynchrony change pattern on design and code smells such as anti-patterns and code clones.Concretely, we conduct an empirical study by detecting Asynchrony change patterns, anti-patterns and code clones occurrences on 22 versions of four software systems and analyzing their fault-proneness. Results show that cloned files that follow the same Asynchrony change patterns have significantly increased fault-proneness with respect to other clones, and that anti-patterns following the same Asynchrony change pattern can be up to five times more risky in terms of fault-proneness as compared to other anti-patterns. Asynchrony change patterns thus seem to be strong indicators of fault-proneness for clones and anti-patterns."
Research article - Recovering software product line architecture of a family of object-oriented product variants,"AbstractSoftware Product Line Engineering (SPLE) aims at applying a pre-planned systematic reuse of large-grained software artifacts to increase the software productivity and reduce the development cost. The idea of SPLE is to analyze the business domain of a family of products to identify the common and the variable parts between the products. However, it is common for companies to develop, in an ad-hoc manner (e.g. clone and own), a set of products that share common services and differ in terms of others. Thus, many recent research contributions are proposed to re-engineer existing product variants to a software product line. These contributions are mostly focused on managing the variability at the requirement level. Very few contributions address the variability at the architectural level despite its major importance. Starting from this observation, we propose an approach to reverse engineer the architecture of a set of product variants. Our goal is to identify the variability and dependencies among architectural-element variants. Our work relies on formal concept analysis to analyze the variability. To validate the proposed approach, we evaluated on two families of open-source product variants; Mobile Media and Health Watcher. The results of precision and recall metrics of the recovered architectural variability and dependencies are 81%, 91%, 67% and 100%, respectively."
Research article - Evaluating Lehman’s Laws of software evolution within software product lines industrial projects,"AbstractThe evolution of a single system is a task where we deal with the modification of a single product. Lehman’s Laws of software evolution were broadly evaluated within this type of system and the results shown that these single systems evolve according to his stated laws over time. However, considering Software Product Lines (SPL), we need to deal with the modification of several products which include common, variable, and product specific assets. Because of the several assets within SPL, each stated law may have a different behavior for each asset kind. Nonetheless, we do not know if all of the stated laws are still valid for SPL since they were partially evaluated in this context. Thus, this paper details an empirical investigation where Lehman’s Laws (LL) of Software Evolution were used in two SPL industrial projects to understand how the SPL assets evolve over time. These projects are related to an application in the medical domain and another in the financial domain, developed by medium-size companies in Brazil. They contain a total of 71 modules and a total of 71.442 bug requests in their tracking system, gathered along the total of more than 10 years. We employed two techniques - the KPSS Test and linear regression analysis, to assess the relationship between LL and SPL assets. Results showed that one law was completely supported (conservation of organizational stability) for all assets within both empirical studies. Two laws were partially supported for both studies depending on the asset type (continuous growth and conservation of familiarity). Finally, the remaining laws had differences among their results for all assets (continuous change, increasing complexity, and declining quality)."
Research article - Exploring quality measures for the evaluation of feature models: a case study,"AbstractEvaluating the quality of a feature model is essential to ensure that errors in the early stages do not spread throughout the Software Product Line (SPL). One way to evaluate the feature model is to use measures that could be associated with the feature model quality characteristics and their quality attributes. In this paper, we aim at investigating how measures can be applied to the quality assessment of SPL feature models. We performed an exploratory case study using the COfFEE maintainability measures catalog and the S.P.L.O.T. feature models repository. In order to support this case study, we built a dataset (denoted by MAcchiATO) containing the values of 32 measures from COfFEE for 218 software feature models, extracted from S.P.L.O.T. This research approach allowed us to explore three different data analysis techniques. First, we applied the Spearman’s rank correlation coefficient in order to identify relationships between the measures. This analysis showed that not all 32 measures in COfFEE are necessary to reveal the quality of a feature model and just 15 measures could be used. Next, the 32 measures in COfFEE were grouped by applying the Principal Component Analysis and a set of 9 new grouped measures were defined. Finally, we used the Tolerance Interval technique to define statistical thresholds for these 9 new grouped measures. So, our findings suggest that measures can be effectively used to support the quality evaluation of SPL feature models."
Research article - Automating the license compatibility process in open source software with SPDX,"AbstractFree and Open Source Software (FOSS) promotes software reuse and distribution at different levels for both creator and users, but at the same time imposes some challenges in terms of FOSS licenses that can be selected and combined. The main problem linked to this selection is the presence of a large set of licenses that define different rights and obligations in software use. The problem becomes more evident in cases of complex combinations of software that carries different – often conflicting – licenses. In this paper we are presenting our work on automating license compatibility by proposing a process that examines the structure of Software Package Data Exchange (SPDX) for license compatibility issues assisting in their correct use and combination. We are offering the possibility to detect license violations in existing software projects and make suggestions on appropriate combinations of different software packages. We are also elaborating on the complexity and ambiguity of licensing detection in software products through representative case studies. Our work constitutes a useful process towards automating the analysis of software systems in terms of license use and compatibilities."
Research article - Construction and utilization of problem-solving knowledge in open source software environments,"AbstractOpen Source Software (OSS) has become an important environment where developers can share reusable software assets in a collaborative manner. Although developers can find useful software assets to reuse in the OSS environment, they may face difficulties in finding solutions to problems that occur while integrating the assets with their own software. In OSS, sharing the experiences of solving similar problems among developers usually plays an important role in reducing problem-solving efforts. We analyzed how developers interact with each other to solve problems in OSS, and found that there is a common pattern of exchanging information about symptoms and causes of a problem. In particular, we found that many problems involve multiple symptoms and causes and it is critical to identify those symptoms and causes early to solve the problems more efficiently. We developed a Bayesian network based approach to semi-automatically construct a knowledge base for dealing with problems, and to recommend potential causes of a problem based on multiple symptoms reported in OSS. Our experiments showed that the approach is effective to recommend the core causes of a problem, and contributes to solving the problem in an efficient manner."
Research article - Xtraitj: Traits for the Java platform,"AbstractTraits were proposed as a mechanism for fine-grained code reuse to overcome many limitations of class-based inheritance. A trait is a set of methods that is independent from any class hierarchy and can be flexibly used to build other traits or classes by means of a suite of composition operations. In this paper we present the new version of Xtraitj, a trait-based programming language that features complete compatibility and interoperability with the Java platform. Xtraitj is implemented in Xtext and Xbase, and it provides a full Eclipse IDE that supports an incremental adoption of traits in existing Java projects. The new version of Xtraitj allows traits to be accessed from any Java project or library, even if the original Xtraitj source code is not available, since traits can be accessed in their byte-code format. This allows developers to create Xtraitj libraries that can be provided in their binary only format. We detail the technique we used to achieve such an implementation; this technique can be reused in other languages implemented in Xtext for the Java platform. We formalize our traits by means of flattening semantics and we provide some performance benchmarks that show that the runtime overhead introduced by our traits is acceptable."
Research article - Reverse engineering reusable software components from object-oriented APIs,"AbstractObject-oriented Application Programing Interfaces (APIs) support software reuse by providing pre-implemented functionalities. Due to the huge number of included classes, reusing and understanding large APIs is a complex task. Otherwise, software components are accepted to be more reusable and understandable entities than object-oriented ones. Thus, in this paper, we propose an approach for reengineering object-oriented APIs into component-based ones. We mine components as a group of classes based on the frequency they are used together and their ability to form a quality-centric component. To validate our approach, we experimented on 100 Java applications that used four APIs."
Research article - Stepwise API usage assistance using n-gram language models,"AbstractReusing software involves learning third-party APIs, a process that is often time-consuming and error-prone. Recommendation systems for API usage assistance based on statistical models built from source code corpora are capable of assisting API users through code completion mechanisms in IDEs. A valid sequence of API calls involving different types may be regarded as a well-formed sentence of tokens from the API vocabulary. In this article we describe an approach for recommending subsequent tokens to complete API sentences using n-gram language models built from source code corpora. The provided system was integrated in the code completion facilities of the Eclipse IDE, providing contextualized completion proposals for Java taking into account the nearest lines of code. The approach was evaluated against existing client code of four widely used APIs, revealing that in more than 90% of the cases the expected subsequent token is within the 10-top-most proposals of our models. The high score provides evidence that the recommendations could help on API learning and exploration, namely through the assistance on writing valid API sentences."
Research article - A Formal Approach to implement java exceptions in cooperative systems,"AbstractThe increasing number of systems that work on the top of cooperating elements have required new techniques to control cooperation on both normal and abnormal behaviors of systems. The controllability of the normal behaviors has received more attention because they are concerned with the users expectations, while for the abnormal behaviors it is left to designers and programmers. However, for cooperative systems, the abnormal behaviors, mostly represented by exceptions at programming level, become an important issue in software development because they can affect the overall system behavior. If an exception is raised and not handled accordingly, the system may collapse. To avoid such situation, certain concepts and models have been proposed to coordinate propagation and recovering of exceptional behaviors, including the Coordinated Atomic Actions (CAA). Regardless of the effort in creating these conceptual models, an actual implementation of them in real systems is not very straightforward.This article provides a reliable framework for the implementation of Java exceptions propagation and recovery using CAA concepts. To do this, a Java framework (based on a formal specification) is presented, together with a set of properties to be preserved and proved with the Java Pathfinder (JPF) model checker. In practice, to develop new systems based on the given coordination concepts, designers/programmers can instantiate the framework to implement the exceptional behavior and then verify the correctness of the resulting code using JPF. Therefore, by using the framework, designers/programmers can reuse the provided CAA implementation and instantiate fault-tolerant Java systems."
Research article - An exploratory study on the usage of common interface elements in android applications,"AbstractThe number of mobile applications has increased drastically in the past few years. A recent study has shown that reusing source code is a common practice for Android application development. However, reuse in mobile applications is not necessarily limited to the source code (i.e., program logic). User interface (UI) design plays a vital role in constructing the user-perceived quality of a mobile application. The user-perceived quality reflects the users’ opinions of a product. For mobile applications, it can be quantified by the number of downloads and raters. In this study, we extract commonly used UI elements, denoted as Common Element Sets (CESs), from user interfaces of applications. Moreover, we highlight the characteristics of CESs that can result in a high user-perceived quality by proposing various metrics. Through an empirical study on 1292 mobile applications, we observe that (i) CESs of mobile applications widely occur among and across different categories; (ii) certain characteristics of CESs can provide a high user-perceived quality; and (iii) through a manual analysis, we recommend UI templates that are extracted and summarized from CESs for developers. Developers and quality assurance personnel can use our guidelines to improve the quality of mobile applications."
Research article - CollabRDL: A language to coordinate collaborative reuse,"AbstractCoordinating software reuse activities is a complex problem when considering collaborative software development. This is mainly motivated due to the difficulty in specifying how the artifacts and the knowledge produced in previous projects can be applied in future ones. In addition, modern software systems are developed in group working in separate geographical locations. Therefore, techniques to enrich collaboration on software development are important to improve quality and reduce costs. Unfortunately, the current literature fails to address this problem by overlooking existing reuse techniques. There are many reuse approaches proposed in academia and industry, including Framework Instantiation, Software Product Line, Transformation Chains, and Staged Configuration. But, the current approaches do not support the representation and implementation of collaborative instantiations that involve individual and group roles, the simultaneous performance of multiple activities, restrictions related to concurrency and synchronization of activities, and allocation of activities to reuse actors as a coordination mechanism. These limitations are the main reasons why the Reuse Description Language (RDL) is unable to promote collaborative reuse, i.e., those related to reuse activities in collaborative software development. To overcome these shortcomings, this work, therefore, proposes CollabRDL, a language to coordinate collaborative reuse by providing essential concepts and constructs for allowing group-based reuse activities. For this purpose, we extend RDL by introducing three new commands, including role, parallel, and doparallel. To evaluate CollabRDL we have conducted a case study in which developer groups performed reuse activities collaboratively to instantiate a mainstream Java framework. The results indicated that CollabRDL was able to represent critical workflow patterns, including parallel split pattern, synchronization pattern, multiple-choice pattern, role-based distribution pattern, and multiple instances with decision at runtime. Overall, we believe that the provision of a new language that supports group-based activities in framework instantiation can help enable software organizations to document their coordinated efforts and achieve the benefits of software mass customization with significantly less development time and effort."
"Research article - Scope-aided test prioritization, selection and minimization for software reuse","AbstractSoftware reuse can improve productivity, but does not exempt developers from the need to test the reused code into the new context. For this purpose, we propose here specific approaches to white-box test prioritization, selection and minimization that take into account the reuse context when reordering or selecting test cases, by leveraging possible constraints delimiting the new input domain scope. Our scope-aided testing approach aims at detecting those faults that under such constraints would be more likely triggered in the new reuse context, and is proposed as a boost to existing approaches. Our empirical evaluation shows that in test suite prioritization we can improve the average rate of faults detected when considering faults that are in scope, while remaining competitive considering all faults; in test case selection and minimization we can considerably reduce the test suite size, with small to no extra impact on fault detection effectiveness considering both in-scope and all faults. Indeed, in minimization, we improve the in-scope fault detection effectiveness in all cases."
Research article - Reusing business components and objects for modeling business systems: The influence of decomposition characteristics and analyst experience,"AbstractComponent-based development (CBD) relies on the use of pre-fabricated business components to develop new application systems, rather than developing them from scratch. It provides an attractive alternative to more established development methods such as object-oriented analysis and design (OOAD). Given the growing demands for using agile methods for software development, we examine if systems analysts are more effective at modeling business systems by reusing business components than by reusing objects. We also examine whether the influence of component or object reuse on modeling performance is moderated by prior experience in systems analysis and design. We evaluate the representational constructs of the two based on a set of decomposition characteristics, and postulate hypotheses comparing the two based on theories in cognitive psychology and human factors. We find that models generated by reusing business components are of higher accuracy than those developed by reusing objects. An interesting finding of our study is that IT professionals who are less experienced in systems analysis and design perform on par with experienced professionals, when modeling business systems by reusing components. We argue that the decomposition characteristics of components—with respect to granularity, quality, and focus—enable less experienced analysts to perform on par with more experienced analysts."
Research article - A method to generate reusable safety case argument-fragments from compositional safety analysis,"AbstractSafety-critical systems usually need to be accompanied by an explained and well-founded body of evidence to show that the system is acceptably safe. While reuse within such systems covers mainly code, reusing accompanying safety artefacts is limited due to a wide range of context dependencies that need to be satisfied for safety evidence to be valid in a different context. Currently, the most commonly used approaches that facilitate reuse lack support for systematic reuse of safety artefacts.To facilitate systematic reuse of safety artefacts we provide a method to generate reusable safety case argument-fragments that include supporting evidence related to compositional safety analysis. The generation is performed from safety contracts that capture safety-relevant behaviour of components in assumption/guarantee pairs backed up by the supporting evidence. We evaluate the feasibility of our approach in a real-world case study where a safety related component developed in isolation is reused within a wheel-loader."
