title,abstract
Research article - PARS: A parallel model for scaled processing of complex events,"AbstractThe need for fast processing of high volume of event streams has triggered the deployment of parallel processing models and techniques for complex event processing. It is however hard to parallelize stateful operators, making the implementations of distributed complex event processing systems very challenging. A well-defined parallel processing model is a pre-requisite to the implementation of any high performance distributed complex event processing system. Most recent works use partition key or shared memory for parallelizing stateful operators but suffer from imbalanced distribution of keys and slow access time to shared memory. This paper proposes a new parallel model called PARS using three partitioning techniques to allow scalable processing of complex events without using partition key or shared memory. We define PARS formally and use this formalism to prove its soundness and completeness. To present a proof-of-concept and evaluate PARS, we use an event generator to simulate event sources and show significant improvement in system scalability. Compared to other works, we experience 10%–20% higher throughput in our experimental results. Our experiments demonstrate processing speeds of up to 5,200,000 complex event detection per second on a multi-machine cluster."
Research article - Partially safe evolution of software product lines,"AbstractSoftware Product Lines allow the automatic generation of related products built with reusable artefacts. In this context, developers may need to perform changes and check whether products are affected. A strategy to perform such analysis is verifying behaviour preservation through the use of formal theories. The product line refinement notion requires behaviour preservation for all existing products. Nevertheless, in evolution scenarios like bug fixes, some products intentionally have their behaviour changed. To support developers in these and other unsafe scenarios, we define a theory of partial product line refinement that helps to precisely understand which products are affected by a change. This provides a kind of impact analysis that could, for example, reduce test effort, since only affected products need to be tested. We provide properties such as compositionality, which deals with changes to a specific product line element, and general properties to support developers when safe and partially safe scenarios are combined. We also define a set of transformation templates, which are classified according to their compatibility to specific types of product lines. To evaluate our work, we analyse two product lines: Linux and Soletta, to discover if our templates could be helpful in evolving these systems."
Research article - Software project scheduling problem in the context of search-based software engineering: A systematic review,"AbstractThis work provides a systematic literature review of the software project scheduling problem, in the context of search-based software engineering, and summarizes the main models, techniques, search algorithms and evaluation criteria applied to solve this problem. We also discuss trends and research opportunities. Our keyword search found 438 papers, published in the last 20 years. After considering the inclusion and exclusion criteria and performing the snowballing procedure, we have analyzed 37 primary studies. The results show the predominance of the use of evolutionary algorithms. The static model, in which the scheduling is performed once during the project, is considered in the majority of the papers. Synthetic instances are commonly used to validate the heuristic and hypervolume and execution time are the mostly applied evaluating criteria."
Research article - Sentiment based approval prediction for enhancement reports,"AbstractThe maintenance and evolution of the software application is a continuous phase in the industry. Users are frequently proposing enhancement requests for further functionalities. However, although only a small part of these requests are finally adopted, developers have to go through all of such requests manually, which is tedious and time consuming. To this end, in this paper we propose a sentiment based approach to predict how likely enhancement reports would be approved or rejected so that developers can first handle likely-to-be-approved requests. This could help the software applications to compete in the industry by upgrading their features in time as per user’s requirements. First, we preprocess enhancement reports using natural language preprocessing techniques. Second, we identify the words having positive and negative sentiments in the summary attribute of the enhancements reports and calculate the sentiment of each enhancement report. Finally, with the history data of real software application, we train a machine learning based classifier to predict whether a given enhancement report would be approved. The proposed approach has been evaluated with the history data from real software applications. The cross-application validation suggests that the proposed approach outperforms the state-of-the-art. The evaluation results suggest that the proposed approach increases the accuracy from 70.94% to 77.90% and improves the F-measure significantly from 48.50% to 74.53%."
Research article - BrownoutCon: A software system based on brownout and containers for energy-efficient cloud computing,"AbstractVM consolidation and Dynamic Voltage Frequency Scaling approaches have been proved to be efficient to reduce energy consumption in cloud data centers. However, the existing approaches cannot function efficiently when the whole data center is overloaded. An approach called brownout has been proposed to solve the limitation, which dynamically deactivates or activates optional microservices or containers. In this paper, we propose a brownout-based software system for container-based clouds to handle overloads and reduce power consumption. We present its design and implementation based on Docker Swarm containers. The proposed system is integrated with existing Docker Swarm without the modification of their configurations. To demonstrate the potential of BrownoutCon software in offering energy-efficient services in brownout situation, we implemented several policies to manage containers and conducted experiments on French Grid’5000 cloud infrastructure. The results show the currently implemented policies in our software system can save about 10%–40% energy than the existing baselines while ensuring quality of services."
Research article - Genetic algorithm based test data generation for MPI parallel programs with blocking communication,"AbstractParallel computing is one of mainstream techniques for high-performance computation in which MPI parallel programs have gained more and more attention. Genetic algorithms (GAs) have been widely employed in automated test data generation, leading to a major family of search-based software testing techniques. However, previous GA-based methods have limitations when testing MPI parallel programs with blocking communication. In this paper, we focus on the path coverage problem for MPI parallel programs with blocking communication, and formulate the problem as an optimization problem with its decision variable being the program input and the execution order of sending nodes. In addition, we develop target amending strategies for candidates when solving the problem using genetic algorithms. The proposed method is evaluated and compared with several state-of-the-art methods through a series of controlled experiments on five typical programs. The experimental results show that the proposed method can effectively and efficiently generate test data for path coverage."
Research article - A routing algorithm for wireless sensor networks based on clustering and an fpt-approximation algorithm,"AbstractClustering sensor nodes is an effective method for routing in Wireless Sensor Networks (WSNs), which maximizes the network lifetime and reduces the energy consumption. However, in a clustered-WSN, the Cluster Heads (CHs) bear a higher load compared to the other nodes, which leads to their earlier death. Therefore, minimizing the maximum load of the CHs is an important problem, which is called the Load-Balanced Clustering Problem (LBCP). LBCP is an NP-hard problem and the best-known approximation factor for this problem is 1.5. Moreover, it has been shown that there is no polynomial-time approximation algorithm that solves this problem with a better approximation factor. In this paper, we propose a Fixed Parameter Tractable (FPT) approximation algorithm with an approximation factor of 1.2 for LBCP. We also propose an energy-efficient and energy-balanced routing algorithm for routing between the CHs and the sink. The simulation results show that the proposed algorithm is practical for large-scale WSNs and performs better than the other similar algorithms."
Review article - Holistic resource management for sustainable and reliable cloud computing: An innovative solution to global challenge,"AbstractMinimizing the energy consumption of servers within cloud computing systems is of upmost importance to cloud providers toward reducing operational costs and enhancing service sustainability by consolidating services onto fewer active servers. Moreover, providers must also provision high levels of availability and reliability, hence cloud services are frequently replicated across servers that subsequently increases server energy consumption and resource overhead. These two objectives can present a potential conflict within cloud resource management decision making that must balance between service consolidation and replication to minimize energy consumption whilst maximizing server availability and reliability, respectively. In this paper, we propose a cuckoo optimization-based energy-reliability aware resource scheduling technique (CRUZE) for holistic management of cloud computing resources including servers, networks, storage, and cooling systems. CRUZE clusters and executes heterogeneous workloads on provisioned cloud resources and enhances the energy-efficiency and reduces the carbon footprint in datacenters without adversely affecting cloud service reliability. We evaluate the effectiveness of CRUZE against existing state-of-the-art solutions using the CloudSim toolkit. Results indicate that our proposed technique is capable of reducing energy consumption by 20.1% whilst improving reliability and CPU utilization by 17.1% and 15.7% respectively without affecting other Quality of Service parameters."
Research article - Re-implementing a legacy system,"AbstractRe-implementation is one of the alternatives to migrate a legacy software system next to conversion, wrapping and redevelopment. It is a compromise solution between automated conversion and complete redevelopment. The technical architecture can be revised and the code replaced, but the functional architecture – the use cases remains as it was. The challenge of this approach is to preserve the functionality while changing the technical implementation. This approach is taken when conversion is not feasible and redevelopment is too expensive or too great a risk. It entails more than a 1:1 transformation but less than a total rewrite. The same components remain with different contents. In this paper the case for reimplementation is presented and the process described. The tools required to support the process are identified and their use illustrated. Finally, two industrial case studies are presented, one with a VisualAge/ PL/I-DB2 system and one with a COBOL-IMS application."
"Research article - Feature analysis using information retrieval, community detection and structural analysis methods in product line adoption","AbstractIn industrial practice the clone-and-own strategy is often applied when in the pressure of high demand of customized features. The adoption of software product line (SPL) architecture is a large one time investment that affects both technical and organizational issues. The analysis of the feature structure is a crucial point in the SPL adoption process involving domain experts working at a higher level of abstraction and developers working directly on the program code. We propose automatic methods to extract feature-to-program links starting from very high level set of features provided by domain experts. For this purpose we combine call graph information with textual similarity between code and high level features. In addition, in depth understanding of the feature structure is supported by finding communities between programs and relating them to features. As features are originated from domain experts, community analysis reveals discrepancies between expert view and internal code structure. We found that communities correspond well to the high level features, with usually more than half of feature code located in specialized communities. We report experiments at two levels of features and more than 2000 Magic 4GL programs in an industrial SPL adoption project."
