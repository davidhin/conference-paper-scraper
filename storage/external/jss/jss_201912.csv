title,abstract
Research article - Mahtab: Phase-wise acceleration of regression testing for C,"AbstractSoftware regression testing consists of offline, online, and execution phases which are executed sequentially. The offline phase involves code instrumentation and test-coverage collection. Subsequently, the online phase performs program differencing, test-suite selection and prioritization. Finally, the selected test-cases are executed against the new version of software for its re-validation. Regression testing is a time-consuming process and is often on the critical path of the project. To improve the turn-around time of software development cycle, our goal is to reduce regression testing time across all phases using multi-core parallelization. This poses several challenges that stem from I/O, dependence on third-party libraries, and inherently sequential components in the overall testing process. We propose parallelization test-windows to effectively partition test-cases across threads. To measure the benefit of prioritization coupled with multi-threaded execution, we propose a new metric, EPSilon, for rewarding failure observation frequency in the timeline of test-execution. To measure the rate of code-change coverage due to regression test prioritization, we introduce ECC, a variant of the widely used APFD metric. We illustrate the effectiveness of our approach using the popular Software-artifact Infrastructure Repository (SIR) and five real-world projects from GitHub."
Research article - ECOS: An efficient task-clustering based cost-effective aware scheduling algorithm for scientific workflows execution on heterogeneous cloud systems,"AbstractCloud Computing provides an attractive execution environment for scientific workflow execution. However, due to the increasingly high charge cost of using cloud service, cost minimization for workflows execution on cloud systems has become a crucial issue. Traditional work are adopting the sophisticated scheduling techniques to address such issue. Differently, this paper has proposed an efficient task-clustering based cost-effective aware scheduling algorithm (ECOS) to minimize the cost without comprising the deadline constraint. First, with respect to the characteristics of multi-type workflows, cloud heterogeneity and cost model, we have formulated the problem of task-clustering to simplify the structure of workflows and workflow scheduling to minimize cost within the deadline constraint. Then, we have devised ECOS with two key steps: (1) vertical clustering is with the time consideration that selectively merges the sequential tasks to reduce the transferring time within the workflow; (2) horizontal clustering and greedy allocation is to aggregate the parallel tasks and greedily allocate resources to that tasks with the aim of minimizing cost within deadline. Last, we have conducted the experiment that compare with well-known task-clustering based algorithms via WorkflowSim platform. The results have demonstrated that ECOS can efficiently merge tasks and minimize the total cost without comprising the deadline constraint both in small and large datasets. Moreover, we have discussed the ECOS in terms of various schedulers and number of tasks to validate the performance of ECOS."
Research article - DeepLink: Recovering issue-commit links based on deep learning,"AbstractThe links between issues in an issue-tracking system and commits resolving the issues in a version control system are important for a variety of software engineering tasks (e.g., bug prediction, bug localization and feature location). However, only a small portion of such links are established by manually including issue identifiers in commit logs, leaving a large portion of them lost in the evolution history. To recover issue-commit links, heuristic-based and learning-based techniques leverage the metadata and text/code similarity in issues and commits; however, they fail to capture the embedded semantics in issues and commits and the hidden semantic correlations between issues and commits. As a result, this semantic gap inhibits the accuracy of link recovery.To bridge this gap, we propose a semantically-enhanced link recovery approach, named DeepLink, which is built on top of deep learning techniques. Specifically, we develop a neural network architecture, using word embedding and recurrent neural network, to learn the semantic representation of natural language descriptions and code in issues and commits as well as the semantic correlation between issues and commits. In experiments, to quantify the prevalence of missing issue-commit links, we analyzed 1078 highly-starred GitHub Java projects (i.e., 583,795 closed issues) and found that only 42.2% of issues were linked to corresponding commits. To evaluate the effectiveness of DeepLink, we compared DeepLink with a state-of-the-art link recovery approach FRLink using ten GitHub Java projects and demonstrated that DeepLink can outperform FRLink in terms of F-measure."
Research article - LDFR: Learning deep feature representation for software defect prediction,"AbstractSoftware Defect Prediction (SDP) aims to detect defective modules to enable the reasonable allocation of testing resources, which is an economically critical activity in software quality assurance. Learning effective feature representation and addressing class imbalance are two main challenges in SDP. Ideally, the more discriminative the features learned from the modules and the better the rescue performed on the imbalance issue, the more effective it should be in detecting defective modules. In this study, to solve these two challenges, we propose a novel framework named LDFR by Learning Deep Feature Representation from the defect data for SDP. Specifically, we use a deep neural network with a new hybrid loss function that consists of a triplet loss to learn a more discriminative feature representation of the defect data and a weighted cross-entropy loss to remedy the imbalance issue. To evaluate the effectiveness of the proposed LDFR framework, we conduct extensive experiments on a benchmark dataset with 27 defect data (each with three types of features), using three traditional and three effort-aware indicators. Overall, the experimental results demonstrate the superiority of our LDFR framework in detecting defective modules when compared with 27 baseline methods, except in terms of the indicator of Precision."
Research article - Automated code-based test selection for software product line regression testing,"AbstractRegression testing for software product lines (SPLs) is challenging and can be expensive because it must ensure that all the products of a product family are correct whenever changes are made. SPL regression testing can be made efficient through a test case selection method that selects only the test cases relevant to the changes. Some approaches for SPL test case selection have been proposed but either they were not efficient by requiring intervention from human experts or they cannot be used if requirements specifications, architecture and/or traceabilities for test cases are not available or partially eroded. To address these limitations, we propose an automated method of source code-based regression test selection for SPLs. Our method reduces the repetition of the selection procedure and minimizes the in-depth analysis effort for source code and test cases based on the commonality and variability of a product family. Evaluation results of our method using six product lines show that our method reduces the overall time to perform regression testing by 14.8% ∼ 49.1% on average compared to an approach of repetitively applying Ekstazi, which is the state-of-the-art regression test selection method for a single product, to each product of a product family."
Research article - Toward collisions produced in requirements rankings: A qualitative approach and experimental study,"AbstractRequirements prioritization is an important issue that determines the way requirements are selected and processed in software projects. There already exist specific methods to classify and prioritize requirements, most of them based on quantitative measures. However, most of existing approaches do not consider collisions, which are an important concern in large-scale requirements sets and, more specifically, in agile development processes where requirements have to be uniquely selected for each software increment. In this paper, we propose QMPSR (Qualitative Method for Prioritizing Software Requirements), an approach that features the prioritization of requirements by considering qualitative elements that are related to the project's priorities. Our approach highlights a prioritization method that has proven to reduce collisions in software requirements rankings. Furthermore, QMPSR improves accuracy in classification when facing large-scale requirements sets, featuring no scalability problems as the number of requirements increases. We formally introduce QMPSR and then define prioritization effort and collision metrics to carry out comprehensive experiments involving different sets of requirements, comparing our approach with well-known existing prioritization methods. The experiments have provided satisfactory results, overcoming existing approaches and ensuring scalability."
Research article - An empirical study on bug propagation through code cloning,"AbstractCode clones are identical or nearly similar code fragments in a code-base. According to the existing studies, code clones are directly related to bugs. Code cloning, creating code clones, is suspected to propagate temporarily hidden bugs from one code fragment to another. However, there is no study on the intensity of bug-propagation through code cloning. In this paper, we define two clone evolutionary patterns that reasonably indicate bug propagation through code cloning. By analyzing software evolution history, we identify those code clones that evolved following the bug propagation patterns. According to our study on thousands of commits of seven subject systems, overall 18.42% of the clone fragments that experience bug-fixes contain propagated bugs. Type-3 clones are primarily involved with bug-propagation. Bug propagation is more likely to occur in the clone fragments that are created in the same commit rather than in different commits. Moreover, code clones residing in the same file have a higher possibility of containing propagated bugs compared to those residing in different files. Severe bugs can sometimes get propagated through code cloning. Automatic support for immediately identifying occurrences of bug-propagation can be beneficial for software maintenance. Our findings are important for prioritizing code clones for management."
Research article - AndroidOff:Offloading android application based on cost estimation,"AbstractComputation offloading is a promising way of improving the performance and reducing the battery power consumption, since it moves some time-consuming computation activities to nearby servers. Although various approaches have been proposed to support computation offloading, we argue that there is still sufficient space for improvements, since existing approaches cannot accurately estimate the execution costs. As a result, we find that their offloading plans are less optimized. To handle the problem, in this paper, given an Android application, we propose a novel approach, called AndroidOff, that supports offloading at the granularity of objects. Supporting such capability is challenging due to the two reasons: (1) through dynamic execution, it is feasible to collect the execution costs of only partial methods, and (2) it is difficult to accurately estimate the execution costs of the remaining methods. To overcome the challenges, given an Android application, AndroidOff first combines static and dynamic analysis to predict the execution costs of all its methods. After all the costs are estimated, AndroidOff synthesizes an offloading plan, in which determines the offloading details. We evaluate AndroidOff on a real-world application, with two mobile devices. Our results show that, compared with other approaches, AndroidOff saves the response time by 8%–49% and reduces the energy consumption by 12%–49% on average for computation-intensive applications."
Research article - Software architecture design in global software development: An empirical study,"AbstractIn Global Software Development (GSD), the additional complexity caused by global distance requires processes to ease collaboration difficulties, reduce communication overhead, and improve control. How development tasks are broken down, shared and prioritized is key to project success.While the related literature provides some support for architects involved in GSD, guidelines are far from complete. This paper presents a GSD Architectural Practice Framework reflecting the views of software architects, all of whom are working in a distributed setting. In-depth interviews with architects from seven different GSD organizations revealed a complex set of challenges and practices.We found that designing software for distributed teams requires careful selection of practices that support understanding and adherence to defined architectural plans across sites. Teams used Scrum which aided communication, and Continuous Integration which helped solve synchronization issues. However, teams deviated from the design, causing conflicts. Furthermore, there needs to be a balance between the self-organizing Scrum team methodology and the need to impose architectural design decisions across distributed sites.The research presented provides an enhanced understanding of architectural practices in GSD companies. Our GSD Architectural Practice Framework gives practitioners a cohesive set of warnings, which for the most part, are matched by recommendations."
Research article - Collaborative configuration approaches in software product lines engineering: A systematic mapping study,"AbstractIn the context of software product line engineering, collaborative configuration is a decision-making process where multiple stakeholders contribute in building a single product specification. Several approaches addressing collaboration during configuration have already been proposed, but we still have little hard evidence about their effectiveness and little understanding about how collaborative configuration process should be carried out. This paper presents a classification framework to help understand existing collaborative configuration approaches. To elaborate it, a systematic mapping study was conducted guided by three research questions and 41 primary studies was selected out of 238 identified ones. The proposed framework is composed of four dimensions capturing main aspects related to configuration approaches: purpose, collaboration, process and tool. Each dimension is itself multi-faceted and a set of attributes is associated to each facet. Using this framework, we position and classify existing approaches, structure the representation of each approach characteristics, highlight their strengths and weaknesses, compare them to each other, and identify open issues. This study gives a solid foundation for classifying existing and future approaches for product lines collaborative configuration. Researchers and practitioners can use our framework for identifying existing research/technical gaps to attack, better scoping their own contributions, or understanding existing ones."
Research article - Characterization of implied scenarios as families of common behavior,"AbstractConcurrent systems face a threat to their reliability in emergent behaviors, which are not included in the specification but can happen during runtime. When concurrent systems are modeled in a scenario-based manner, it is possible to detect emergent behaviors as implied scenarios (ISs) which, analogously, are unexpected scenarios that can happen due to the concurrent nature of the system. Until now, the process of dealing with ISs can demand significant time and effort from the user, as they are detected and dealt with in a one by one basis. In this paper, a new methodology is proposed to deal with various ISs at a time, by finding Common Behaviors (CBs) among them. Additionally, we propose a novel way to group CBs into families utilizing a clustering technique using the Smith-Waterman algorithm as a similarity measure. Thus allowing the removal of multiple ISs with a single fix, decreasing the time and effort required to achieve higher system reliability. A total of 1798 ISs were collected across seven case studies, from which 14 families of CBs were defined. Consequently, only 14 constraints were needed to resolve all collected ISs, applying our approach. These results support the validity and effectiveness of our methodology."
Research article - An empirical study of security warnings from static application security testing tools,"AbstractThe Open Web Application Security Project (OWASP) defines Static Application Security Testing (SAST) tools as those that can help find security vulnerabilities in the source code or compiled code of software. Such tools detect and classify the vulnerability warnings into one of many types (e.g., input validation and representation). It is well known that these tools produce high numbers of false positive warnings. However, what is not known is if specific types of warnings have a higher predisposition to be false positives or not. Therefore, our goal is to investigate the different types of SAST-produced warnings and their evolution over time to determine if one type of warning is more likely to have false positives than others. To achieve our goal, we carry out a large empirical study where we examine 116 large and popular C++ projects using six different state-of-the-art open source and commercial SAST tools that detect security vulnerabilities. In order to track a piece of code that has been tagged with a warning, we use a new state of the art framework called cregit+ that traces source code lines across different commits. The results demonstrate the potential of using SAST tools as an assessment tool to measure the quality of a product and the possible risks without manually reviewing the warnings. In addition, this work shows that pattern-matching static analysis technique is a very powerful method when combined with other advanced analysis methods."
Research article - Industry requirements for FLOSS governance tools to facilitate the use of open source software in commercial products,"AbstractVirtually all software products incorporate free/libre and open source software (FLOSS) components. However, ungoverned use of FLOSS components can result in legal and financial risks, and risks to a firm’s intellectual property. To avoid these risks, companies must govern their FLOSS use through open source governance processes and by following industry best practices. A particular challenge is license compliance. To manage the complexity of governance and compliance, companies should use tools and well-defined processes. This paper investigates and presents industry requirements for FLOSS governance tools, followed by an evaluation of the suggested requirements.We chose eleven companies with an advanced understanding of open source governance and interviewed their FLOSS governance experts to derive a theory of industry requirements for tooling. We extended our previous work adding the requirement category on the architecture model for software products.We then analyzed the features of leading governance tools and used this analysis to evaluate two categories of our theory: FLOSS license scanning and FLOSS components in product bills of materials. The result is a list of FLOSS governance requirements. For practical relevance, we cast our theory as a requirements specification for FLOSS governance tools."
Research article - Measuring the reusability of software components using static analysis metrics and reuse rate information,"AbstractNowadays, the continuously evolving open-source community and the increasing demands of end users are forming a new software development paradigm; developers rely more on reusing components from online sources to minimize the time and cost of software development. An important challenge in this context is to evaluate the degree to which a software component is suitable for reuse, i.e. its reusability. Contemporary approaches assess reusability using static analysis metrics by relying on the help of experts, who usually set metric thresholds or provide ground truth values so that estimation models are built. However, even when expert help is available, it may still be subjective or case-specific. In this work, we refrain from expert-based solutions and employ the actual reuse rate of source code components as ground truth for building a reusability estimation model. We initially build a benchmark dataset, harnessing the power of online repositories to determine the number of reuse occurrences for each component in the dataset. Subsequently, we build a model based on static analysis metrics to assess reusability from five different properties: complexity, cohesion, coupling, inheritance, documentation and size. The evaluation of our methodology indicates that our system can effectively assess reusability as perceived by developers."
"Research article - Survey-based investigation, feature extraction and classification of Greek municipalities maturity for open source adoption and migration prospects","AbstractWhile FOSS solutions have attracted a significant amount of attention, alongside with necessary and growing IT related expenditure for the digitalization of public administration, authorities face the question of whether migration to such solutions is feasible and/or worthwhile. The purpose of the presented work is to analyze existing domain research and extract key features, grouped in three major areas (namely Readiness, Ease of Use and Gain from migration), that should be investigated prior to a migration attempt. Following and building upon an extensive survey on Greek municipalities, the latter are categorized via the k-means non-supervised machine learning method to 3 levels of maturity regarding candidate participation in relevant projects. A combined scoring approach, based on similar features of the target FOSS solution as well as the aforementioned municipality categorization, is presented in order to detect a priori good candidate combinations (municipality and software) for minimizing a migration project risk. The method is validated through the aforementioned survey on municipalities with confirmed FOSS usage, indicating that selection in the proposed organized manner can aid in harvesting FOSS benefits. Furthermore, it is compared against a popular maturity model method (BPMMM) in order to comment on the applicability and classification process."
Research article - A topological analysis of communication channels for knowledge sharing in contemporary GitHub projects,"AbstractWith over 28 million developers, success of the GitHub collaborative platform is highlighted through an abundance of communication channels among contemporary software projects. Knowledge is broken into two forms and its sharing (through communication channels) can be described as externalization or combination by the SECI model. Such platforms have revolutionized the way developers work, introducing new channels to share knowledge in the form of pull requests, issues and wikis. It is unclear how these channels capture and share knowledge. In this research, our goal is to analyze these communication channels in GitHub. First, using the SECI model, we are able to map how knowledge is shared through the communication channels. Then in a large-scale topology analysis of seven library package projects (i.e., involving over 70 thousand projects), we extracted insights of the different communication channels within GitHub. Using two research questions, we explored the evolution of the channels and adoption of channels by both popular and unpopular library package projects. Results show that (i) contemporary GitHub Projects tend to adopt multiple communication channels, (ii) communication channels change over time and (iii) communication channels are used to both capture new knowledge (i.e., externalization) and updating existing knowledge (i.e., combination)."
Research article - Finding needles in a haystack: Leveraging co-change dependencies to recommend refactorings,"AbstractA fine-grained co-change dependency arises when two fine-grained source-code entities, e.g., a method, change frequently together. This kind of dependency is relevant when considering remodularization efforts (e.g., to keep methods that change together in the same class). However, existing approaches for recommending refactorings that change software decomposition (such as a move method) do not explore the use of fine-grained co-change dependencies. In this paper we present a novel approach for recommending move method and move field refactorings, which removes co-change dependencies and evolutionary smells, a particular type of dependency that arise when fine-grained entities that belong to different classes frequently change together. First we evaluate our approach using 49 open-source Java projects, finding 610 evolutionary smells. Our approach automatically computes 56 refactoring recommendations that remove these evolutionary smells, without introducing new static dependencies. We also evaluate our approach by submitting pull-requests with the recommendations of our technique, in the context of one large and two medium size proprietary Java systems. Quantitative results show that our approach outperforms existing approaches for recommending refactorings when dealing with co-change dependencies. Qualitative results show that our approach is promising, not only for recommending refactorings but also to reveal opportunities of design improvements."
