title,abstract
Research article - An aggregated coupling measure for the analysis of object-oriented software systems,"AbstractCoupling is a fundamental property of software systems which is strongly connected with the quality of software design and has high impact on program understanding. The coupling between software components influences software maintenance and evolution as well. In order to ease the maintenance and evolution processes it is essential to estimate the impact of changes made in the software system, coupling indicating such a possible impact. This paper introduces a new aggregated coupling measurement which captures both the structural and the conceptual characteristics of coupling between the software components. The proposed measure combines the textual information contained in the source code with the structural relationships between software components. We conduct several experiments which underline that the proposed aggregated coupling measure reveals new characteristics of coupling and is also effective for change impact analysis."
Research article - Combining malleability and I/O control mechanisms to enhance the execution of multiple applications,"AbstractThis work presents a common framework that integrates CLARISSE, a cross-layer runtime for the I/O software stack, and FlexMPI, a runtime that provides dynamic load balancing and malleability capabilities for MPI applications. This integration is performed both at application level, as libraries executed within the application, as well as at central-controller level, as external components that manage the execution of different applications. We show that a cooperation between both runtimes provides important benefits for overall system performance: first, by means of monitoring, the CPU, communication and I/O performances of all executing applications are collected, providing a holistic view of the complete platform utilization. Secondly, we introduce a coordinated way of using CLARISSE and FlexMPI control mechanisms, based on two different optimization strategies, with the aim of improving both the application I/O and overall system performance. Finally, we present a detailed description of this proposal, as well as an empirical evaluation of the framework on a cluster showing significant performance improvements at both application and wide-platform levels. We demonstrate that with this proposal the overall I/O time of an application can be reduced by up to 49% and the aggregated FLOPS of all running applications can be increased by 10% with respect to the baseline case."
Research article - Software project management in high maturity: A systematic literature mapping,"AbstractHigh maturity in software development involves statistically controlling the performance of critical subprocesses and using the predictability thus gained to manage projects with better planning precision and monitoring control. Maturity models such as CMMI mention statistical and other quantitative methods, techniques, and tools supporting high-maturity project management, but do not provide details about them, their use or their available types. Thus, knowledge is lacking on how to support software process improvement initiatives to select and apply statistical and other quantitative methods, techniques and tools in this context. The goal of this study is to identify various methods, techniques, and tools which can assist in high-maturity software project management. By conducting a systematic literature mapping, we identified 108 papers describing 153 contributions. We describe the contributions identified, classifying them by their type, their software technology maturation phase, the method by which they were evaluated, the development methods and characteristics which they support, and the process/indicator areas to which they were applied. We hope this work can help fill the knowledge gap on the statistical and other quantitative methods, techniques and tools actually being proposed, evaluated, experimented with and adopted by organizations to support quantitative high-maturity software project management."
Research article - A distributed data management system to support large-scale data analysis,"AbstractDistributed data management is a key technology to enable efficient massive data processing and analysis in cluster-computing environments. Specifically, in environments where the data volumes are beyond the system capabilities, big data files are required to be summarized by representative samples with the same statistical properties as the whole dataset. This paper proposes a big data management system (BDMS) based on distributed random sample data blocks. It presents a high-level architecture design of the BDMS which extends the current distributed file systems. This system offers certain functionalities for block-level management such as statistically-aware data partitioning, data blocks organization, and data blocks selection. This paper also presents a round-random partitioning scheme to represent a big dataset as a set of non-overlapping data blocks; each block is a random sample of the whole dataset. Based on the presented scheme, two algorithms are introduced as an implementation strategy to convert the HDFS blocks of a big file into a set of random sample data blocks which is also stored in HDFS. The experimental results show that the execution time of partitioning operation is acceptable in the real applications because this operation is only performed once on each input data file."
Research article - Tracking runtime concurrent dependences in java threads using thread control profiling,"AbstractMore than 75% of recent Java projects include some form of concurrent programming. Due to complex interactions between multi-threads, concurrent programs are often harder to understand and test than single threaded programs. To facilitate understanding and testing of concurrent programs, we developed a new profiling method called TCP (Thread Control Profiling). Outputs of TCP presents frequencies of control dependence, which includes thread creation, thread synchronization, interruption, and so on, of the executed thread. TCP first performs static analysis of detailed concurrency syntax and semantics of Java to construct the profiling graph model TCDG (Thread Control Dependence Graph). TCDG is then used for instrumentation and for generating profiles. We have evaluated TCP using a case study and a few experiments. The case study shows that TCP method can effectively prioritize test cases for testing concurrent programs. One experiment shows that outputs from TCP facilitate developers’ understanding of concurrent code. Other experiments evaluate various possible overheads introduced by the TCP method. Results show that TCP can provide rich and useful information with reasonable costs."
Research article - Mobile user behavior based topology formation and optimization in ad hoc mobile cloud,"AbstractAs more and more mobile users use social networks to interact with other people currently, the resource limitation of mobile devices becomes an urgent problem to be solved. However, being a novel type of mobile cloud, the ad hoc mobile cloud allows participating mobile devices to share resources with other neighboring mobile devices, which can help the mobile device process a large amount of compute-intensive applications and conserve energy. In this paper, a mobile user behavior based topology formation and optimization in ad hoc mobile cloud is proposed. The mobile device nodes having similar behavioral features are grouped and formed as an ad hoc mobile cloud, which can reduce network delay and improve efficiency of node interaction. Subsequently, a flower pollination based offloading strategy is presented to reduce response time and save energy consumption. The experiment results show that our proposed ad hoc mobile cloud topology outperforms in scalability, while the offloading algorithm can reduce approximately 25%–50% response time and energy consumption as compared with that of other benchmark algorithms."
Research article - Impact of usability on process lead-time in information systems: A case study,"AbstractTechnological advancements have started to demand the need for information systems to survive in the business world. There is an inherent need to be fast and efficient to conquer the market. In order to reduce time to market there is a need to shorten the lead-time in design and development process of products. Hence, lead-time is a significant performance metric for a product development organizations having information systems. It is important to explore the factors which affect the process lead-time in information systems. The objective of this study is to explore the impact that usability (one of the factors that may affect the process lead-time) has on process lead-time. This paper presents a case study involving qualitative and quantitate data from Volvo Group, which uses an information system called KOLA for their design and development process. Data was collected through 29 interviews, 17 think aloud sessions, 5 document studies, and 73 responses from a survey. This study identifies which usability factors and heuristics that have an impact on process lead-time. Moreover, the results show that the users level of experience determines the effective utilization of accelerators and thereby affecting the process lead-time. Finally, usability plays an important role in shortening the process lead-time in information systems."
Research article - Empirical software engineering: From discipline to interdiscipline,"AbstractEmpirical software engineering has received much attention in recent years and coined the shift from a more design-science-driven engineering discipline to an insight-oriented, and theory-centric one. Yet, we still face many challenges, among which some increase the need for interdisciplinary research. This is especially true for the investigation of social, cultural and human-centric aspects of software engineering. Although we can already observe an increased recognition of the need for more interdisciplinary research in (empirical) software engineering, such research configurations come with challenges barely discussed from a scientific point of view. In this position paper, we critically reflect upon the epistemological setting of empirical software engineering and elaborate its configuration as an Interdiscipline. In particular, we (1) elaborate a pragmatic view on empirical research for software engineering reflecting a cyclic process for knowledge creation, (2) motivate a path towards symmetrical interdisciplinary research, and (3) adopt five rules of thumb from other interdisciplinary collaborations in our field before concluding with new emerging challenges. This supports to elevate empirical software engineering from a developing discipline moving towards a paradigmatic stage of normal science to one that configures interdisciplinary teams and research methods symmetrically."
Research article - Does the fault reside in a stack trace? Assisting crash localization by predicting crashing fault residence,"AbstractGiven a stack trace reported at the time of software crash, crash localization aims to pinpoint the root cause of the crash. Crash localization is known as a time-consuming and labor-intensive task. Without tool support, developers have to spend tedious manual effort examining a large amount of source code based on their experience. In this paper, we propose an automatic approach, namely CraTer, which predicts whether a crashing fault resides in stack traces or not (referred to as predicting crashing fault residence). We extract 89 features from stack traces and source code to train a predictive model based on known crashes. We then use the model to predict the residence of newly-submitted crashes. CraTer can reduce the search space for crashing faults and help prioritize crash localization efforts. Experimental results on crashes of seven real-world projects demonstrate that CraTer can achieve an average accuracy of over 92%."
Research article - Tuning self-adaptation in cyber-physical systems through architectural homeostasis,"AbstractSelf-adaptive software-intensive cyber-physical systems (sasiCPS) encounter a high level of run-time uncertainty. State-of-the-art architecture-based self-adaptation approaches assume designing against a fixed set of situations that warrant self-adaptation. As a result, failures may appear when sasiCPS operate in environment conditions they are not specifically designed for. In response, we propose to increase the homeostasis of sasiCPS, i.e., the capacity to maintain an operational state despite run-time uncertainty, by introducing run-time changes to the architecture-based self-adaptation strategies according to environment stimuli. In addition to articulating the main idea of architectural homeostasis, we introduce four mechanisms that reify the idea: (i) collaborative sensing, (ii) faulty component isolation from adaptation, (iii) enhancing mode switching, and (iv) adjusting guards in mode switching. Moreover, our experimental evaluation of the four mechanisms in two different case studies confirms that allowing a complex system to change its self-adaptation strategies helps the system recover from run-time errors and abnormalities and keep it in an operational state."
