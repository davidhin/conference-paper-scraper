title,abstract
Research article - CrossRec: Supporting software developers by recommending third-party libraries,"AbstractWhen creating a new software system, or when evolving an existing one, developers do not reinvent the wheel but, rather, seek available libraries that suit their purpose. In such a context, open source software repositories contain rich resources that can provide developers with helpful advice to support their tasks. However, the heterogeneity of resources and the dependencies among them are the main obstacles to the effective mining and exploitation of the available data. In this sense, advanced techniques and tools are needed to mine the metadata to bring in meaningful recommendations. In this paper, we present CrossRec, a recommender system to assist open source software developers in selecting suitable third-party libraries. CrossRec exploits a collaborative filtering technique to recommend libraries to developers by relying on the set of dependencies, which are currently included in the project being developed. We perform an empirical evaluation to compare the proposed approach with three state-of-the-art baselines, i.e., LibRec, LibFinder, and LibCUP on three considerably large datasets. The experimental results show that CrossRec overcomes the limitation of the baselines by recommending also libraries with a specific version. More importantly, it outperforms LibRec and LibCUP with respect to various quality metrics."
Research article - Model checking of in-vehicle networking systems with CAN and FlexRay,"AbstractAn in-vehicle networking (IVN) system consists of electronic components that are connected by buses and communicate through multiple protocols according to their requirements. In practice, intelligent vehicles need to exchange data between subsystems that use various protocols, such as the controller area network (CAN) and FlexRay. Such systems are more likely to encounter delays and message loss during transmission, presenting serious safety issues. Moreover, IVN systems are extremely complicated because of their large number of nodes, multiple communication protocols, and diverse topologies. As a result, it is difficult to check the timed properties of the system directly and accurately. In this paper, we present an appropriate abstraction for modeling IVN systems that utilize CAN and FlexRay during the design phase. The timed properties of communication are analyzed using the UPPAAL platform. As there are numerous IVN system structures, a framework is developed to build a model for an IVN system design. The model is to verify the transmission of messages between different protocols. We evaluate the validity, applicability, and reusability of the framework and show the performance of the framework for verifying IVN system models."
Research article - On the testing resource allocation problem: Research trends and perspectives,"AbstractIn testing a software application, a primary concern is how to effectively plan the assignment of resources available for testing to the software components so as to achieve a target goal under given constraints. In the literature, this is known as testing resources allocation problem (TRAP). Researchers spent a lot of effort to propose models for supporting test engineers in this task, and a variety of solutions exist to assess the best trade-off between testing time, cost and quality of delivered products. This article presents a systematic mapping study aimed at systematically exploring the TRAP research area in order to provide an overview on the type of research performed and on results currently available. A sample of 68 selected studies has been classified and analyzed according to defined dimensions. Results give an overview of the state of the art, provide guidance to improve practicability and allow outlining a set of directions for future research and applications of TRAP solutions."
Research article - SPELLing out energy leaks: Aiding developers locate energy inefficient code,"AbstractAlthough hardware is generally seen as the main culprit for a computer’s energy usage, software too has a tremendous impact on the energy spent. Unfortunately, there is still not enough support for software developers so they can make their code more energy-aware.This paper proposes a technique to detect energy inefficient fragments in the source code of a software system. Test cases are executed to obtain energy consumption measurements, and a statistical method, based on spectrum-based fault localization, is introduced to relate energy consumption to the source code. The result of our technique is an energy ranking of source code fragments pointing developers to possible energy leaks in their code. This technique was implemented in the SPELL toolkit.Finally, in order to evaluate our technique, we conducted an empirical study where we asked participants to optimize the energy efficiency of a software system using our tool, while also having two other groups using no tool assistance and a profiler, respectively. We showed statistical evidence that developers using our technique were able to improve the energy efficiency by 43% on average, and even out performing a profiler for energy optimization."
Research article - Predicting failures in multi-tier distributed systems,"AbstractMany applications are implemented as multi-tier software systems, and are executed on distributed infrastructures, like cloud infrastructures, to benefit from the cost reduction that derives from dynamically allocating resources on-demand. In these systems, failures are becoming the norm rather than the exception, and predicting their occurrence, as well as locating the responsible faults, are essential enablers of preventive and corrective actions that can mitigate the impact of failures, and significantly improve the dependability of the systems. Current failure prediction approaches suffer either from false positives or limited accuracy, and do not produce enough information to effectively locate the responsible faults.In this paper, we present PreMiSE, a lightweight and precise approach to predict failures and locate the corresponding faults in multi-tier distributed systems. PreMiSE blends anomaly-based and signature-based techniques to identify multi-tier failures that impact on performance indicators, with high precision and low false positive rate. The experimental results that we obtained on a Cloud-based IP Multimedia Subsystem indicate that PreMiSE can indeed predict and locate possible failure occurrences with high precision and low overhead."
Research article - An OSLC-based environment for system-level functional testing of ERTMS/ETCS controllers,"AbstractProduct and application life-cycle management (PLM/ALM) are the processes that govern a product and a software system, respectively, encompassing the creation, deployment and operation of a system from the beginning to the end of its life. As both PLM and ALM require cross-discipline collaboration and cooperation, tools integration and inter-operation are necessary to enable the efficient and effective usage of tool suites supporting the management of the entire system life-cycle and overcome the limitations of all-in-one solutions from one tool vendor. In this context, the Open Services for Life-cycle Collaboration (OSLC) initiative proposes a set of specifications to allow a seamless integration based on linked data. This paper describes the work performed within the ARTEMIS JU project CRYSTAL to develop an environment for the functional system-level testing of railway controllers, relying on OSLC to enable inter-operation with existing PLM/ALM tools. A concrete realization of the proposed architecture is described also discussing some design and implementation choices. A real industrial case study is used to exemplify the features and the usage of the environment in testing one of the functionalities of the Radio Block Centre, the vital core of the European Rail Traffic Management System/European Train Control System (ERTMS/ETCS) Control System."
Research article - Performance evaluation of web service response time probability distribution models for business process cycle time simulation,"AbstractContextThe adoption of Business Process Management (BPM) is enabling companies to improve the pace of building new capabilities, enhancing existing ones, and measuring process performance to identify bottlenecks. It is essential to compute the cycle time of the process to assess the performance of a business process. The cycle time typically forms part of service level agreements (SLAs) and is a crucial contributor to the overall user experience and productivity. The simulation technique is versatile and has broad applicability for determining realistic cycle time using historical data of web service response time. BPM tools offer inadequate support for modeling input data used in simulation in the form of descriptive statistics or standard probability distributions like normal, lognormal, which results in inaccurate simulation results.ObjectiveWe evaluate the effectiveness of different parametric and non-parametric probability distributions for modeling data of web service response time. We further assess how the choice of probability distribution impacts the accuracy of the simulated cycle time of a business process. The work is the first of such a study using real-world data for encouraging Business Process Simulation Specification (BPSim) standard setters and BPM tools to enhance their support for such distributions in their simulation engine.MethodWe consider several parametric and non-parametric distributions and explore how well these distributions fit web service response time from extensive public and a real-world dataset. The cycle time of the business process of a real-world system is simulated using the identified distributions to model the underlying web service data.ResultsOur results show that kernel distribution is the most suitable choice, followed by Burr. Kernel outperforms Burr by 86.63% for the public and 84.21% for the real-world dataset. The choice of distribution affects the percentile ranks like 90 and above than the median. The use of single-point values underestimates cycle time values at higher percentiles.ConclusionBased on our empirical results, we recommend the addition of kernel and Burr to the current list of distributions supported by BPSim and BPM tools."
Research article - A machine-learning based ensemble method for anti-patterns detection,"AbstractAnti-patterns are poor solutions to recurring design problems. Several empirical studies have highlighted their negative impact on program comprehension, maintainability, as well as fault-proneness. A variety of detection approaches have been proposed to identify their occurrences in source code. However, these approaches can identify only a subset of the occurrences and report large numbers of false positives and misses. Furthermore, a low agreement is generally observed among different approaches. Recent studies have shown the potential of machine-learning models to improve this situation. However, such algorithms require large sets of manually-produced training-data, which often limits their application in practice.In this paper, we present SMAD (SMart Aggregation of Anti-patterns Detectors), a machine-learning based ensemble method to aggregate various anti-patterns detection approaches on the basis of their internal detection rules. Thus, our method uses several detection tools to produce an improved prediction from a reasonable number of training examples. We implemented SMAD for the detection of two well known anti-patterns: God Class and Feature Envy. With the results of our experiments conducted on eight java projects, we show that: (1) Our method clearly improves the so aggregated tools; (2) SMAD significantly outperforms other ensemble methods."
Research article - Component-based development of embedded systems with GPUs,"AbstractOne pressing challenge of many modern embedded systems is to successfully deal with the considerable amount of data that originates from the interaction with the environment. A recent solution comes from the use of GPUs, providing a significantly improved performance for data-parallel applications. Another trend in the embedded systems domain is component-based development. However, existing component-based approaches lack specific support to develop embedded systems with GPUs. As a result, components with GPU capability need to encapsulate all the required GPU information, leading to component specialization to specific platforms, hence drastically impeding component reusability.To facilitate component-based development of embedded systems with GPUs, we introduce the concept of flexible components. This increases the design flexibility by allowing the system developer to decide component allocation (i.e., either the CPU or GPU) at a later stage of the system development, with no change to the component implementation. Furthermore, we provide means to automatically generate code for adapting flexible components corresponding to their hardware placement, as well as code for component communication. Through the introduced support, components with GPU capability are platform-independent, and can be executed, without manual adjustment, on a large variety of hardware (i.e., platforms with different GPU characteristics)."
Research article - A model-driven approach for the development of native mobile applications focusing on the data layer,"AbstractThe data layer access design is a critical task for mobile applications which need constant access to remote data, making them available offline in case of network connectivity problems. Moreover, the variety of mobile operating systems and platforms (fragmentation phenomenon) that handles data storage differently affects the portability of mobile applications. This issue suggests the use of Model-Driven Development (MDD) approach that may ease the smartphone application development for different platforms. Therefore, we propose an extension of the Model-Oriented Web Approach (MoWebA) for the development of native mobile applications focusing on the data layer. MoWebA Mobile (as we name the proposal) covers data persistence concepts to achieve offline applications in case of network connectivity problems. It defines meta-models and the Architecture-Specific Model (ASM) for data persistence and data provider to design the data sources of mobile applications. As well, we propose transformation rules for the generation of Android and Windows Phone applications. The MoWebA Mobile process was illustrated by means of modeling, design, and development of a typical mobile application. Despite the learning curve of the approach, the first evaluation suggests that MoWebA Mobile has the potential for supporting the mobile design applications considering the persistence of data."
Research article - Automatic retrieval and analysis of high availability scenarios from system execution traces: A case study on hot standby router protocol,"AbstractHigh availability (HA) is becoming an increasingly important requirement in a growing number of domains. It is even mandatory for critical systems, such as networking and communications, that cannot afford downtime. Such systems often monitor the state of crucial services and produce huge amounts of execution trace data, where functional and non-functional log entries are intertwined; hence they are hard to dissociate and analyze. Dynamic analysis aims at capturing and analyzing run-time behavior of a system based on its execution traces. In this paper, we apply dynamic analysis to retrieve and analyze HA scenarios from system execution traces. Our proposed approach aims to help analysts understand and report on how a highly available system detects and recovers from failures. As a proof of concept, we have selected the Hot Standby Router Protocol (HSRP) in order to demonstrate the applicability of our approach. We have evaluated empirically the effectiveness of our technique using four real-world case studies of IP networks running HSRP. Results have shown that high availability scenarios were successfully retrieved and analyzed. Moreover, results have shown that our prototype tool HAAnalyzer was able to effectively unveil high availability behavioral and temporal errors, that were seeded in the execution traces."
Research article - On the performance of method-level bug prediction: A negative result,"AbstractBug prediction is aimed at identifying software artifacts that are more likely to be defective in the future. Most approaches defined so far target the prediction of bugs at class/file level. Nevertheless, past research has provided evidence that this granularity is too coarse-grained for its use in practice. As a consequence, researchers have started proposing defect prediction models targeting a finer granularity (particularly method-level granularity), providing promising evidence that it is possible to operate at this level. Particularly, models mixing product and process metrics provided the best results.We present a study in which we first replicate previous research on method-level bug-prediction, by using different systems and timespans. Afterwards, based on the limitations of existing research, we (1) re-evaluate method-level bug prediction models more realistically and (2) analyze whether alternative features based on textual aspects, code smells, and developer-related factors can be exploited to improve method-level bug prediction abilities. Key results of our study include that (1) the performance of the previously proposed models, tested using the same strategy but on different systems/timespans, is confirmed; but, (2) when evaluated with a more practical strategy, all the models show a dramatic drop in performance, with results close to that of a random classifier. Finally, we find that (3) the contribution of alternative features within such models is limited and unable to improve the prediction capabilities significantly. As a consequence, our replication and negative results indicate that method-level bug prediction is still an open challenge."
Research article - LAURA architecture: Towards a simpler way of building situation-aware and business-aware IoT applications,"AbstractThe explosion of smart objects made companies rethink their Business Model (BM) using Wireless Sensor Networks (WSN) and the Internet of Things (IoT) aiming to improve their Business Processes (BP) to achieve competitiveness. Business environments are complex due to the wide variety of technologies, hardware and software solutions that compose heterogeneous enterprise environments. On the other hand, putting real-world IoT scenarios into practice is still a challenge for even experienced developers, because it requires low-level programming skills and, at the same time, specific domain knowledge of a company`s BM. This research paper proposes LAURA – Lean AUtomatic code generation for situation-aware and business-awaRe Applications, a flexible, service-oriented and general open-source conceptual architecture, designed to support the deployment of decoupled IoT applications. Empirical evaluation has shown that LAURA simplifies the development of final Situation-Aware or Business-Aware applications, reducing the need for specialized IoT low-level knowledge, while showing an acceptable performance. LAURA also provides the freedom and independence to modify, adapt or integrate its architecture according to specific needs of the stakeholders."
Research article - Augmenting ant colony optimization with adaptive random testing to cover prime paths,"AbstractTest data generation has a notable impact on the performance of software testing. A well-known approach to automate this activity is search-based test data generation. Most studies in this area use branch coverage as the test criterion. Since the prime path coverage criterion includes branch coverage, it has higher probability to detect software failures than the branch coverage criterion. This paper customizes and improves ant colony optimization (ACO) to provide a test data generation approach for covering prime paths. The proposed approach incorporates the notion of input space partitioning to maintain pheromone values in the search space. In addition, it employs the idea of adaptive random testing in the local search. At last, it uses the information of program predicates in order to make a relation between the logic of the program and pheromone values in the search space. The experimental results confirm the positive effects of the mentioned contributions, especially for programs with complex predicates. Furthermore, they represent that, on average, test suites generated by the proposed approach has 9% better mutation score in comparison to test suites produced by EvoSuite, a well-known test data generation tool."
