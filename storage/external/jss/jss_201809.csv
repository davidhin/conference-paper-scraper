title,abstract
Research article - Enhancing change prediction models using developer-related factors,"AbstractContinuous changes applied during software maintenance risk to deteriorate the structure of a system and are a threat to its maintainability. In this context, predicting the portions of source code where specific maintenance operations should be focused on may be crucial for developers to prevent maintainability issues. Previous work proposed change prediction models relying on product and process metrics as predictors of change-prone source code classes. However, we believe that existing approaches still miss an important piece of information, i.e., developer-related factors that are able to capture the complexity of the development process under different perspectives. In this paper, we firstly investigate three change prediction models that exploit developer-related factors (e.g., number of developers working on a class) as predictors of change-proneness of classes and then we compare them with existing models. Our findings reveal that these factors improve the capabilities of change prediction models. Moreover, we observed interesting complementarities among the prediction models. For this reason, we devised a novel change prediction model exploiting the combination of developer-related factors and product and evolution metrics. The results show that such a combined model is up to 22% more effective than the single models in the identification of change-prone classes."
Research article - Perceived importance of agile requirements engineering practices â€“ A survey,"AbstractContext:Requirements Engineering (RE) is one of the key processes in software development. Since Agile software development advocates continuous improvement, the question arises which Agile RE practices are the most essential and shall be adopted/improved at first? Objective:Investigate and rank the Agile RE practices based on how practitioners perceive their importance for a software project.Method:We conducted a survey asking 136 Agile software development practitioners how they perceive the importance of the 31 Agile RE practices that we had identified in a literature study. We used a ranking method based on the PROMETHEE family methods to create the ranking of relative importance of the practices.Results:The opinions of respondents from a wide range of countries around the globe allowed us to determine the perceived importance of the Agile RE practices and create a seven-tier ranking of the practices. Moreover, the analysis concerning demographic data let us identify some relationships between the experience of the respondents and their view on the importance of the Agile RE practices.Conclusions:Our findings suggest the most critical Agile RE practices are those supporting iterative development with emergent requirements and short feedback loop. Moreover, in many cases, the perceived importance of practices seems to depend on the context of the project (e.g., methodology, domain). We also learned that the popularity of the practices is highly correlated with their perceived importance."
Research article - A novel TOPSIS evaluation scheme for cloud service trustworthiness combining objective and subjective aspects,"AbstractCloud computing has been paid more attention due to its outstanding advantages. However, trust issue greatly affects the adoption of cloud services. Selecting trustworthy cloud service from those with same functionality but different qualities has become a significant challenge. Since trustworthiness evaluation is multi-dimensional, how to assign weight for each influence factor is a non-trivial problem. In this paper, we put forward a novel TOPSIS evaluation scheme for cloud service trustworthiness combining objective and subjective aspects. First, we consider the objectivity of cloud services from two facets. For one thing, we concern the reliability of QoS information source and utilize monitored values of QoS attributes rather than feedback ratings from consumers. For another, we employ objective entropy weight for different QoS attributes to decrease the effect of false or artificial parameter information. Second, we introduce trust preference that reflects the subjectivity of trust. Most important of all, we propose the combined weight integrating the two aspects and apply it to TOPSIS method to develop a novel evaluation scheme. The results of two experiments based on the existing QWS dataset from real Web services demonstrate its feasibility, effectiveness, and better Satisfaction Degree from the perspective of consumers."
Research article - A likelihood-free Bayesian derivation method for service variants,"AbstractApplication programming interfaces (API), allowing systems to be accessed by the services they expose, have proliferated on the Internet and gained strategic interest in the IT industry. However, integration opportunities for larger, enterprise systems are hampered by complex and overloaded operations of their interfaces, having hundreds of parameters and multiple levels of nesting, corresponding to multiple business entities. Static (code) analysis techniques have been proposed to analyse service interfaces of enterprise systems. They support the derivation of business entities and relationships from the parameters of interface operations, allowing the restructure of operations, based on individual entities. In this paper, we extend the repertoire of static interface analysis to derive service variants, whereby subsets of operation parameters correspond to multiple nested business entity subtypes of variants. Specifically, we apply a Monte Carlo sampling method, based on likelihood-free Bayesian sampling, to traverse large parameter spaces, based on higher probabilistic tree search, to efficiently find subsets of parameters related to prospective subtypes. The results demonstrate a method with significant success rates in massive search spaces, as applied to the FedEx Shipment interface whose operations have in excess of 1000 parameters."
Research article - A novel dynamic resource adjustment architecture for virtual tenant networks in SDN,"AbstractThis work proposes a novel dynamic resource adjustment architecture that includes a routing planning mechanism and a bandwidth resource planning mechanism for a virtual tenant network (VTN) with software-defined networking (SDN). The routing planning mechanism helps to assign the routing paths in a physical network to solve the problem of their uneven distribution, with the ultimate goal of improving the utilization of the network routing. The bandwidth resource planning mechanism helps to manage the tenant demand for bandwidth resources to solve the problem of an excessive bandwidth requirement and thereby to improve the utilization of network bandwidth resources. The proposed architecture helps not only to plan routing paths in a physical network to satisfy a VTN user request but also to guarantee bandwidth usage based on overall network conditions. Analytical results indicate that the proposed routing planning mechanism increases the efficiency of routing assignment and that the proposed bandwidth resource scheduling increases bandwidth utilization by 10.92%."
Research article - Feedback-based integrated prediction: Defect prediction based on feedback from software testing process,"AbstractTest resource constraints is a common phenomenon in software testing. Using defect prediction to guide the resource allocation can significantly improve the efficiency and effectiveness of available test resources. However, traditional defect prediction (t-DP) is a static strategy, where the predictor cannot be dynamically adjusted during the software testing process (STP). This paper combines defect prediction with feedback control in STP and proposes a feedback-based defect prediction model, where the test results generated during STP is used as feedback information for on-line adjustment of predictor to optimize the prediction result. In addition, a novel approach called feedback-based integrated prediction (FIP) is proposed to improve the prediction accuracy, where a global predictor and a local predictor are employed to make an integrated prediction using the weight to adjust the effects of predictors at different test stages. A systematic experiment is conducted to investigate the performance of the FIP over 10 public data sets. Results show that FIP has better prediction efficiency and better robustness for external data than the t-DP, especially when the percentage of the test modules is 40%."
Research article - Execution anomaly detection in large-scale systems through console log analysis,"AbstractExecution anomaly detection is important for development, maintenance and performance tuning in large-scale systems. System console logs are the significant source of troubleshooting and problem diagnosis. However, manually inspecting logs to detect anomalies is unfeasible due to the increasing volume and complexity of log files. Therefore, this is a substantial demand for automatic anomaly detection based on log analysis. In this paper, we propose a general method to mine console logs to detect system problems. We first give some formal definitions of the problem, and then extract the set of log statements in the source code and generate the reachability graph to reveal the reachable relations of log statements. After that, we parse the log files to create log messages by combining information about log statements with information retrieval techniques. These messages are grouped into execution traces according to their execution units. We propose a novel anomaly detection algorithm that considers traces as sequence data and uses a probabilistic suffix tree based method to organize and differentiate significant statistical properties possessed by the sequences. Experiments on a CloudStack testbed and a Hadoop production system show that our method can effectively detect running anomalies in comparison with existing four detection algorithms."
Research article - Mutomvo: Mutation testing framework for simulated cloud and HPC environments,"AbstractMany current applications provide high performance to process large volumes of data. These applications usually run in highly distributed environments, like cloud and HPC systems. Nevertheless, the large and complex architectures required for deploying these applications may not be available during the development phase. This limitation can be overcome by using simulation platforms to model a wide range of distributed system configurations and execute these applications in the modeled system. Usually, these applications are tested against a small number of test cases that are manually designed by the testers. It is desirable to have effective test suites in order to detect failures in the application models. In this paper we propose a mutation testing framework for detecting errors in distributed applications executed in simulated environments. The execution of a test suite against the set of mutated models allows to determine its effectiveness for detecting different errors. The proposal has been implemented in a tool called MuTomVo. In order to support the feasibility of the proposal, we have carried out a case study over three applications running in different distributed systems: a client/server model, intensive computation and scientific pipeline."
Research article - Cross lifecycle variability analysis: Utilizing requirements and testing artifacts,"AbstractVariability analysis is an essential activity that supports increasing and systemizing reuse across similar software products. Current studies use different types of artifacts for analyzing variability, most notably are architecture or design, requirements, and code. While architecture, design, and code help understand and model the differences in solutions and realizations, requirements enable capturing differences in a higher level of abstraction through the intended use of the software products or their behavior. However, analyzing variability based on requirements may result in inaccurate outcomes, due to the informal and incomplete nature of requirements. To tackle this deficiency, we call for augmenting requirements-based variability analysis with other behavior-related cross-lifecycle artifacts. Particularly, we extend an approach that compares and analyzes software behaviors based on requirements taking into account both ontological and semantic considerations. Using test cases and their relations to requirements, our extension, named SOVA R-TC, extract software behaviors more comprehensively, including their preconditions, post-conditions, and expected results. The outputs of SOVA R-TC are feature diagrams, which group similar behaviors and present variability of software products in a tree structure. Empirically evaluating outcomes of SOVA R-TC, they seem to be perceived as significantly better than outcomes generated based on requirements only."
Research article - Coordinated actor model of self-adaptive track-based traffic control systems,"AbstractSelf-adaptation is a well-known technique to handle growing complexities of software systems, where a system autonomously adapts itself in response to changes in a dynamic and unpredictable environment. With the increasing need for developing self-adaptive systems, providing a model and an implementation platform to facilitate integration of adaptation mechanisms into the systems and assuring their safety and quality is crucial. In this paper, we target Track-based Traffic Control Systems (TTCSs) in which the traffic flows through pre-specified sub-tracks and is coordinated by a traffic controller. We introduce a coordinated actor model to design self-adaptive TTCSs and provide a general mapping between various TTCSs and the coordinated actor model. The coordinated actor model is extended to build large-scale self-adaptive TTCSs in a decentralized setting. We also discuss the benefits of using Ptolemy II as a framework for model-based development of large-scale self-adaptive systems that supports designing multiple hierarchical MAPE-K feedback loops interacting with each other. We propose a template based on the coordinated actor model to design a self-adaptive TTCS in Ptolemy II that can be instantiated for various TTCSs. We enhance the proposed template with a predictive adaptation feature. We illustrate applicability of the coordinated actor model and consequently the proposed template by designing two real-life case studies in the domains of air traffic control systems and railway traffic control systems in Ptolemy II."
Research article - Efficient synthesis of robust models for stochastic systems,"AbstractWe describe a tool-supported method for the efficient synthesis of parametric continuous-time Markov chains (pCTMC) that correspond to robust designs of a system under development. The pCTMCs generated by our RObust DEsign Synthesis (RODES) method are resilient to changes in the systemâ€™s operational profile, satisfy strict reliability, performance and other quality constraints, and are Pareto-optimal or nearly Pareto-optimal with respect to a set of quality optimisation criteria. By integrating sensitivity analysis at designer-specified tolerance levels and Pareto optimality, RODES produces designs that are potentially slightly suboptimal in return for less sensitivityâ€”an acceptable trade-off in engineering practice. We demonstrate the effectiveness of our method and the efficiency of its GPU-accelerated tool support across multiple application domains by using RODES to design a producer-consumer system, a replicated file system and a workstation cluster system."
Research article - DiVM: Model checking with LLVM and graph memory,"AbstractIn this paper, we introduce the concept of a virtual machine with graph-organised memory as a versatile backend for both explicit-state and abstraction-driven verification of software. Our virtual machine uses the LLVM IR as its instruction set, enriched with a small set of hypercalls. We show that the provided hypercalls are sufficient to implement a small operating system, which can then be linked with applications to provide a POSIX-compatible verification environment. Finally, we demonstrate the viability of the approach through a comparison with a more traditionally-designed LLVM model checker."
Research article - Characterizing and predicting blocking bugs in open source projects,"AbstractSoftware engineering researchers have studied specific types of issues such reopened bugs, performance bugs, dormant bugs, etc. However, one special type of severe bugs is blocking bugs. Blocking bugs are software bugs that prevent other bugs from being fixed. These bugs may increase maintenance costs, reduce overall quality and delay the release of the software systems. In this paper, we study blocking bugs in eight open source projects and propose a model to predict them early on. We extract 14 different factors (from the bug repositories) that are made available within 24 hours after the initial submission of the bug reports. Then, we build decision trees to predict whether a bug will be a blocking bugs or not. Our results show that our prediction models achieve F-measures of 21%â€“54%, which is a two-fold improvement over the baseline predictors. We also analyze the fixes of these blocking bugs to understand their negative impact. We find that fixing blocking bugs requires more lines of code to be touched compared to non-blocking bugs. In addition, our file-level analysis shows that files affected by blocking bugs are more negatively impacted in terms of cohesion, coupling complexity and size than files affected by non-blocking bugs."
Research article - Positive affect through interactions in meetings: The role of proactive and supportive statements,"AbstractSoftware projects are dominated by meetings. For participants, not all meetings are useful and enjoyable. However, interaction within a meeting has an impact on individual and group affects. Group affect influences team performance and project success. Despite frequent yet vague dissatisfaction with some meetings, many software engineers are not aware of the crucial importance of their behavior in those meetings. This can set the tone for the entire project. By influencing group affect, meeting interaction influences success without participants even noticing. Due to this lack of awareness, it depends on good or bad luck whether software teams will adopt a promising meeting style.In a study of 32 student projects with 155 participants, we coded fine-grained interaction elements during the first internal meeting of each team. The analysis of resulting codes showed that constructive remarks had a positive impact on positive group affect tone (PGAT). However, this effect was only observed when constructive remarks were followed by supportive utterances. We were able to show a complete mediation of this statistically significant effect. Seemingly subtle behavior patterns influence group affect. Software projects could significantly benefit from supportive meeting behavior. We propose practical interventions to improve meeting quality."
