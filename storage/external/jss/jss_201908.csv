title,abstract
Research article - FM-CF: A framework for classifying feature model building approaches,"AbstractSoftware product line engineering has emerged as a prominent software engineering paradigm, as it comprises a set of core assets sharing functionality and quality attributes. Feature modelling is one of the most frequently used techniques for modelling the variability within a software product line. There are several proposals for building Feature Models which rely on semi-automated or fully automated means. Unfortunately, automatic feature model construction has been addressed from different viewpoints, so it is not easy to know which is the best approach for automating the building of variability models. In fact, there is no clarity regarding common elements, and the main differences that characterise such approaches. Additionally, the wide variety of terms used to refer to the process of building a Feature Model (e.g. synthesis, location, re-engineering, and weaving) means that approaches are varied and very heterogeneous, making them complex to understand and classify. This paper introduces FM-CF, which is a Conceptual experience-based Framework for classifying approaches for the automatic building of Feature Models. The framework considers a set of categories mainly focused on characterising some aspects, such as input sources, methods and techniques, results, and types of evaluation. A literature review of (semi-) automated Feature Model construction was performed to identify approaches for building Feature Models by (semi-)automatic means, and the main terms used by those approaches. Then the completeness of the framework was evaluated by mapping the set of dimensions and their items, and the terms extracted from the literature.The conceptual framework provides guidance to researchers for choosing the appropriate aspects with which to build Feature Models, and helps in the understanding and clarification of the proposed approaches."
Research article - Integrating UX principles and practices into software development organizations: A case study of influencing events,"AbstractCurrent studies on User eXperience (UX) integration often do not investigate or reflect on the transition companies go through from only developing Graphical User Interfaces (GUI) to also considering usability and more recently UX. Understanding this transition provides a more holistic and realistic picture of integration and can be a rich source of knowledge for improving UX integration in the software industry. Applying case study and grounded theory research we show that UX integration, like other organizational changes, can include a mixture of planned and emergent initiatives, and is influenced by various intertwined events; not only those that reside inside an organization but also those external to it. We also show that different decisions that are made outside the authority of UX practitioners have an inevitable impact on enabling or prohibiting UX integration. In addition, we found that for a successful integration, practitioners need to explicitly consider and address the characteristics of UX, otherwise, the integration efforts may have a lopsided focus on the pragmatic aspect of UX, consequently, leave the hedonic aspect unaddressed. Based on our findings, we present four lessons learned and five pitfalls companies should consider to go beyond GUI design and usability to also address UX."
Research article - TSTSS: A two-stage training subset selection framework for cross version defect prediction,"AbstractCross Version Defect Prediction (CVDP) is a practical scenario by training the classification model on the historical data of the prior version and then predicting the defect labels of modules in the current version. Unfortunately, the differences of data distribution across versions may hinder the effectiveness of the trained CVDP model. Thus, it is not trivial to select a suitable training subset from the prior version to promote the CVDP performance. In this paper, we propose a novel method, called Two-Stage Training Subset Selection (TSTSS), to address this challenging issue. In the first stage, TSTSS utilizes a sparse modeling representative selection method to select an initial module subset from the prior version which can well reconstruct the data of the prior version. In the second stage, TSTSS leverages a dissimilarity-based sparse subset selection method to further refine the selected module subset, which enables the selected modules to well represent the modules of the current version. Finally, we use a novel weighted extreme learning machine classifier to construct the CVDP model. We evaluate the CVDP performance of TSTSS on 50 cross-version pairs using 6 indicators. The experiments show that TSTSS can efficiently improve the CVDP performance compared with 11 baseline methods."
Research article - The role of Sprint planning and feedback in game development projects: Implications for game quality,"AbstractGame development projects adopt Scrum to leverage their flexibility, as game concepts and the customer preferences are highly abstract and unpredictable. The most desirable features in an original game will not be easily identified during the first phase of development but will emerge later in a clear pattern as developers and testers continuously playtest the game. Thus, game development projects use feedback from game testers to understand what they think of various features and concepts, to obtain a better understanding of problem spaces. This study proposes that game tester feedback moderates the effect of Sprint planning on game quality. A field study was conducted using a pair-matched questionnaire in which 102 game development projects participated. Results showed that Sprint planning has a positive effect on game quality. The results also revealed that iterative feedback has a moderating effect on the relationship between Sprint planning and game quality. Theoretical and practical implications are discussed."
Research article - A systematic mapping study on higher order mutation testing,"AbstractContext: Higher Order Mutants (HOMs) present some advantages concerning the First-Order Mutants (FOMs). HOMs can better simulate real and subtle faults, reduce the number of generated mutants and test cases, and so on.Objective: In order to characterize the Higher Order Mutation Testing (HOMT) field, this paper presents results of a mapping study, by synthesizing characteristics of the HOMT approaches, HOM generation strategies, evaluation aspects, trends and research opportunities.Method: We followed a research plan to locate, assess, extract and group the outcomes from relevant studies. We found 69 primary studies, which were classified based on dimensions related to aspects of the conducted evaluation, purpose and use of HOMs.Results: Java is the preferred language. Most approaches use Second-Order Mutants (SOMs). We found 50 different techniques used to generate/select HOMs. We observed that from 39 primary studies which apply a strategy,  ≈49% use search-based techniques.Conclusions: HOMT has been arising interest in the last years. The results herein presented provide researchers the start-of-the-art on HOMT, allowing them to understand existing approaches, and how the HOMs have been used and evaluated. Furthermore, this paper points out open issues and not addressed topics, which require more investigation, discussing trends and research opportunities in the field."
Research article - Improved schedulability analysis of the contention-free policy for real-time systems,"AbstractReal-time scheduling is the primary research for designing real-time systems whose correctness is determined by not only logical correctness but also timely execution. Real-time scheduling involves two fundamental issues: scheduling algorithm design and schedulability analysis development, which aim at developing a prioritization policy for real-time tasks and offering their timing guarantees at design time, respectively. Among the numerous scheduling algorithms and schedulability analysis for a multiprocessor platform, the contention-free (CF) policy and response-time analysis (RTA) have received considerable attention owing to their wide applicability and high analytical performance, respectively. Notwithstanding their effectiveness, it has been conjectured that it is not feasible to exploit the two techniques together. In this study, we propose a new schedulability analysis for the CF policy, referred to as pseudo-response time analysis (PRTA), which exploits a new notion of pseudo-response time effectively capturing the time instant at which the schedulability of a task is guaranteed under the CF policy. To demonstrate the effectiveness of PRTA, we apply PRTA to the existing earliest deadline first and rate monotonic scheduling algorithms employing the CF policy, and show that up to 46.4% and 18.3% schedulability performance improvement can be achieved, respectively, compared to those applying the existing schedulability analysis."
Research article - Are architectural smells independent from code smells? An empirical study,"AbstractBackground. Architectural smells and code smells are symptoms of bad code or design that can cause different quality problems, such as faults, technical debt, or difficulties with maintenance and evolution. Some studies show that code smells and architectural smells often appear together in the same file. The correlation between code smells and architectural smells, however, is not clear yet; some studies on a limited set of projects have claimed that architectural smells can be derived from code smells, while other studies claim the opposite.Objective. The goal of this work is to understand whether architectural smells are independent from code smells or can be derived from a code smell or from one category of them.Method. We conducted a case study analyzing the correlations among 19 code smells, six categories of code smells, and four architectural smells.Results. The results show that architectural smells are correlated with code smells only in a very low number of occurrences and therefore cannot be derived from code smells.Conclusion. Architectural smells are independent from code smells, and therefore deserve special attention by researchers, who should investigate their actual harmfulness, and practitioners, who should consider whether and when to remove them."
Research article - Revisiting the impact of common libraries for android-related investigations,"AbstractThe packaging model of Android apps requires the entire code to be shipped into a single APK file in order to be installed and executed on a device. This model introduces noises to Android app analyses, e.g., detection of repackaged applications, malware classification, as not only the core developer code but also the other assistant code will be visited. Such assistant code is often contributed by common libraries that are used pervasively by all apps.Despite much effort has been put in our community to investigate Android libraries, the momentum of Android research has not yet produced a complete and reliable set of common libraries for supporting thorough analyses of Android apps. In this work, we hence leverage a dataset of about 1.5 million apps from Google Play to identify potential common libraries, including advertisement libraries, and their abstract representations. With several steps of refinements, we finally collect 1113 libraries supporting common functions and 240 libraries for advertisement. For each library, we also collected its various abstract representations that could be leveraged to find new usages, including obfuscated cases.Based on these datasets, we further empirically revisit three popular Android app analyses, namely (1) repackaged app detection, (2) machine learning-based malware detection, and (3) static code analysis, aiming at measuring the impact of common libraries on their analysing performance. Our experimental results demonstrate that common library can indeed impact the performance of Android app analysis approaches. Indeed, common libraries can introduce both false positive and false negative results to repackaged app detection approaches. The existence of common libraries in Android apps may also impact the performance of machine learning-based classifications as well as that of static code analysers. All in all, the aforementioned results suggest that it is essential to harvest a reliable list of common libraries and also important to pay special attention to them when conducting Android-related investigations."
Research article - Using acceptance tests to predict files changed by programming tasks,"AbstractIn a collaborative development context, conflicting code changes might compromise software quality and developers productivity. To reduce conflicts, one could avoid the parallel execution of potentially conflicting tasks. Although hopeful, this strategy is challenging because it relies on the prediction of the required file changes to complete a task. As predicting such file changes is hard, we investigate its feasibility for BDD (Behaviour-Driven Development) projects, which write automated acceptance tests before implementing features. We develop a tool that, for a given task, statically analyzes Cucumber tests and infers test-based interfaces (files that could be executed by the tests), approximating files that would be changed by the task. To assess the accuracy of this approximation, we measure precision and recall of test-based interfaces of 513 tasks from 18 Rails projects on GitHub. We also compare such interfaces with randomly defined interfaces, interfaces obtained by textual similarity of test specifications with past tasks, and interfaces computed by executing tests. Our results give evidence that, in the specific context of BDD, Cucumber tests might help to predict files changed by tasks. We find that the better the test coverage, the better the predictive power. A hybrid approach for computing test-based interfaces is promising."
Research article - Who should make decision on this pull request? Analyzing time-decaying relationships and file similarities for integrator prediction,"AbstractIn pull-based development model, integrators are responsible for making decisions about whether to accept pull requests and integrate code contributions. Ideally, pull requests are assigned to integrators and evaluated within a short time after their submissions. However, the volume of incoming pull requests is large in popular projects, and integrators often encounter difficulties in processing pull requests in a timely fashion. Therefore, an automatic integrator prediction approach is required to assign appropriate pull requests to integrators. In this paper, we propose an approach TRFPre which analyzes Time-decaying Relationships and File similarities to predict integrators. We evaluate the effectiveness of TRFPre on 24 projects containing 138,373 pull requests. Experimental results show that TRFPre makes accurate integrator predictions in terms of accuracies and Mean Reciprocal Rank. Less than 2 predictions are needed to find correct integrator in 91.67% of projects. In comparison with state-of-the-art approaches cHRev, WRC, TIE, CoreDevRec and ACRec, TRFPre improves top-1 accuracy by 68.2%, 73.9%, 49.3%, 14.3% and 46.4% on average across 24 projects."
Research article - Pro-IDTV: A sociotechnical process model for designing IDTV applications,"AbstractTV is considered a pervasive social medium; with the arrival of digital technology, interactive Digital TV (iDTV) has come into existence, and TV usage has changed. On the one hand, these changes increase TV's potential in terms of its processing capacity, its potential to connect with other devices, and its ability to connect people to broadcasting companies in a two-way trade. On the other hand, these changes also increase the complexity of user interaction, as well as the social and technical challenges for designing solutions. Broadcasting companies may not be prepared to include iDTV application design in their production chain, and there are few theoretical references that can help them. This paper proposes a process model for iDTV named Pro-iDTV: a process for designing iDTV applications grounded on a sociotechnical approach that uses principles of Participatory Design and Organizational Semiotics. This process model was created and applied in a TV company, providing a practical understanding of the real forces that govern the organization, for, in addition, the process has the ability to tailor solutions to fit to this context. This study's results reflect the viability and benefits of the Pro-iDTV for supporting the design of iDTV applications in a practical setting."
Review article - FogBus: A Blockchain-based Lightweight Framework for Edge and Fog Computing,"AbstractRecently much emphasize is given on integrating Edge, Fog and Cloud infrastructures to support the execution of various latency sensitive and computing intensive Internet of Things (IoT) applications. Although different real-world frameworks attempt to assist such integration, they have limitations in respect of platform independence, security, resource management and multi-application execution. To address these limitations, we propose a framework, named FogBus that facilitates end-to-end IoT-Fog(Edge)-Cloud integration. FogBus offers platform independent interfaces to IoT applications and computing instances for execution and interaction. It not only assists developers to build applications but also helps users to run multiple applications at a time and service providers to manage their resources. Moreover, FogBus applies Blockchain, authentication and encryption techniques to secure operations on sensitive data. Due to its simplified and cross platform software systems, it is easy to deploy, scalable and cost efficient. We demonstrate the effectiveness of FogBus by creating a computing environment with it that integrates finger pulse oximeters as IoT devices with Smartphone-based gateway and Raspberry Pi-based Fog nodes for Sleep Apnea analysis. We also evaluate the characteristics of FogBus in respect of other existing frameworks and the impact of various FogBus settings on system parameters through deployment of a real-world IoT application. The experimental results show that FogBus is comparatively lightweight and responsive, and different FogBus settings can tune the computing environment as per the situation demands."
Review article - ROUTER: Fog enabled cloud based intelligent resource management approach for smart home IoT devices,"AbstractThere is a growing requirement for Internet of Things (IoT) infrastructure to ensure low response time to provision latency-sensitive real-time applications such as health monitoring, disaster management, and smart homes. Fog computing offers a means to provide such requirements, via a virtualized intermediate layer to provide data, computation, storage, and networking services between Cloud datacenters and end users. A key element within such Fog computing environments is resource management. While there are existing resource manager in Fog computing, they only focus on a subset of parameters important to Fog resource management encompassing system response time, network bandwidth, energy consumption and latency. To date no existing Fog resource manager considers these parameters simultaneously for decision making, which in the context of smart homes will become increasingly key. In this paper, we propose a novel resource management technique (ROUTER) for fog-enabled Cloud computing environments, which leverages Particle Swarm Optimization to optimize simultaneously. The approach is validated within an IoT-based smart home automation scenario, and evaluated within iFogSim toolkit driven by empirical models within a small-scale smart home experiment. Results demonstrate our approach results a reduction of 12% network bandwidth, 10% response time, 14% latency and 12.35% in energy consumption."
Research article - Model based system assurance using the structured assurance case metamodel,"AbstractAssurance cases are used to demonstrate confidence in system properties of interest (e.g. safety and/or security). A number of system assurance approaches are adopted by industries in the safety-critical domain. However, the task of constructing assurance cases remains a manual, lenghty and informal process. The Structured Assurance Case Metamodel (SACM) is a standard specified by the Object Management Group (OMG). SACM provides a richer set of features than existing system assurance languages/approaches. SACM provides a foundation for model-based system assurance, which bears great application potentials in growing technology domains such as Open Adaptive Systems. However, the intended usage of SACM has not been sufficiently explained. In addition, there has not been support to interoperate between existing assurance case (models) and SACM models.In this article, we explain the intended usage of SACM based on our involvement in the OMG specification process of SACM. In addition, to promote a model-based approach, we provide SACM compliant metamodels for existing system assurance approaches (the Goal Structuring Notation and Claims-Arguments-Evidence), and the transformations from these models to SACM. We also briefly discuss the tool support for model-based system assurance which helps practitioners make the transition from existing system assurance approaches to model-based system assurance using SACM."
