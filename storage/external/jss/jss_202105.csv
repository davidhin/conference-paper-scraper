title,abstract
Research article - FaaSten your decisions: A classification framework and technology review of function-as-a-Service platforms,"AbstractFunction-as-a-Service (FaaS) is a cloud service model enabling developers to offload event-driven executable snippets of code. The execution and management of such functions becomes a FaaS provider’s responsibility, therein included their on-demand provisioning and automatic scaling. Key enablers for this cloud service model are FaaS platforms, e.g., AWS Lambda, Microsoft Azure Functions, or OpenFaaS. At the same time, the choice of the most appropriate FaaS platform for deploying and running a serverless application is not trivial, as various organizational and technical aspects have to be taken into account. In this work, we present (i) a FaaS platform classification framework derived using a multivocal review and (ii) a technology review of the ten most prominent FaaS platforms, based on the proposed classification framework. We also present a FaaS platform selection support system, called FaaStener, which can help researchers and practitioners to choose the FaaS platform most suited for their requirements."
Research article - A deductive reasoning approach for database applications using verification conditions,"AbstractDeductive verification has gained paramount attention from both academia and industry. Although intensive research in this direction covers almost all mainstream languages, the research community has paid little attention to the verification of database applications. This paper proposes a comprehensive set of Verification Conditions (VCs) generation techniques from database programs, adapting Symbolic Execution, Conditional Normal Form, and Weakest Precondition. The validity checking of the generated VCs for a database program determines its correctness w.r.t. the annotated database properties. The developed prototype DBverify based on our theoretical foundation allows us to instantiate VC generation from PL/SQL codes, yielding to detailed performance analysis of the three approaches under different circumstances. With respect to the literature, the proposed approach shows its competence to support crucial SQL features (aggregate functions, nested queries, NULL values, and set operations) and the embedding of SQL codes within a host imperative language. For the chosen set of benchmark PL/SQL codes annotated with relevant properties of interest, our experiment shows that only 38% of procedures are correct, while 62% violate either all or part of the annotated properties. The primary cause for the latter case is mostly due to the acceptance of runtime inputs in SQL statements without proper checking."
Research article - FollowMe@LS: Electricity price and source aware resource management in geographically distributed heterogeneous datacenters,"AbstractWith rapid availability of renewable energy sources and growing interest in their use in the datacenter industry presents opportunities for service providers to reduce their energy related costs, as well as, minimize the ecological impact of their infrastructure. However, renewables are largely intermittent and can, negatively affect users’ applications and their performance, therefore, the profit of the service providers. Furthermore, services could be offered from those geographical locations where electricity is relatively cheaper than other locations; which may degrade the applications’ performance and potentially increase users’ costs. To ensure larger providers’ profits and lower users’ costs, certain non-interactive workloads could be either: moved and executed in geographical locations offering the lowest energy prices; or could be queued and delayed to execute later (in day or night time) when renewables, such as solar and wind energies, are at peak. However, these may have negative impacts on the energy consumption, workloads performance, and users’ costs. Therefore, to ensure energy, performance and cost efficiencies, appropriate workload scheduling, placement, migration, and resource management techniques are required to mange the infrastructure resources, workloads, and energy sources. In this paper, we propose a workload placement and three different migration policies that maximize the providers’ revenues, ensure the workload performance, reduce energy consumption, along with reducing ecological impacts and users’ costs. Using real workload traces and electricity prices for several geographical locations and distributed, heterogeneous, datacenters, our experimental evaluation suggest that the proposed approaches could save significant amount of energy (∼15.26%), reduces service monetary costs (∼0.53% - ∼19.66%), improves (∼1.58%) or, at least, maintains the expected level of applications’ performance, and increases providers’ revenue along with environmental sustainability, against the well-known first fit (FF), best fit (BF) heuristic algorithms, and other closest rivals."
Research article - Fast and accurate incremental feedback for students’ software tests using selective mutation analysis,"AbstractAs incorporating software testing into programming assignments becomes routine, educators have begun to assess not only the correctness of students’ software, but also the adequacy of their tests. In practice, educators rely on code coverage measures, though its shortcomings are widely known. Mutation analysis is a stronger measure of test adequacy, but it is too costly to be applied beyond the small programs developed in introductory programming courses. We demonstrate how to adapt mutation analysis to provide rapid automated feedback on software tests for complex projects in large programming courses. We study a dataset of 1389 student software projects ranging from trivial to complex. We begin by showing that although the state-of-the-art in mutation analysis is practical for providing rapid feedback on projects in introductory courses, it is prohibitively expensive for the more complex projects in subsequent courses. To reduce this cost, we use a statistical procedure to select a subset of mutation operators that maintains accuracy while minimizing cost. We show that with only 2 operators, costs can be reduced by a factor of 2–3 with negligible loss in accuracy. Finally, we evaluate our approach on open-source software and report that our findings may generalize beyond our educational context."
Research article - A ground-truth dataset and classification model for detecting bots in GitHub issue and PR comments,"AbstractBots are frequently used in Github repositories to automate repetitive activities that are part of the distributed software development process. They communicate with human actors through comments. While detecting their presence is important for many reasons, no large and representative ground-truth dataset is available, nor are classification models to detect and validate bots on the basis of such a dataset. This paper proposes a ground-truth dataset, based on a manual analysis with high interrater agreement, of pull request and issue comments in 5,000 distinct Github accounts of which 527 have been identified as bots. Using this dataset we propose an automated classification model to detect bots, taking as main features the number of empty and non-empty comments of each account, the number of comment patterns, and the inequality between comments within comment patterns. We obtained a very high weighted average precision, recall and F1-score of 0.98 on a test set containing 40% of the data. We integrated the classification model into an open source command-line tool to allow practitioners to detect which accounts in a given Github repository actually correspond to bots."
Research article - ProDSPL: Proactive self-adaptation based on Dynamic Software Product Lines,"AbstractDynamic Software Product Lines (DSPLs) are a well-accepted approach to self-adaptation at runtime. In the context of DSPLs, there are plenty of reactive approaches that apply countermeasures as soon as a context change happens. In this paper we propose a proactive approach, ProDSPL, that exploits an automatically learnt model of the system, anticipates future variations of the system and generates the best DSPL configuration that can lessen the negative impact of future events on the quality requirements of the system. Predicting the future fosters adaptations that are good for a longer time and therefore reduces the number of reconfigurations required, making the system more stable.ProDSPL formulates the problem of the generation of dynamic reconfigurations as a proactive controller over a prediction horizon, which includes a mapping of the valid configurations of the DSPL into linear constraints. Our approach is evaluated and compared with a reactive approach, DAGAME, also based on a DSPL, which uses a genetic algorithm to generate quasi-optimal feature model configurations at runtime. ProDSPL has been evaluated using a strategy mobile game and a set of randomly generated feature models. The evaluation shows that ProDSPL gives good results with regard to the quality of the configurations generated when it tries anticipate future events. Moreover, in doing so, ProDSPL enforces the system to make as few reconfigurations as possible."
Research article - GEML: A grammar-based evolutionary machine learning approach for design-pattern detection,"AbstractDesign patterns (DPs) are recognised as a good practice in software development. However, the lack of appropriate documentation often hampers traceability, and their benefits are blurred among thousands of lines of code. Automatic methods for DP detection have become relevant but are usually based on the rigid analysis of either software metrics or specific properties of the source code. We propose GEML, a novel detection approach based on evolutionary machine learning using software properties of diverse nature. Firstly, GEML makes use of an evolutionary algorithm to extract those characteristics that better describe the DP, formulated in terms of human-readable rules, whose syntax is conformant with a context-free grammar. Secondly, a rule-based classifier is built to predict whether new code contains a hidden DP implementation. GEML has been validated over five DPs taken from a public repository recurrently adopted by machine learning studies. Then, we increase this number up to 15 diverse DPs, showing its effectiveness and robustness in terms of detection capability. An initial parameter study served to tune a parameter setup whose performance guarantees the general applicability of this approach without the need to adjust complex parameters to a specific pattern. Finally, a demonstration tool is also provided."
Research article - Blended graphical and textual modelling for UML profiles: A proof-of-concept implementation and experiment,"AbstractDomain-specific modelling languages defined by extending or constraining the Unified Modelling Language (UML) through the profiling mechanism have historically relied on graphical notations to maximise human understanding and facilitate communication among stakeholders. Other notations, such as text-, form-, or table-based are, however, often preferred for specific modelling purposes, due to the nature of a specific domain or the available tooling, or for personal preference. Currently, the state of the art support for UML-based languages provides an almost completely detached, or even entirely mutually exclusive, use of graphical and textual modelling. This becomes inadequate when dealing with the development of modern systems carried out by heterogeneous stakeholders. Our intuition is that a modelling framework based on seamless blended multi-notations can disclose several benefits, among which: flexible separation of concerns, multi-view modelling based on multiple notations, convenient text-based editing operations (inside and outside the modelling environment), and eventually faster modelling activities.In this paper we report on: (i) a proof-of-concept implementation of a framework for UML and profiles modelling using blended textual and graphical notations, and (ii) an experiment on the framework, which eventually shows that blended multi-notation modelling performs better than standard single-notation modelling."
Research article - Transforming abstract to concrete repairs with a generative approach of repair values,"AbstractSoftware models, often comprise of interconnected diagrams, change continuously, and developers often fail in keeping these diagrams consistent. Detecting inconsistencies quickly and efficiently is state of the art. However, repairing them is not trivial, because there are typically multiple model elements that need to be repaired, leading to an exponentially growing space of combinations of repair choices. Despite extensive research on consistency checking, existing approaches either provide abstract repairs only (i.e., identifying the model element but failing to describe the change), which is not satisfactory. This paper presents a novel approach that provides concrete repair choices based on values from the inconsistent models. Thus, our approach first retrieves repair values from the model, turn them to repair choices, and groups them based on their effects. This grouping lets our approach explore the repair space in its entirety, providing quick example-like feedback for all possible repairs. Our approach and its tool implementation have been empirically assessed on 10 case studies from industry, academia, and GitHub to demonstrate its feasibility and scalability. A comparison with three versioned models shows that our approach identifies useful repair values that developers have chosen."
Research article - Ensemble Effort Estimation using dynamic selection,"AbstractThe Software Effort Estimation (SEE) process has been approached in different ways in the literature, including models built from Machine Learning (ML). The combination of these models (Ensemble) is an important research topic in ML, and has lead to improvements in accuracy compared to individual models. This paper proposes heterogeneous and dynamic ensemble selection (DES) models, composed by a set of regressors dynamically selected by classifiers to estimate software development effort. In the training phase, a pool of regression algorithms is trained using training data and a validation data set to validate the models. Next, some classifiers are trained to identify the best regression model from the pool for each training instance. In the test phase each trained classifier is used to dynamically select a regressor model from the pool for predicting the effort for each test instance. The final prediction is given by the combination of the predictions of the regressors selected by the classifiers. An experimental analysis considering a relevant set of software effort estimation problems is reported. The experiments demonstrate that the proposed method outperforms individual regressors and some state of the art models of the literature."
Research article - Product metrics for spreadsheets—A systematic review,"AbstractSoftware product metrics allow practitioners to improve their products and to optimize development processes based on quantifiable characteristics of source code. To facilitate similar benefits for spreadsheet programs, researchers proposed various product metrics for spreadsheets over the last decades. However, to our knowledge, no comprehensive overview of those efforts is currently available. In this paper, we close this gap by conducting a literature review of research works that either inherently or explicitly define product metrics for spreadsheets. We scanned five major digital libraries for scientific papers that define or use spreadsheet product metrics. Based on the identified 37 papers, we created a novel catalog of product metrics for spreadsheets. The catalog can be used by practitioners and researchers as a central reference for spreadsheet product metrics. In the paper, we (i) describe the proposed metrics in detail, (ii) report how often and for what purposes the metrics are used, (iii) identify significant discrepancies in the naming and definition of the metrics, and (iv) investigate how the appropriateness of the metrics was evaluated."
Research article - A Kubernetes controller for managing the availability of elastic microservice based stateful applications,"AbstractThe architectural style of microservices has been gaining popularity in recent years. In this architectural style, small and loosely coupled modules are deployed and scaled independently to compose cloud-native applications. Carrier-grade service providers are migrating their legacy applications to a microservice based architecture running on Kubernetes which is an open source platform for orchestrating containerized microservice based applications. However, in this migration, service availability remains a concern. Service availability is measured as the percentage of time the service is provisioned. High Availability (HA) is achieved when the service is available at least 99.999% of the time. In this paper, we identify possible architectures for deploying stateful microservice based applications with Kubernetes and evaluate Kubernetes from the perspective of availability it provides for its managed applications. The results of our experiments show that the repair actions of Kubernetes cannot satisfy HA requirements, and in some cases cannot guarantee service recovery. Therefore, we propose an HA State Controller which integrates with Kubernetes and allows for application state replication and automatic service redirection to the healthy microservice instances by enabling service recovery in addition to the repair actions of Kubernetes. Based on experiments we evaluate our solution and compare the different architectures from the perspective of availability and scaling overhead. The results of our investigations show that our solution can improve the recovery time of stateful microservice based applications by 50%."
Research article - Adaptive distributed monitors of spatial properties for cyber–physical systems,"AbstractCyber–physical systems increasingly feature highly-distributed and mobile deployments of devices spread over large physical environments: in these contexts, it is generally very difficult to engineer trustworthy critical services, mostly because formal methods generally hardly scale with the number of involved devices, especially when faults, continuous changes, and dynamic topologies are the norm. To start addressing this problem, in this paper we devise a formally correct and self-adaptive implementation of distributed monitors for spatial properties. We start from the Spatial Logic of Closure Spaces, and provide a compositional translation that takes a formula and yields a distributed program that provides runtime verification of its validity. Such programs are expressed in terms of the field calculus, a recently emerged computational model that focusses on global-level outcomes instead of single-device behaviour, and expresses distributed computations by pure functions and the functional composition mechanism. By reusing previous results and tools of the field calculus, we prove correctness of the translation, self-stabilisation of the derived monitors, and empirically evaluate adaptivity of such monitors in a realistic smart city scenario of safe crowd monitoring and control."
