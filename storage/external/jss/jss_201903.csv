title,abstract
Research article - Search-Based test case prioritization for simulation-Based testing of cyber-Physical system product lines,"AbstractCyber-Physical Systems (CPSs) integrate computation with physical processes. These systems are usually highly configurable to address different customer needs and are evolving to be CPS product lines. The variability of CPS product lines is large, which implies that they can be set into millions of configurations. As a result, different cost-effective methods are needed to optimize the test process of these systems. We propose a search-based approach that aims to cost-effectively optimize the test process of CPS product lines by prioritizing the test cases that are executed in specific products at different test levels. The prioritized test suite aims at reducing the fault detection time, the simulation time and the time required to cover functional and non-functional requirements.We compared our approach by integrating five search algorithms as well as Random Search (RS) using four case studies. As compared with RS, the search algorithms managed to reduce fault detection time by 47%, the simulation time by 23%, the functional requirements covering time by 22% and the non-functional requirements covering time by 47%. Moreover, we observed that the performance of search algorithms varied for different case studies but the local search algorithms were more effective than the global search algorithms."
Research article - Peak resource analysis of concurrent distributed systems,"AbstractTraditional cost analysis frameworks have been defined for cumulative resources which keep on increasing along the computation. Examples of cumulative resources are execution time, number of executed steps, and energy consumption. Non-cumulative resources are acquired and (possibly) released along the execution. Examples of non-cumulative cost are number of connections established that are later closed, or resources requested to a virtual host which are released after using them. We present a static analysis framework to infer the peak cost for non-cumulative types of resources in concurrent distributed systems. Our framework is generic w.r.t. the type of resource and can be instantiated with any of the above mentioned resources as well as with any other resource that is measurable by inspecting the instructions of the program. The concurrent distributed language that we consider allows creating distributed locations dynamically within the program and spawning tasks that execute concurrently at any of the existing locations. Our analysis infers, for the different distributed locations, the maximum (or peak) amount of resources that they may require along any execution. This information can be used, among other applications, to detect bottlenecks in the system and to efficiently dimension the processing capacity and storage that the locations of the concurrent distributed system require."
Research article - Adopting integrated application lifecycle management within a large-scale software company: An action research approach,"AbstractContextApplication Lifecycle Management (ALM) is a paradigm for integrating and managing the various activities related to the governance, development and maintenance of software products. In the last decade, several ALM tools have been proposed to support this process, and an increasing number of companies have started to adopt ALM.ObjectiveWe aim to investigate the impact of adopting ALM in a real industrial context to understand and justify both the benefits and obstacles of applying integrated ALM.MethodAs a research methodology, we apply action research that we have carried out within HAVELSAN, a large-scale IT company. The research was carried out over a period of seven years starting in 2010 when the ALM initiative has been started in the company to increase productivity and decrease maintenance costs.ResultsThe paper presents the results of the action research that includes the application of ALM practices. The transitions among the different steps are discussed in detail, together with the identified obstacles, benefits and lessons learned.ConclusionsOur seven-year study shows that the adoption of ALM processes is not trivial and its success is related to many factors. An important conclusion is that a piecemeal solution as provided by ALM 1.0 is not feasible for the complex process and tool integration problems of large enterprises. Hence the transition to ALM 2.0 was found necessary to cope with the organizational and business needs. Although ALM 2.0 appeared to be a more mature ALM approach, there are still obstacles that need attention from both researchers and practitioners."
"Research article - End-user development, end-user programming and end-user software engineering: A systematic mapping study","AbstractEnd-User Development (EUD), End-Programming (EUP) and End-User Software Engineering (EUSE) are three related research fields that study methods and techniques for empowering end users to modify and create digital artifacts. This paper presents a systematic mapping study aimed at identifying and classifying scientific literature about EUD, EUP and EUSE in the time range January 2000–May 2017. We selected 165 papers found through a manual selection of papers from specific conferences, journal special issues, and books, integrated with an automatic search on the most important digital libraries. The answer to our research question was built through a classification of the selected papers on seven dimensions: type of approach, interaction technique, phase in which the approach is adopted, application domain, target use, class of users, and type of evaluation. Our findings suggest that EUD, EUP and EUSE are active research topics not only in Human–Computer Interaction, but also in other research communities. However, little cross-fertilization exists among the three themes, as well as unifying frameworks and approaches for guiding novice designers and practitioners. Other findings highlight trends and gaps related to the analysis’ dimensions, which have implications on the design of future tools and suggest open issues for further investigations."
"Research article - Touch gesture-based authentication on mobile devices: The effects of user posture, device size, configuration, and inter-session variability","AbstractTouch dynamics is a behavioral biometric that authenticates users by analyzing the characteristics of the touch gestures executed on mobile devices. Current research in this field has mostly focused on identifying the best algorithms and attributes to improve authentication performance. However, such systems must also be resilient against environmental variables.In this paper, we demonstrate that the user’s posture, device size and configuration have a significant impact on the performance of touch-based authentication systems. Our results indicate that authentication accuracy increases with the device size. Furthermore, we conclude that using a device’s 3-D orientation is necessary to attain better authentication performance. Our findings indicate that the features used in state-of-the-art touch-based authentication systems are insufficient to provide constant, reliable performance when any of the studied environmental variables change. With this paper, we release a new data set. Unlike the currently publicly available touch-based authentication datasets, our collection protocols control for all the studied variables. Our research study demonstrates threats to validity that noisy environmental conditions introduce to these currently available public datasets.This work is an extension of a previous publication. Presented user authentication approaches are unique and may have immediate benefits to the development of better touch-based authentication systems."
Research article - State of the art of cyber-physical systems security: An automatic control perspective,"AbstractCyber-physical systems are integrations of computation, networking, and physical processes. Due to the tight cyber-physical coupling and to the potentially disrupting consequences of failures, security here is one of the primary concerns. Our systematic mapping study sheds light on how security is actually addressed when dealing with cyber-physical systems from an automatic control perspective. The provided map of 138 selected studies is defined empirically and is based on, for instance, application fields, various system components, related algorithms and models, attacks characteristics and defense strategies. It presents a powerful comparison framework for existing and future research on this hot topic, important for both industry and academia."
Research article - An empirical study on decision making for quality requirements,"AbstractContextQuality requirements are important for product success yet often handled poorly. The problems with scope decision lead to delayed handling and an unbalanced scope.ObjectiveThis study characterizes the scope decision process to understand influencing factors and properties affecting the scope decision of quality requirements.MethodWe studied one company's scope decision process over a period of five years. We analyzed the decisions artifacts and interviewed experienced engineers involved in the scope decision process.ResultsFeatures addressing quality aspects explicitly are a minor part (4.41%) of all features handled. The phase of the product line seems to influence the prevalence and acceptance rate of quality features. Lastly, relying on external stakeholders and upfront analysis seems to lead to long lead-times and an insufficient quality requirements scope.ConclusionsThere is a need to make quality mode explicit in the scope decision process. We propose a scope decision process at a strategic level and a tactical level. The former to address long-term planning and the latter to cater for a speedy process. Furthermore, we believe it is key to balance the stakeholder input with feedback from usage and market in a more direct way than through a long plan-driven process."
Research article - Hierarchical scheduling of real-time tasks over Linux-based virtual machines,"AbstractVirtualization has made feasible the full isolation of virtual machines (VMs) among each other. When applications running within VMs have real-time constraints, threads implementing the virtual cores must be scheduled in a predictable manner over the physical cores. In this paper, we propose a possible implementation of such a predictable VM scheduling based on Linux and kvm (a hosted hypervisor). The proposed implementation is based on vanilla Linux kernels and standard qemu/kvm, and does not require to apply any patch or to use custom software. We also show that previous work makes some assumptions that are unrealistic in practical situations. Motivated by these considerations, we finally propose a principled methodology to practically implement hierarchical scheduling with Linux. Finally, an extensive set of experiments based on Linux and kvm illustrates how the VMs and host scheduler can be set-up to match theoretical results with experiments."
Research article - Alone or Together? Inter-organizational affiliations of open source communities,"AbstractMany of today's open source software (OSS) communities operate beneath an umbrella organization, while others are organized entirely independently, and yet others follow a strategy somewhere in between, sharing certain resources and services. In our paper, we analyze four mature OSS communities (GENIVI, PolarSys, LibreOffice and PostgreSQL) representing different organizational forms. Our qualitative case studies illustrate that OSS communities preferring to control all of their resources are organized autonomously, while those focused mainly on software development are integrated into an umbrella organization. An interjacent strategy is pursued by OSS communities affiliated with an intermediary form of organization that takes care of legal and financial issues, without prescribing organizational structures or a specific license. The findings of our case studies show that there is no one-size-fits-all approach for OSS communities and each strategy has specific advantages and disadvantages. Arguing with the theoretical concepts of Resource Dependence Theory (RDT) and Transaction Cost Economics (TCE), we are able to relate the findings of our qualitative empirical study to theoretical concepts explaining different organizational behavior. Therefore, this study contributes new insights concerning the inter-organizational affiliations of OSS communities thus responding to the question why different forms of OSS community governance exist."
Research article - Scalable complex event processing using adaptive load balancing,"AbstractAn essential requirement of large-scale event-driven systems is the real-time detection of complex patterns of events from a large number of basic events and derivation of higher-level events using complex event processing (CEP) mechanisms. Centralized CEP mechanisms are however not scalable and thus inappropriate for large-scale domains with many input events and complex patterns, rendering the horizontal scaling of CEP mechanisms a necessity. In this paper, we propose CCEP as a mechanism for clustering of heterogeneous CEP engines to provide horizontal scalability using adaptive load balancing. We experimentally compare the performance of CCEP with the performances of three CEP clustering mechanisms, namely VISIRI, SCTXPF, and RR. The results of experiments show that CCEP increases throughput by 40 percent and thus it is more scalable than the other three chosen mechanisms when the input event rate changes at runtime. Although CCEP increases the network utilization by about 40 percent, it keeps the load of the system two times more balanced and reduces the input event loss three times."
Research article - Black-box model-based regression testing of fail-safe behavior in web applications,"AbstractThis paper provides an approach for selective black-box model-based regression testing for web applications, emphasizing testing proper mitigation of external failures in web applications. This approach uses an existing model of web applications, FSMWeb, and extends its test generation capabilities to include selective regression testing of fail-safe behavior. It classifies existing tests as reusable, retestable, and obsolete. The approach reduces the number of tests compared to a full retest between 3% to 81% depending on the type of changes in the example and by over 99% in the case study. Removing reusable requirements reduced test requirements between 49% to 65% in the case study. The approach also uses partial regeneration for new tests wherever possible."
Research article - A mixed-method empirical study of Function-as-a-Service software development in industrial practice,"AbstractFunction-as-a-Service (FaaS) describes cloud computing services that make infrastructure components transparent to application developers, thus falling in the larger group of “serverless” computing models. When using FaaS offerings, such as AWS Lambda, developers provide atomic and short-running code for their functions, and FaaS providers execute and horizontally scale them on-demand. Currently, there is no systematic research on how developers use serverless, what types of applications lend themselves to this model, or what architectural styles and practices FaaS-based applications are based on. We present results from a mixed-method study, combining interviews with practitioners who develop applications and systems that use FaaS, a systematic analysis of grey literature, and a Web-based survey. We find that successfully adopting FaaS requires a different mental model, where systems are primarily constructed by composing pre-existing services, with FaaS often acting as the “glue” that brings these services together. Tooling availability and maturity, especially related to testing and deployment, remains a major difficulty. Further, we find that current FaaS systems lack systematic support for function reuse, and abstractions and programming models for building non-trivial FaaS applications are limited. We conclude with a discussion of implications for FaaS providers, software developers, and researchers."
Research article - Empirical research for software architecture decision making: An analysis,"AbstractContextDespite past empirical research in software architecture decision making, we have not yet systematically studied how to perform such empirical research. Software architecture decision making involves humans, their behavioral issues and practice. As such, research on decision making needs to involve not only engineering but also social science research methods.ObjectiveThis paper studies empirical research on software architecture decision making. We want to understand what research methods have been used to study human decision making in software architecture. Further, we want to provide guidance for future studies.MethodWe analyzed research papers on software architecture decision making. We classified the papers according to different sub-dimensions of empirical research design like research logic, research purpose, research methodology and process. We introduce the study focus matrix and the research cycle to capture the focus and the goals of a software architecture decision making study. We identify gaps in current software architecture decision making research according to the classification and discuss open research issues inspired by social science research.ConclusionWe show the variety of research designs and identify gaps with respect to focus and goals. Few papers study decision making behavior in software architecture design. Also these researchers study mostly the process and much less the outcome and the factors influencing decision making. Furthermore, there is a lack of improvements for software architecture decision making and in particular insights into behavior have not led to new practices. The study focus matrix and the research cycle are two new instruments for researchers to position their research clearly. This paper provides a retrospective for the community and an entry point for new researchers to design empirical studies that embrace the human role in software architecture decision making."
Research article - Landscaping systematic mapping studies in software engineering: A tertiary study,"AbstractContextA number of Systematic Mapping Studies (SMSs) that cover Software Engineering (SE) are reported in literature. Tertiary studies synthesize the secondary studies to provide a holistic view of an area.ObjectivesWe synthesize SMSs in SE to provide insights into existing SE areas and to investigate the trends and quality of SMSs.MethodologyWe use Systematic Literature Review protocol to analyze and map the SMSs in SE, till August 2017, to SE Body of Knowledge (SWEBOK).ResultsWe analyze 210 SMSs and results show that: (1) Software design and construction are most active areas in SE; (2) Some areas lack SMSs, including mathematical foundations, software configuration management, and SE tools; (3) The quality of SMSs is improving with time; (4) SMSs in journals have higher quality than SMSs in conferences and are cited more often; (5) Low quality in SMSs can be attributed to a lack of quality assessment in SMSs and not reporting information about the primary studies.ConclusionThere is a potential for more SMSs in some SE areas. A number of SMSs do not provide the required information for an SMS, which leads to a low quality score."
Research article - Software product lines and variability modeling: A tertiary study,"AbstractContext: A software product line is a means to develop a set of products in which variability is a central phenomenon captured in variability models. The field of SPLs and variability have been topics of extensive research over the few past decades. Objective: This research characterizes systematic reviews (SRs) in the field, studies how SRs analyze and use evidence-based results, and identifies how variability is modeled. Method: We conducted a tertiary study as a form of systematic review. Results: 86 SRs were included. SRs have become a widely adopted methodology covering the field broadly otherwise except for variability realization. Numerous variability models exist that cover different development artifacts, but the evidence is insufficient in quantity and immature, and we argue for better evidence. SRs perform well in searching and selecting studies and presenting data. However, their analysis and use of the quality of and evidence in the primary studies often remains shallow, merely presenting of what kinds of evidence exist. Conclusions: There is a need for actionable, context-sensitive, and evaluated solutions rather than novel ones. Different kinds of SRs (SLRs and Maps) need to be better distinguished, and evidence and quality need to be better used in the resulting syntheses."
Research article - Disaster recovery solutions for IT systems: A Systematic mapping study,"AbstractContext: Organizations are spending an unprecedented amount of money towards the cost of keeping Information Technology (IT) systems operational. Hence, these systems need to be designed using effective fault-tolerant techniques like Disaster Recovery (DR) solutions. Even though research has been done in the DR field, it is necessary to assess the current state of research and practice, to provide practitioners with evidence that enables foster its further development. Objective: This paper has the following goals: to investigate state-of-the-art solutions for DR, as well as to systematically analyze the current published research and identify different strategies available in the literature. Method: A systematic mapping study was conducted, in which 49 studies, dated from 2007 to 2017, were evaluated. Results: Various DR practices are being investigated. The results identified a number of relevant issues, including reasons to adopt DR solutions, strategies used to implement DR solutions, approaches employed to analyze DR solutions, and metrics considered during the analyses of DR solutions. Conclusion: The number of strategies and reasons for adopting DR solutions is overwhelming. Hence, there was a need to provide a consolidated view of the field. Also, the results can help to direct future research efforts in this critical area."
Research article - On Haskell and energy efficiency,"AbstractBackgroundRecent work has studied diverse affecting factors on software energy efficiency.ObjectiveThis paper attempts to shed light on the energy behavior of programs written in a lazy, purely functional programming language, Haskell.MethodologyWe conducted two in-depth and complementary studies to analyze the energy efficiency of programs from two different perspectives: strictness and concurrency.ResultsWe found that small changes can make a big difference. In one benchmark, under a specific configuration, choosing the MVar data sharing primitive over TMVar can yield 60% energy savings. In another benchmark, TMVar can yield up to 30% savings over MVar. Thus, tools that support developers in refactoring a program to switch between primitives can be very useful. In addition, the relationship between energy consumption and performance is not always clear. In sequential benchmarks, high performance is an accurate proxy for low energy consumption. However, for one of our concurrent benchmarks, the variants with the best performance also exhibited the worst energy consumption. We report on deviating cases.ConclusionsTo support developers, we have extended existing performance analysis tools to also gather and present data about energy consumption. Furthermore, we provide a set of guidelines to help Haskell developers save energy."
Review article - A survey of many-objective optimisation in search-based software engineering,"AbstractSearch-based software engineering (SBSE) is changing the way traditional software engineering (SE) activities are carried out by reformulating them as optimisation problems. The natural evolution of SBSE is bringing new challenges, such as the need of a large number of objectives to formally represent the many decision criteria involved in the resolution of SE tasks. This suggests that SBSE is moving towards many-objective optimisation, an emerging area that provides advanced techniques to cope with high-dimensional optimisation problems. To analyse this phenomenon, this paper surveys relevant SBSE literature focused on the resolution of many-objective problems. From the gathered knowledge, current limitations regarding problem formulation, algorithm selection, experimental design and industrial applicability are discussed. Through the analysis of observed trends, this survey provides a historical perspective and future lines of research concerning the adoption of many-objective optimisation within SBSE."
Review article - Towards a knowledge driven framework for bridging the gap between software and data engineering,"AbstractIn this paper we present a collection of ontologies specifically designed to model the information exchange needs of combined software and data engineering. Effective, collaborative integration of software and big data engineering for Web-scale systems, is now a crucial technical and economic challenge. This requires new combined data and software engineering processes and tools. Our proposed models have been deployed to enable: tool-chain integration, such as the exchange of data quality reports; cross-domain communication, such as interlinked data and software unit testing; mediation of the system design process through the capture of design intents and as a source of context for model-driven software engineering processes. These ontologies are deployed in web-scale, data-intensive, system development environments in both the commercial and academic domains. We exemplify the usage of the suite on case-studies emerging from two complex collaborative software and data engineering scenarios: one from the legal sector and the other from the Social sciences and Humanities domain."
Research article - Minimum/maximum delay testing of product lines with unbounded parametric real-time constraints,"AbstractNon-functional requirements like real-time behaviors are of ever-growing interest in many application domains of software product lines. Consequently, existing modeling formalisms and analysis techniques for reasoning about time-critical behaviors have to be adapted to product-line engineering, too. Featured timed automata (FTA) extend timed automata (TA) by feature constraints to enable efficient family-based verification of real-time properties. Here, we present configurable parametric timed automata (CoPTA) to further extend expressiveness of FTA by freely configurable and a-priori unbounded timing intervals of real-time constraints. Hence, CoPTA models impose infinite configuration spaces which makes variant-by-variant analysis practically infeasible. Instead, we present a family-based test-suite generation methodology for CoPTA models ensuring symbolical location coverage for every model configuration. Furthermore, we define a novel coverage criterion, called Minimum/Maximum Delay (M/MD) coverage, requiring every location in a CoPTA model to be reached by test cases with minimum/maximum possible durations, for systematically investigating best-case/worst-case execution times. We extend our family-based test-suite generation methodology to also achieve M/MD coverage on CoPTA models. Our evaluation results, obtained from applying our CoPTA tool to a collection of subject systems, reveal efficiency improvements of family-based test-suite generation, as compared to a variant-by-variant strategy in case of finite configuration spaces."
Research article - Evaluating the extension mechanisms of the knowledge discovery metamodel for aspect-oriented modernizations,"AbstractCrosscutting concerns are an intrinsic problem of legacy systems, hindering their maintenance and evolution. A possible solution is to modernize these systems employing aspect-orientation, which provides suitable abstractions for modularizing these kind of concerns. Architecture-Driven Modernization is a more specific kind of software reengineering focused on employing standard metamodels along the whole process, promoting interoperability and reusability across different tools/vendors. Its main metamodel is the Knowledge Discovery Metamodel (KDM), which is able to represent a significant amount of system details. However, up to this moment, there is no extension of this metamodel for aspect-orientation, preventing software engineers from conducting Aspect-Oriented Modernizations. Therefore, in this paper we present our experience on creating a heavyweight and a lightweight extension of KDM for aspect-orientation. We conducted two evaluations. The first one showed all aspect-oriented concepts were represented in both extensions. The second one was a experiment, in which we have analyzed the productivity of software engineers using both extensions. The results showed that the heavyweight extension propitiate a more productive environment in terms of time and number of errors when compared to the lightweight one."
Research article - A model-driven IT governance process based on the strategic impact evaluation of services,"AbstractModel-driven engineering is used for managing software systems development. In most cases, requirements representations are transformed into design diagrams themselves transformed into code, ensuring traceability. Such an approach can nevertheless take roots on a more abstract level and be used for IT governance. Goal-oriented requirements engineering can be adapted to model strategic objectives aimed to lead the organization to an enhanced competitive position in the long-term. Most IT governance and management frameworks are driven by the concept of service. The latter allows to package the work offer of an IT provider. Within their realization, such services align or misalign with strategic objectives. This paper proposes a model-driven IT governance process allowing to evaluate the alignment of business IT services to strategic objectives; it follows the 3 stages of IT governance: evaluate, direct and monitor. The approach allows to integrate the governance level as a (graphical) strategic layer made of long-term objectives that business IT services potentially contribute or hamper to attain. The strategic layer is custom developed for each organization and linked with organizational representations in a model-driven fashion to study business and IT alignment. The framework is called MoDrIGo; it is applied onto a case study in a hospital."
Research article - DelDroid: An automated approach for determination and enforcement of least-privilege architecture in android,"AbstractAndroid is widely used for the development and deployment of autonomous and smart systems, including software targeted for IoT and mobile devices. Security of such systems is an increasingly important concern. Android relies on a permission model to secure the system’s resources and apps. In Android, since the permissions are granted at the granularity of apps, and all components in an app inherit those permissions, an app’s components are over-privileged, i.e., components are granted more privileges than they actually need. Systematic violation of least-privilege principle in Android is the root cause of many security vulnerabilities. To mitigate this issue, we have developed DelDroid, an automated system for determination of least privilege architecture in Android and its enforcement at runtime. A key contribution of DelDroid is the ability to limit the privileges granted to apps without modifying them. DelDroid utilizes static analysis techniques to extract the exact privileges each component needs. A Multiple-Domain Matrix representation of the system’s architecture is then used to automatically analyze the security posture of the system and derive its least-privilege architecture. Our experiments on hundreds of real-world apps corroborate DelDroid’s ability in effectively establishing the least-privilege architecture and its benefits in alleviating the security threats."
Research article - Semantics-based platform for context-aware and personalized robot interaction in the internet of robotic things,"AbstractRobots are moving from well-controlled lab environments to the real world, where an increasing number of environments has been transformed into smart sensorized IoT spaces. Users will expect these robots to adapt to their preferences and needs, and even more so for social robots that engage in personal interactions. In this paper, we present declarative ontological models and a middleware platform for building services that generate interaction tasks for social robots in smart IoT environments. The platform implements a modular, data-driven workflow that allows developers of interaction services to determine the appropriate time, content and style of human-robot interaction tasks by reasoning on semantically enriched IoT sensor data. The platform also abstracts the complexities of scheduling, planning and execution of these tasks, and can automatically adjust parameters to the personal profile and current context. We present motivational scenarios in three environments: a smart home, a smart office and a smart nursing home, detail the interfaces and executional paths in our platform and present a proof-of-concept implementation."
Research article - An empirical study of tactical vulnerabilities,"AbstractArchitectural security tactics (e.g., authorization, authentication) are used to achieve stakeholders’ security requirements. Security tactics allow the system to react, resist, detect and recover from attacks. Flaws in the adoption of these tactics into the system’s architecture, an incorrect implementation of security tactics, or deterioration of tactic implementations over time can introduce severe vulnerabilities that are exploitable by attackers. Therefore, in this work, we present the Common Architectural Weakness Enumeration (CAWE), a catalog of known weaknesses rooted in the design or implementation of security tactics which can result in tactical vulnerabilities. We categorized all known software weaknesses as tactic-related and non-tactic related. This way, our CAWE catalog enumerates common weaknesses in a security architecture that can lead to tactical vulnerabilities. From our CAWE catalog, we found 223 different types of tactical vulnerabilities. In this work, we also used this catalog to study tactical vulnerabilities in three large-scale open source projects: Chromium, PHP, and Thunderbird. In a detailed analysis, we identified the most occurring vulnerability types on these projects. From this study we observed that (i) Improper Input Validation and Improper Access Control were the most occurring vulnerability types in Chromium, PHP and Thunderbird and (ii) “Validate Inputs” and “Authorize Actors” were the security tactics mostly affected by these tactical vulnerabilities. Moreover, in a qualitative analysis of 632 tactical vulnerabilities and their fixes in these systems, we characterized their root causes and investigated the way the original developers of each system fixed these vulnerabilities. From this qualitative analysis, we found 44 distinct root causes that lead to these tactical vulnerabilities. The results of this study not only show how architectural weaknesses in systems have created severe vulnerabilities, but also provide recommendations driven by empirical data for addressing such security problems."
"Research article - Continuously analyzing finite, message-driven, time-synchronous component & connector systems during architecture evolution","AbstractUnderstanding the semantic differences of continuously evolving system architectures by semantic analyses facilitates engineers during evolution analysis in understanding the impact of the syntactical changes between two architecture versions. To enable effective semantic differencing usable in practice, this requires means to fully automatically check whether one version of a system admits behaviors that are not possible in another version. Previous work produced very general system models for message-driven time-synchronous (MDTS) systems that impede fully automated semantic differencing but very adequately describe such systems from a black-box viewpoint abstracting from hidden internal component behavior. This paper presents a system model for MDTS systems from a white-box viewpoint (assuming component implementation availability) and presents a sound and complete method for semantic differencing of finite MDTS system architectures. This method relies on representing (sub-)architectures as channel automata and a reduction from the semantic differencing problem for such automata to the language inclusion problem for Büchi automata. The system model perfectly captures the logical basics of MDTS systems from a white-box viewpoint and the method enables to fully automatically calculate semantic differences between two finite MDTS systems on push-button basis, yields witnesses, and ultimately facilitates semantic evolution analysis of such systems."
