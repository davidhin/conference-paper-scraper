title,abstract
Research article - A general method for rendering static analyses for diverse concurrency models modular,"AbstractShared-memory multi-threading and the actor model both share the notion of processes featuring communication, respectively by modifying shared state and by sending messages. Existing static analyses for concurrent programs either model every possible process interleavings and therefore suffer from the state explosion problem, or feature modularity but lack in precision or in their support for dynamic processes. In this paper we present a general method for obtaining a scalable analysis of concurrent programs featuring dynamic process creation. Our ModConc method transforms an abstract concurrent semantics modeling processes and communication into a modular static analysis treating the behavior of processes separately from their communication. We present ModConc in a generic way and demonstrate its applicability by instantiating it for multi-threaded and actor-based programs. The resulting analyses are evaluated in terms of precision, performance, scalability, and soundness. While a typical non-modular static analysis time out on half of our 56 benchmarks with a 30 min timeout, ModConc analyses successfully analyze all of them in less than 30 s, while remaining on par in terms of precision. Analyzing concurrent processes in isolation while modeling their communications is the key ingredient in supporting scalable analysis of concurrent programs featuring dynamic process communication."
Research article - Business process model refactoring applying IBUPROFEN. An industrial evaluation,"AbstractBusiness process models are recognized as being important assets for companies, since appropriate management of them provides companies with a competitive advantage. Quality assurance of business process models has become a critical issue, especially when companies carry out reverse engineering techniques to retrieve their business process models. Thus, companies have to deal with several quality faults, such as unmeaningful elements, fine-grained granularity or incompleteness, which seriously affect understandability and modifiability of business process models. The most widely-used method to reduce these faults is refactoring. Although several refactoring operators exist in the literature, there are no refactoring techniques specially developed for business process models obtained by process mining and other reverse engineering techniques. Therefore, this paper presents the use of IBUPROFEN, a business process model refactoring technique for those models obtained by reverse engineering. IBUPROFEN is applied in an in-depth case study with a real-life information system belonging to a European bank company. The goal of this industrial evaluation is to prove that the refactoring operators improve the understandability and modifiability of the business process model after being refactored. In addition, the scalability of the technique is assessed to demonstrate the feasibility of its application."
Research article - Energy-aware virtual machine allocation for cloud with resource reservation,"AbstractTo reduce the price of pay-as-you-go style cloud applications, an increasing number of cloud service providers offer resource reservation-based services that allow tenants to customize their virtual machines (VMs) with specific time windows and physical resources. However, due to the lack of efficient management of reserved services, the energy efficiency of host physical machines cannot be guaranteed. In today’s highly competitive cloud computing market, such low energy efficiency will significantly reduce the profit margin of cloud service providers. Therefore, how to explore energy efficient VM allocation solutions for reserved services to achieve maximum profit is becoming a key issue for the operation and maintenance of cloud computing. To address this problem, this paper proposes a novel and effective evolutionary approach for VM allocation that can maximize the energy efficiency of a cloud data center while incorporating more reserved VMs. Aiming at accurate energy consumption estimation, our approach needs to simulate all the VM allocation updates, which is time-consuming using traditional cloud simulators. To overcome this, we have designed a simplified simulation engine for CloudSim that can accelerate the process of our evolutionary approach. Comprehensive experimental results obtained from both simulation on CloudSim and real cloud environments show that our approach not only can quickly achieve an optimized allocation solution for a batch of reserved VMs, but also can consolidate more VMs with fewer physical machines to achieve better energy efficiency than existing methods. To be specific, the overall profit improvement and energy savings achieved by our approach can be up to 24% and 41% as compared to state-of-the-art methods, respectively. Moreover, our approach could enable the cloud data center to serve more tenant requests."
Research article - TDQN: Trace-driven analytic queuing network modeling of computer systems,"AbstractMany current performance studies of computer systems are evaluated by subjecting the system under evaluation to a workload obtained from publicly available traces. Many current performance studies of computer systems are evaluated by using publicly available traces as inputs. This approach provides credibility to performance studies but generally precludes the use of computationally efficient analytic models that rely on strict stochastic assumptions. Simulation or prototype implementations, less general and more time consuming methods, are used when the use of traces limits the applicability of analytic performance models. Analytic models (e.g., queuing theory for single queues or queuing networks) have extensively been used to estimate job execution times under steady state conditions. This paper discusses the use of closed Queuing Network (QN) models during finite time intervals to estimate the execution time of jobs submitted to a computer system. The paper presents the Trace-Driven Queuing Network (TDQN) algorithm that allows job traces to be used as input to analytic models. Validations against experimental results showed that the absolute relative error between measurements and execution time predictions obtained with the TDQN algorithm is below 10% in most cases. Additionally, the paper shows how the TDQN algorithm can be used to estimate the makespan of a stream of jobs submitted to a scheduler, which decides which computer of a cluster will process the job."
Research article - A bibliometric assessment of software engineering scholars and institutions (2010–2017),"AbstractThis paper presents the findings of a bibliometric study, targeting an eight-year period (2010–2017), with the aim of identifying: (a) emerging research directions, (b) the top-20 institutions, and (c) top-20 early stage, consolidated, and experienced scholars in the field of software engineering. To perform this goal, we performed a bibliometric study, by applying the mapping study technique on top-quality software engineering venues, and developed a dataset of 14,456 primary studies. As the ranking metric for institutions, we used the count of papers in which authors affiliated with this institute have been identified in the obtained dataset, whereas regarding scholars we computed the corresponding rankings based on the number of published papers and the average number of citations. Finally, we identified the top-20 rising scholars in the SE research community, based on their recent publication record (between 2015 and 2017) and their research age."
Research article - Online model learning for self-aware computing infrastructures,"AbstractPerformance models are valuable and powerful tools for performance prediction. However, the creation of performance models usually requires significant manual effort. Furthermore, as the modeled structures are subject to frequent change in modern infrastructures, such performance models need to be adapted as well. We therefore propose a reference architecture for online model learning in virtualized environments, which enables the automatic extraction of the aforementioned performance models. We follow an agent-based approach, which enables us to incorporate the extraction of information about the application structure as well as the virtualization structures present in modern computing centers. Our evaluation shows that our collaborating agents are able to reduce the manual effort of performance model extraction by 85.4%. The resulting performance model is able to predict the system utilization with an absolute error of less than 4% and the end-to-end response time with a relative error of less than 21%."
Research article - Emotion-oriented requirements engineering: A case study in developing a smart home system for the elderly,"AbstractSmart home technology has received growing interest in recent years with a focus on automation and assistance, for example, Alexa, Google Home, Apple HomePod, and many cheap IoT devices. Better supporting elderly people to continue live in their home using smart home technology is a key target application. However, most of the existing smart home solutions for the elderly are not designed with people’s emotional goals in mind, leading to lack of adoption, lack of engagement, and failure of the technology. In this paper, we introduce an emotion-oriented requirements engineering approach to help identifying, modeling and evaluating emotional goals. We also explain how we used this technique to help us develop SofiHub - a new smart home platform for elderly people. SofiHub comprises a range of devices and software for sensing, interaction, passive monitoring, and emergency assistance. We have conducted multiple trials including initial field trials for elderly people in real houses. We have used our emotion-oriented requirements techniques to evaluate the participants’ emotional reactions before, during, and after trials to understand the impact of such technology on elderly people’s emotions to the SofiHub solution. Our analysis shows that SofiHub successfully alleviates their loneliness, makes them feel safer and cared about. We also found that the trial participants developed a strong relation with the system and hence, felt frustrated when SofiHub did not respond in ways expected or desired. We reflect on the lessons learned from the trials related to our emotion-oriented design and evaluation experimental approach, including refining our set of evaluation tools."
Research article - Determining relevant training data for effort estimation using Window-based COCOMO calibration,"AbstractContextA software estimation model is often built using historical project data. As software development practices change over time, however, a model based on past data may not make accurate predictions for a new project.ObjectivesWe investigate the use of moving windows to determine relevant training data for COCOMO calibration.MethodWe present a windowing calibration approach to calibrating COCOMO and assess performance of effort estimation models calibrated using windows and all data.ResultsOur results show that calibrating COCOMO using small windows of the most recently completed projects generates superior estimates than using all available historical projects. Large windows tend to produce worse estimates.ConclusionsThis study provides empirical evidence to support the use of small windows of projects completed so far to calibrate models when COCOMO-like data is available. Additionally, when the change in software development over time is rapid, the use of windows is more justifiable for improving estimation accuracy."
Research article - Retest test selection for product-line regression testing of variants and versions of variants,"AbstractTesting is a crucial activity of product-line engineering. Due to shared commonality, testing each variant individually results in redundant testing processes. By adopting regression testing strategies, variants are tested incrementally by focusing on the variability between variants to reduce the overall testing effort. However, product lines evolve during their life-cycle to adapt, e.g., to changing requirements. Hence, quality assurance has also to be ensured after product-line evolution by efficiently testing respective versions of variants. In this paper, we propose retest test selection for product-line regression testing of variants and versions of variants. Based on delta-oriented test modeling, we capture the commonality and variability of an evolving product line by means of differences between variants and versions of variants. We exploit those differences to apply change impact analyses, where we reason about changed dependencies to be retested when stepping from a variant or a version of a variant to its subsequent one by selecting test cases for reexecution. We prototypically implemented our approach and evaluated its effectiveness and efficiency by means of two evolving product lines showing positive results."
Research article - On the refinement of spreadsheet smells by means of structure information,"AbstractSpreadsheet users are often unaware of the risks imposed by poorly designed spreadsheets. One way to assess spreadsheet quality is to detect smells which attempt to identify parts of spreadsheets that are hard to comprehend or maintain and which are more likely to be the root source of bugs. Unfortunately, current spreadsheet smell detection techniques suffer from a number of drawbacks that lead to incorrect or redundant smell reports. For example, the same quality issue is often reported for every copy of a cell, which may overwhelm users. To deal with these issues, we propose to refine spreadsheet smells by exploiting inferred structural information for smell detection. We therefore first provide a detailed description of our static analysis approach to infer clusters and blocks of related cells. We then elaborate on how to improve existing smells by providing three example refinements of existing smells that incorporate information about cell groups and computation blocks. Furthermore, we propose three novel smell detection techniques that make use of the inferred spreadsheet structures. Empirical evaluation of the proposed techniques suggests that the refinements successfully reduce the number of incorrectly and redundantly reported smells, and novel deficits are revealed by the newly introduced smells."
Research article - On the analysis of spectrum based fault localization using hitting sets,"AbstractCombining spectrum-based fault localization (SBFL) with other techniques is generally regarded as a feasible approach as advantages from both techniques would be preserved. Sendys which combines SBFL with slicing-hitting-set-computation is one of the promising techniques. However, all current evaluations on Sendys were obtained via empirical studies, which have inevitable threats to validity. Besides, purely empirical studies cannot reveal the essential reason that why Sendys performs well or badly, and whether all the complicated computations are necessary. Therefore, in this paper, we provide an in-depth theoretical analysis on Sendys, which can give definite and convincing conclusions. We generalize our previous theoretical framework on SBFL, to make it applicable to combined techniques like Sendys. We first provide a variant of current Sendys by patching its loophole of ignoring “zero or negative risk values” in normalization. This variant plays as a substitution of the original Sendys, as well as one of the baselines in our analysis. Then, by modifying a few steps of this variant, we propose an enhanced Sendys and theoretically prove its superiority over several other methods in single-fault scenario. Moreover, we provide a short-cut reformulation of the enhanced Sendys by preserving its performance, but only requiring very simple computations. And it is proved to be even better than traditional SBFL maximal formulas. As a complementary, our empirical studies with 13 subject programs demonstrate the obvious superiority of the enhanced Sendys, as well as its stability across different formulas in single-fault scenario. For multiple-fault cases, the variant of Sendys is observed to have the best performance. Besides, this variant has shown great helpfulness in improving the bad performance of the original Sendys when encountering the NOR problem."
Research article - Robustness of spectrum-based fault localisation in environments with labelling perturbations,"AbstractMost fault localisation techniques take as inputs a faulty program and a test suite, and produce as output a ranked list of suspicious code locations at which the program may be defective. If only a small portion of the executions are labelled erroneously, we expect a fault localisation technique to be robust to these errors. However, it is not known which fault localisation techniques with high accuracy are robust and which techniques are best at finding faults under the trade-off between accuracy and robustness.In this paper, a theoretical analysis of the impacts of labelling perturbations on spectrum-based fault localisation techniques (SBFL) is presented from different aspects first. We theoretically analyse the influence of labelling perturbations on three relations among risk evaluation formulas and the effect of mislabelling cases on the ranking of faulty statements. Then, we conduct controlled experiments on 18 programs with 3079 faulty versions from different domains to compare the robustness of 23 classes of risk evaluation formulas. Besides, experiments are conducted for evaluating the robustness of two neural network-based techniques. The impacts of perturbation degrees, number of faults and types of labelling perturbation on the robustness of formulas are empirically studied, and several interesting findings are obtained."
Research article - Exoneration-based fault localization for SQL predicates,"AbstractSpectrum-based fault localization (SFL) techniques automatically localize faults in program entities (statements, predicates, SQL clauses, etc.) by analyzing information collected from test executions. One application of SFL techniques is to find faulty SQL statements in database applications. However, prior techniques treated each SQL statement as one program entity, thus they could not find faulty elements inside SQL statements. Since SQL statements can be complex, identifying the faulty element within a faulty SQL statement is still time-consuming.In our previous paper, we developed a novel fault localization method based on row-based dynamic slicing and delta debugging techniques that can localize faults in individual clauses within SQL predicates. We call this technique exoneration-based fault localization because it can exonerate “innocent” elements and precisely identify the faulty element, whereas previous SFL techniques simply ranked all the elements in an SQL statement based on suspiciousness.This paper improves the exoneration-based fault localization technique with a new algorithm that considerably reduces the execution time. We also conducted an empirical study that compared nine existing SFL techniques with the exoneration-based technique in localizing faulty clauses in SQL predicates. Results indicate that the new exoneration-based technique surpasses the other techniques both in terms of effectiveness and efficiency."
