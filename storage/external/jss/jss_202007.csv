title,abstract
Research article - QMM-VANET: An efficient clustering algorithm based on QoS and monitoring of malicious vehicles in vehicular ad hoc networks,"AbstractVehicular ad hoc networks (VANETs) are considered as a subset of mobile ad hoc networks (MANETs) that can be used in the transportation field. These networks considerably improve the traffic safety and accident prevention. Because of the characteristics of VANETs such as self-organization, frequent link disconnections and rapid topology changes, developing efficient routing protocols is a challenging task. To address this issue, clustering is an appropriate approach in a mobile environment. Clustering aims to partition the vehicles into a number of clusters based on some predefined metrics such as velocity, distance and location. In this paper, a clustering routing protocol, named QMM-VANET, which considers Quality of Service (QoS) requirements, the distrust value parameters and mobility constraints, is proposed. This protocol specifies a reliable and stable cluster and increases the stability and connectivity during communications. This protocol is composed of three parts: (1) computing the QoS of vehicles and electing a trustier vehicle as a cluster-head, (2) selecting a set of proper neighboring nodes as gateways for retransmitting the packets and (3) using gateway recovery algorithm to choose another gateway in case of failure of the link. NS-2 simulator is utilized to illustrate the performance of our proposed protocol in a highway scenario. The performance analyses display that the QMM-VANET protocol can achieve low end-to-end delay and high packet delivery ratio and maintain the network stability."
Research article - Substructure similarity search for engineering service-based systems,"AbstractWith the broad application of service-oriented architecture in service-oriented software engineering, service-based systems (SBSs) are becoming ever more widely used. As a result, the selection of appropriate component services destined to fulfill the functional requirements becomes a critical challenge for successfully building SBSs, especially when the pre-specified SBS plan involves a complicated structure. Because building an exact SBS is often too restrictive, a similarity search for complex functional requirements becomes an essential operation that must be efficiently supported. We thus investigate in this work the substructure similarity search problem of building a SBS. To solve this new research problem, we propose a feature-based method, called the substructure similarity search for service-based systems (5S), to help users find similar SBS solutions by progressively relaxing a SBS plan. The 5S approach models each SBS as a set of features and transforms the task of relaxation of a SBS into the maximum allowed missing features, which can filter many SBSs directly without costly structure comparisons. 5S thus opens a new paradigm for efficient SBS engineering that shortens the build cycle. Finally, we discuss a series of experiments using real-world Web service datasets that demonstrate the effectiveness and efficiency of the proposed approach."
Research article - Systematic literature review of empirical studies on mental representations of programs,"AbstractProgrammers are frequently tasked with modifying, enhancing, and extending applications. To perform these tasks, programmers must understand existing code by forming mental representations. Empirical research is required to determine the mental representations constructed during program comprehension to inform the development of programming languages, instructional practices, and tools. To make recommendations for future work a systematic literature review was conducted that summarizes the empirical research on mental representations formed during program comprehension, how the methods and tasks have changed over time, and the research contributions.The data items included in the systematic review are empirical studies of programmers that investigated the comprehension and internal representation of code written in a formal programming language. The eligibility criteria used in the review were meant to extract studies with a focus on knowledge representation as opposed to knowledge utilization.The results revealed a lack of incremental research and a dramatic decline in the research meaning that newly developed or popularized languages and paradigms have not been a part of the research reviewed. Accordingly, we argue that there needs to be a resurgence of empirical research on the psychology of programming to inform the design of tools and languages, especially in new and emerging paradigms."
Research article - Improving software bug-specific named entity recognition with deep neural network,"AbstractThere is a large volume of bug data in the bug repository, which contains rich bug information. Existing studies on bug data mining mainly rely on using information retrieval (IR) technology to search relevant historical bug reports. These studies basically treat a bug report as a closed unit, ignoring the semantic and structural information within it. Named-entity recognition (NER) is an important task of information extraction (IE) technology. Based on NER, fine-grained factual information could be comprehensively extracted to further form structured data, which provides a new way to improve the accessibility of bug information. However, bug NER is different from general NER tasks. Bug reports are free-form text, which include a mixed language environment studded with code, abbreviations and software-specific vocabularies. In this paper, we propose a deep neural network approach for bug-specific entity recognition called DBNER using bidirectional long short-term memory (LSTM) with Conditional Random Fields decoding model (CRF). DBNER extracts multiple features from the massive bug data and uses attention mechanism to improve the consistency of entity tags in the bug reports. Experiment results show that the F1-score reaches an average of 91.19%. In addition, in cross-project experiments, the DBNER’s F1-score reaches an average of 84%."
Research article - GAP: Forecasting commit activity in git projects,"AbstractAbandonment of active developers poses a significant risk for many open source software projects. This risk can be reduced by forecasting the future activity of contributors involved in such projects. Focusing on the commit activity of individuals involved in git repositories, this paper proposes a practicable probabilistic forecasting model based on the statistical technique of survival analysis. The model is empirically validated on a wide variety of projects accounting for 7528 git repositories and 5947 active contributors. We found that a model based on the last 20 observed days of commit activity per contributor provides the best concordance. We also found that the predictions provided by the model are generally close to actual observations, with slight underestimations for low probability predictions and slight overestimations for higher probability predictions. This model is implemented as part of an open source tool, called GAP, that predicts future commit activity."
Research article - On tracking Java methods with Git mechanisms,"AbstractMethod-level historical information is useful in various research on mining software repositories such as fault-prone module detection or evolutionary coupling identification. An existing technique named Historage converts a Git repository of a Java project to a finer-grained one. In a finer-grained repository, each Java method exists as a single file. Treating Java methods as files has an advantage, which is that Java methods can be tracked with Git mechanisms. The biggest benefit of tracking methods with Git mechanisms is that it can easily connect with any other tools and techniques build on Git infrastructure. However, Historage’s tracking has an issue of accuracy, especially on small methods. More concretely, in the case that a small method is renamed or moved to another class, Historage has a limited capability to track the method. In this paper, we propose a new technique, FinerGit, to improve the trackability of Java methods with Git mechanisms. We implement FinerGit as a system and apply it to 182 open source software projects, which include 1,768K methods in total. The experimental results show that our tool has a higher capability of tracking methods in the case that methods are renamed or moved to other classes."
Research article - Adaptive metamorphic testing with contextual bandits,"AbstractMetamorphic Testing is a software testing paradigm which aims at using necessary properties of a system under test, called metamorphic relations, to either check its expected outputs, or to generate new test cases. Metamorphic Testing has been successful to test programs for which a full oracle is not available or to test programs for which there are uncertainties on expected outputs such as learning systems. In this article, we propose Adaptive Metamorphic Testing as a generalization of a simple yet powerful reinforcement learning technique, namely contextual bandits, to select one of the multiple metamorphic relations available for a program. By using contextual bandits, Adaptive Metamorphic Testing learns which metamorphic relations are likely to transform a source test case, such that it has higher chance to discover faults. We present experimental results over two major case studies in machine learning, namely image classification and object detection, and identify weaknesses and robustness boundaries. Adaptive Metamorphic Testing efficiently identifies weaknesses of the tested systems in context of the source test case."
Research article - The effects of database complexity on SQL query formulation,"AbstractIn Structured Query Language (SQL) education, students often execute queries against a simple exercise database. Recently, databases that are more realistic have been utilized to the effect that students find exercises more interesting and useful, as these databases more accurately mimic databases students are likely to encounter in their future work environments. However, using even the most engaging database can be counterproductive to learning, if a student is not able to formulate correct queries due to the complexity of the database schema. Scientific evidence on the effects of database complexity on student’s query formulation is limited, and with queries from 744 students against three databases of varying logical complexity, we set out to study how database complexity affects the success rates in query formulation. The success rates against a simple database were significantly higher than against a semi-complex and a complex database, which indicates that it is easier for students to write SQL queries against simpler databases. This suggests, at least in the scale of our exercise databases, that educators should also consider the negative effects of more realistic databases, even though they have been shown to increase student engagement."
Research article - Software-testing education: A systematic literature mapping,"AbstractContextWith the rising complexity and scale of software systems, there is an ever-increasing demand for sophisticated and cost-effective software testing. To meet such a demand, there is a need for a highly-skilled software testing work-force (test engineers) in the industry. To address that need, many university educators worldwide have included software-testing education in their software engineering (SE) or computer science (CS) programs. Many papers have been published in the last three decades (as early as 1992) to share experience from such undertakings.ObjectiveOur objective in this paper is to summarize the body of experience and knowledge in the area of software-testing education to benefit the readers (both educators and researchers) in designing and delivering software testing courses in university settings, and to also conduct further education research in this area.MethodTo address the above need, we conducted a systematic literature mapping (SLM) to synthesize what the community of educators have published on this topic. After compiling a candidate pool of 307 papers, and applying a set of inclusion/exclusion criteria, our final pool included 204 papers published between 1992 and 2019.ResultsThe topic of software-testing education is becoming more active, as we can see by the increasing number of papers. Many pedagogical approaches (how to best teach testing), course-ware, and specific tools for testing education have been proposed. Many challenges in testing education and insights on how to overcome those challenges have been proposed.ConclusionThis paper provides educators and researchers with a classification of existing studies within software-testing education. We further synthesize challenges and insights reported when teaching software testing. The paper also provides a reference (“index”) to the vast body of knowledge and experience on teaching software testing. Our mapping study aims to help educators and researchers to identify the best practices in this area to effectively plan and deliver their software testing courses, or to conduct further education-research in this important area."
Research article - An ontology-based learning approach for automatically classifying security requirements,"AbstractAlthough academia has recognized the importance of explicitly specifying security requirements in early stages of system developments for years, in reality, many projects mix security requirements with other types of requirements. Thus, there is a strong need for precisely and efficiently classifying such security requirements from other requirements in requirement specifications. Existing studies leverage lexical evidence to build probabilistic classifiers, which are domain-dependent by design and cannot effectively classify security requirements from different application domains. In this paper, we propose an ontology-driven learning approach to automatically classify security requirements. Our approach consists of a conceptual layer and a linguistic layer, which understands security requirements based on not only lexical evidence but also conceptual domain knowledge. In particular, we apply a systematic approach to identify linguistic features of security requirements based on an extended security requirements ontology and linguistic knowledge, connecting the conceptual layer with the linguistic layer. Such linguistic features are then used to train domain-independent security requirements classifiers by using machine learning techniques. We have carried out a series of experiments to evaluate the performance and generalization ability of our proposal against existing approaches. The results of the experiments show that the proposed approach outperforms existing approaches with a significant increase of F1 score (0.63 VS. 0.44) when the training dataset and the testing dataset come from different application domains, i.e., the classifiers trained by our approach can be generalized to classify security requirements from different domains."
Research article - Formal analysis and verification of the PSTM architecture using CSP,"AbstractStarting with the analysis of the source codes of the Python Software Transactional Memory (PSTM) architecture, this paper applies process algebra CSP to formally verify the architecture at a fine-grained level. We analyze the communication process and components of the architecture from multiple perspectives and establish models describing the communication behaviors of the PSTM architecture. We use model checker PAT to automatically simulate and verify the established model. After adapting the traditional transactional properties to the PSTM architecture, we analyze and verify five properties for the PSTM architecture, including deadlock freeness, atomicity, isolation, consistency and optimism. The verification results indicate that all the properties are valid. Based on the judgement of the execution logic of the communication procedure in the PSTM architecture, we can conclude that the architecture can have a proper communication and can guarantee atomicity, isolation, consistency and optimism. Besides, we also provide a case study with an application scenario and propose a corollary that the value of the shared counter is equal to the number of parallel processes. We verify whether the case study system can satisfy all the conditions of corollary from both positive and negative perspectives. The results show that the corollary is tenable."
Research article - Scalability Assessment of Microservice Architecture Deployment Configurations: A Domain-based Approach Leveraging Operational Profiles and Load Tests,"AbstractMicroservices have emerged as an architectural style for developing distributed applications. Assessing the performance of architecture deployment configurations — e.g., with respect to deployment alternatives — is challenging and must be aligned with the system usage in the production environment. In this paper, we introduce an approach for using operational profiles to generate load tests to automatically assess scalability pass/fail criteria of microservice configuration alternatives. The approach provides a Domain-based metric for each alternative that can, for instance, be applied to make informed decisions about the selection of alternatives and to conduct production monitoring regarding performance-related system properties, e.g., anomaly detection.We have evaluated our approach using extensive experiments in a large bare metal host environment and a virtualized environment. First, the data presented in this paper supports the need to carefully evaluate the impact of increasing the level of computing resources on performance. Specifically, for the experiments presented in this paper, we observed that the evaluated Domain-based metric is a non-increasing function of the number of CPU resources for one of the environments under study. In a subsequent series of experiments, we investigate the application of the approach to assess the impact of security attacks on the performance of architecture deployment configurations."
Research article - Enhancing example-based code search with functional semantics,"AbstractAs the quality and quantity of open source code increase, effective and efficient search for code implementing certain semantics, or semantics-based code search, has become an emerging need for software developers to retrieve and reuse existing source code. Previous techniques in semantics-based code search encode the semantics of loop-free Java code snippets as constraints and utilize an SMT solver to find encoded snippets that match an input/output (IO) query. We present in this article the Quebio approach to semantics-based search for Java methods. Quebio advances the state-of-the-art by supporting important language features like invocation to library APIs and enabling the search to handle more data types like array/List, Set, and Map. Compared with existing approaches, Quebio also integrates a customized keyword-based search that uses as the input a textual, behavioral summary of the desired methods to quickly prune the methods to be checked against the IO examples. To evaluate the effectiveness and efficiency of Quebio, we constructed a repository of 14,792 methods from 723 open source Java projects hosted on GitHub and applied the approach to resolve 47 queries extracted from StackOverflow. Quebio was able to find methods correctly implementing the specified IO behaviors for 43 of the queries, significantly outperforming the existing semantics-based code search techniques. The average search time with Quebio was 213.2 seconds for each query."
Research article - Bug severity prediction using question-and-answer pairs from Stack Overflow,"AbstractNowadays, bugs have been common in most software systems. For large-scale software projects, developers usually conduct software maintenance tasks by utilizing software artifacts (e.g., bug reports). The severity of bug reports describes the impact of the bugs and determines how quickly it needs to be fixed. Bug triagers often pay close attention to some features such as severity to determine the importance of bug reports and assign them to the correct developers. However, a large number of bug reports submitted every day increase the workload of developers who have to spend more time on fixing bugs. In this paper, we collect question-and-answer pairs from Stack Overflow and use logical regression to predict the severity of bug reports. In detail, we extract all the posts related to bug repositories from Stack Overflow and combine them with bug reports to obtain enhanced versions of bug reports. We achieve severity prediction on three popular open source projects (e,g., Mozilla, Ecplise, and GCC) with Naïve Bayesian, k-Nearest Neighbor algorithm (KNN), and Long Short-Term Memory (LSTM). The results of our experiments show that our model is more accurate than the previous studies for predicting the severity. Our approach improves by 23.03%, 21.86%, and 20.59% of the average F-measure for Mozilla, Eclipse, and GCC by comparing with the Naïve Bayesian based approach which performs the best among all baseline approaches."
