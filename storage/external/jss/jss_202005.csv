title,abstract
Research article - Traceability Link Recovery between Requirements and Models using an Evolutionary Algorithm Guided by a Learning to Rank Algorithm: Train control and management case,"AbstractTraceability Link Recovery (TLR) has been a topic of interest for many years within the software engineering community. In recent years, TLR has been attracting more attention, becoming the subject of both fundamental and applied research. However, there still exists a large gap between the actual needs of industry on one hand and the solutions published through academic research on the other.In this work, we propose a novel approach, named Evolutionary Learning to Rank for Traceability Link Recovery (TLR-ELtoR). TLR-ELtoR recovers traceability links between a requirement and a model through the combination of evolutionary computation and machine learning techniques, generating as a result a ranking of model fragments that can realize the requirement.TLR-ELtoR was evaluated in a real-world case study in the railway domain, comparing its outcomes with five TLR approaches (Information Retrieval, Linguistic Rule-based, Feedforward Neural Network, Recurrent Neural Network, and Learning to Rank). The results show that TLR-ELtoR achieved the best results for most performance indicators, providing a mean precision value of 59.91%, a recall value of 78.95%, a combined F-measure of 62.50%, and a MCC value of 0.64. The statistical analysis of the results assesses the magnitude of the improvement, and the discussion presents why TLR-ELtoR achieves better results than the baselines."
Research article - MT-EA4Cloud: A Methodology For testing and optimising energy-aware cloud systems,"AbstractCurrently, using conventional techniques for checking and optimising the energy consumption in cloud systems is unpractical, due to the massive computational resources required. An appropriate test suite focusing on the parts of the cloud to be tested must be efficiently synthesised and executed, while the correctness of the test results must be checked. Additionally, alternative cloud configurations that optimise the energetic consumption of the cloud must be generated and analysed accordingly, which is challenging.To solve these issues we present MT-EA4Cloud, a formal approach to check the correctness – from an energy-aware point of view – of cloud systems and optimise their energy consumption. To make the checking of energy consumption practical, MT-EA4Cloud combines metamorphic testing, evolutionary algorithms and simulation. Metamorphic testing allows to formally model the underlying cloud infrastructure in the form of metamorphic relations. We use metamorphic testing to alleviate both the reliable test set problem, generating appropriate test suites focused on the features reflected in the metamorphic relations, and the oracle problem, using the metamorphic relations to check the generated results automatically. MT-EA4Cloud uses evolutionary algorithms to efficiently guide the search for optimising the energetic consumption of cloud systems, which can be calculated using different cloud simulators."
Research article - Inner source software development: Current thinking and an agenda for future research,"AbstractContextInner source software development (ISSD) has been viewed as an alternative approach in which organisations adopt open source software development (OSSD) practices and exploit its benefits internally.ObjectiveIn this paper, we aim to provide an extensive review of current research on ISSD and to establish a research agenda on this domain.MethodThe review is primarily performed using a systematic literature review protocol.ResultsWe identified, critically evaluated and integrated the findings of 37 primary studies, describing 25 empirical research papers, 10 frameworks/methods, models and tools to support the implementation of inner source, as well as a set of benefits and challenges associated with ISSD.ConclusionThis study presents four main contributions. First, the study provides an in-depth review of ISSD to date, i.e. the evolution of research across inner source, contributions of existing research developments, and theories, models and frameworks used to study inner source. Second, our review applies the OSSD approach framework as the lens to analyse ISSD. Third, the review updates the key challenges associated with ISSD from a management perspective. The final contribution is the establishment of a research agenda to advance knowledge on ISSD."
Research article - In-the-field monitoring of functional calls: Is it feasible?,"AbstractCollecting data about the sequences of function calls executed by an application while running in the field can be useful to a number of applications, including failure reproduction, profiling, and debugging. Unfortunately, collecting data from the field may introduce annoying slowdowns that negatively affect the quality of the user experience. So far, the impact of monitoring has been mainly studied in terms of the overhead that it may introduce in the monitored applications, rather than considering if the introduced overhead can be really recognized by users. In this paper we take a different perspective studying to what extent collecting data about sequences of function calls may impact the quality of the user experience, producing recognizable effects. Interestingly we found that, depending on the nature of the executed operation and its execution context, users may tolerate a non-trivial overhead. This information can be potentially exploited to collect significant amount of data without annoying users."
Research article - Partially observable Markov decision process to generate policies in software defect management,"AbstractBug repositories are dynamic in nature and as new bugs arrive, the old ones are closed. In a typical software project, bugs and their dependencies are reported manually and gradually using a issue tracking system. Thus, not all of the bugs in the system are available at any time, creating uncertainty in the dependency structure of the bugs. In this research, we propose to construct a dependency graph based on the reported dependency-blocking information in a issue tracking system. We use two graph metrics, depth and degree, to measure the extent of blocking bugs. Due to the uncertainty in the dependency structure, simply ordering bugs in the descending order of depth and/or degree may not be the best policy to prioritize bugs. Instead, we propose a Partially Observable Markov Decision Process model for sequential decision making and Partially Observable Monte Carlo Planning to identify the best policy for this sequential decision-making process. We validated our proposed approach by mining the data from two open source projects, and a commercial project. We compared our proposed framework with three baseline policies. The results on all datasets show that our proposed model significantly outperforms the other policies with respect to average discounted return."
Research article - Cloud reliability and efficiency improvement via failure risk based proactive actions,"AbstractDue to the huge magnitude and complexity of cloud computing systems (CCS), failures are inevitable, which lead to reliability and efficiency losses. Failure mitigation, fault tolerance, and recovery actions can be performed to improve CCS reliability and efficiency. Using data collected during CCS operation, failure prediction and risk identification techniques could anticipate such failure occurrences. In this paper, we develop a framework to combine risk identification with follow-up proactive actions for CCS reliability and efficiency improvement. We start by analyzing cloud failures and the related operational data. Then a tree based predictive model is trained to diagnose high risk cloud tasks. By proactively terminating these high risk tasks, both the number of CCS failures and the resource consumption could be significantly reduced. The impact of these proactive actions can be simulated to quantify the improvement to both system reliability and efficiency. The new approach has been applied on the Google cluster dataset, covering approximately 400GB of operational data over 29 consecutive days, to demonstrate its viability and effectiveness."
Research article - Cyber-physical modelling in Modelica with model-reduction techniques,"AbstractObject-oriented modelling of cyber-physical systems with Modelica and similar environments has brought many advantages, especially the efficient re-use of models and thus the possibility of creating powerful multi-domain libraries. Unfortunately, the models have become highly complex, which causes serious problems during processing and execution. Consequently, verification and debugging is becoming an increasingly challenging task. The continuous investigation of simplifications and reductions in all phases of model developments is thus urgent.The present paper deals with reduction methods based on metric ranking and preserve realisation, which means that the structure and the parameters of the model remain physically interpretable. Two model-reduction methods are described and implemented in Open Modelica. The first operates on a set of differential-algebraic equations, and the second is based on modified bond-graphs-reduction techniques. The latter approach is suitable for component-based models in Modelica that are usually represented graphically with object diagrams. The paper briefly describes the research area, the problems of the adoption of the developed model reduction techniques to the Modelica environments, and the final implementation. Both proposed approaches are tested on the model of a car suspension system and briefly discussed."
Research article - A novel Security-by-Design methodology: Modeling and assessing security by SLAs with a quantitative approach,"AbstractRecent software development methodologies, as DevOps or Agile, are very popular and widely used, especially for the development of cloud services and applications. They dramatically reduce the time-to-market of developed software but, at the same time, they can be hardly integrated with security design and risk management methodologies. These cannot be easily automated and require big economic investments, due to the necessity of security experts in the development team and to the lack of automatic tools to evaluate risk and to assess security in the design and operation phases. This paper presents a novel Security-by-Design methodology based on Security Service Level Agreements (SLAs), which can be integrated within modern development processes and that is able to support the risk management life-cycle in an almost-completely automated way. In particular, it relies upon a guided risk analysis process and a completely automated security assessment phase, which enable to assess the security properties granted by a cloud application and to report them in a Security SLA. We validated the proposed methodology with respect to a real case study, which showed its effectiveness in improving the awareness of designer and developer teams on security aspects and in reducing the secure design process time."
Research article - Relation-based test case prioritization for regression testing,"AbstractTest case prioritization (TCP), which aims at detecting faults as early as possible is broadly used in program regression testing. Most existing TCP techniques exploit coverage information with the hypothesis that higher coverage has more chance to catch bugs. Static structure information such as function and statement are frequently employed as coverage granularity. However, the former consumes less costs but presents lower capability to detect faults, the latter typically incurs more overhead.In this paper, dynamic function call sequences are argued that can guide TCP effectively. Same set of functions/statements can exhibit very different execution behaviors. Therefore, mapping program behaviors to unit-based (function/statement) coverage may not be enough to predict fault detection capability. We propose a new approach AGC (Additional Greedy method Call sequence). Our approach leverages dynamic relation-based coverage as measurement to extend the original additional greedy coverage algorithm in TCP techniques.We conduct our experiments on eight real-world java open source projects and systematically compare AGC against 22 state-of-the-art TCP techniques with different granularities. Results show that AGC outperforms existing techniques on large programs in terms of bug detection capability, and also achieves the highest mean APFD value. The performance demonstrates a growth trend as the size of the program increases."
Research article - Analyzing bug fix for automatic bug cause classification,"AbstractDuring the bug fixing process, developers usually need to analyze the source code to induce the bug cause, which is useful for bug understanding and localization. The bug fixes of historical bugs usually reflects the bug causes when fixing them. This paper aims at exploiting the corresponding relationship between bug causes and bug fixes to automatically classify bugs into their cause categories. First, we define the code-related bug classification criterion from the perspective of the cause of bugs. Then, we propose a new model to exploit the knowledge in the bug fix by constructing fix trees from the diff source code at Abstract Syntax Tree (AST) level, and representing each fix tree based on the encoding method of Tree-based Convolutional Neural Network (TBCNN). Finally, the corresponding relationship between bug causes and bug fixes is analyzed by automatically classifying bugs into their cause categories. We collected 2000 real-world bugs from two open source projects Mozilla and Radare2 to evaluate our approach. The experimental results show the existence of observational correlation between the bug fix and the cause of the historical bugs, and the proposed fix tree can effectively express the characteristics of the historical bugs for bug cause classification."
Research article - Diversified keyword search based web service composition,"AbstractTo assist system engineers in efficiently building service-based software systems, the keyword search based service composition approach on service connection graphs (scgraphs) has been proposed recently. However, due to the ambiguity of keywords, a keyword query may represent a bunch of different user requirements. Thus the current approach that only returns the composition with the optimal Quality of Service (QoS) cannot guarantee to hit the spot. In this paper, in order to satisfy the various possible requirements underlying a given keyword query, we formally introduce the top-k diverse service composition problem, and present a novel diversified keyword search approach on scgraphs to address it. Specifically, we firstly propose an All-Then-Diversify (ATD) algorithm that enumerates all potential compositions by searching a scgraph and then derives the top-k diverse subsets by deriving the maximal independent sets of a similarity graph. Then, due to the possibly large number of potential compositions, we present a Pop-And-Diversify (PAD) algorithm that only maintains a similarity graph of the top compositions that have been found so far during the search and computes its maximal independent sets incrementally until convergence, thereby reducing unnecessary computation overheads. Moreover, we propose two composition similarity measurements w.r.t. the categories or descriptions of services respectively. Lastly, the experimental results on ProgrammableWeb.com demonstrate that, our approach outperforms another state-of-the-art composition diversification approach on both metrics of density and redundancy, and meanwhile, improves the efficiency of diversification significantly."
Research article - An Android application risk evaluation framework based on minimum permission set identification,"AbstractAndroid utilizes a security mechanism that requires apps to request permission for accessing sensitive user data, e.g., contacts and SMSs, or certain system features, e.g., camera and Internet access. However, Android apps tend to be overprivileged, i.e., they often request more permissions than necessary. This raises the security problem of overprivilege. To alleviate the overprivilege problem, this paper proposes MPDroid, an approach that combines static analysis and collaborative filtering to identify the minimum permissions for an Android app based on its app description and API usage. Given an app, MPDroid first employs collaborative filtering to identify the initial minimum permissions for the app. Then, through static analysis, the final minimum permissions that an app really needs are identified. Finally, it evaluates the overprivilege risk by inspecting the app’s extra privileges, i.e., the unnecessary permissions requested by the app. Experiments are conducted on 16,343 popular apps collected from Google Play. The results show that MPDroid outperforms the state-of-the-art approach significantly."
