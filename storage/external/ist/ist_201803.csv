title,abstract
Research article - Innovation Initiatives in Large Software Companies: A Systematic Mapping Study,"AbstractContextTo keep the competitive advantage and adapt to changes in the market and technology, companies need to innovate in an organised, purposeful and systematic manner. However, due to their size and complexity, large companies tend to focus on the structure in maintaining their business, which can potentially lower their agility to innovate.ObjectiveThe aims of this study are to provide an overview of the current research on innovation initiatives and to identify the challenges of implementing those initiatives in the context of large software companies.MethodThe investigation was primarily performed using a systematic mapping approach of published literature on corporate innovation and entrepreneurship, which was then complemented with interviews with four experts with rich industry experience.ResultsOur mapping study results suggest that, there is a lack of high quality empirical studies on innovation initiative in the context of large software companies. A total of 7 studies are conducted in the context of large software companies, which reported 5 types of initiatives: intrapreneurship, bootlegging, internal venture, spin-off and crowdsourcing. Our study offers three contributions. First, this paper represents the map of existing literature on innovation initiatives inside large companies. The second contribution of this paper is to provide an innovation initiative tree. The third contribution is to identify key challenges faced by each initiative in large software companies.ConclusionsAt the strategic and tactical levels, there is no difference between large software companies and other companies. At the operational level, large software companies are highly influenced by the advancement of Internet technology. In addition, large software companies use open innovation paradigm as part of their innovation initiatives. We envision our future work is to further empirically evaluate the innovation initiative tree in large software companies. More practitioners from different companies should be involved in the future studies."
Research article - Do the informal & formal software modeling notations satisfy practitioners for software architecture modeling?,"AbstractContextSoftware architectures can be modeled using semantically informal (i.e., ambiguous) or formal (i.e., mathematically precise) software modeling notations.ObjectiveIn this paper, 115 different practitioners from 28 different countries who work in companies that perform software development have been surveyed. The goal is to understand practitioners’ knowledge and experience about informal and formal modeling notations in specifying software architectures.MethodThe survey consists of 35 different questions, divided into three parts, i.e., the participant profile, the informal modeling, and formal modeling. The results of the survey lead to many interesting findings:Results(1) Informal software modeling notations (especially UML) are practitioners’ top choice in specifying software architectures (94%). (2) Despite many informal languages, some practitioners (40%) insist using informal ad-hoc techniques (i.e., simple boxes/lines and natural languages) to specify complex design decisions (e.g., structure, behaviour, and interaction). (3) Practitioners using informal notations are impressed by their low learning-curve (79%), visuality (62%), and general-purpose scope (66%). (4) Practitioners still criticise informal notations too, essentially for the lack of support for complex design decisions and their exhaustive analysis. (5) While formal modeling notations bridge this gap mentioned in step-4, many practitioners (70%) rarely use formal notations due essentially to the high-learning curve, the lack of knowledge among stakeholders, and the lack of popularity in industry. (6) Among the considered formal notations (i.e., process algebras, high-level formal specification languages, and architecture description languages (ADLs)), process algebras are never used and ADLs are the least used formal languages are ADLs (i.e., 12% frequently use ADLs). (7) Practitioners complain about ADLs’ weak tool support (38%) and their lack of tutorials/guidance/etc (33%).ConclusionThe survey results will let the community realise the advantages and disadvantages of informal and formal modeling notations for software architecture modeling from practitioners’ viewpoint."
Research article - Continuous and collaborative technology transfer: Software engineering research with real-time industry impact,"AbstractContext: Traditional technology transfer models rely on the assumption that innovations are created in academia, after which they are transferred to industry using a sequential flow of activities. This model is outdated in contemporary software engineering research that is done in close collaboration between academia and industry and in large consortia rather than on a one-on-one basis. In the new setup, research can be viewed as continuous co-experimentation, where industry and academia closely collaborate and iteratively and jointly discover problems and develop, test, and improve solutions.Objective: The objective of the paper is to answer the following research questions: How can high-quality, ambitious software engineering research in a collaborative setup be conducted quickly and on a large scale? How can real-time business feedback to continuously improve candidate solutions be gained?Method: The proposed model has been created, refined, and evaluated in two large, national Finnish software research programs. For this paper, we conducted thematic interviews with representatives of four companies who participated in these programs.Results: The fundamental change is in the mindset of the participants from technology push by academia to technology pull by companies, resulting in co-creation. Furthermore, continuous cooperation between participants enables solutions to evolve in rapid cycles and forms a scalable model of interaction between research institutes and companies.Conclusions: The multifaceted nature of software engineering research calls for numerous approaches. In particular, when working with human-related topics such as company culture and development methods, many discoveries result from seamless collaboration between companies and research institutes."
Research article - ARSENAL-GSD: A framework for trust estimation in virtual teams based on sentiment analysis,"AbstractContextTechnology advances has enabled the emergence of virtual teams. In these teams, people are in different places and possibly over different time zones, making use of computer mediated communication to interact. At the same time distribution brings benefits, it poses challenges as the difficulty to develop trust, which is essential for team efficiency.ObjectiveIn this paper, we present ARSENAL-GSD, an automatic framework for detecting trust among members of global software development teams based on sentiment analysis.MethodsTo design ARSENAL-GSD we made a literature review to identify trust evidences, especially those that could be captured or inferred from the automatic analysis of data generated by members’ interactions in a versioning system. We applied a survey to validate the framework and evidences found.ResultsOn a scale of 0–9, evidences were evaluated as having importance greater or equal to 5.23, and the extraction techniques used to estimate them were considered as good enough. Regarding differences between subjects profile, no difference was found in responses of participants with theoretical knowledge/none and those with medium/high knowledge in GSD, except for the evidence mimicry, which was considered more important for the group of participants with medium/high knowledge in GSD.ConclusionWe concluded that our framework is valid and trust information provided by it could be used to allocate members to a new team and/or, to monitor them during project development."
Review article - Reporting systematic reviews: Some lessons from a tertiary study,"AbstractContextMany of the systematic reviews published in software engineering are related to research or methodological issues and hence are unlikely to be of direct benefit to practitioners or teachers. Those that are relevant to practice and teaching need to be presented in a form that makes their findings usable with minimum interpretation.ObjectiveWe have examined a sample of the many systematic reviews that have been published over a period of six years, in order to assess how well these are reported and identify useful lessons about how this might be done.MethodWe undertook a tertiary study, performing a systematic review of systematic reviews. Our study found 178 systematic reviews published in a set of major software engineering journals over the period 2010–2015. Of these, 37 provided recommendations or conclusions of relevance to education and/or practice and we used the DARE criteria as well as other attributes related to the systematic review process to analyse how well they were reported.ResultsWe have derived a set of 12 ‘lessons’ that could help authors with reporting the outcomes of a systematic review in software engineering. We also provide an associated checklist for use by journal and conference referees.ConclusionThere are several areas where better reporting is needed, including quality assessment, synthesis, and the procedures followed by the reviewers. Researchers, practitioners, teachers and journal referees would all benefit from better reporting of systematic reviews, both for clarity and also for establishing the provenance of any findings."
Research article - A community’s perspective on the status and future of peer review in software engineering,"AbstractContext:Pre-publication peer review of scientific articles is considered a key element of the research process in software engineering, yet it is often perceived as not to work fully well.Objective:We aim at understanding the perceptions of and attitudes towards peer review of authors and reviewers at one of software engineering’s most prestigious venues, the International Conference on Software Engineering (ICSE).Method:We invited 932 ICSE 2014/15/16 authors and reviewers to participate in a survey with 10 closed and 9 open questions.Results:We present a multitude of results, such as: Respondents perceive only one third of all reviews to be good, yet one third as useless or misleading; they propose double-blind or zero-blind reviewing regimes for improvement; they would like to see showable proofs of (good) reviewing work be introduced; attitude change trends are weak.Conclusion:The perception of the current state of software engineering peer review is fairly negative. Also, we found hardly any trend that suggests reviewing will improve by itself over time; the community will have to make explicit efforts. Fortunately, our (mostly senior) respondents appear more open for trying different peer reviewing regimes than we had expected."
Research article - Investigating the impact of fault data completeness over time on predicting class fault-proneness,"AbstractContextThe adequacy of fault-proneness prediction models in representing the relationship between the internal quality of classes and their fault-proneness relies on several factors. One of these factors is the completeness of the fault data. A fault-proneness prediction model that is built using fault data collected during testing or after a relatively short period of time after release may be inadequate and may not be reliable enough in predicting faulty classes.ObjectiveWe empirically study the relationship between the time interval since a system is released and the performance of the fault-proneness prediction models constructed based on the fault data reported within the time interval.MethodWe construct prediction models using fault data collected at several time intervals since a system has been released and study the performance of the models in representing the relationship between the internal quality of classes and their fault-proneness. In addition, we empirically explore the relationship between the performance of a prediction model and the percentage of increase in the number of classes detected faulty (PIF) over time.ResultsOur results show evidence in favor of the expectation that predictions models that use more complete fault data, to a certain extent, more adequately represent the expected relationship between the internal quality of classes and their fault-proneness and have better performance. A threshold based on the PIF value can be used as an indicator for deciding when to stop collecting fault data. When reaching this threshold, collecting additional fault data will not significantly improve the prediction ability of the constructed model.ConclusionWhen constructing fault-proneness prediction models, researchers and software engineers are advised to rely on systems that have relatively long maintenance histories. Researchers and software engineers can use the PIF value as an indicator for deciding when to stop collecting fault data."
Review article - Interactions between environmental sustainability goals and software product quality: A mapping study,"AbstractContextSustainability is considered as either a quality requirement or a quality characteristic that should be included in software when environmental protection concerns are being taken into account. However, addressing sustainability in software projects might have an impact on the quality of the software product delivered. Conflicting goals between sustainability and particular software product characteristics should be studied when developing application software, since achieving users’ requirements can be a hindrance in the quest to meet sustainability goals.ObjectiveThis paper aims to provide an overview of the approaches found in the literature for dealing with interactions between software product quality and sustainability in the context of application software.MethodA systematic mapping study is conducted to identify practices for managing interactions between software quality characteristics and sustainability. The selected papers are classified according to the quality characteristic considered and their influence on sustainability.ResultsMost of the 66 selected papers focused on validating current technologies concerning their support for sustainability (46%%). The interaction between performance efficiency and energy efficiency is what is reported most and there is a fairly positive interaction. In addition, reliability and usability point to a positive interaction with energy efficiency, while security shows a conflicting interaction with energy efficiency. Functional suitability and maintainability can present both positive and negative interaction, with different goals derived from environmental sustainability.ConclusionsInteractions between software quality and sustainability have been addressed within an explorative approach. There is a need for additional research work to characterize the impact of interaction on both software quality and sustainability. Furthermore, proposals should be validated in industrial settings."
Research article - Sustainability analysis and ease of learning in artifact-based requirements engineering: The newest member of the family of studies (It’s a girl!),"AbstractContext: Artifact-based requirements engineering promises to deliver results of high quality while allowing for flexibility in the development process and the project settings. Tailored for analyzing sustainability, it can offer tangible insights on potential benefits and risks of a system under development. However, as of now there is still relatively little empirical evidence available that would prove this quality, flexibility, and insight potential. Previous studies, specifically on the first two characteristics, differ in their socio-economic contexts and make the findings hard to generalize.Objective: Our goal is to investigate the advantages and limitations in the application of artifact-based requirements engineering by new, inexperienced requirements engineers to extend our family of studies. In addition, the secondary goal is to evaluate the suitability of the sustainability analysis artifact for a sustainability analysis of the system planned for development.Method: We report on a new member in a family of studies with 20 participants for evaluating artifact models in a sustainability application context. We use a graduate block course as case. Our data collection is performed via survey at the end of the course, based on the same instrument used in previous studies, and extended with a new section on evaluating the suitability of a particular artifact for sustainability analysis.Results: Both from the quantitative and the qualitative feedback, the results indicate that the students have benefitted from the artifact-based approach to analyzing sustainability in requirements engineering. Usability, syntactic and semantic quality were all rated high and the rationales were positive, as was the feedback on the sustainability analysis artifact.Conclusion: The results contribute to a reliable database on artifact-oriented requirements engineering and strengthen our confidence in the general benefits of artifact-orientation. Relating the old and new data provides some more insight into the trajectory of the wider transfer of artifact-based requirements engineering into practice."
Research article - Variability models for generating efficient configurations of functional quality attributes,"AbstractContext: Quality attributes play a critical role in the architecture elicitation phase. Software Sustainability and energy efficiency is becoming a critical quality attribute that can be used as a selection criteria to choose from among different design or implementation alternatives. Energy efficiency usually competes with other non-functional requirements, like for instance, performance.Objective: This paper presents a process that helps developers to automatically generate optimum configurations of functional quality attributes in terms of energy efficiency and performance. Functional quality attributes refer to the behavioral properties that need to be incorporated inside a software architecture to fulfill a particular quality attribute (e.g., encryption and authentication for the security quality attribute, logging for the usability quality attribute).Method: Quality attributes are characterized to identify their design and implementation variants and how the different configurations influence both energy efficiency and performance. A usage model for each characterized quality attribute is defined. The variability of quality attributes, as well as the energy efficiency and performance experiment results, are represented as a constraint satisfaction problem with the goal of formally reasoning about it. Then, a configuration of the selected functional quality attributes is automatically generated, which is optimum with respect to a selected objective function.Results: Software developers can improve the energy efficiency and/or performance of their applications by using our approach to perform a richer analysis of the energy consumption and performance of different alternatives for functional quality attributes. We show quantitative values of the benefits of using our approach and discuss the threats to validity.Conclusions: The process presented in this paper will help software developers to build more energy efficient software, whilst also being aware of how their decisions affect other quality attributes, such as performance."
Research article - The hunt for the guzzler: Architecture-based energy profiling using stubs,"AbstractContextSoftware producing organizations have the ability to address the energy impact of their software products through their source code and software architecture. In spite of that, the focus often remains on hardware aspects, which limits the contribution of software towards energy efficient ICT solutions.ObjectiveNo methods exist to provide software architects information about the energy consumption of the different components in their software product. The objective of this paper is to bring software producing organizations in control of this qualitative aspect of their software.MethodTo achieve the objective, we developed the StEP Method to systematically investigate the effects of software units through the use of software stubs in relation to energy concerns. To evaluate the proposed method, an experiment involving three different versions of a commercial software product has been conducted. In the experiment, two versions of a software product were stubbed according to stakeholder concerns and stressed according to a test case, whilst energy consumption measurements were performed. The method provided guidance for the experiment and all activities were documented for future purposes.ResultsComparing energy consumption differences across versions unraveled the energy consumption related to the products’ core functionality. Using the energy profile, stakeholders could identify the major energy consuming elements and prioritize software engineering efforts to maximize impact.ConclusionsWe introduce the StEP Method and demonstrate its applicability in an industrial setting. The method identified energy hotspots and thereby improved the control stakeholders have over the sustainability of a software product. Despite promising results, several concerns are identified that require further attention to improve the method. For instance, we recommend the investigation of software operation data to determine, and possibly automatically create, stubs."
Research article - Design and preliminary evaluation of a cyber Security Requirements Education Game (SREG),"AbstractContext: Security, in digitally connected organizational environments of today, involves many different perspectives, including social, physical, and technical factors. In order to understand the interactions among these correlated aspects and elicit potential threats geared towards a given organization, different security requirements analysis approaches are proposed in the literature. However, the body of knowledge is yet to unleash its full potential due to the complex nature of security problems, and inadequate ways to improve security awareness of key players in the organization. Objective: Objective(s) of the research study is to improve the security awareness of players utilizing serious games via: (i) Know-how of security concepts and security protection; (ii) guided process of identifying valuable assets and vulnerabilities in a given organizational setting; (iii) guided process of defining successful security attacks to the organization. Method: Important methods used to address the above objectives include: (i) a comprehensive review of the literature to better understand security and game design elements; (ii) designing a serious game using cyber security knowledge and game-based techniques combined with security requirements engineering concepts; (iii) using empirical evaluation (observation and survey) to verify the effectiveness of the proposed game design. Result: The solution proposed is a serious game for security requirements education, which: (i) can be an effective and fun way of learning security related concepts; (ii) mimics a real life problem setting in a presentable and understandable way; (iii) motivates players to learn more about security related concepts in future. Conclusion: From this study, we conclude that the proposed Security Requirement Education Game (SREG) has positive results and is helpful for players of the game to get an understanding of security attacks and vulnerabilities."
Review article - A systematic mapping study on game-related methods for software engineering education,"AbstractContextThe use of games in software engineering education is not new. However, recent technologies have provided new opportunities for using games and their elements to enhance learning and student engagement.ObjectiveThe goal of this paper is twofold. First, we discuss how game-related methods have been used in the context of software engineering education by means of a systematic mapping study. Second, we investigate how these game-related methods support specific knowledge areas from software engineering. By achieving these goals, we aim not only to characterize the state of the art on the use of game-related methods on software engineering education, but also to identify gaps and opportunities for further research.MethodWe carried out a systematic mapping study to identify primary studies which address the use, proposal or evaluation of games and their elements on software engineering education. We classified primary studies based on type of approaches, learning goals based on software engineering knowledge areas, and specific characteristics of each type of approach.ResultsWe identified 156 primary studies, published between 1974 and June 2016. Most primary studies describe the use of serious games (86) and game development (57) for software engineering education, while Gamification is the least explored method (10). Learning goals of these studies and their development of skills are mostly related to the knowledge areas of “Software Process”, “Software Design”, and “Professional Practices”.ConclusionsThe use of games in software engineering education is not new. However, there are some knowledge areas where the use of games can still be further explored. Gamification is a new trend and existing research in the field is quite preliminary. We also noted a lack of standardization both in the definition of learning goals and in the classification of game-related methods."
Research article - How to design gamification? A method for engineering gamified software,"AbstractContextSince its inception around 2010, gamification has become one of the top technology and software trends. However, gamification has also been regarded as one of the most challenging areas of software engineering. Beyond traditional software design requirements, designing gamification requires the command of disciplines such as (motivational/behavioral) psychology, game design, and narratology, making the development of gamified software a challenge for traditional software developers. Gamification software inhabits a finely tuned niche of software engineering that seeks for both high functionality and engagement; beyond technical flawlessness, gamification has to motivate and affect users. Consequently, it has also been projected that most gamified software is doomed to fail.ObjectiveThis paper seeks to advance the understanding of designing gamification and to provide a comprehensive method for developing gamified software.MethodWe approach the research problem via a design science research approach; firstly, by synthesizing the current body of literature on gamification design methods and by interviewing 25 gamification experts, producing a comprehensive list of design principles for developing gamified software. Secondly, and more importantly, we develop a detailed method for engineering of gamified software based on the gathered knowledge and design principles. Finally, we conduct an evaluation of the artifacts via interviews of ten gamification experts and implementation of the engineering method in a gamification project.ResultsAs results of the study, we present the method and key design principles for engineering gamified software. Based on the empirical and expert evaluation, the developed method was deemed as comprehensive, implementable, complete, and useful. We deliver a comprehensive overview of gamification guidelines and shed novel insights into the nature of gamification development and design discourse.ConclusionThis paper takes first steps towards a comprehensive method for gamified software engineering."
"Research article - MEdit4CEP-Gam: A model-driven approach for user-friendly gamification design, monitoring and code generation in CEP-based systems","AbstractContextGamification has been proven to increase engagement and motivation in multiple and different non-game contexts such as healthcare, education, workplace, and marketing, among others. However, many of these applications fail to achieve the desired benefits of gamification, mainly because of a poor design.ObjectiveThis paper explores the conceptualization, implementation and monitoring phases of meaningful gamification strategies and proposes a solution for strategy experts that hides the implementation details and helps them focus only on what is crucial for the success of the strategy. The solution makes use of Model-Driven Engineering (MDE) and Complex Event Processing (CEP) technology.MethodAn easy-to-use graphical editor is used to provide the high-level models that represent the design of the gamification strategy and its deployment and monitoring. These models contain the event pattern definitions to be automatically transformed into code. This code is then deployed both in a CEP engine to detect the conditions expressed in such patterns and in an enterprise service bus to execute the corresponding pattern actions.ResultsThe paper reports on the use of both a graphical modeling editor for gamification domain definition and a graphical modeling editor for gamification strategy design, monitoring and code generation in event-based systems. It also shows how the proposal can be used to design and automate the implementation and monitoring of a gamification strategy in an educational domain supported by a well-known Learning Management System (LMS) such as Moodle.ConclusionIt can be concluded that this unprecedented model-driven approach leveraging gamification and CEP technology provides strategy experts with the ability to graphically define gamification strategies, which can be directly transformed into code executable by event-based systems. Therefore, this is a novel solution for bringing CEP closer to any strategy expert, positively influencing the gamification strategy design, implementation and real-time monitoring processes."
Research article - Is seeding a good strategy in multi-objective feature selection when feature models evolve?,"AbstractContext: When software architects or engineers are given a list of all the features and their interactions (i.e., a Feature Model or FM) together with stakeholders’ preferences – their task is to find a set of potential products to suggest the decision makers. Software Product Lines Engineering (SPLE) consists in optimising those large and highly constrained search spaces according to multiple objectives reflecting the preference of the different stakeholders. SPLE is known to be extremely skill- and labour-intensive and it has been a popular topic of research in the past years.Objective: This paper presents the first thorough description and evaluation of the related problem of evolving software product lines. While change and evolution of software systems is the common case in the industry, to the best of our knowledge this element has been overlooked in the literature. In particular, we evaluate whether seeding previous solutions to genetic algorithms (that work well on the general problem) would help them to find better/faster solutions.Method: We describe in this paper a benchmark of large scale evolving FMs, consisting of 5 popular FMs and their evolutions – synthetically generated following an experimental study of FM evolution. We then study the performance of a state-of-the-art algorithm for multi-objective FM selection (SATIBEA) when seeded with former solutions.Results:Our experiments show that we can improve both the execution time and the quality of SATIBEA by feeding it with previous configurations. In particular, SATIBEA with seeds proves to converge an order of magnitude faster than SATIBEA alone.Conclusion: We show in this paper that evolution of FMs is not a trivial task and that seeding previous solutions can be used as a first step in the optimisation - unless the difference between former and current FMs is high, where seeding has a limited impact."
Research article - Beyond evolutionary algorithms for search-based software engineering,"AbstractContextEvolutionary algorithms typically require large number of evaluations (of solutions) to converge – which can be very slow and expensive to evaluate.ObjectiveTo solve search-based software engineering (SE) problems, using fewer evaluations than evolutionary methods.MethodInstead of mutating a small population, we build a very large initial population which is then culled using a recursive bi-clustering chop approach. We evaluate this approach on multiple SE models, unconstrained as well as constrained, and compare its performance with standard evolutionary algorithms.ResultsUsing just a few evaluations (under 100), we can obtain comparable results to state-of-the-art evolutionary algorithms.ConclusionJust because something works, and is widespread use, does not necessarily mean that there is no value in seeking methods to improve that method. Before undertaking search-based SE optimization tasks using traditional EAs, it is recommended to try other techniques, like those explored here, to obtain the same results with fewer evaluations."
Research article - A benchmark study on the effectiveness of search-based data selection and feature selection for cross project defect prediction,"AbstractContextPrevious studies have shown that steered training data or dataset selection can lead to better performance for cross project defect prediction(CPDP). On the other hand, feature selection and data quality are issues to consider in CPDP.ObjectiveWe aim at utilizing the Nearest Neighbor (NN)-Filter, embedded in genetic algorithm to produce validation sets for generating evolving training datasets to tackle CPDP while accounting for potential noise in defect labels. We also investigate the impact of using different feature sets.MethodWe extend our proposed approach, Genetic Instance Selection (GIS), by incorporating feature selection in its setting. We use 41 releases of 11 multi-version projects to assess the performance GIS in comparison with benchmark CPDP (NN-filter and Naive-CPDP) and within project (Cross-Validation(CV) and Previous Releases(PR)). To assess the impact of feature sets, we use two sets of features, SCM+OO+LOC(all) and CK+LOC(ckloc) as well as iterative info-gain subsetting(IG) for feature selection.ResultsGIS variant with info gain feature selection is significantly better than NN-Filter (all,ckloc,IG) in terms of F1 (p=values≪0.001, Cohen’s d={0.621,0.845,0.762}) and G (p=values≪0.001, Cohen’s d={0.899,1.114,1.056}), and Naive CPDP (all,ckloc,IG) in terms of F1 (p=values≪0.001, Cohen’s d={0.743,0.865,0.789}) and G (p=values≪0.001, Cohen’s d={1.027,1.119,1.050}). Overall, the performance of GIS is comparable to that of within project defect prediction (WPDP) benchmarks, i.e. CV and PR. In terms of multiple comparisons test, all variants of GIS belong to the top ranking group of approaches.ConclusionsWe conclude that datasets obtained from search based approaches combined with feature selection techniques is a promising way to tackle CPDP. Especially, the performance comparison with the within project scenario encourages further investigation of our approach. However, the performance of GIS is based on high recall in the expense of a loss in precision. Using different optimization goals, utilizing other validation datasets and other feature selection techniques are possible future directions to investigate."
Research article - Empirical evaluation of software maintainability based on a manually validated refactoring dataset,"AbstractContextRefactoring is a technique for improving the internal structure of software systems. It has a solid theoretical background while being used in development practice also. However, we lack empirical research results on the real effect of code refactoring and its application.ObjectiveThis paper presents a manually validated subset of a previously published dataset containing the refactorings extracted by the RefFinder tool, code metrics, and maintainability of 7 open-source systems. We found that RefFinder had around 27% overall average precision on the subject systems, thus our manually validated subset has substantial added value. Using the dataset, we studied several aspects of the refactored and non-refactored source code elements (classes and methods), like the differences in their maintainability and source code metrics.MethodWe divided the source code elements into a group containing the refactored elements and a group with non-refactored elements. We analyzed the elements’ characteristics in these groups using correlation analysis, Mann–Whitney U test and effect size measures.ResultsSource code elements subjected to refactorings had significantly lower maintainability than elements not affected by refactorings. Moreover, refactored elements had significantly higher size related metrics, complexity, and coupling. Also these metrics changed more significantly in the refactored elements. The results are mostly in line with our previous findings on the not validated dataset, with the difference that clone metrics had no strong connection with refactoring.ConclusionsCompared to the preliminary analysis using a not validated dataset, the manually validated dataset led to more significant results, which suggests that developers find targets for refactorings based on some internal quality properties of the source code, like their size, complexity or coupling, but not clone related metrics as reported in our previous studies. They do not just use these properties for identifying targets, but also control them with refactorings."
Research article - Exploiting load testing and profiling for Performance Antipattern Detection,"AbstractContext: The performance assessment of complex software systems is not a trivial task since it depends on the design, code, and execution environment. All these factors may affect the system quality and generate negative consequences, such as delays and system failures. The identification of bad practices leading to performance flaws is of key relevance to avoid expensive rework in redesign, reimplementation, and redeployment.Objective: The goal of this manuscript is to provide a systematic process, based on load testing and profiling data, to identify performance issues with runtime data. These performance issues represent an important source of knowledge as they are used to trigger the software refactoring process. Software characteristics and performance measurements are matched with well-known performance antipatterns to document common performance issues and their solutions.Method: We execute load testing based on the characteristics of collected operational profile, thus to produce representative workloads. Performance data from the system under test is collected using a profiler tool to create profiler snapshots and get performance hotspot reports. From such data, performance issues are identified and matched with the specification of antipatterns. Software refactorings are then applied to solve these performance antipatterns.Results: The approach has been applied to a real-world industrial case study and to a representative laboratory study. Experimental results demonstrate the effectiveness of our tool-supported approach that is able to automatically detect two performance antipatterns by exploiting the knowledge of domain experts. In addition, the software refactoring process achieves a significant performance gain at the operational stage in both case studies.Conclusion: Performance antipatterns can be used to effectively support the identification of performance issues from load testing and profiling data. The detection process triggers an antipattern-based software refactoring that in our two case studies results in a substantial performance improvement."
Research article - Cloud restriction solver: A refactoring-based approach to migrate applications to the cloud,"AbstractContextThe migration of legacy systems to the Platform as a Service (PaaS) model provides several benefits, but also brings new challenges, such as dealing with the restrictions imposed by the service provider. Furthermore, factors such as time, training and the extensive reengineering activities make the migration process time consuming and error prone. Although there exist several techniques for partial or total migration of legacy applications to the cloud, only a few specifically address the resolution of these constraints.ObjectiveThis paper proposes a novel semi-automatic approach, called Cloud Restriction Solver (CRS), for migrating applications to a PaaS environment that avoids the cloud restrictions through user-defined refactorings.MethodsThe approach is supported by two open and extensible tools. The first one, called CRSAnalyzer, identifies the pieces of code that violate the restrictions of the chosen PaaS platform, while the second one, CRSRefactor, changes those pieces by equivalent cloud-enabled services.ResultsThe applicability of the proposed approach is presented by showing its instantiation for Google App Engine as an Eclipse plugin and by migrating three Java applications to that PaaS successfully. In addition, an instantiation for IBM Bluemix has been created and used to compare the migration of the same application using the developed tools for both cloud providers.ConclusionThe proposed approach fosters software reuse, is cloud-independent, and facilitates the migration of applications to PaaS platforms."
Research article - Performance-driven software model refactoring,"AbstractContextSoftware refactoring is a common practice aimed at addressing requirements or fixing bugs during the software development. While refactoring related to functional requirements has been widely studied in the last few years, non-functional-driven refactoring is still critical, mostly because non-functional characteristics of software are hard to assess and appropriate refactoring actions can be difficult to identify. In the context of performance, which is the focus of this paper, antipatterns represent effective instruments to tackle this issue, because they document common mistakes leading to performance problems as well as their solutions.ObjectiveIn order to effectively reuse the knowledge beyond performance antipatterns, automation is required to detect and remove them. In this paper we introduce a framework that enables, in an unique tool context, the refactoring of software models driven by performance antipattern detection and removal.MethodWe have implemented, within the EPSILON platform, detection rules and refactoring actions on UML models for a set of well-known performance antipatterns. By exploiting the EPSILON languages to check properties and apply refactoring on models, we enable three types of refactoring sessions.ResultsWe experiment our framework on a Botanical Garden Management System to show, on one side, that antipatterns can effectively drive software refactoring towards models that satisfy performance requirements and, on the other side, that the automation introduced by EPSILON-based sessions enables to inspect multiple paths and to propose a variety of solutions.ConclusionThis work demonstrates that automation in performance-driven software model refactoring can be beneficial, and that performance antipatterns can be powerful instruments in the hands of software engineers for detecting (and solving) performance problems usually hidden to traditional bottleneck analysis. This work also opens the road to the integration of well-known techniques for software refactoring driven by functional requirements with novel techniques addressing non-functional requirements like performance."
