title,abstract
Research article - What software reuse benefits have been transferred to the industry? A systematic mapping study,"AbstractContextThe term software reuse was first used in 1968 at the NATO conference. Since then, work in the scientific literature has stated that the application of software reuse offers benefits such as increase in quality and productivity. Nonetheless, in spite of many publications reporting software reuse experiences, evidence that such benefits having reached industrial settings is scarce.ObjectiveTo identify and classify the benefits transferred to real-world settings by the application of software reuse strategies.MethodWe conducted a systematic mapping study (SMS). Our search strategies retrieved a set of 2,413 papers out of which 49 were selected as primary studies. We defined five facets to classify these studies: (a) the type of benefit, (b) the reuse process, (c) the industry's domain, (d) the type of reuse and (e) the type of research reported.ResultsQuality increase (28 papers) and Productivity increase (25 papers) were the two most mentioned benefits. Component-Based Development (CBD) was the most reported reuse strategy (41%), followed by Software Product Lines (SPL, 30%). The selected papers mentioned fourteen industrial domains, of which four stand out: aerospace and defense, telecommunications, electronics and IT services. The application of systematic reuse was reported in 78% of the papers. Regarding the research type, 50% use evaluation research as the investigation method. Finally, 13 papers (27%) reported validity threats for the research method applied.ConclusionsThe literature analyzed presents a lack of empirical data, making it difficult to evaluate the effective transfer of benefits to the industry. This work did not find any relationship between the reported benefits and the reuse strategy applied by the industry or the industry domain. Although the most reported research method was industrial case studies (25 works), half of these works (12) did not report threats to validity."
Review article - A conceptual model of agile software development in a safety-critical context: A systematic literature review,"AbstractContextSafety-critical software systems are increasingly being used in new application areas, such as personal medical devices, traffic control, and detection of pathogens. A current research debate is regarding whether safety-critical systems are better developed with traditional waterfall processes or agile processes that are purportedly faster and promise to lead to better products.ObjectiveTo identify the issues and disputes in agile development of safety-critical software and the key qualities as found in the extant research literature.MethodWe conducted a systematic literature review as an interpretive study following a research design to search, assess, extract, group, and understand the results of the found studies.ResultsThere are key issues and propositions that we elicit from the literature and combine into a conceptual model for understanding the foundational challenges of agile software development of safety-critical systems. The conceptual model consists of four problematic practice areas and five relationships, which we find to be even more important than the problematic areas. From this review, we suggest that there are important research gaps that need to be investigated.ConclusionsWe suggest that future research should have a primary focus on the relationships in the resulting conceptual model and specifically on the dynamics of the field as a whole, on incremental versus iterative development, and on how to create value with minimal but sufficient effort."
Research article - Multi-objective regression test selection in practice: An empirical study in the defense software industry,"AbstractContextExecuting an entire regression test-suite after every code change is often costly in large software projects. To cope with this challenge, researchers have proposed various regression test-selection techniques.ObjectiveThis paper was motivated by a real industrial need to improve regression-testing practices in the context of a safety-critical industrial software in the defence domain in Turkey. To address our objective, we set up and conducted an “action-research” collaborative project between industry and academia.MethodAfter a careful literature review, we selected a conceptual multi-objective regression-test selection framework (called MORTO) and adopted it to our industrial context by developing a custom-built genetic algorithm (GA) based on that conceptual framework. GA is able to provide full coverage of the affected (changed) requirements while considering multiple cost and benefit factors of regression testing. e.g., minimizing the number of test cases, and maximizing cumulative number of detected faults by each test suite.ResultsThe empirical results of applying the approach on the Software Under Test (SUT) demonstrate that this approach yields a more efficient test suite (in terms of costs and benefits) compared to the old (manual) test-selection approach, used in the company, and another applicable approach chosen from the literature. With this new approach, regression selection process in the project under study is not ad-hoc anymore. Furthermore, we have been able to eliminate the subjectivity of regression testing and its dependency on expert opinions.ConclusionSince the proposed approach has been beneficial in saving the costs of regression testing, it is currently in active use in the company. We believe that other practitioners can apply our approach in their regression-testing contexts too, when applicable. Furthermore, this paper contributes to the body of evidence in regression testing by offering a success story of successful implementation and application of multi-objective regression testing in practice."
Research article - What happened to my application? Helping end users comprehend evolution through variation management,"AbstractContext: Millions of end users are creating software applications. These end users typically do not have clear requirements in mind; instead, they debug their programs into existence and reuse their own or other persons’ code. These behaviors often result in the creation of numerous variants of programs. Current end-user programming environments do not provide support for managing such variants.Objective: We wish to understand the variant creation behavior of end user programmers. Based on this understanding we wish to develop an automated system to help end user programmers efficiently manage variants.Method: We conducted an on-line survey to understand when and how end-user programmers create program variants and how they manage them. Our 124 survey respondents were recruited via email from among non-computer science majors who had taken at least one course in the computer science department at our university; the respondents were involved in the Engineering, Sciences, Arts, and Management fields. Based on the results of this survey we identified a set of design requirements for providing variation management support for end users. We implemented variation management support in App Inventor – a drag and drop programming environment for creating mobile applications. Our support, AppInventorHelper, is meant to help end-user programmers visualize the provenance of and relationships among variants. We conducted a think-aloud study with 10 participants to evaluate the usability of AppInventorHelper. The participants were selected on a first-come, first-served basis from those who responded to our recruitment email sent to list-servers. They were all end users majoring in electrical engineering, mechanical engineering, or physics. None had formal training in software engineering methods, but all had some experience with visual programming languages.Results: Our (user study) results indicate that AppInventorHelper can help end users navigate through variants and find variants that could be utilized cost-effectively as examples or actual code upon which to build new applications. For example, in one of our empirical studies end users explored variants of a paint application in order to find a variant that could easily be extended to incorporate a new feature.Conclusions: Our survey results show that end users do indeed reuse program variants and suggest that understanding the differences between variants is important. Further, end users prefer running code and looking at outputs, accessing source code and meta information such as filenames, referring to the creation and update dates of programs, and having information on the authors of code. When selecting variants users prefer to look at their major features such as correctness, similarity and authorship information. End users rely primarily on memory to track changes. They seldom make use of online or configuration management tools. Hence, integrated domain-specific variation management tools like AppInventorHelper can significantly help improve users’ interactions with the system. A key contribution of our work is a set of design requirements for end-user programming environments that facilitate the management and understanding of the provenance of program variants."
Research article - An empirical study on catalog of non-functional requirement templates: Usefulness and maintenance issues,"AbstractContext. Non-functional requirements (NFRs) are not easy to elicit and formulate. Therefore, some experts advocate using templates, i.e., statement patterns with parameters and optional parts. Unfortunately, there is still scarcity of evidence showing the benefits of this approach.Objective.We aim at evaluating the usefulness of catalog of NFR templates in the context of inexperienced requirements elicitors and the effort required to maintain such catalog.Method.To investigate the usefulness of NFR templates, an experiment was conducted with 107 participants. The participants, individually or in teams, elicited NFRs based on a business case concerning an e-commerce system.To study the maintenance effort, we analyzed 2231 NFRs, 41 industrial projects to simulate the development of a catalog of NFR templates. We investigated how the characteristics of the catalog, essential from the maintenance perspective, change over a series of projects (a counterpart of elapsing time).Results.The participants using NFR templates provided NFRs that were more complete, less ambiguous, more detailed, and better from the point of view of verifiability than their counterparts using the ad hoc approach. However, the catalog of templates did not speed up the elicitation process.As regards the maintenance effort, we introduced the notion of mature catalog. In our case, ca. 40 projects were needed to make the catalog mature and then it contained 400 templates but less than 10% of them were used by a single project. The mature catalog subjected to the Pareto principle—about 20% of templates resulted in almost 80% of NFRs. Moreover, when updating the catalog after each project, less than 10% of templates had to be modified or added.Conclusions.Catalog of NFR templates seems useful. It increases the quality of NFRs and does not hinder elicitation speed. However, it takes time to make such catalog mature."
Research article - Object constraint language for code generation from activity models,"AbstractContextAchieving hundred percent automation in code generation process from Unified Modeling Language (UML) models will make a drastic advancement in software industry. UML does not use a fully formalized semantics. So it leads to ambiguity during automatic implementation of UML models. These ambiguities can be avoided to a large extent using Object Constraint Language (OCL). OCL is formal and user friendly which is also familiar to industry people.ObjectiveThis paper examines how to improve the code generation from UML models, with the help of Object Constraint Language. It also explores the possibilities to incorporate OCL in UML activity models and generate code from the OCL enhanced activity diagrams.MethodMeta models for the association of OCL expressions with the UML activity diagram is proposed in the paper. OCL expressions are added as part of the UML activity models to improve the code generation and to specify assertions and behavior. Moreover a tool, called ActivityOCLKode, is implemented which follows the algorithm for code generation. The algorithm is depicted in the text.ResultsThe tool which is implemented based on the proposed method gives a promising result. More than 80% of source code is generated using the tool. In addition, the average execution time for our approach is only 11.46 ms.ConclusionThe meta model proposed in the paper gives the strong theoretical back ground to attach OCL statements with each element in the UML activity diagrams. The proposed method of code generation will improve the productivity of the software industries, since it reduces the software development effort and time. Since UML and OCL are commonly used in software industry, our method is easily adaptable by software programmers in industry."
Review article - Development of a human error taxonomy for software requirements: A systematic literature review,"AbstractBackgroundHuman-centric software engineering activities, such as requirements engineering, are prone to error. These human errors manifest as faults. To improve software quality, developers need methods to prevent and detect faults and their sources.AimsHuman error research from the field of cognitive psychology focuses on understanding and categorizing the fallibilities of human cognition. In this paper, we applied concepts from human error research to the problem of software quality.MethodWe performed a systematic literature review of the software engineering and psychology literature to identify and classify human errors that occur during requirements engineering.ResultsWe developed the Human Error Taxonomy (HET) by adding detailed error classes to Reason's well-known human error taxonomy of Slips, Lapses, and Mistakes.ConclusionThe process of identifying and classifying human error identification provides a structured way to understand and prevent the human errors (and resulting faults) that occur during human-centric software engineering activities like requirements engineering. Software engineering can benefit from closer collaboration with cognitive psychology researchers."
Research article - The WGB method to recover implemented architectural rules,"AbstractContext: The identification of architectural rules, which specify allowed dependencies among architectural modules, is a key challenge in software architecture recovery. Existing approaches either retrieve a large set of rules, compromising their practical use, or are limited to supporting the understanding of such rules, which are manually recovered.Objective: To propose and evaluate a method to recover architectural rules, focusing on those implemented in the source code, which may differ from planned or conceptual rules.Method: We propose the WGB method, which analyzes dependencies among architectural modules as a graph, adding weights that correspond to the proposed module dependency strength (MDS) metric and identifies the set of implemented architectural rules by solving a mathematical optimization problem. We evaluated our method with a case study and an empirical study that compared rules extracted by the method with the conceptual architecture and source code dependencies of six systems. These comparisons considered efficiency and effectiveness of our method.Results: Regarding efficiency, our method took 45.55 s to analyze the largest system evaluated. Considering effectiveness, our method captured package dependencies as extracted rules with a reduction of 87.6%, on average, to represent this information. Using allowed architectural dependencies as a reference point (but not a gold standard), provided rules achieved 37.1% of precision and 37.8% of recall.Conclusion: Our empirical evaluation shows that the implemented architectural rules recovered by our method consist of abstract representations of (a large number of) module dependencies, providing a concise view of dependencies that can be inspected by developers to identify occurrences of architectural violations and undocumented rules."
Short communication - Source code optimization using equivalent mutants,"AbstractContext: A mutant is a program obtained by syntactically modifying a program’s source code; an equivalent mutant is a mutant, which is functionally equivalent to the original program. Mutants are primarily used in mutation testing, and when deriving a test suite, obtaining an equivalent mutant is considered to be highly negative, although these equivalent mutants could be used for other purposes.Objective: We present an approach that considers equivalent mutants valuable, and utilizes them for source code optimization. Source code optimization enhances a program’s source code preserving its behavior.Method: We showcase a procedure to achieve source code optimization based on equivalent mutants and discuss proper mutation operators.Results: Experimental evaluation with Java and C programs demonstrates the applicability of the proposed approach.Conclusion: An algorithmic approach for source code optimization using equivalent mutants is proposed. It is showcased that whenever applicable, the approach can outperform traditional compiler optimizations."
Review article - Agile development in the cloud computing environment: A systematic review,"AbstractBackground: Agile software development is based on a set of values and principles. The twelve principles are inferred from agile values. Agile principles are composition of evolutionary requirement, simple design, continuous delivery, self-organizing team and face-to-face communication. Due to changing market demand, agile methodology faces problems such as scalability, more effort and cost required in setting up hardware and software infrastructure, availability of skilled resource and ability to build application from multiple locations. Twelve (12) principles may be practiced more appropriately with the support of cloud computing. This merger of agile and cloud computing may provide infrastructure optimization and automation benefits to agile practitioners.Objective: This Systematic Literature Review (SLR) identifies the techniques employed in cloud computing environment that are useful for agile development. In addition, SLR discusses the significance of cloud and its challenges.Method: By applying the SLR procedure, the authors select thirty-seven (37) studies out of six-hundred-forty-seven (647) from 2010 to 2017.Result: The result of SLR shows that the techniques using existing tools were reported in 35%, simulations in 20% and application developed in 15% of the studies. Evaluation of techniques was reported in 32% of the studies. The impact of cloud computing was measured by the classification of four major categories such as transparency 32%, collaboration 50%, development infrastructure 29% and cloud quality attributes in 39%. Furthermore, a large number of tools were reported in primary studies. The challenges posed by cloud adoption in agile was reported as interoperability 13%, security & privacy 18% and rest of the primary studies did not report any other research gaps.Conclusions: The study concludes that agile development in cloud computing environment is an important area in software engineering. There are many open challenges and gaps. In particular, more quality tools, evaluation research and empirical studies are required in this area."
Short communication - Performance mutation testing: Hypothesis and open questions,"AbstractPerformance bugs are common, costly, and elusive. Performance tests aim to detect performance bugs by running the program with specific inputs and determining whether the observed behaviour is acceptable. There not exist mechanisms, however, to assess the effectiveness of performance tests. Mutation testing is a technique to evaluate and enhance functional test suites by seeding artificial faults in the program under test. In this new idea paper, we explore the applicability of mutation testing to assess and improve performance tests. This novel approach is motivated with examples and open questions."
Research article - Diversity driven adaptive test generation for concurrent data structures,"AbstractContextTesting concurrent data structures remains a notoriously challenging task, due to the nondeterminism of multi-threaded tests and the exponential explosion on the number of thread schedules.ObjectiveWe propose an automated approach to generate a series of concurrent test cases in an adaptive manner, i.e., the next test cases are generated with the guarantee to discover the thread schedules that have not yet been activated by the previous test cases.MethodTwo diversity metrics are presented to induce such adaptive test cases from a static and a dynamic perspective, respectively. The static metric enforces the diversity in the program structures of the test cases; while the dynamic one enforces the diversity in their capabilities of exposing untested thread schedules. We implement three adaptive test generation approaches for C/C++ concurrent data structures, based on the state-of-the-art active testing engine Maple.ResultsWe then report an empirical study with 9 real-world C/C++ concurrent data structures, which demonstrates the efficiency of our test generation approaches in terms of the number of thread schedules discovered, as well as the time and the number of tests required for testing a concurrent data structure.ConclusionHence, by using diverse test cases derived from the static and dynamic perspectives, our adaptive test generation approaches can deliver a more efficient coverage of the thread schedules of the concurrent data structure under test."
Research article - Improving regression test efficiency with an awareness of refactoring changes,"AbstractContext. Developers often improve software quality through refactorings—the practice of behavior-preserving changes to existing code. Recent studies showed that, despite their awareness of tool support for automated refactorings, developers prefer manual refactorings. This practice can be often error-prone and increase testing cost.Objective. To address the problem, we present the Refactorings Investigation and Testing technique, called Rit. Rit improves the testing efficiency for validating refactoring changes and providing confidence that changed parts behave as intended. As testing is expensive for developers of high-assurance software, Rit reduces a considerable amount of its costs by only identifying dependent statements on a failure in each test and by detecting specific refactoring edits responsible for testing failures.Method. Our approach identifies refactorings by analyzing original and edited versions of a program. It then uses the semantic impact of a set of identified refactoring changes to detect tests whose behavior may have been affected and modified by refactoring edits. Given each failed asserts after running regression test suites, Rit helps developers focus their attention on logically related program statements by applying program slicing for minimizing each test. For debugging purposes, Rit determines specific failure-inducing refactoring edits, separating from other changes that only affect other asserts or tests.Results. We evaluated Rit on three open source projects, and found that Rit detected tests affected by refactorings with 80.9% accuracy on average. Furthermore, it identified and formed partitions relating program statements only dependent on failed asserts with 97.2% accuracy on average.Conclusion. Rit, which combines a refactoring reconstruction technique with change impact analysis to localize failure-inducing program edits, helps developers localize fault causes by focusing on refactoring changes as opposed to all the code fragments in the new version."
Research article - Fragment retrieval on models for model maintenance: Applying a multi-objective perspective to an industrial case study,"AbstractContextTraceability Links Recovery (TLR), Bug Localization (BL), and Feature Location (FL) are amongst the most relevant tasks performed during software maintenance. However, most research in the field targets code, while models have not received enough attention yet.ObjectiveThis paper presents our approach (FROM, Fragment Retrieval on Models) that uses an Evolutionary Algorithm to retrieve the most relevant model fragments for three different types of input queries: natural language requirements for TLR, bug descriptions for BL, and feature descriptions for FL.MethodFROM uses an Evolutionary Algorithm that generates model fragments through genetic operations, and assesses the relevance of each model fragment with regard to the provided query through a fitness configuration. We analyze the influence that four fitness configurations have over the results of FROM, combining three objectives: Similitude, Understandability, and Timing. To analyze this, we use a real-world case study from our industrial partner, which is a worldwide leader in train manufacturing. We record the results in terms of recall, precision, and F-measure. Moreover, results are compared against those obtained by a baseline, and a statistical analysis is performed to provide evidences of the significance of the results.ResultsThe results show that FROM can be applied in our industrial case study. Also, the results show that the configurations and the baseline have significant differences in performance for TLR, BL, and FL tasks. Moreover, our results show that there is no single configuration that is powerful enough to obtain the best results in all tasks.ConclusionsThe type of task performed (TLR, BL, and FL) during the retrieval of model fragments has an actual impact on the results of the configurations of the Evolutionary Algorithm. Our findings suggest which configuration offers better results as well as the objectives that do not contribute to improve the results."
Review article - A business model for commercial open source software: A systematic literature review,"AbstractContextCommercial open source software (COSS) and community open source software (OSS) are two types of open source software. The former is the newer concept with the grounds for research such as business model. However, in the literature of open source software, the revenue model has been studied as a business model, which is one component of the business model. Therefore, there is a need for a more complete review of the COSS business model with all components.ObjectiveThe purpose of this research is to describe and present the COSS business model with all its components.MethodA systematic literature review of the COSS business model was conducted and 1157 studies were retrieved through search in six academic databases. The result of the process of selecting the primary studies was 21 studies. By backward snowballing, we discovered 10 other studies, and thus a total of 31 studies were found. Then, the grounded theory coding procedures were used to determine the characteristics and components of the COSS business model.ResultsThe COSS business model was presented with value proposition, value creation & delivery, and value capture. This business model includes eight components: COSS products and complementarities, COSS clients and users, COSS competitive strategies, organizational aspects of COSS, position of COSS producers in the value network, resources and capabilities of COSS business, COSS revenue sources, and COSS cost-benefit.ConclusionThis study provides a complete illustration of the COSS business model. Identifies COSS generic competitive strategies. By cost-benefit component, we have considered both tangible and intangible components. This business model is especially effective in developing countries. In future research, it is necessary to review the management of the COSS community, the organization, the new revenue models for disruptive ability of open source software, and the localization of open source software."
Research article - Improving the experience for software-measurement system end-users: A story of two companies,"AbstractContextSoftware measurement systems are used in large companies to provide developers with up-to-date feedback and metrics.ObjectiveHowever, the front-ends of these systems are often not ready to provide a real-time experience for the end-users, who sometimes have to wait minutes before visualizations are provided.MethodIn this paper, we present the case studies of two large international companies and compare alternative technological setups for measurement system front-ends used and explored within these companies. We use a publicly available data-set for a performance evaluation and to analyze the results.ResultsFor both companies we found significant performance differences between the alternative setups. However, in one of the companies these differences are not there for large data-sets. Furthermore, we found that not all setups enable the visualization of the newest available data.ConclusionsOur results indicate that the choice of the visualization component has a larger impact on the performance than the choice of the data storage. However, companies are also willing to invest into setups that ensure that visualized measures are always up-to-date."
Research article - Using simulation for understanding and reproducing distributed software development processes in the cloud,"AbstractContext: Organizations increasingly develop software in a distributed manner. The Cloud provides an environment to create and maintain software-based products and services. Currently, it is unknown which software processes are suited for Cloud-based development and what their effects in specific contexts are.Objective: We aim at better understanding the software process applied to distributed software development using the Cloud as development environment. We further aim at providing an instrument, which helps project managers comparing different solution approaches and to adapt team processes to improve future project activities and outcomes.Method: We provide a simulation model, which helps analyzing different project parameters and their impact on projects performed in the Cloud. To evaluate the simulation model, we conduct different analyses using a Scrumban process and data from a project executed in Finland and Spain. An extra adaptation of the simulation model for Scrum and Kanban was used to evaluate the suitability of the simulation model to cover further process models.Results: A comparison of the real project data with the results obtained from the different simulation runs shows the simulation producing results close to the real data, and we could successfully replicate a distributed software project. Furthermore, we could show that the simulation model is suitable to address further process models.Conclusion: The simulator helps reproducing activities, developers, and events in the project, and it helps analyzing potential tradeoffs, e.g., regarding throughput, total time, project size, team size and work-in-progress limits. Furthermore, the simulation model supports project managers selecting the most suitable planning alternative thus supporting decision-making processes."
Research article - A case study on software vulnerability coordination,"AbstractContext: Coordination is a fundamental tenet of software engineering. Coordination is required also for identifying discovered and disclosed software vulnerabilities with Common Vulnerabilities and Exposures (CVEs). Motivated by recent practical challenges, this paper examines the coordination of CVEs for open source projects through a public mailing list.Objective: The paper observes the historical time delays between the assignment of CVEs on a mailing list and the later appearance of these in the National Vulnerability Database (NVD). Drawing from research on software engineering coordination, software vulnerabilities, and bug tracking, the delays are modeled through three dimensions: social networks and communication practices, tracking infrastructures, and the technical characteristics of the CVEs coordinated.Method: Given a period between 2008 and 2016, a sample of over five thousand CVEs is used to model the delays with nearly fifty explanatory metrics. Regression analysis is used for the modeling.Results: The results show that the CVE coordination delays are affected by different abstractions for noise and prerequisite constraints. These abstractions convey effects from the social network and infrastructure dimensions. Particularly strong effect sizes are observed for annual and monthly control metrics, a control metric for weekends, the degrees of the nodes in the CVE coordination networks, and the number of references given in NVD for the CVEs archived. Smaller but visible effects are present for metrics measuring the entropy of the emails exchanged, traces to bug tracking systems, and other related aspects. The empirical signals are weaker for the technical characteristics.Conclusion: Software vulnerability and CVE coordination exhibit all typical traits of software engineering coordination in general. The coordination perspective elaborated and the case studied open new avenues for further empirical inquiries as well as practical improvements for the contemporary CVE coordination."
