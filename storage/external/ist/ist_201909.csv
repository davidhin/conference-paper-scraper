title,abstract
Research article - A systematic literature review of test breakage prevention and repair techniques,"AbstractContextWhen an application evolves, some of the developed test cases break. Discarding broken test cases causes a significant waste of effort and leads to test suites that are less effective and have lower coverage. Test repair approaches evolve test suites along with applications by repairing the broken test cases.ObjectiveNumerous studies are published on test repair approaches every year. It is important to summarise and consolidate the existing knowledge in the area to provide directions to researchers and practitioners. This research work provides a systematic literature review in the area of test case repair and breakage prevention, aiming to guide researchers and practitioners in the field of software testing.MethodWe followed the standard protocol for conducting a systematic literature review. First, research goals were defined using the Goal Question Metric (GQM). Then we formulate research questions corresponding to each goal. Finally, metrics are extracted from the included papers. Based on the defined selection criteria a final set of 41 primary studies are included for analysis.ResultsThe selection process resulted in 5 journal papers, and 36 conference papers. We present a taxonomy that lists the causes of test case breakages extracted from the literature. We found that only four proposed test repair tools are publicly available. Most studies evaluated their approaches on open-source case studies.ConclusionThere is significant room for future research on test repair techniques. Despite the positive trend of evaluating approaches on large scale open source studies, there is a clear lack of results from studies done in a real industrial context. Few tools are publicly available which lowers the potential of adaption by industry practitioners."
Research article - Interactive semi-automated specification mining for debugging: An experience report,"AbstractContextSpecification mining techniques are typically used to extract the specification of a software in the absence of (up-to-date) specification documents. This is useful for program comprehension, testing, and anomaly detection. However, specification mining can also potentially be used for debugging, where a faulty behavior is abstracted to give developers a context about the bug and help them locating it.ObjectiveIn this project, we investigate this idea in an industrial setting. We propose a very basic semi-automated specification mining approach for debugging and apply that on real reported issues from an AutoPilot software system from our industry partner, MicroPilot Inc. The objective is to assess the feasibility and usefulness of the approach in a real-world setting.MethodThe approach is developed as a prototype tool, working on C code, which accept a set of relevant state fields and functions, per issue, and generates an extended finite state machine that represents the faulty behavior, abstracted with respect to the relevant context (the selected fields and functions).ResultsWe qualitatively evaluate the approach by a set of interviews (including observational studies) with the company’s developers on their real-world reported bugs. The results show that (a) our approach is feasible, (b) it can be automated to some extent, and (c) brings advantages over only using their code-level debugging tools. We also compared this approach with traditional fully automated state-merging algorithms and reported several issues when applying those techniques on a real-world debugging context.ConclusionThe main conclusion of this study is that the idea of an “interactive” specification mining rather than a fully automated mining tool is NOT impractical and indeed is useful for the debugging use case."
Research article - Adopting configuration management principles for managing experiment materials in families of experiments,"AbstractContextReplication is a key component of experimentation for verifying previous results and findings. Experiment replication requires products like documentation describing the baseline experiment and a version of the experimental material. When replicating an experiment, changes may have to be made to some of the products, leading to new or modified versions of materials. After the replication has been conducted, part of or all the materials should be added to the family history or to the baseline experiment documentation. As the number of replications increases, more versions of the materials are generated. This can lead to product management chaos in replications sharing the same protocol.ObjectiveThe aim of this paper is to adopt configuration management principles to manage experimental materials. We apply and validate these principles in a code inspection technique comparison experiment and a personality quasi-experiment.MethodThe study was conducted within a research group with lengthy experience in experiment replication. This research group has had trouble with the management of the materials used to run some of the experiments replicated by other colleagues. This is a suitable context for applying action research. We used action research to adopt the configuration management principles and build a materials management framework.ResultWe generated the instances of an experiment and a quasi-experiment, identifying the status and traceability of the materials. Additionally, we documented the workload required for instantiation in person-hours. We also checked the ease of use and understanding of the framework for instantiating the personality quasi-experiment configuration plan executed by researchers who did not develop the framework, as well as its usefulness for managing the experimental materials.ConclusionThe experimental materials management framework is useful for establishing the status and traceability of the experimental materials. Additionally, it improves the storage, search, location and retrieval of the experimental material versions."
Research article - Multi-armed bandits in the wild: Pitfalls and strategies in online experiments,"AbstractContextDelivering faster value to customers with online experimentation is an emerging practice in industry. Multi-Armed Bandit (MAB) based experiments have the potential to deliver even faster results with a better allocation of resources over traditional A/B experiments. However, the incorrect use of MAB-based experiments can lead to incorrect conclusions that can potentially hurt the company's business.ObjectiveThe objective of this study is to understand the pitfalls and restrictions of using MABs in online experiments, as well as the strategies that are used to overcome them.MethodThis research uses a multiple case study method with eleven experts across five software companies and simulations to triangulate the data of some of the identified limitations.ResultsThis study analyzes some limitations faced by companies using MAB and discusses strategies used to overcome them. The results are summarized into practitioners’ guidelines with criteria to select an appropriated experimental design.ConclusionMAB algorithms have the potential to deliver even faster results with a better allocation of resources over traditional A/B experiments. However, potential mistakes can occur and hinder the potential benefits of such approach. Together with the provided guidelines, we aim for this paper to be used as reference material for practitioners during the design of an online experiment."
Research article - Specifying quantities in software models,"AbstractContextAn essential requirement for the design and development of any engineering application that deals with real-world physical systems is the formal representation and processing of physical quantities, comprising both measurement uncertainty and units. Although solutions exist for several programming languages and simulation frameworks, this problem has not yet been fully solved for software models.ObjectiveThis paper shows how both measurement uncertainty and units can be effectively incorporated into software models, becoming part of their basic type systems.MethodWe introduce the main concepts and mechanisms needed for representing and handling physical quantities in software models. More precisely, we describe an extension of basic type Real, called Quantity, and a set of operations defined for the values of that type, together with a ready-to-use library of dimensions and units, which can be added to any modeling project.ResultsWe show how our approach permits modelers to safely represent and operate with physical quantities, statically ensuring type- and unit-safe assignments and operations, prior to any simulation of the system or implementation in any programming language.ConclusionOur approach improves the expressiveness and type-safety of software models with respect to measurement uncertainty and units of physical quantities, and its effective use in modeling projects of physical systems."
Research article - An HMM-based approach for automatic detection and classification of duplicate bug reports,"AbstractContextSoftware projects rely on their issue tracking systems to guide maintenance activities of software developers. Bug reports submitted to the issue tracking systems carry crucial information about the nature of the crash (such as texts from users or developers and execution information about the running functions before the occurrence of a crash). Typically, big software projects receive thousands of reports every day.ObjectiveThe aim is to reduce the time and effort required to fix bugs while improving software quality overall. Previous studies have shown that a large amount of bug reports are duplicates of previously reported ones. For example, as many as 30% of all reports in for Firefox are duplicates.MethodWhile there exist a wide variety of approaches to automatically detect duplicate bug reports by natural language processing, only a few approaches have considered execution information (the so-called stack traces) inside bug reports. In this paper, we propose a novel approach that automatically detects duplicate bug reports using stack traces and Hidden Markov Models.ResultsWhen applying our approach to Firefox and GNOME datasets, we show that, for Firefox, the average recall for Rank k = 1 is 59%, for Rank k = 2 is 75.55%. We start reaching the 90% recall from k = 10. The Mean Average Precision (MAP) value is up to 76.5%. For GNOME, The recall at k = 1 is around 63%, while this value increases by about 10% for k = 2. The recall increases to 97% for k = 11. A MAP value of up to 73% is achieved.ConclusionWe show that HMM and stack traces are a powerful combination for detecting and classifying duplicate bug reports in large bug repositories."
Research article - On the use of usage patterns from telemetry data for test case prioritization,"AbstractContextModern applications contain pervasive telemetry to ensure reliability and enable monitoring and diagnosis. This presents a new opportunity in the area of regression testing techniques, as we now have the ability to consider usage profiles of the software when making decisions on test execution.ObjectiveThe results of our prior work on test prioritization using telemetry data showed improvement rate on test suite reduction, and test execution time. The objective of this paper is to further investigate this approach and apply prioritization based on multiple prioritization algorithms in an enterprise level cloud application as well as open source projects. We aim to provide an effective prioritization scheme that practitioners can implement with minimum effort. The other objective is to compare the results and the benefits of this technique factors with code coverage-based prioritization approaches, which is the most commonly used test prioritization technique.MethodWe introduce a method for identifying usage patterns based on telemetry, which we refer to as “telemetry fingerprinting.” Through the use of various algorithms to compute fingerprints, we conduct empirical studies on multiple software products to show that telemetry fingerprinting can be used to more effectively prioritize regression tests.ResultsOur experimental results show that the proposed techniques were able to reduce over 30% in regression test suite run times compared to the coverage-based prioritization technique in detecting discoverable faults. Further, the results indicate that fingerprints are effective in identifying usage patterns, and that the fingerprints can be applied to improve regression testing techniques.ConclusionIn this research, we introduce the concept of fingerprinting software usage patterns through telemetry. We provide various algorithms to compute fingerprints and conduct empirical studies that show that fingerprints are effective in identifying distinct usage patterns. By applying these techniques, we believe that regression testing techniques can be improved beyond the current state-of-the-art, yielding additional cost and quality benefits."
Research article - M-Lean: An end-to-end development framework for predictive models in B2B scenarios,"AbstractContextThe need for business intelligence has led to advances in machine learning in the business domain, especially with the rise of big data analytics. However, the resulting predictive systems often fail to maintain a satisfactory level of performance in production. Besides, for predictive systems used in business-to-business scenarios, user trust is subject to the model performance. Therefore, the processes of creating, evaluating, and deploying machine learning systems in the business domain need innovative solutions to solve the critical challenges of assuring the quality of the resulting systems.ObjectiveApplying machine learning in business-to-business situations imposes specific requirements. This paper aims at providing an integrated solution to businesses to help them transform their data into actions.MethodThe paper presents MLean, an end-to-end framework, that aims at guiding businesses in designing, developing, evaluating, and deploying business-to-business predictive systems. The framework employs the Lean Startup methodology and aims at maximizing the business value while eliminating wasteful development practices.ResultsTo evaluate the proposed framework, with the help of our industrial partner, we applied the framework to a case study to build a predictive product. The case study resulted in a predictive system to predict the risks of software license cancellations. The system was iteratively developed and evaluated while adopting the management and end-user perspectives.ConclusionIt is concluded that, in industry, it is important to be aware of the businesses requirements before considering the application of machine learning. The framework accommodates business perspective from the beginning to produce a holistic product. From the results of the case study, we think that this framework can help businesses define the right opportunities for applying machine learning, developing solutions, evaluating the effectiveness of these solutions, and maintaining their performance in production."
