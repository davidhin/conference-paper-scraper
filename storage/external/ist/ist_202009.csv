title,abstract
Review article - Model composition in Model Driven Engineering: A systematic literature review,"AbstractContextModel Driven Engineering (MDE) aims to alleviate complexity and improve reusability in software development. The development of complex software implies to divide it into independent parts before then assembled. This is how the problem of model composition has become an interesting and stills an emerging topic in MDE.ObjectiveOur goal is to analyze the current state of the art in model composition in the context of Model Driven Engineering.MethodWe use the systematic literature review based on the guidelines proposed by Biolchini et al., Brereton et al., and Kitchenham and Charters. We propose five research questions and six quality assessments.ResultsOf the 9270 search results, 56 have been considered relevant studies. These studies have resulted in 36 primary studies.ConclusionThe evaluation shows that most of approaches allow more than two models as inputs of the composition, allow composing heterogeneous models and enable the tuning of the composition schema, while the important limitations are about the maturity of implementations and the lack on the management of future evolutions or backwards compatibility."
Research article - Automated isolation for white-box test generation,"AbstractContext: White-box test generation is a technique used for automatically selecting test inputs using only the code under test. However, such techniques encounter challenges when applying them to complex programs. One of the challenges is handling invocations to external modules or dependencies in the code under test.Objective: Without using proper isolation, like mocks, generated tests cannot cover all parts of the source code. Moreover, invoking external dependencies may cause unexpected side effects (e.g., accessing the file system or network). Our goal was to tackle this issue while maintaining the advantages of white-box test generation.Method: In this paper, we present an automated approach addressing the external dependency challenge for white-box test generation. This technique isolates the test generation and execution by transforming the code under test and creating a parameterized sandbox with generated mocks. We implemented the approach in a ready-to-use tool using Microsoft Pex as a test generator, and evaluated it on 10 open-source projects from GitHub having more than 38.000 lines of code in total.Results: The results from the evaluation indicate that if the lack of isolation hinders white-box test generation, then our approach is able to help: it increases the code coverage reached by the automatically generated tests, while it prevents invoking any external module or dependency. Also, our results act as a unique baseline for the test generation performance of Microsoft Pex on open-source projects.Conclusion: Based on the results, our technique might serve well for handling external dependencies in white-box test generation as it increases the coverage reached in such situations, while maintaining the practical applicability of the tests generated on the isolated code."
Research article - CodeGRU: Context-aware deep learning with gated recurrent unit for source code modeling,"AbstractContext: Recently deep learning based Natural Language Processing (NLP) models have shown great potential in the modeling of source code. However, a major limitation of these approaches is that they take source code as simple tokens of text and ignore its contextual, syntactical and structural dependencies.Objective: In this work, we present CodeGRU, a gated recurrent unit based source code language model that is capable of capturing source code’s contextual, syntactical and structural dependencies.Method: We introduce a novel approach which can capture the source code context by leveraging the source code token types. Further, we adopt a novel approach which can learn variable size context by taking into account source code’s syntax, and structural information.Results: We evaluate CodeGRU with real-world data set and it shows that CodeGRU outperforms the state-of-the-art language models and help reduce the vocabulary size up to 24.93%. Unlike previous works, we tested CodeGRU with an independent test set which suggests that our methodology does not requisite the source code comes from the same domain as training data while providing suggestions. We further evaluate CodeGRU with two software engineering applications: source code suggestion, and source code completion.Conclusion: Our experiment confirms that the source code’s contextual information can be vital and can help improve the software language models. The extensive evaluation of CodeGRU shows that it outperforms the state-of-the-art models. The results further suggest that the proposed approach can help reduce the vocabulary size and is of practical use for software developers."
Research article - A large scale study on how developers discuss code smells and anti-pattern in Stack Exchange sites,"AbstractContext: In this paper, we investigate how developers discuss code smells and anti-patterns across three technical Stack Exchange sites. Understanding developers perceptions of these issues is important to inform and align future research efforts and direct tools vendors to design tailored tools that best suit developers. Method: we mined three Stack Exchange sites and used quantitative and qualitative methods to analyse more than 4000 posts that discuss code smells and anti-patterns.Results: results showed that developers often asked their peers to smell their code, thus utilising those sites as an informal, crowd-based code smell/anti-pattern detector. The majority of questions (556) asked were focused on smells like Duplicated Code, Spaghetti Code, God and Data Classes. In terms of languages, most of discussions centred around popular languages such as C# (772 posts), JavaScript (720) and Java (699), however greater support is available for Java compared to other languages (especially modern languages such as Swift and Kotlin). We also found that developers often discuss the downsides of implementing specific design patterns and ‘flag’ them as potential anti-patterns to be avoided. Some well-defined smells and anti-patterns are discussed as potentially being acceptable practice in certain scenarios. In general, developers actively seek to consider trade-offs to decide whether to use a design pattern, an anti-pattern or not.Conclusion: our results suggest that there is a need for: 1) more context and domain sensitive evaluations of code smells and anti-patterns, 2) better guidelines for making trade-offs when applying design patterns or eliminating smells/anti-patterns in industry, and 3) a unified, constantly updated, catalog of smells and anti-patterns. We conjecture that the crowd-based detection approach considers contextual factors and thus tend to be more trusted by developers than automated detection tools."
Research article - Semantically find similar binary codes with mixed key instruction sequence,"AbstractContextSoftware similarity comparison has always been a common technique for software reuse detection, plagiarism detection, and defect detection.ObjectiveConsidering the role of API calls and arithmetic operations in software execution, a semantic-based dynamic software analysis method–mixed key instruction sequence (MKIS) is proposed.MethodMKIS embeds key value sets into a vector and constructs a novel software execution sequence that contains API calls and arithmetic operations during software execution. To determine the location of key values, a key-value equivalent matching algorithm is proposed, combined with the longest common subsequence algorithm to optimize the software execution sequence.ResultsExperiments show that MKIS can accurately compare the similarity of binary programs without obtaining the software source code, and has better resiliency and credibility.ConclusionMoreover, in the case when the software source code is changed with some main function-independent modification and code obfuscator, software reuse can be successfully detected."
Research article - Comparing manual and automated feature location in conceptual models: A Controlled experiment,"AbstractContextMaintenance activities cannot be completed without locating the set of software artifacts that realize a particular feature of a software system. Manual Feature Location (FL) is widely used in industry, but it becomes challenging (time-consuming and error prone) in large software repositories. To reduce manual efforts, automated FL techniques have been proposed. Research efforts in FL tend to make comparisons between automated FL techniques, ignoring manual FL techniques. Moreover, existing research puts the focus on code, neglecting other artifacts such as models.ObjectiveThis paper aims to compare manual FL against automated FL in models to answer important questions about performance, productivity, and satisfaction of both treatments.MethodWe run an experiment for comparing manual and automated FL on a set of 18 subjects (5 experts and 13 non-experts) in the domain of our industrial partner, BSH, manufacturer of induction hobs for more than 15 years. We measure performance (recall, precision, and F-measure), productivity (ratio between F-measure and spent time), and satisfaction (perceived ease of use, perceived usefulness, and intention to use) of both treatments, and perform statistical tests to assess whether the obtained differences are significant.ResultsRegarding performance, manual FL significantly outperforms automated FL in precision and F-measure (up to 27.79% and 19.05%, respectively), whereas automated FL significantly outperforms manual FL in recall (up to 32.18%). Regarding productivity, manual FL obtains 3.43%/min, which improves automated FL significantly. Finally, there are no significant differences in satisfaction for both treatments.ConclusionsThe findings of our work can be leveraged to advance research to improve the results of manual and automated FL techniques. For instance, automated FL in industry faces issues such as low discrimination capacity. In addition, the obtained satisfaction results have implications for the usage and possible combination of manual, automated, and guided FL techniques."
Research article - On an optimal analogy-based software effort estimation,"AbstractContext: An analogy-based software effort estimation technique estimates the required effort for a new software project based on the total effort used in completing past similar projects. In practice, offering high accuracy can be difficult for the technique when the new software project is not similar to any completed projects. In this case, the accuracy will rely heavily on a process called effort adaptation, where the level of difference between the new project and its most similar past projects is quantified and transformed to the difference in the effort. In the past, attempts to adapt to the effort used machine learning algorithms; however, no algorithm was able to offer a significantly higher performance. On the contrary, only a simple heuristic such as scaling the effort by consulting the difference in software size was adopted.Objective:More recently, million-dollar prize data-science competitions have fostered the rapid development of more powerful machine learning algorithms, such as the Gradient boosting machine and Deep learning algorithm. Therefore, this study revisits the comparison of software effort adaptors that are based on heuristics and machine learning algorithms.Method:A systematic comparison of software effort estimators, which they all were fully optimized by Bayesian optimization technique, was carried out on 13 standard benchmark datasets. The comparison was supported by robust performance metrics and robust statistical test methods.Conclusion:The results suggest a novel strategy to construct a more accurate analogy-based estimator by adopting a combined effort adaptor. In particular, the analogy-based model that adapts to the effort by integrating the Gradient boosting machine algorithm and a traditional adaptation technique based on productivity adjustment has performed the best in the study. Particularly, this model significantly outperformed various state-of-the-art effort estimation techniques, including a current standard benchmark algorithmic-based technique, analogy-based techniques, and machine learning-based techniques."
Research article - ManQ: Many-objective optimization-based automatic query reduction for IR-based bug localization,"AbstractContextAn information retrieval-based bug localization (IRBL) method is proposed to localize buggy files using a bug report as a query. The performance of this method strongly depends on the quality of the query. However, these queries contain noise terms that hinder their use for IRBL. To improve the quality of a query, an automatic query reduction (AQR) technique that removes noise words from the query is needed.ObjectiveOur objective is to develop an AQR method for IRBL. Most existing AQR techniques are based on single objective optimization, which presents issues in terms of biased and limited performance. To solve these issues, it is necessary to find a subquery that comprehensively satisfies all of their objectives.MethodWe propose an AQR technique called ManQ, which is a many-objective optimization-based AQR method for IRBL. We design 15 objective functions to (1) maintain the query quality properties, (2) maintain the important terms, (3) maintain the initial information, and (4) minimize the query length. ManQ finds a final subquery that maximize the return values of these objective functions.ResultsThe experimental results show that ManQ improves the quality of poor queries. We also show that if we select the best query among the candidates generated by ManQ, we can increase the number of improved queries by more than 53.4% of all queries.ConclusionManQ improves the performance of IRBL by improving the quality of queries through a many-objective optimization approach."
Research article - Developer portraying: A quick approach to understanding developers on OSS platforms,"AbstractContextMillions of software developers are using open-source software (OSS) platforms to host their code and collaborate with each other. They possess different programming skills, styles, and preferences, etc., and it is important to understand them for making collaborative decisions such as programming task assignment. Existing OSS platforms do not provide sufficient information about developers, and we need to spend significant effort in searching the OSS platforms for such information.ObjectiveDifferent than the basic developer information displayed on OSS platforms, we propose portraying developers as a quick approach for characterizing and understanding them. We discuss how to build developer portraits to make them concise yet informative.MethodWe propose a multi-dimensional developer portrait model to specify the attributes of various aspects concerning software development about developers. Then, a method that leverages text analysis, web data analysis, and code analysis techniques is presented to analyze a developer’s various sources of data on OSS platforms for constructing the portrait.ResultsThe constructed portraits can be vividly displayed on the web to help people quickly understand developers and make better decisions during collaborative software development. Case studies on two representative problems in the software engineering area—code recommendation and programming task assignment—are conducted, and the results show the improvement in recommendation and the potential for proper assignments when using our portraits.ConclusionThe developer portrait is an effective form to characterize developers. It can help people quickly understand the developers and can be applied to various applications in the software development process."
