title,abstract
Short communication - The evolution of agile UXD,"AbstractContextAgile User eXperience Design (Agile UXD) is a current theme and a trending topic for the future of software development. The integration of UX Design within Agile development is seen as one of the frontiers for Agile Methods as a balance between upfront design as advocated by UX and the you-ain't-gonna-need-it (YAGNI) principle from the agile community must be found.ObjectiveIn this paper, we analyze the evolution and current state of Agile UXD to provide a brief overview of the topic and to point out still unaddressed gaps, challenges, and future trends.MethodWe systematically analyzed the existing research literature on how this topic evolved over time. We identified three categories with distinctive sets of work and classified them as Early, Middle and Recent years.ResultsWe noticed that the Process and Practice dimension has already crossed the line that separates Agile and UXD, the People and Social dimension is crossing this line right now, and the Technology and Artifact is the dimension that took the longest to be addressed, and it did not cross the line yet. Crossing the line means that there is already a full understanding from the Agile side of UX needs and vice versa.ConclusionAgile UXD is a need for today's software development teams. However, integrated teams still need to understand that UXD is not a role, but discipline and culture for the whole Agile environment."
Research article - Debugging and maintaining pragmatically reused test suites,"AbstractContextPragmatic software reuse is a common activity in industry, involving the reuse of software artifacts not designed to anticipate that reuse.ObjectiveThere are two key issues in such tasks that have not been previously explored. (1) Subtle bugs can be inserted due to mistakes on the part of a developer performing the pragmatic reuse. The reused code, integrated in the target system, should be (re-)validated there. But it is not clear what validation strategies would be employed by professional developers, and which of these strategies would be most effective to detect and to repair these inserted bugs. (2) Although semi-automated reuse of the associated test suite has been previously proposed as a strategy to detect such inserted bugs, it is unknown if the reused test suite would be maintainable in practice and how its maintenance characteristics would compare against alternative strategies.MethodWe present two empirical studies with industrial developers to address these open issues.ResultsWe find that industrial developers use a few strategies including test suite reuse, but that test suite reuse is more reliably effective at discovering and repairing bugs inserted during pragmatic reuse. We also find that, in general, semi-automatically reused test suites are slightly more maintainable than manually reused test suites, in pragmatic reuse scenarios; specific situations can vary wildly however. Participants suggested specific extensions to tool support for semi-automated reuse of test suites.ConclusionsWhile various validation strategies are employed by industrial developers in the context of pragmatic reuse, none is as reliable and effective as test case reuse at discovering and repairing bugs inserted during pragmatic reuse. Despite the fact that semi-automatically reused test cases contain non-trivial adaptive code, their maintainability is equivalent to or exceeds that of manually reused test suites. The approach could be improved, however, by adopting the suggestions of our participants to increase usability."
Research article - Model-driven architecture based testing: A systematic literature review,"AbstractContextModel-driven architecture based testing (MDABT) adopts architectural models of a system under test and/or its environment to derive test artifacts. In the literature, different MDABT approaches have been provided together with the corresponding lessons results and lessons learned.ObjectiveThe overall objective of this paper is to identify the published concerns for applying MDABT, identify the proposed solutions, and describe the current research directions for MDABT.MethodTo this end we have provided a systematic literature review (SLR) that is conducted by a multi-phase study selection process using the published literature in major software engineering journals and conference proceedings.ResultsWe reviewed 739 papers that are discovered using a well-planned review protocol, and 31 of them were assessed as primary studies related to our research questions. Based on the analysis of the data extraction process, we discuss the primary trends and approaches and present the identified obstacles.ConclusionThis study shows that although a generic process the approaches different in various ways with different goals, modeling abstractions and results. Further, based on the synthesis process in the SLR we can state that the potential of MDABT has not been fully exploited yet."
Research article - User interface design smell: Automatic detection and refactoring of Blob listeners,"AbstractContext. User Interfaces (UIs) intensively rely on event-driven programming: interactive objects send UI events, which capture users’ interactions, to dedicated objects called controllers. Controllers use several UI listeners that handle these events to produce UI commands.Objective. First, we reveal the presence of design smells in the code that describes and controls UIs. Second, we demonstrate that specific code analyses are necessary to analyze and refactor UI code, because of its coupling with the rest of the code.Method. We conducted an empirical study on four large Java software systems. We studied to what extent the number of UI commands that a UI listener can produce has an impact on the change- and fault-proneness of the UI listener code. We developed a static code analysis for detecting UI commands in the code.Results. We identified a new type of design smell, called Blob listener, that characterizes UI listeners that can produce more than two UI commands. We proposed a systematic static code analysis procedure that searches for Blob listener that we implement in InspectorGuidget. We conducted experiments on the four software systems for which we manually identified 53 instances of Blob listener. InspectorGuidget successfully detected 52 Blob listeners out of 53. The results exhibit a precision of 81.25% and a recall of 98.11%. We then developed a semi-automatically and behavior-preserving refactoring process to remove Blob listeners. 49.06% of the 53 Blob listeners were automatically refactored. Patches have been accepted and merged. Discussions with developers of the four software systems assess the relevance of the Blob listener.Conclusion. This work shows that UI code also suffers from design smells that have to be identified and characterized. We argue that studies have to be conducted to find other UI design smells and tools that analyze UI code must be developed."
Research article - Particle swarm optimization-based ensemble learning for software change prediction,"AbstractContextVarious researchers have successfully established the association between Object-Oriented metrics and change prone nature of a class. However, they actively continue to explore effective classifiers for developing efficient change prediction models. Recent developments have ascertained that ensemble methodology can be used to improve the prediction performance of individual classifiers.ObjectiveThis study proposes four strategies of ensemble learning to predict change prone classes by combining seven individual Particle Swarm Optimization (PSO) based classifiers as constituents of ensembles and aggregating them using weighted voting.MethodThe weights allocated to individual classifiers are based on their accuracy and their ability to correctly predict “hard instances” i.e. classes which are frequently misclassified by a majority of classifiers. Each individual PSO based classifier uses a different fitness function. The ensembles are constructed on the premises that change in fitness functions leads to variation in the results of a search-based algorithm such as PSO. Therefore, it is important to combine them to obtain a better classifier with improved accuracy using the ensemble methodology.ResultsThe proposed strategies of ensemble learning were found effective in predicting software change. The statistical analysis of the results indicates improved performance of the proposed ensemble classifiers as compared to individual classifiers. Furthermore, the results of the proposed voting ensemble classifiers were found competent with those of machine-learning ensemble classifiers for determination of change prone classes.ConclusionThe accuracy and diversity of the individual classifiers were instrumental in the superior performance of the proposed voting ensemble classifiers."
Review article - Stakeholder quantification and prioritisation research: A systematic literature review,"AbstractContextStakeholder quantification and prioritisation (SQP) is executed to quantify and prioritise stakeholders of the system based on their impacts. Selecting and involving the appropriate stakeholders are considered one of the major factors for producing a successful system.ObjectiveThe objectives of this paper is to provide precise investigation regarding the SQP domain with respect to its impact on prioritising requirements, identifying SQP attributes, critically investigating the existing techniques, and presenting the challenges and recommended future works.MethodThe systematic literature review (SLR) guidelines proposed by Kitchenham are adopted to guide the review process. The identified related studies underwent a rigorous study selection process. Thus, 31 out of 210 identified studies were selected as primary studies to address adequately the formulated research questions.ResultsFindings demonstrate that SQP is a crucial process in requirement prioritisation (RP) because of its ability to identify stakeholders’ impact on the systems requirements that lead to the production of a correctly prioritised list of requirements. Seventeen SQP attributes are revealed along with their description, usage impact, and degree of importance. Furthermore, nine techniques that focus on quantification and prioritisation of the stakeholders are identified and critically analysed in terms of their description, SQP process involved, SQP attributes used, types, and limitations. The findings reveal that these techniques face some challenges with respect to the lack of low-level implementation details, lack of automation and intelligence level, and heavy reliance on the involvement of experts.ConclusionSQP has been extensively discussed in stakeholder analysis and requirement prioritisation domains. Based on the findings, a new intelligent solution is suggested to minimise the need for expert participation in conducting the SQP process along with proposing measurement criteria for the attributes used to evaluate the stakeholders. The deficiency of research works regarding the selection of SQP techniques is also observed."
Research article - A software quality framework for large-scale mission-critical systems engineering,"AbstractContext:In the industry of large-scale mission-critical systems, software is a pivotal asset and a key business driver. Production and maintenance costs of systems in domains like air/naval traffic control or homeland security are largely dependent on the quality of software, and there are numerous examples where poor software quality is blamed for major business failures. Because of the size, the complexity and the nature of systems and engineering processes in this industry, there is a strong need yet a slow shift toward innovation in software quality management.Objective:We present SVEVIA, a framework for software quality assessment and strategic decisions support for large-scale mission-critical systems engineering, and its application in a three years long industry-academy cooperation.Method:We started with the analysis of the industrial software quality management processes, and identified the key challenges toward a satisfying quality-cost-time trade-off. We defined new methods for product/process quality assessment, prediction, planning and optimization. We experimented them on the industrial partner systems and processes. They finally conflated in the SVEVIA framework.Results:SVEVIA was integrated into the industrial process, and tested with hundreds of software (sub)systems. More than 20 millions of lines of code – deployed in about 20 sites in Italy and UK – have come under the new quality measurement and improvement chain. The framework proved its ability to support systematic management of software quality and key decisions for productivity improvement.Conclusion:SVEVIA supports team leaders and managers coping with software quality in mission-critical industries, yielding figures and projections about quality and productivity trends for a prompt and informed decision-making."
"Review article - A tertiary study on technical debt: Types, management strategies, research trends, and base information for practitioners","AbstractContextThe concept of technical debt (TD) contextualizes problems faced during software evolution considering the tasks that are not carried out adequately during its development. Currently, it is common to associate any impediment related to the software product and its development process to the definition of TD. This can bring confusion and ambiguity in the use of the term. Besides, due to the increasing amount of work in the area, it is difficult to have a comprehensive view of the plethora of proposals on TD management.ObjectiveThis paper intends to investigate the current state of research on TD by identifying what research topics have been considered, organizing research directions and practical knowledge that has already been defined, identifying the known types of TD, and organizing what activities, strategies and tools have been proposed to support the management of TD.MethodA tertiary study was performed based on a set of five research questions. In total, 13 secondary studies, dated from 2012 to March 2018, were evaluated.ResultsThe results of this tertiary study are beneficial for both practitioners and researchers. We evolved a taxonomy of TD types, identified a list of situations in which debt items can be found in software projects, and organized a map representing the state of the art of activities, strategies and tools to support TD management. Besides, we also summarized some research directions and practical knowledge, and identified the research topics that have been more considered in secondary studies.ConclusionThis tertiary study revisited the TD landscape. Its results can help to identify points that still require further investigation in TD research."
Review article - Mapping the field of software life cycle security metrics,"AbstractContext: Practitioners establish a piece of software’s security objectives during the software development process. To support control and assessment, practitioners and researchers seek to measure security risks and mitigations during software development projects. Metrics provide one means for assessing whether software security objectives have been achieved. A catalog of security metrics for the software development life cycle could assist practitioners in choosing appropriate metrics, and researchers in identifying opportunities for refinement of security measurement.Objective: The goal of this research is to support practitioner and researcher use of security measurement in the software life cycle by cataloging security metrics presented in the literature, their validation, and the subjects they measure.Method: We conducted a systematic mapping study, beginning with 4818 papers and narrowing down to 71 papers reporting on 324 unique security metrics. For each metric, we identified the subject being measured, how the metric has been validated, and how the metric is used. We categorized the metrics, and give examples of metrics for each category.Results: In our data, 85% of security metrics have been proposed and evaluated solely by their authors, leaving room for replication and confirmation through field studies. Approximately 60% of the metrics have been empirically evaluated, by their authors or by others. The available metrics are weighted heavily toward the implementation and operations phases, with relatively few metrics for requirements, design, and testing phases of software development. Some artifacts and processes remain unmeasured. Measured by phase, Testing received the least attention, with 1.5% of the metrics.Conclusions: At present, the primary application of security metrics to the software development life cycle in the literature is to study the relationship between properties of source code and reported vulnerabilities. The most-cited and most used metric, vulnerability count, has multiple definitions and operationalizations. We suggest that researchers must check vulnerability count definitions when making comparisons between papers. In addition to refining vulnerability measurement, we see research opportunities for greater attention to metrics for the requirement, design, and testing phases of development. We conjecture from our data that the field of software life cycle security metrics has yet to converge on an accepted set of metrics."
Research article - The impact of IR-based classifier configuration on the performance and the effort of method-level bug localization,"AbstractContextIR-based bug localization is a classifier that assists developers in locating buggy source code entities (e.g., files and methods) based on the content of a bug report. Such IR-based classifiers have various parameters that can be configured differently (e.g., the choice of entity representation).ObjectiveIn this paper, we investigate the impact of the choice of the IR-based classifier configuration on the top-k performance and the required effort to examine source code entities before locating a bug at the method level.MethodWe execute a large space of classifier configuration, 3172 in total, on 5266 bug reports of two software systems, i.e., Eclipse and Mozilla.ResultsWe find that (1) the choice of classifier configuration impacts the top-k performance from 0.44% to 36% and the required effort from 4395 to 50,000 LOC; (2) classifier configurations with similar top-k performance might require different efforts; (3) VSM achieves both the best top-k performance and the least required effort for method-level bug localization; (4) the likelihood of randomly picking a configuration that performs within 20% of the best top-k classifier configuration is on average 5.4% and that of the least effort is on average 1%; (5) configurations related to the entity representation of the analyzed data have the most impact on both the top-k performance and the required effort; and (6) the most efficient classifier configuration obtained at the method-level can also be used at the file-level (and vice versa).ConclusionOur results lead us to conclude that configuration has a large impact on both the top-k performance and the required effort for method-level bug localization, suggesting that the IR-based configuration settings should be carefully selected and the required effort metric should be included in future bug localization studies."
Research article - Dependability-enhanced unified modeling and simulation methodology for Critical Infrastructures,"AbstractContextCritical infrastructures (CIs) are mission-critical, large-scale systems that provide essential products and services for everyday life. Modeling and simulation (M&S) technique is one of useful methods for understanding the complex and emergent behavior of CIs. However, the several characteristics of CIs, such as interdependence, adaptability, dependability, etc., can disturb modelers’ activities to develop the quality models for CIs. The quality of these models can affect the reliability of simulation results as well as the reusability of the models in further simulations.ObjectiveIn this paper, we propose a M&S methodology that aims to improve the quality of CI models by means of focusing on dependability, which is one of important characteristics of CIs.MethodFirst, we propose a system to model and simulate CIs based on discrete event system specification, a belief-desire-intention model, and a role-oriented command hierarchy model. Next, we integrate several modeling methods, such as goal modeling, agent-based modeling, and object-oriented modeling, into a unified M&S methodology (UMAS) to seamlessly develop CI models from initial requirements to final modeling artifacts. Finally, we propose a dependability-enhanced UMAS (DUMAS) that can deal with the dependability of CI models.ResultsIn our case study of applying the DUMAS into the M&S of a smart grid, we show how the DUMAS reflects the characteristics of CIs on the overall modeling artifacts.ConclusionThe DUMAS provides a novel method that can improve the quality of CI models through elaborating on the activities of requirements engineering with regard to the dependability of CIs. Therefore, modelers can systematically develop quality models from requirements analysis to implementation in compliance with the DUMAS."
