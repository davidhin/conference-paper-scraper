title,abstract
Research article - An empirical study of sentiments in code reviews,"AbstractContextModern code reviews are supported by tools to enhance developers’ interactions allowing contributors to submit their opinions for each committed change in form of comments. Although the comments are aimed at discussing potential technical issues, the text might enclose harmful sentiments that could erode the benefits of suggested changes.ObjectiveIn this paper, we study empirically the impact of sentiment embodied within developers’ comments on the time and outcome of the code review process.MethodBased on historical data of four long-lived Open Source Software (OSS) projects from a code review system we investigate whether perceived sentiments have any impact on the interval time of code changes acceptance.ResultsWe found that (1) contributors frequently express positive and negative sentiments during code review activities; (2) the expressed sentiments differ among the contributors depending on their position within the social network of the reviewers (e.g., core vs peripheral contributors); (3) the sentiments expressed by contributors tend to be neutral as they progress from the status of newcomer in an OSS project to the status of core team contributors; (4) the reviews with negative comments on average took more time to complete than the reviews with positive/neutral comments, and (5) the reviews with controversial comments took significantly longer time in one project.ConclusionThrough this work, we provide evidences that text-based sentiments have an impact on the duration of the code review process as well as the acceptance or rejection of the suggested changes."
Review article - A systematic mapping addressing Hyper-Heuristics within Search-based Software Testing,"AbstractContextSearch-based Software Testing (SBST) is a research field where testing a software product is formulated as an optimization problem. It is an active sub-area of Search-based Software Engineering (SBSE) where many studies have been published and some reviews have been carried out. The majority of studies in SBST has been adopted meta-heuristics while hyper-heuristics have a long way to go. Moreover, there is still a lack of studies to perceive the state-of-the-art of the use of hyper-heuristics within SBST.ObjectiveThe objective of this work is to investigate the adoption of hyper-heuristics for Software Testing highlighting the current efforts and identifying new research directions.MethodA Systematic mapping study was carried out with 5 research questions considering papers published up to may/2019, and 4 different bases. The research questions aims to find out, among other things, what are the hyper-heuristics used in the context of Software Testing, for what problems hyper-heuristics have been applied, and what are the objective functions in the scope of Software Testing.ResultsA total of 734 studies were found via the search strings and 164 articles were related to Software Testing. However, from these, only 26 papers were actually in accordance with the scope of this research and 3 more papers were considered due to snowballing or expert’s suggestion, totalizing 29 selected papers. Few different problems and application domains where hyper-heuristics have been considered were identified.ConclusionDifferently from other communities (Operational Research, Artificial Intelligence), SBST has little explored the benefits of hyper-heuristics which include generalization and less difficulty in parameterization. Hence, it is important to further investigate this area in order to alleviate the effort of practitioners to use such an approach in their testing activities."
Research article - Employment of multiple algorithms for optimal path-based test selection strategy,"AbstractContextExecuting various sequences of system functions in a system under test represents one of the primary techniques in software testing. The natural method for creating effective, consistent and efficient test sequences is to model the system under test and employ an algorithm to generate tests that satisfy a defined test coverage criterion. Several criteria for preferred test set properties can be defined. In addition, to optimize the test set from an economic viewpoint, the priorities of the various parts of the system model under test must be defined.ObjectiveUsing this prioritization, the test cases exercise the high-priority parts of the system under test by more path combinations than those with low priority (this prioritization can be combined with the test coverage criterion that determines how many path combinations of the individual parts of the system are tested). Evidence from the literature and our observations confirm that finding a universal algorithm that produces a test set with preferred properties for all test coverage criteria is a challenging task. Moreover, for different individual problem instances, different algorithms provide results with the best value of a preferred property. In this paper, we present a portfolio-based strategy to perform the best test selection.MethodThe proposed strategy first employs a set of current algorithms to generate test sets; then, a preferred property of each test set is assessed in terms of the selected criterion, and finally, the test set with the best value of a preferred property is chosen.ResultsThe experimental results confirm the validity and usefulness of this strategy. For individual instances of 50 system under test models, different algorithms provided results having the best preferred property value; these results varied by the required test coverage level, the size of the priority parts of the model, and the selected test set preferred property criteria.ConclusionIn addition to the used algorithms, the proposed strategy can be used to assess the optimality of different path-based testing algorithms and choose a suitable algorithm for the testing."
Research article - Model driven transformation development (MDTD): An approach for developing model to model transformation,"AbstractContextIn the Model Driven Development (MDD) approach, model transformations are responsible for the semi-automation of software development process converting models between different abstraction levels. The development of model transformations involves a complexity inherent to the transformation domain, in addition to the complexity of software development in general. Therefore, the construction of model transformations requires software engineering feature such as processes and languages to facilitate its development and maintenance.ObjectiveThis paper presents a framework to develop unidirectional relational model transformation using the MDD approach itself, which integrates: (i) a software development process suitable for the model transformation domain (ii) a Domain specific language for transformation modeling (iii) a transformation chain, to (semi) automate the proposed process, and (iv) a development environment to support it.MethodsThe proposal systematizes the development of model transformation, following the MDD principles. An iterative and incremental process guides transformation development from requirement specification to transformation codification. The proposal has been evaluated through a case study and a controlled experiment.ResultsThe framework enables model transformation specification at a high abstraction level and (semi) automatically transforms it into models at a low abstraction level until the transformation code. The results of the case study showed that people with different levels of knowledge of MDD, or without experience in transformation languages, were able to develop transformations through the framework and generated executable code.ConclusionsThe framework integrates the essential elements involved in the development of model transformation and enables the abstraction of technological details. The results of the case study and controlled experiment showed the feasibility of the proposal and its use in dealing with the complexity involved in model transformation development."
Research article - Startup ecosystem effect on minimum viable product development in software startups,"AbstractContextSoftware startups develop innovative products through which they scale their business rapidly, and thus, provide value to the economy, including job generation. However, most startups fail within two years of their launch because of a poor problem-solution fit and negligence of the learning process during minimum viable product (MVP) development. An ideal startup ecosystem can assist in MVP development by providing the necessary entrepreneurial education and technical skills to founding team members for identifying problem-solution fit for their product idea, allowing them to find the right product-market fit. However, existing knowledge on the effect of the startup ecosystem elements on the MVP development is limited.ObjectiveThe empirical study presented in this article aims to identify the effect of the six ecosystem elements (entrepreneurs, technology, market, support factors, finance, and human capital) on MVP development.MethodWe conducted a study with 13 software startups and five supporting organizations (accelerators, incubator, co-working space, and investment firm) in the startup ecosystem of the city of Oulu in Finland. Data were collected through semi-structured interviews, observation, and materials.ResultsThe study results showed that internal sources are most common for identifying requirements for the product idea for MVP development. The findings indicate that supporting factors, such as incubators and accelerators, can influence MVP development by providing young founders with the necessary entrepreneurship skills and education needed to create the right product-market fit.ConclusionsWe conclude from this study of a regional startup ecosystem that the MVP development process is most affected by founding team members’ experiences and skill sets and by advanced technologies. Furthermore, a constructive startup ecosystem around software startups can boost up the creation of an effective MVP to test product ideas and find a product-market fit."
Research article - On the use of virtual reality in software visualization: The case of the city metaphor,"AbstractBackground: Researchers have been exploring 3D representations for visualizing software. Among these representations, one of the most popular is the city metaphor, which represents a target object-oriented system as a virtual city. Recently, this metaphor has been also implemented in interactive software visualization tools that use virtual reality in an immersive 3D environment medium.Aims: We assessed the city metaphor displayed on a standard computer screen and in an immersive virtual reality with respect to the support provided in the comprehension of Java software systems.Method: We conducted a controlled experiment where we asked the participants to fulfill program comprehension tasks with the support of (i) an integrated development environment (Eclipse) with a plugin for gathering code metrics and identifying bad smells; and (ii) a visualization tool of the city metaphor displayed on a standard computer screen and in an immersive virtual reality.Results: The use of the city metaphor displayed on a standard computer screen and in an immersive virtual reality significantly improved the correctness of the solutions to program comprehension tasks with respect to Eclipse. Moreover, when carrying out these tasks, the participants using the city metaphor displayed in an immersive virtual reality were significantly faster than those visualizing with the city metaphor on a standard computer screen.Conclusions: Virtual reality is a viable means for software visualization."
Research article - A novel approach for automatic remodularization of software systems using extended ant colony optimization algorithm,"AbstractContextSoftware modularization is extremely important to streamline the inner structure of the program modules without influencing its core functionality. As the framework advances during the upkeep stage, the pristine design of the software package gets disintegrated and hence it is arduous to understand and maintain. There are many existing approaches being carried out to automatically remodularize using optimization techniques to ease the maintenance and improve the quality of the system. The outcomes are rather insufficiently optimal and depend on problem-specific operators, which in turn expands the time multifaceted nature to land at an answer. Apart from these limitations, the issues, such as time complexity, scalability and performance need to be addressed.ObjectiveIn this paper, an efficient automatic software remodularization using extended Ant Colony Optimization (ACO) has been proposed to remodularize the software systems.MethodThe proposed approach mainly includes two phases: optimised traversal of software system using ACO for finding the order of software files to be processed and remodularization of software system using the proposed approach of extended ACO.ResultsWe experimented our proposed approach on seven software systems. The performance is evaluated by using Turbo modularization quality (MQ) which supports Module dependency graph (MDG) that have edge weights. The time complexity of remodularized software system is evaluated based on number of Turbo MQ.ConclusionIt can be concluded that when the performance has been compared with the subsisting methodologies, for example, Genetic algorithm (GA), Hill climbing (HC) and Interactive genetic algorithms (I-GAs), the proposed approach has higher Turbo MQ value with lesser time complexity in the evaluated software systems."
Research article - Log mining to re-construct system behavior: An exploratory study on a large telescope system,"AbstractContextA large amount of information about system behavior is stored in logs that record system changes. Such information can be exploited to discover anomalies of a system and the operations that cause them. Given their large size, manual inspection of logs is hard and infeasible in a desired timeframe (e.g., real-time), especially for critical systems.ObjectiveThis study proposes a semi-automated method for reconstructing sequences of tasks of a system, revealing system anomalies, and associating tasks and anomalies to code components.MethodThe proposed approach uses unsupervised machine learning (Latent Dirichlet Allocation) to discover latent topics in messages of log events and introduces a novel technique based on pattern recognition to derive the semantic of such topics (topic labelling). The approach has been applied to the big data generated by the ALMA telescope system consisting of more than 2000 log events collected in about five hours of telescope operation.ResultsWith the application of our approach to such data, we were able to model the behavior of the telescope over 16 different observations. We found five different behavior models and three different types of errors. We use the models to interpret each error and discuss its cause.ConclusionsWith this work, we have also been able to discuss some of the known challenges in log mining. The experience we gather has been then summarized in lessons learned."
Research article - Scheduling sequence selection for generating test data to cover paths of MPI programs,"AbstractContext: As one of key tasks in software testing, test data generation has been receiving widespread attention in recent years. Message-passing Interface (MPI) programs, which are one representative type of parallel programs, have the characteristic of non-determinism, which is reflected by the non-deterministic execution under different scheduling sequences against the same program input. Previous studies have shown that different difficulties are raised in generating test data under different scheduling sequences, suggesting that selecting appropriate scheduling sequences is beneficial to a high efficiency.Objective: We propose a method of selecting a superior and feasible scheduling sequence for generating test data in the criterion of path coverage against each target path of an MPI program.Method: In the proposed method, a number of program inputs are first sampled by Latin hypercube sampling. Then, the program is executed against each sample under each scheduling sequence, and all the scheduling sequences are sorted according to the similarities between the paths traversed by these samples and the target one. Finally, the feasibility of a scheduling sequence with the best quality is investigated based on the symbolic execution.Results: We apply the proposed method to seven typical MPI programs and compare it with the random one. The experimental results show that test data covering the target path can be generated under the selected scheduling sequence with high success rate and low time consumption.Conclusion: The proposed method takes the influence of scheduling sequences on generating test data into consideration, thus providing a competent way to test parallel programs."
Research article - Improving defect prediction with deep forest,"AbstractContextSoftware defect prediction is important to ensure the quality of software. Nowadays, many supervised learning techniques have been applied to identify defective instances (e.g., methods, classes, and modules).ObjectiveHowever, the performance of these supervised learning techniques are still far from satisfactory, and it will be important to design more advanced techniques to improve the performance of defect prediction models.MethodWe propose a new deep forest model to build the defect prediction model (DPDF). This model can identify more important defect features by using a new cascade strategy, which transforms random forest classifiers into a layer-by-layer structure. This design takes full advantage of ensemble learning and deep learning.ResultsWe evaluate our approach on 25 open source projects from four public datasets (i.e., NASA, PROMISE, AEEEM and Relink). Experimental results show that our approach increases AUC value by 5% compared with the best traditional machine learning algorithms.ConclusionThe deep strategy in DPDF is effective for software defect prediction."
Research article - DevOps in practice: A multiple case study of five companies,"AbstractContext: DevOps is considered important in the ability to frequently and reliably update a system in operational state. DevOps presumes cross-functional collaboration and automation between software development and operations. DevOps adoption and implementation in companies is non-trivial due to required changes in technical, organisational and cultural aspects.Objectives: This exploratory study presents detailed descriptions of how DevOps is implemented in practice. The context of our empirical investigation is web application and service development in small and medium sized companies.Method: A multiple-case study was conducted in five different development contexts with successful DevOps implementations since its benefits, such as quick releases and minimum deployment errors, were achieved. Data was mainly collected through interviews with 26 practitioners and observations made at the companies. Data was analysed by first coding each case individually using a set of predefined themes and thereafter perform a cross-case synthesis.Results: Our analysis yielded some of the following results: (i) software development team attaining ownership and responsibility to deploy software changes in production is crucial in DevOps. (ii) toolchain usage and support in deployment pipeline activities accelerates the delivery of software changes, bug fixes and handling of production incidents. (ii) the delivery speed to production is affected by context factors, such as manual approvals by the product owner (iii) steep learning curve for new skills is experienced by both software developers and operations staff, who also have to cope with working under pressure.Conclusion: Our findings contributes to the overall understanding of DevOps concept, practices and its perceived impacts, particularly in small and medium sized companies. We discuss two practical implications of the results."
Research article - A conceptual perspective on interoperability in context-aware software systems,"AbstractContextContext-aware software systems can interact with different devices to complete their tasks and act according to the context, regardless of their development and organizational differences. Interoperability is a big challenge in the engineering of such systems.ObjectiveTo discuss how interoperability has been addressed in context-aware software systems, strengthening the scientific basis for its understanding and conceptualization.MethodA quasi-systematic literature review was undertaken to observe interoperability in such context-aware software systems to support the discussions. Its dataset includes 17 from 408 papers identified in the technical literature. The extracted information was qualitatively analyzed by following the principles of Grounded Theory.ResultsThe analysis allowed to identify ten interoperability concepts, organized into a Theoretical Framework according to structural and behavioral perspectives, which deals with interoperability as the ability of things (an object, a place, an application or anything that can engage interaction with a system) to interact for a particular purpose, once their differences (development platforms, data formats, culture, legal issues) have been overcome. Once the interoperability is established from structural concepts (context, perspective, purpose, the level of provided support and system attributes), it can be measured, improved and observed from the behavioral concepts (evaluation method, challenges, issues, and benefits).ConclusionsThe Interoperability Theoretical Framework provides relevant information to organize the knowledge related to interoperability, considering context, and can be used to guide the evolution of software systems regarding changes focused on interoperability."
"Research article - A large-scale, in-depth analysis of developers’ personalities in the Apache ecosystem","AbstractContextLarge-scale distributed projects are typically the results of collective efforts performed by multiple developers with heterogeneous personalities.ObjectiveWe aim to find evidence that personalities can explain developers’ behavior in large scale-distributed projects. For example, the propensity to trust others — a critical factor for the success of global software engineering — has been found to influence positively the result of code reviews in distributed projects.MethodIn this paper, we perform a quantitative analysis of ecosystem-level data from the code commits and email messages contributed by the developers working on the Apache Software Foundation (ASF) projects, as representative of large scale-distributed projects.ResultsWe find that there are three common types of personality profiles among Apache developers, characterized in particular by their level of Agreeableness and Neuroticism. We also confirm that developers’ personality is stable over time. Moreover, personality traits do not vary with their role, membership, and extent of contribution to the projects. We also find evidence that more open developers are more likely to make contributors to Apache projects.ConclusionOverall, our findings reinforce the need for future studies on human factors in software engineering to use psychometric tools to control for differences in developers’ personalities."
Research article - Resilience of distributed student teams to stress factors: A longitudinal case-study,"AbstractContext: Teaching global software engineering is continuously evolving and improving to prepare future software engineers adequately. Geographically distributed work in project-oriented software development courses is both demanding and rewarding for student teams, who are susceptible to various risks stemming from different internal and external factors, being the sources of stress and impacting team performance.Objective: In this paper, we analyze the resilience of teams of students working in a geographically fully distributed setting. Resilience is analyzed in relation to two representative stress factors: non-contributing team members and changes to customer project requirements. We also reason on team collaboration patterns and analyze potential dependencies among these collaboration patterns, team resilience and stress factors.Method: We conduct a longitudinal case-study over five years on our Distributed Software Development (DSD) course. Based on empirical data, we study team resilience to two stress factors by observing their impact on process and product quality aspects of team performance. The same performance aspects are studied for identified collaboration patterns, and bidirectional influence between patterns and resilience is investigated.Results: Teams with up to two non-contributing members experience graceful degradation of performance indicators. A large number of non-contributing students almost guarantees the occurrence of educationally undesirable collaboration patterns. Exposed to requirements change stress, less resilient teams tend to focus on delivering the functional product rather than retaining a proper development process.Conclusions: Practical recommendations to be applied in contexts similar to our case have been provided at the end of the study. They include suggestions to mitigate the sources of stress, for example, by careful planning the team organization and balancing the number of regular and exchange students, or by discussing the issue of changing requirements with the external customers before the start of the project."
Research article - Pareto efficient multi-objective black-box test case selection for simulation-based testing,"AbstractContext: In many domains, engineers build simulation models (e.g., Simulink) before developing code to simulate the behavior of complex systems (e.g., Cyber-Physical Systems). Those models are commonly heavy to simulate which makes it difficult to execute the entire test suite. Furthermore, it is often difficult to measure white-box coverage of test cases when employing such models. In addition, the historical data related to failures might not be available.Objective: The objective of the approach presented in this paper is to cost-effectively select test cases without making use of white-box coverage information or historical data related to fault detection.Method: We propose a cost-effective approach for test case selection that relies on black-box data related to inputs and outputs of the system. The approach defines in total six effectiveness measures and one cost measure followed by deriving in total 21 objective combinations and integrating them within Non-Dominated Sorting Genetic Algorithm-II (NSGA-II). The proposed six effectiveness metrics are specific to simulation models and are based on anti-patterns and similarity measures.Results: We empirically evaluated our approach with these 21 combinations using six case studies by employing mutation testing to assess the fault revealing capability. We compared our approach with Random Search (RS), two many-objective algorithm, as well as three white-box metrics. The results demonstrated that our approach managed to improve Random Search by up to around 28% in terms of the Hypervolume quality indicator. Similarly, black-box metrics-based test case selection also significantly outperformed those of white-box metrics.Conclusion: We demonstrate that test case selection is a non-trivial problem in the context of simulation models. We also show that the proposed effectiveness metrics performed significantly better than traditional white-box metrics. Thus, we show that black-box test selection approaches are appropriate to solve the test case selection problem within simulation models."
Research article - Standing on the shoulders of giants: Seeding search-based multi-objective optimization with prior knowledge for software service composition,"AbstractContextSearch-Based Software Engineering, in particular multi-objective evolutionary algorithm, is a promising approach to engineering software service composition while simultaneously optimizing multiple conflicting Quality-of-Service (QoS) objectives. Yet, existing applications of evolutionary algorithms have failed to consider domain knowledge about the problem into the optimization, which is a perhaps obvious but challenging task.ObjectiveThis paper aims to investigate different strategies of exploring and injecting knowledge about the problem into the Multi-Objective Evolutionary Algorithm (MOEA) by seeding. Further, we investigate various factors that contribute to the effectiveness of seeding, including the number of seeds, the importance of crossover operation and the similarity of historical problems.MethodWe conduced empirical evaluations with NSGA-II, MOEA/D and IBEA based on a wide spectrum of problem instances, including 10 different workflow structures, from 5 to 100 abstract services and 510 to 5.529  × 10203 candidate concrete services with diverse QoS on latency, throughput and cost, which was chosen from the real-world WS-DREAM dataset that contains 4500 QoS values.ResultsWe found that, (i) all seeding strategies generally outperform their non-seeded counterparts under the same search budget with large statistical significance. Yet, they may involve relatively smaller compromise on one or two of the quality aspects among convergence, uniformity and spread. (ii) The implication of the number of seeds on the service composition problems is minimal in general (except for IBEA). (iii) In contrast to the non-seeded counterparts, the seeding strategies suffer much less implications by the crossover operation. (iv) The differences of historical problems, which are important for two proposed seeding strategies, can indeed affect the results in a non-linear manner; however, the results are still greatly better than the non-seeded counterparts even with up to 90% difference of the problem settings.ConclusionThe paper concludes that (i) When applying the seeding strategies, the number of seeds to be placed in is less important in general, except for the pre-optimization based strategies under IBEA. (ii) Eliminating or having less crossover is harmful for multi-objective service composition optimization, but the seeding strategies are much less sensitive to this operator than their non-seeded counterparts. (iii) For the history based seeding strategies, the seeds do not have to come from the most similar historical composition problem to achieve the best HV value, but a largely different historical problem should usually be avoided, unless they are the only available seeds."
