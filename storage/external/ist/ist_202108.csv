title,abstract
Review article - Measuring the cognitive load of software developers: An extended Systematic Mapping Study,"AbstractContext:Cognitive load in software engineering refers to the mental effort users spend while reading software artifacts. The cognitive load can vary according to tasks and across developers. Researchers have measured developers’ cognitive load for different purposes, such as understanding its impact on productivity and software quality. Thus, researchers and practitioners can use cognitive load measures for solving many aspects of software engineering problems.Problem:However, a lack of a classification of dimensions on cognitive load measures in software engineering makes it difficult for researchers and practitioners to obtain research trends to advance scientific knowledge or apply it in software projects.Objective:This article aims to classify different aspects of cognitive load measures in software engineering and identify challenges for further research.Method:We conducted a Systematic Mapping Study (SMS), which started with 4,175 articles gathered from 11 search engines and then narrowed down to 63 primary studies.Results:Our main findings are: (1) 43% (27/63) of the primary studies focused on applying a combination of sensors; (2) 81% (51/63) of the selected works were validation studies; (3) 83% (52/63) of the primary studies analyzed cognitive load while developers performed programming tasks. Moreover, we created a classification scheme based on the answers to our research questions.Conclusion:despite the production of a significant amount of studies on cognitive load in software engineering, there are still many challenges to be solved in this particular field for effectively measuring the cognitive load in software engineering. Therefore, this work provided directions for future studies on cognitive load measurement in software engineering."
Review article - Automation of systematic literature reviews: A systematic literature review,"AbstractContextSystematic Literature Review (SLR) studies aim to identify relevant primary papers, extract the required data, analyze, and synthesize results to gain further and broader insight into the investigated domain. Multiple SLR studies have been conducted in several domains, such as software engineering, medicine, and pharmacy. Conducting an SLR is a time-consuming, laborious, and costly effort. As such, several researchers developed different techniques to automate the SLR process. However, a systematic overview of the current state-of-the-art in SLR automation seems to be lacking.ObjectiveThis study aims to collect and synthesize the studies that focus on the automation of SLR to pave the way for further research.MethodA systematic literature review is conducted on published primary studies on the automation of SLR studies, in which 41 primary studies have been analyzed.ResultsThis SLR identifies the objectives of automation studies, application domains, automated steps of the SLR, automation techniques, and challenges and solution directions.ConclusionAccording to our study, the leading automated step is the Selection of Primary Studies. Although many studies have provided automation approaches for systematic literature reviews, no study has been found to apply automation techniques in the planning and reporting phase. Further research is needed to support the automation of the other activities of the SLR process."
Research article - BCI-CFI: A context-sensitive control-flow integrity method based on branch correlation integrity,"AbstractContextAs part of the arms race, one emerging attack methodology has been control-hijacking attacks, e.g., return-oriented programming (ROP). Control-flow integrity (CFI) is a generic and effective defense against most control-hijacking attacks. However, existing CFI mechanisms have poor security as demonstrated by their equivalence class (EC) sizes, which are sets of targets that CFI policies cannot distinguish. Adversaries can choose an illegitimate control transfer within an EC that is included in the resulting CFG and incorrectly allowed by CFI protection policies.ObjectiveThe paper introduces a context-sensitive control-flow integrity method, which aims to improve the security of CFI and prevent ROP attacks.MethodThe paper presents BCI-CFI, a context-sensitive CFI technique based on branch correlation integrity (BCI), which can effectively break down EC sizes and improve the security of CFI. BCI-CFI takes the branch correlation relationship (i.e., a new type of context for CFI) as contextual information to refine the CFI policy and identify the BCI pairs in the target program via static analysis. Furthermore, the paper introduces a state machine MCFI for BCI-CFI to conduct target validation for the indirect control-flow transfer (ICT) instructions in the target program at runtime.ResultsOur results show that, (i) BCI-CFI prevented adversaries from manipulating the control data and launching ROP attacks, (ii) protected both forward and backward ICT in the target program, and improved the security and effectiveness of CFI, and (iii) BCI-CFI introduced a 19.67% runtime overhead on average and a maximum runtime overhead of 31.2%ConclusionBCI-CFI is a context-sensitive CFI technique aiming to prevent adversaries from manipulating the control data of the target program to launch ROP attacks. BCI-CFI can reduce EC sizes and improve the security of CFI while incurring a moderate runtime overhead on average."
Research article - BGNN4VD: Constructing Bidirectional Graph Neural-Network for Vulnerability Detection,"AbstractContext:Previous studies have shown that existing deep learning-based approaches can significantly improve the performance of vulnerability detection. They represent code in various forms and mine vulnerability features with deep learning models. However, the differences of code representation forms and deep learning models make various approaches still have some limitations. In practice, their false-positive rate (FPR) and false-negative rate (FNR) are still high.Objective:To address the limitations of existing deep learning-based vulnerability detection approaches, we propose BGNN4VD (Bidirectional Graph Neural Network for Vulnerability Detection), a vulnerability detection approach by constructing a Bidirectional Graph Neural-Network (BGNN).Method:In Phase 1, we extract the syntax and semantic information of source code through abstract syntax tree (AST), control flow graph (CFG), and data flow graph (DFG). Then in Phase 2, we use vectorized source code as input to Bidirectional Graph Neural-Network (BGNN). In Phase 3, we learn the different features between vulnerable code and non-vulnerable code by introducing backward edges on the basis of traditional Graph Neural-Network (GNN). Finally in Phase 4, a Convolutional Neural-Network (CNN) is used to further extract features and detect vulnerabilities through a classifier.Results:We evaluate BGNN4VD on four popular C/C++ projects from NVD and GitHub, and compare it with four state-of-the-art (Flawfinder, RATS, SySeVR, and VUDDY) vulnerab ility detection approaches. Experiment results show that, when compared these baselines, BGNN4VD achieves 4.9%, 11.0%, and 8.4% improvement in F1-measure, accuracy and precision, respectively.Conclusion:The proposed BGNN4VD achieves a higher precision and accuracy than the state-of-the-art methods. In addition, when applied on the latest vulnerabilities reported by CVE, BGNN4VD can still achieve a precision at 45.1%, which demonstrates the feasibility of BGNN4VD in practical application."
Research article - An empirical study on clone consistency prediction based on machine learning,"AbstractContext:Code Clones have been accepted as a common phenomenon in software, thanks to the increasing demand for rapid production of software. The existence of code clones is recognized by developers in the form of clone group, which includes several pieces of clone fragments that are similar to one another. A change in one of these clone fragments may indicate necessary “consistent changes” are required for the rest of the clones within the same group, which can increase extra maintenance costs. A failure in making such consistent change when it is necessary is commonly known as a “clone consistency-defect”, which can adversely impact software maintainability.Objective:Predicting the need for “clone consistent changes” after successful clone-creating or clone-changing operations can help developers maintain clone changes effectively, avoid consistency-defects and reduce maintenance cost.Method:In this work, we use several sets of attributes in two scenarios of clone operations (clone-creating and clone-changing), and conduct an empirical study on five different machine-learning methods to assess each of their clone consistency predictability — whether any one of the clone operations will require or be free of clone consistency maintenance in future.Results:We perform our experiments on eight open-source projects. Our study shows that such predictions can be reasonably effective both for clone-creating and changing operating instances. We also investigate the use of five different machine-learning methods for predictions and show that our selected features are effective in predicting the needs of consistency-maintenance across all selected machine-learning methods.Conclusion:The empirical study conducted here demonstrates that the models developed by different machine-learning methods with the specified sets of attributes have the ability to perform clone-consistency prediction."
Research article - From a Scrum Reference Ontology to the Integration of Applications for Data-Driven Software Development,"AbstractContextOrganizations often use different applications to support the Scrum process, including project management tools, source repository and quality assessment tools. These applications store useful data for decision-making. However, data items often remain spread in different applications, each of which adopt different data and behavioral models, posing a barrier for integrated data usage. As a consequence, data-driven decisions in agile development are uncommon, missing valuable opportunities for informed decision making.ObjectiveConsidering the need to address semantic issues to properly integrate applications that support the agile development process, we aim to provide a common and comprehensive conceptualization about Scrum in the software development context and apply this conceptualization to support application integration.MethodWe have developed the Scrum Reference Ontology (SRO) and used it to semantically integrate Azure DevOps and Clockify.ResultsSRO served as a reference model to build software artifacts in a semantic integration architecture that enables applications to automatically share, exchange and combine data and services. The integrated solution was used in the software development unit of a Brazilian government agency. Results demonstrate that the integrated solution contributed to improving estimates, provided data that helped allocate teams, manage team productivity and project performance, and enabled to identify and fix problems in the Scrum process execution.ConclusionsSRO can serve as an interlingua for application integration in the context of Scrum-process support. By capturing the conceptualization underlying Scrum, the reference ontology can address semantic conflicts and thereby support the development of integrated data-driven solutions for decision making."
Research article - Exploring the communication functions of comments during bug fixing in Open Source Software projects,"AbstractContext:Bug fixing is a frequent and important task in Open Source Software (OSS) development and involves the communication of messages, which can serve for multiple purposes and affect the efficiency and effectiveness of corrective software activities.Objective:This work is aimed at studying the communication functions of bug comments and their associations with fast and complete bug fixing in OSS development.Method:Over 500K comments and 89K bugs of 100 OSS projects were extracted from three Issue Tracking Systems. Six thousand comments were manually tagged to create a corpus of communication functions. The extracted comments were automatically tagged using machine learning algorithms and the corpus of communication functions. Statistical and correlation analyses were performed and the most frequent comments communicated during fast and successful bug fixing were identified.Results:Significant differences in the distribution of comments of fixed and not fixed bugs were found. Variations in the distribution of comments of bugs with different fixing time were also found. Referential comments that provided objective information were found to be the most frequent messages. Results showed that the percentages of conative and emotive comments are greater when bugs are resolved without the requested fixes and when fixes are implemented in a long time.Conclusion:Associations between communication functions and bug fixing exist. The results of this work could be used to improve corrective tasks in OSS development and some other specific linguistic aspects should be studied in detail in OSS communities."
Research article - Insights on the relationship between decision-making style and personality in software engineering,"AbstractContext:Software development involves many activities, and decision making is an essential one. Various factors can impact a decision-making process, and by understanding such factors, one can improve the process. Since people are the ones making decisions, some human-related aspects are amongst those influencing factors. One such aspect is the decision maker’s personality.Objective:This research investigates the relationship between decision-making style and personality within the context of software project development.Method:We conducted a survey in a population of Brazilian software engineers to gather data on their personality and decision-making style.Results:Data from 63 participants was gathered and resulted in the identification of seven statistically significant correlations between decision-making style and personality (personality factor and personality facets). Furthermore, we built a regression model in which decision-making style (DMS) was the response variable and personality factors the independent variables. The backward elimination procedure selected only agreeableness to explain 4.2% of DMS variation. The model accuracy was evaluated and deemed good enough. Regarding the moderation effect of demographic variables (age, educational level, experience, and role) on the relationship between DMS and Agreeableness, the analysis showed that only software engineers’ role has such effect.Conclusion:This paper contributes toward understanding the relationship between DMS and personality. Results show that the personality variable agreeableness can explain the variation in decision-making style. Furthermore, someone’s role in a software development project can impact the strength of the relationship between DMS and agreeableness."
Research article - A methodology to automatically translate user requirements into visualizations: Experimental validation,"AbstractContext:Information visualization is paramount for the analysis of Big Data. The volume of data requiring interpretation is continuously growing. However, users are usually not experts in information visualization. Thus, defining the visualization that best suits a determined context is a very challenging task for them. Moreover, it is often the case that users do not have a clear idea of what objectives they are building the visualizations for. Consequently, it is possible that graphics are misinterpreted, making wrong decisions that lead to missed opportunities. One of the underlying problems in this process is the lack of methodologies and tools that non-expert users in visualizations can use to define their objectives and visualizations.Objective:The main objectives of this paper are to (i) enable non-expert users in data visualization to communicate their analytical needs with little effort, (ii) generate the visualizations that best fit their requirements, and (iii) evaluate the impact of our proposal with reference to a case study, describing an experiment with 97 non-expert users in data visualization.Methods:We propose a methodology that collects user requirements and semi-automatically creates suitable visualizations. Our proposal covers the whole process, from the definition of requirements to the implementation of visualizations. The methodology has been tested with several groups to measure its effectiveness and perceived usefulness.Results:The experiments increase our confidence about the utility of our methodology. It significantly improves over the case when users face the same problem manually. Specifically: (i) users are allowed to cover more analytical questions, (ii) the visualizations produced are more effective, and (iii) the overall satisfaction of the users is larger.Conclusion:By following our proposal, non-expert users will be able to more effectively express their analytical needs and obtain the set of visualizations that best suits their goals."
"Research article - Motivations, benefits, and issues for adopting Micro-Frontends: A Multivocal Literature Review","AbstractContext:Micro-Frontends are increasing in popularity, being adopted by several large companies, such as DAZN, Ikea, Starbucks and may others. Micro-Frontends enable splitting of monolithic frontends into independent and smaller micro applications. However, many companies are still hesitant to adopt Micro-Frontends, due to the lack of knowledge concerning their benefits. Additionally, provided online documentation is often times perplexed and contradictory.Objective:The goal of this work is to map the existing knowledge on Micro-Frontends, by understanding the motivations of companies when adopting such applications as well as possible benefits and issues.Method:For this purpose, we surveyed the academic and grey literature by means of the Multivocal Literature Review process, analysing 173 sources, of which 43 reported motivations, benefits and issues.Results:The results show that existing architectural options to build web applications are cumbersome if the application and development team grows, and if multiple teams need to develop the same frontend application. In such cases, companies adopted Micro-Frontends to increase team independence and to reduce the overall complexity of the frontend. The application of the Micro-Frontend, confirmed the expected benefits, and Micro-Frontends resulted to provide the same benefits as microservices on the back end side, combining the development team into a fully cross-functional development team that can scale processes when needed. However, Micro-Frontends also showed some issues, such as the increased payload size of the application, increased code duplication and coupling between teams, and monitoring complexity.Conclusions:Micro-Frontends allow companies to scale development according to business needs in the same way microservices do with the back end side. In addition, Micro-Frontends have a lot of overhead and require careful planning if an advantage is achieved by using Micro-Frontends. Further research is needed to carefully investigate this new hype, by helping practitioners to understand how to use Micro-Frontends as well as understand in which contexts they are the most beneficial."
Research article - RLTCP: A reinforcement learning approach to prioritizing automated user interface tests,"AbstractContext:User interface testing validates the correctness of an application through visual cues and interactive events emitted in real-world usages. Performing user interface tests is a time-consuming process, and thus, many studies have focused on prioritizing test cases to help maintain the effectiveness of testing while reducing the need for full execution.Objective:This paper describes a novel test prioritization method called RLTCP whose goal is to maximize the number of test faults detected while reducing the amount of test.Methods:We define a weighted coverage graph to model the underlying association among test cases for the user interface testing. Our method combines Reinforcement Learning (RL) and the coverage graph to prioritize test cases. While RL is found to be suitable for rapidly changing projects with abundant historical data, the coverage graph considers in-depth the event-based aspects of user interface testing and provides a fine-grained level at which the RL system can gain more insights into individual test cases.Results:We experiment and assess the proposed method using nine data sets obtained from two mature web applications, finding that the method outperforms the six, including the state-of-the-art, methods.Conclusions:The use of both reinforcement learning and the underlying structure of user interface tests modeled via the coverage has the potential to improve the performance of test prioritization methods. Our study also shows the benefit of using the coverage graph to gain insights into test cases, their relationship and execution history."
