title,abstract
Review article - Research patterns and trends in software effort estimation,"AbstractContextSoftware effort estimation (SEE) is most crucial activity in the field of software engineering. Vast research has been conducted in SEE resulting into a tremendous increase in literature. Thus it is of utmost importance to identify the core research areas and trends in SEE which may lead the researchers to understand and discern the research patterns in large literature dataset.ObjectiveTo identify unobserved research patterns through natural language processing from a large set of research articles on SEE published during the period 1996 to 2016.MethodA generative statistical method, called Latent Dirichlet Allocation (LDA), applied on a literature dataset of 1178 articles published on SEE.ResultsAs many as twelve core research areas and sixty research trends have been revealed; and the identified research trends have been semantically mapped to associate core research areas.ConclusionsThis study summarises the research trends in SEE based upon a corpus of 1178 articles. The patterns and trends identified through this research can help in finding the potential research areas."
Research article - Investigating comprehension and learnability aspects of use cases for software specification problems,"AbstractContext: Availability of multiple use case templates to document software requirements inevitably requires their characterization in terms of their relevance, usefulness, and the degree of the formality of the expressions.Objective: This paper reports two experimental studies that separately investigate two usability aspects, namely the comprehension and the learnability of use case templates for software specification problems.Method: We judged the comprehension aspect by evaluating the subjects’ understanding of the requirements, specified in eight different use case templates, and the ease with which the changes were made by them in the requirement specifications. The learnability aspect was judged by assessing the completeness, the correctness, and the redundancy of the use case specifications developed by the subjects using these eight use case templates for three software specification problems.Results: Our results suggested that the Kettenis’s use case template was found to be significantly more understandable, and the templates by Tiwari, Yue and Somé were found to be significantly more flexible to adapt to the changes. On the learnability aspect, the way we formulated it, we found different templates to be more complete (Kettenis), correct (Somé), and non-redundant (Tiwari).Conclusion: The specifications documented using a more detailed use case template with an intermediate degree of formality can be more comprehensible and flexible to adapt to the required changes to be made in the specification. A more formal template seems to enhance the learnability as well."
Research article - Understanding inactive yet available assignees in GitHub,"AbstractContextIn GitHub, an issue or a pull request can be assigned to a specific assignee who is responsible for working on this issue or pull request. Due to the principle of voluntary participation, available assignees may remain inactive in projects. If assignees ever participate in projects, they are active assignees; otherwise, they are inactive yet available assignees (inactive assignees for short).ObjectiveOur objective in this paper is to provide a comprehensive analysis of inactive yet available assignees in GitHub.MethodWe collect 2,374,474 records of activities in 37 popular projects, and 797,756 records of activities in 687 projects belonging to 8 organizations. We compute the percentage of inactive assignees in projects, and compare projects with and without inactive assignees. Then we analyze datasets to explore why some assignees are inactive. Finally, we send questionnaires to understand impacts of inactive assignees.ResultsWe find that some projects have high percentage of inactive yet available assignees. For example, 66.35% of assignees never participate in the project paperclip. The project paperclip belongs to the organization thoughtbot. In the organization thoughtbot, 84.4% of projects have more than 80% of inactive assignees. We further observe that the main reason for developers being inactive assignees is that developers work for organizations and automatically become available assignees of some projects in the organizations. However, these developers do not work on projects. 37.25% of developers that we have surveyed agree that inactive assignees affect open source software development (i.e., causing unresolved issues or pull requests, and delaying software development).ConclusionSome organizations should improve team management, and carefully select developers to become assignees in projects. Future studies about assignees should be careful to perform data cleaning, since some available assignees are added by virtue of their employment and do not really work on projects."
Research article - Analysing problem handling schemes in software projects,"AbstractContextAlthough many papers have been published on software development and maintenance processes, there is still a need for deeper exploration of software repositories related to real projects to evaluate these processes.ObjectiveThe aim of this study is to present and evaluate different schemes of handling problems (bugs) during software development and maintenance. It is based on exploring information comprised in various software repositories.MethodHaving analysed the contents of software repositories we have introduced a graph model describing problem handling processes and a scheme of analysing the effectiveness of these processes. This analysis is targeted at structural, throughput and timing aspects. The presented methodology has been verified on real data comprised in open source and commercial projects. Moreover, having identified some drawbacks and anomalies, we point out possible improvements in problem reports and their management.ResultsDetailed software repositories provide a wider scope of possible measures to assess the relevant problem handling processes. This is helpful in controlling single projects (local perspective) as well as in managing these processes in the whole company (global perspective).ConclusionDetailed problem handling reports extend the space and quality of statistical analysis, the presented graph model with proposed metrics provides a useful tool to evaluate and refine problem resolution schemes."
Research article - Vision for SLR tooling infrastructure: Prioritizing value-added requirements,"AbstractContextEven with the increasing use of Systematic Literature Reviews (SLR) in software engineering (SE), there are still a number of barriers faced by SLR authors. These barriers increase the cost of conducting SLRs.ObjectiveFor many of these barriers, appropriate tool support could reduce their impact. In this paper, we use interactions with the SLR community in SE to identify and prioritize a set of requirements for SLR tooling infrastructure.MethodThis paper analyzes and combines the results from three studies on SLR process barriers and SLR tool requirements to produce a prioritized list of functional requirements for SLR tool support. Using this list of requirements, we perform a feature analysis of the current SLR support tools to identify requirements that are supported as well as identify the need for additional tooling infrastructure.ResultsThe analysis resulted in a list 112 detailed requirements (consolidated into a set of composite requirements) that SE community desires in SLR support tools. The requirements span all the phases of the SLR process. The results show that, while recent tools cover more of the requirements, there are a number of high-priority requirements that are not yet fully covered by any of the existing tools.ConclusionThe existing set of SLR tools do not cover all the requirements posed by the community. The list of requirements in this paper is useful for tool developers and researchers wishing to provide support to the SLR community with SE."
Review article - Choreography in the embedded systems domain: A systematic literature review,"AbstractContextSoftware companies that develop their products on a basis of service-oriented architecture can expect various improvements as a result of choreography. Current choreography practices, however, are not yet used extensively in the embedded systems domain even though service-oriented architecture is increasingly used in this domain.ObjectiveThe objective of this study is to identify current features of the use of choreography in the embedded systems domain for practitioners and researchers by systematically analysing current developments in the scientific literature, strategies for choreography adaption, choreography specification and execution types, and implicit assumptions about choreography.MethodTo fulfil this objective, a systematic literature review of scientific publications that focus on the use of choreography in the embedded systems domain was carried out. After a systematic screening of 6823 publications, 48 were selected as primary studies and analysed using thematic synthesis.ResultsThe main results of the study showed that there are differences in how choreography is used in embedded and non-embedded systems domain. In the embedded systems domain, it is used to capture the service interactions of a single organisation, while, for example, in the enterprise systems domain it captures the service interactions among multiple organisations. Additionally, the results indicate that the use of choreography can lead to improvements in system performance and that the languages that are used for choreography modelling in the embedded systems domain are insufficiently expressive to capture the complexities that are typical in this domain.ConclusionThe selection of the key information resources and the identified gaps in the existing literature offer researchers a foundation for further investigations and contribute to the advancement of the use of choreography in the embedded systems domain. The study results facilitate the work of practitioners by allowing them to make informed decisions about the applicability of choreography in their organisations."
Review article - Key factors that influence task allocation in global software development,"AbstractContextPlanning and managing task allocation in Global Software Development (GSD) projects is both critical and challenging. To date, a number of models that support task allocation have been proposed, including cost models and risk-based multi-criteria optimization models.ObjectiveThe objective of this paper is to identify the factors that influence task allocation in the GSD project management context.MethodFirst, we implemented a formal Systematic Literature Review (SLR) approach and identified a set of factors that influence task allocation in GSD projects. Second, a questionnaire survey was developed based on the SLR, and we collected feedback from 62 industry practitioners.ResultsThe findings of this combined SLR and questionnaire survey indicate that site technical expertise, time zone difference, resource cost, task dependency, task size and vendor reliability are the key criteria for the distribution of work units in a GSD project. The results of the t-test show that there is no significant difference between the findings of the SLR and questionnaire survey. However, the industry study data indicates that resource cost and task dependency are more important to a centralized GSD project structure while task size is a key factor in a decentralized GSD project structure.ConclusionGSD organizations should try to consider the identified task allocation factors when managing their global software development activities to better understand, plan and manage work distribution decisions."
Research article - A method for generation and design of business processes with business rules,"AbstractContext: Business Processes provide a universal method of describing operational aspects of business. Business Rules, in turn, support declarative specification of business domain knowledge. Although there is a difference in abstraction levels between these both modeling techniques, rules can be complementary to processes. Rules can be efficiently used to specify process low-level logic, while processes can serve as a procedural specification of the workflow, including the inference control.Objective: One of the research problems in this area is supporting business analytics in the modeling of processes integrated with rules. Such a support can take advantage of new design method for such models.Method: We describe a model of procedural Business Process as well as the model and method of creating Attribute Relationship Diagrams. Based on these two representations, we provide a formalized model combining a process model with rules. Using these models, we introduce an algorithm that generates an executable process model along with decision table schemas for rules (rule templates for rule sets grouped in decision tables).Results: The paper provides an automated approach for generation of Business Process models from Attribute Relationship Diagrams. The approach was evaluated based on the selected benchmark cases, which were deployed and tested in the provided modeling and execution environment for such integrated models.Conclusion: The paper presents an efficient and formalized method for design of processes with rules that allows for generating BPMN models integrated with the rules from the Semantic Knowledge Engineering approach. Such a model can be treated as a structured rule base that provides explicit inference flow determined by the process control flow."
Research article - A dedicated approach for model composition traceability,"AbstractContext: Software systems are often too complex to be expressed by a single model. Recognizing this, the Model Driven Engineering (MDE) proposes multi-modeling approaches to allow developers to describe a system from different perspectives. In this context, model composition has become important since the combination of those partial representations is inevitable. Nevertheless, no approach has been defined for keeping track of the composition effects, and this operation has been overshadowed by model transformations.ObjectiveThis paper presents a traceability approach dedicated to the composition of models. Two aspects of quality are considered: producing relevant traces; and dealing with scalability.MethodThe composition of softgoal trees has been selected to motivate the need for tracing the composition of models and to illustrate our approach. The base principle is to augment the specification of the composition with the behavior needed to generate the expected composed model accompanied with a trace model. This latter includes traces of the execution details. For that, traceability is considered as a crosscutting concern and encapsulated in an aspect. As part of the proposal, an Eclipse plug-in has been implemented as a tool support. Besides, a comparative experiment has been conducted to assess the traces relevance. We also used the regression method to validate the scalability of the tool support.ResultsOur experiments show that the proposed approach allows generating relevant traces. In addition, the obtained results reveal that tracing a growing number of elements causes an acceptable increase of response time.ConclusionThis paper presents a traceability approach dedicated to the composition of models and its application to softgoal trees. The experiment results reveal that our proposal considers the composition specificities for producing valuable traceability information while supporting scalability."
Research article - Open source software ecosystems: A Systematic mapping,"AbstractContext: Open source software (OSS) and software ecosystems (SECOs) are two consolidated research areas in software engineering. OSS influences the way organizations develop, acquire, use and commercialize software. SECOs have emerged as a paradigm to understand dynamics and heterogeneity in collaborative software development. For this reason, SECOs appear as a valid instrument to analyze OSS systems. However, there are few studies that blend both topics together.Objective: The purpose of this study is to evaluate the current state of the art in OSS ecosystems (OSSECOs) research, specifically: (a) what the most relevant definitions related to OSSECOs are; (b) what the particularities of this type of SECO are; and (c) how the knowledge about OSSECO is represented.Method: We conducted a systematic mapping following recommended practices. We applied automatic and manual searches on different sources and used a rigorous method to elicit the keywords from the research questions and selection criteria to retrieve the final papers. As a result, 82 papers were selected and evaluated. Threats to validity were identified and mitigated whenever possible.Results: The analysis allowed us to answer the research questions. Most notably, we did the following: (a) identified 64 terms related to the OSSECO and arranged them into a taxonomy; (b) built a genealogical tree to understand the genesis of the OSSECO term from related definitions; (c) analyzed the available definitions of SECO in the context of OSS; and (d) classified the existing modelling and analysis techniques of OSSECOs.Conclusion: As a summary of the systematic mapping, we conclude that existing research on several topics related to OSSECOs is still scarce (e.g., modelling and analysis techniques, quality models, standard definitions, etc.). This situation calls for further investigation efforts on how organizations and OSS communities actually understand OSSECOs."
Research article - An anomaly detection system based on variable N-gram features and one-class SVM,"AbstractContext: Run-time detection of system anomalies at the host level remains a challenging task. Existing techniques suffer from high rates of false alarms, hindering large-scale deployment of anomaly detection techniques in commercial settings. Objective: To reduce the false alarm rate, we present a new anomaly detection system based on a novel feature extraction technique, which combines the frequency with the temporal information from system call traces, and on one-class support vector machine (OC-SVM) detector.Method: The proposed feature extraction approach starts by segmenting the system call traces into multiple n-grams of variable length and mapping them to fixed-size sparse feature vectors, which are then used to train OC-SVM detectors.Results: The results achieved on a real-world system call dataset show that our feature vectors with up to 6-grams outperform the term vector models (using the most common weighting schemes) proposed in related work. More importantly, our anomaly detection system using OC-SVM with a Gaussian kernel, trained on our feature vectors, achieves a higher-level of detection accuracy (with a lower false alarm rate) than that achieved by Markovian and n-gram based models as well as by the state-of-the-art anomaly detection techniques.Conclusion: The proposed feature extraction approach from traces of events provides new and general data representations that are suitable for training standard one-class machine learning algorithms, while preserving the temporal dependencies among these events."
