title,abstract
Research article - Analyzing and predicting effort associated with finding and fixing software faults,"AbstractContext: Software developers spend a significant amount of time fixing faults. However, not many papers have addressed the actual effort needed to fix software faults.Objective: The objective of this paper is twofold: (1) analysis of the effort needed to fix software faults and how it was affected by several factors and (2) prediction of the level of fix implementation effort based on the information provided in software change requests.Method: The work is based on data related to 1200 failures, extracted from the change tracking system of a large NASA mission. The analysis includes descriptive and inferential statistics. Predictions are made using three supervised machine learning algorithms and three sampling techniques aimed at addressing the imbalanced data problem.Results: Our results show that (1) 83% of the total fix implementation effort was associated with only 20% of failures. (2) Both post-release failures and safety-critical failures required more effort to fix than pre-release and non-critical counterparts, respectively; median values were two or more times higher. (3) Failures with fixes spread across multiple components or across multiple types of software artifacts required more effort. The spread across artifacts was more costly than spread across components. (4) Surprisingly, some types of faults associated with later life-cycle activities did not require significant effort. (5) The level of fix implementation effort was predicted with 73% overall accuracy using the original, imbalanced data. Oversampling techniques improved the overall accuracy up to 77% and, more importantly, significantly improved the prediction of the high level effort, from 31% to 85%.Conclusions: This paper shows the importance of tying software failures to changes made to fix all associated faults, in one or more software components and/or in one or more software artifacts, and the benefit of studying how the spread of faults and other factors affect the fix implementation effort."
Research article - Applying process mining techniques in software process appraisals,"AbstractContextProcess assessments are performed to identify the current maturity of organizations in relation to best practices. Existing process assessment methods, although widely used, have limitations such as: dependence on the competencies of appraisers; high amount of effort and resources required; subjectivity to analyze data and to judge on the implementation of practices; low confidence in sampling and its representativeness. Currently, due to the increasing use of information systems to support process execution, detailed information on the implementation of processes are recorded as event logs, transaction logs, etc. This fact enables the usage of process mining techniques as a powerful tool for process analysis. It allows using a significant amount of data with agility and reliability for process assessments.ObjectiveThe objective of this paper is to present the development and application of a feasible, usable and useful method, which reduces the limitations of current SCAMPI method and defines how to apply process mining techniques in SCAMPI-based process appraisals.MethodResearch method comprises nine steps that were performed in a manner that raised questions in the first four steps were answered by the last four steps of the research design.ResultsThe method “Process Mining Extension to SCAMPI” was designed, developed, applied in two cases and submitted for review by experts who judged it viable, usable, and useful.ConclusionsAs per this research, process mining techniques are suitable to be applied in software process assessments since they are aligned with the purposes of data collection and analysis tasks. In addition to that, the resulting method was judged by experts as something that reduces identified limitations of one of the most used process assessment method."
Research article - A case study of TTCN-3 test scripts clone analysis in an industrial telecommunication setting,"AbstractContext: This paper presents a novel experiment focused on detecting and analyzing clones in test suites written in TTCN-3, a standard telecommunication test script language, for different industrial projects.Objective: This paper investigates frequencies, types, and similarity distributions of TTCN-3 clones in test scripts from three industrial projects in telecommunication. We also compare the distribution of clones in TTCN-3 test scripts with the distribution of clones in C/C++ and Java projects from the telecommunication domain. We then perform a statistical analysis to validate the significance of differences between these distributions.Method: Similarity is computed using CLAN, which compares metrics syntactically derived from script fragments. Metrics are computed from the Abstract Syntax Trees produced by a TTCN-3 parser called Titan developed by Ericsson as an Eclipse plugin. Finally, clone classification of similar script pairs is computed using the Longest Common Subsequence algorithm on token types and token images.Results: This paper presents figures and diagrams reporting TTCN-3 clone frequencies, types, and similarity distributions. We show that the differences between the distribution of clones in test scripts and the distribution of clones in applications are statistically significant. We also present and discuss some lessons that can be learned about the transferability of technology from this study.Conclusion: About 24% of fragments in the test suites are cloned, which is a very high proportion of clones compared to what is generally found in source code. The difference in proportion of Type-1 and Type-2 clones is statistically significant and remarkably higher in TTCN-3 than in source code. Type-1 and Type-2 clones represent 82.9% and 15.3% of clone fragments for a total of 98.2%. Within the projects this study investigated, this represents more and easier potential re-factoring opportunities for test scripts than for code."
Research article - A design theory for software engineering,"AbstractContext: Software Engineering is a discipline that has been shaped by over 50 years of practice. Many have argued that its theoretical basis has been slow to develop and that, in fact, a substantial theory of Software Engineering is still lacking.Objective: We propose a design theory for Software Engineering as a contribution to the debate. Having done this, we extend it to a design theory for socio-technical systems.Method: We elaborate our theory based on Gregor’s influential ‘meta-theoretical’ exploration of the structural nature of a theory in the discipline of Information Systems, with particular attention to ontological and epistemological arguments.Results: We argue how, from an ontological perspective, our theory embodies a view of Software Engineering as the practice of framing, representing and transforming Software Engineering problems. As such, theory statements concern the characterisation of individual problems and how problems relate and transform to other problems as part of an iterative, potentially backtracking, problem solving process, accounting for the way Software Engineering transforms the physical world to meet a recognised need. From an epistemological perspective, we argue how the theory has developed through research cycles including both theory-then-(empirical-)research and (empirical-)research-then-theory strategies spanning over a decade; both theoretical statements and related empirical evidence are included.Conclusion: The resulting theory provides descriptions and explanations for many phenomena observed in Software Engineering and in the combination of software with other technologies, and embodies analytic, explanatory and predictive properties. There are however acknowledged limitations and current research to overcome them is outlined."
"Research article - Using argumentation theory to analyse software practitioners’ defeasible evidence, inference and belief","AbstractContextSoftware practitioners are often the primary source of information for software engineering research. They naturally produce information about their experiences of software practice, and the beliefs they infer from their experiences. Researchers must evaluate the quality and quantity of this information for their research.ObjectiveTo examine how concepts and methods from argumentation research can be used to study practitioners’ evidence, inference and beliefs so as to better understand and improve software practice.MethodWe develop a preliminary framework and preliminary methodology, and use those to identify, extract and structure practitioners’ evidence, inference and beliefs. We illustrate the application of the framework and methodology with examples from a practitioner's blog post.ResultThe practitioner uses (factual) stories, analogies, examples and popular opinion as evidence, and uses that evidence in defeasible reasoning to justify his beliefs and to rebut the beliefs of other practitioners.ConclusionThe framework, methodology and examples could provide a foundation for software engineering researchers to develop a more sophisticated understanding of, and appreciation for, practitioners’ defeasible evidence, inference and belief. Further work needs to automate (parts of) the methodology to support larger-scale application of the methodology."
Research article - Investigating styles in variability modeling: Hierarchical vs. constrained styles,"AbstractContextA common way to represent product lines is with variability modeling. Yet, there are different ways to extract and organize relevant characteristics of variability. Comprehensibility of these models and the ease of creating models are important for the efficiency of any variability management approach.ObjectiveThe goal of this paper is to investigate the comprehensibility of two common styles to organize variability into models – hierarchical and constrained – where the dependencies between choices are specified either through the hierarchy of the model or as cross-cutting constraints, respectively.MethodWe conducted a controlled experiment with a sample of 90 participants who were students with prior training in modeling. Each participant was provided with two variability models specified in Common Variability Language (CVL) and was asked to answer questions requiring interpretation of provided models. The models included 9–20 nodes and 8–19 edges and used the main variability elements. After answering the questions, the participants were asked to create a model based on a textual description.ResultsThe results indicate that the hierarchical modeling style was easier to comprehend from a subjective point of view, but there was also a significant interaction effect with the degree of dependency in the models, that influenced objective comprehension. With respect to model creation, we found that the use of a constrained modeling style resulted in higher correctness of variability models.ConclusionsPrior exposure to modeling style and the degree of dependency among elements in the model determine what modeling style a participant chose when creating the model from natural language descriptions. Participants tended to choose a hierarchical style for modeling situations with high dependency and a constrained style for situations with low dependency. Furthermore, the degree of dependency also influences the comprehension of the variability model."
Research article - Minimizing the stakeholder dissatisfaction risk in requirement selection for next release planning,"AbstractContextThe requirements to be delivered in the next software release are selected according to the stakeholders’ perceived value, expected implementation cost, budget availability, and precedence and technical dependency constraints. Existing approaches to the requirement selection problem do not take into account the risk of stakeholders’ dissatisfaction possibly resulting from divergence in the stakeholders’ estimates of the requirement value.ObjectiveWe present a novel risk-aware, multi-objective approach to the next release problem that aims at reducing the stakeholder dissatisfaction risk in a given cost/value region of interest provided by stakeholders.MethodWe have devised an exact algorithm to address the risk-aware formulation of the next release problem and implemented the algorithm using two well-known SMT solvers, Yices and Z3. To allow the application of the proposed formulation to large size problems, we have also implemented an approximate algorithm based on the NSGA-II metaheuristic.ResultsResults show that (1) the stakeholder dissatisfaction risk can be minimised with minimum impact on cost/value, and (2) our approach is scalable when NSGA-II is used. SMT solvers scale up to problems that are not overly large in terms of the number of requirements and/or are not too sparse in terms of dependencies, but the metaheuristic can quickly find good solutions even for large size problems.ConclusionWe recommend the users of our approach to apply an SMT solver and to resort to a metaheuristic algorithm only if the SMT solver does not terminate within reasonable time, due to the actual combination of number of requirements and dependency density."
"Research article - Investigating the relationship between price, rating, and popularity in the Blackberry World App Store","AbstractContext: App stores provide a software development space and a market place that are both different from those to which we have become accustomed for traditional software development: The granularity is finer and there is a far greater source of information available for research and analysis. Information is available on price, customer rating and, through the data mining approach presented in this paper, the features claimed by app developers. These attributes make app stores ideal for empirical software engineering analysis.Objective: This paper1 exploits App Store Analysis to understand the rich interplay between app customers and their developers.Method: We use data mining to extract app descriptions, price, rating, and popularity information from the Blackberry World App Store, and natural language processing to elicit each apps’ claimed features from its description.Results: The findings reveal that there are strong correlations between customer rating and popularity (rank of app downloads). We found evidence for a mild correlation between app price and the number of features claimed for the app and also found that higher priced features tended to be lower rated by their users. We also found that free apps have significantly (p-value < 0.001) higher ratings than non-free apps, with a moderately high effect size (A^12=0.68). All data from our experiments and analysis are made available on-line to support further investigations."
Research article - Uncertainty-wise evolution of test ready models,"AbstractContextCyber-Physical Systems (CPSs), when deployed for operation, are inherently prone to uncertainty. Considering their applications in critical domains (e.g., healthcare), it is important that such CPSs are tested sufficiently, with the explicit consideration of uncertainty. Model-based testing (MBT) involves creating test ready models capturing the expected behavior of a CPS and its operating environment. These test ready models are then used for generating executable test cases. It is, therefore, necessary to develop methods that can continuously evolve, based on real operational data collected during the operation of CPSs, test ready models and uncertainty captured in them, all together termed as Belief Test Ready Models (BMs)ObjectiveOur objective is to propose a model evolution framework that can interactively improve the quality of BMs, based on operational data. Such BMs are developed by one or more test modelers (belief agents) with their assumptions about the expected behavior of a CPS, its expected physical environment, and potential future deployments. Thus, these models explicitly contain subjective uncertainty of the test modelers.MethodWe propose a framework (named as UncerTolve) for interactively evolving BMs (specified with extended UML notations) of CPSs with subjective uncertainty developed by test modelers. The key inputs of UncerTolve include initial BMs of CPSs with known subjective uncertainty and real data collected from the operation of CPSs. UncerTolve has three key features: 1) Validating the syntactic correctness and conformance of BMs against real operational data via model execution, 2) Evolving objective uncertainty measurements of BMs via model execution, and 3) Evolving state invariants (modeling test oracles) and guards of transitions (modeling constraints for test data generation) of BMs with a machine learning technique.ResultsAs a proof-of-concept, we evaluated UncerTolve with one industrial CPS case study, i.e., GeoSports from the healthcare domain. Using UncerTolve, we managed to evolve 51% of belief elements, 18% of states, and 21% of transitions as compared to the initial BM developed in an industrial setting.ConclusionUncerTolve can successfully evolve model elements of the initial BM, in addition to objective uncertainty measurements using real operational data. The evolved model can be used to generate additional test cases covering evolved model elements and objective uncertainty. These additional test cases can be used to test the current and future deployments of a CPS to ensure that it will handle uncertainty gracefully during its operations."
Research article - REASSURE: Requirements elicitation for adaptive socio-technical systems using repertory grid,"AbstractContextSocio-technical systems are expected to understand the dynamics of the execution environment and behave accordingly. Significant work has been done on formalizing and modeling requirements of such adaptive systems. However, not enough attention is paid on eliciting requirements from users and introducing flexibility in the system behavior at an early phase of requirements engineering. Most of the work is based on an assumption that general users’ cognitive level would be able to support the inherent complexity of variability acquisition.ObjectiveOur main focus is on providing help to the users with ordinary cognitive level to express their expectations from the complex system considering various contexts. This work also helps the designers to explore the design variability based on the general users’ preferences.MethodWe explore the idea of using a cognitive technique Repertory Grid (RG) to acquire knowledge from users and experts along multiple dimensions of problem and design space. We propose REASSURE methodology which guides requirements engineers to explore the intentional and design variability in an organized way. We also provide a tool support to analyze the knowledge captured in multiple repertory grid files and detect potential conflicts in the intentional variability. Finally, we evaluate the proposed idea by performing an empirical study using smart home system domain.ResultsThe result of our study shows that a greater number of requirements can be elicited after applying our approach. With the help of the provided tool support, it is even possible to detect a greater number of conflicts in user’s requirements than the traditional practices.ConclusionWe envision RG as a technique to filter design options based on the intentional variability in various contexts. The promising results of empirical study open up new research questions: “how to elicit requirements from multiple stakeholders and reach consensus for multi-dimensional problem domain”."
Research article - Systematic literature review and empirical investigation of barriers to process improvement in global software development: Client–vendor perspective,"AbstractContextIncreasingly, software development organizations are adopting global software development (GSD) strategies, mainly because of the significant return on investment they produce. However, there are many challenges associated with GSD, particularly with regards to software process improvement (SPI). SPI can play a significant role in the successful execution of GSD projects.ObjectiveThe aim of the present study was to identify barriers that can negatively affect SPI initiatives in GSD organizations from both client and vendor perspectives.MethodA systematic literature review (SLR) and survey questionnaire were used to identify and validate the barriers.ResultsTwenty-two barriers to successful SPI programs were identified. Results illustrate that the barriers identified using SLR and survey approaches have more similarities However, there were significant differences between the ranking of these barriers in the SLR and survey approaches, as indicated by the results of t-tests (for instance, t = 2.28, p = 0.011 < 0.05). Our findings demonstrate that there is a moderate positive correlation between the ranks obtained from the SLR and the empirical study (rs (22) = 0.567, p = 0.006).ConclusionsThe identified barriers can assist both client and vendor GSD organizations during initiation of an SPI program. Client-vendor classification was used to provide a broad picture of SPI programs, and their respective barriers. The top-ranked barriers can be used as a guide for GSD organizations prior to the initiation of an SPI program. We believe that the results of this study can be useful in tackling the problems associated with the implementation of SPI, which is vital to the success and progression of GSD organizations."
Research article - TLEL: A two-layer ensemble learning approach for just-in-time defect prediction,"AbstractContextDefect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time [1].ObjectiveEnsemble learning becomes a hot topic in recent years. There have been several studies about applying ensemble learning to defect prediction [2–5]. Traditional ensemble learning approaches only have one layer, i.e., they use ensemble learning once. There are few studies that leverages ensemble learning twice or more. To bridge this research gap, we try to hybridize various ensemble learning methods to see if it will improve the performance of just-in-time defect prediction. In particular, we focus on one way to do this by hybridizing bagging and stacking together and leave other possibly hybridization strategies for future work.MethodIn this paper, we propose a two-layer ensemble learning approach TLEL which leverages decision tree and ensemble learning to improve the performance of just-in-time defect prediction. In the inner layer, we combine decision tree and bagging to build a Random Forest model. In the outer layer, we use random under-sampling to train many different Random Forest models and use stacking to ensemble them once more.ResultsTo evaluate the performance of TLEL, we use two metrics, i.e., cost effectiveness and F1-score. We perform experiments on the datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. Also, we compare our approach with three baselines, i.e., Deeper, the approach proposed by us [6], DNC, the approach proposed by Wang et al. [2], and MKEL, the approach proposed by Wang et al. [3]. The experimental results show that on average across the six datasets, TLEL could discover over 70% of the bugs by reviewing only 20% of the lines of code, as compared with about 50% for the baselines. In addition, the F1-scores TLEL can achieve are substantially and statistically significantly higher than those of three baselines across the six datasets.ConclusionTLEL can achieve a substantial and statistically significant improvement over the state-of-the-art methods, i.e., Deeper, DNC and MKEL. Moreover, TLEL could discover over 70% of the bugs by reviewing only 20% of the lines of code."
Research article - Visualizing and exploring software version control repositories using interactive tag clouds over formal concept lattices,"AbstractContext: version control repositories contain a wealth of implicit information that can be used to answer many questions about a project’s development process. However, this information is not directly accessible in the repositories and must be extracted and visualized.Objective: the main objective of this work is to develop a flexible and generic interactive visualization engine called ConceptCloud that supports exploratory search in version control repositories.Method: ConceptCloud is a flexible, interactive browser for SVN and Git repositories. Its main novelty is the combination of an intuitive tag cloud visualization with an underlying concept lattice that provides a formal structure for navigation. ConceptCloud supports concurrent navigation in multiple linked but individually customizable tag clouds, which allows for multi-faceted repository browsing, and scriptable construction of unique visualizations.Results: we describe the mathematical foundations and implementation of our approach and use ConceptCloud to quickly gain insight into the team structure and development process of three projects. We perform a user study to determine the usability of ConceptCloud. We show that untrained participants are able to answer historical questions about a software project better using ConceptCloud than using a linear list of commits.Conclusion: ConceptCloud can be used to answer many difficult questions such as “What has happened in this project while I was away?” and “Which developers collaborate?”. Tag clouds generated from our approach provide a visualization in which version control data can be aggregated and explored interactively."
Research article - Stable and predictable Voronoi treemaps for software quality monitoring,"AbstractContext: Voronoi treemaps can be used to effectively visualize software quality attributes of a given software system. Algorithms for computing Voronoi treemaps are non-deterministic making them unsuited for monitoring the development of such attributes over time.Objective: We adapt an existing sweep line algorithm to efficiently compute Voronoi treemaps and we introduce a novel algorithm that adds stability and predictability.Method: We introduce stable and predictable Voronoi treemaps based on additively weighted power Voronoi diagrams. We employ scaled Hilbert curves to place Voronoi sites in the plane, retaining the order in which sites are placed along the curve for easy comparison with revisions of the same software system.Results: Our algorithm achieves a predictable first good approximation of the final location of the sites. We show that our algorithm not only provides more stability, but also that because of better placement it needs fewer iterations to compute its result. As part of our implementation we introduce a visualization to show the difference between two versions of a software system. We also present a small case study in which we use a web based application that implements our work to investigate the usefulness of stability and predictability of visualizations.Conclusion: It is possible to achieve stable and predictable visualizations of software system attributes, while, as a pleasant side effect, decreasing the number of iterations necessary to arrive at the visualization."
Research article - Software landscape and application visualization for system comprehension with ExplorViz,"AbstractContext: The number of software applications deployed in organizations is constantly increasing. Those applications – often several hundreds – form large software landscapes.Objective: The comprehension of such landscapes and their applications is often impeded by, for instance, architectural erosion, personnel turnover, or changing requirements. Therefore, an efficient and effective way to comprehend such software landscapes is required.Method: In our ExplorViz visualization, we introduce hierarchical abstractions aiming at solving system comprehension tasks fast and accurately for large software landscapes. Besides hierarchical visualization on the landscape level, ExplorViz provides multi-level visualization from the landscape to the level of individual applications. The 3D application-level visualization is empirically evaluated with a comparison to the Extravis approach, with physical models and in virtual reality. To evaluate ExplorViz, we conducted four controlled experiments. We provide packages containing all our experimental data to facilitate the verifiability, reproducibility, and further extensibility of our results.Results: We observed a statistical significant increase in task correctness of the hierarchical visualization compared to the flat visualization. The time spent did not show any significant differences. For the comparison with Extravis, we observed that solving program comprehension tasks using ExplorViz leads to a significant increase in correctness and in less or similar time spent. The physical models improved the team-based program comprehension process for specific tasks by initiating gesture-based interaction, but not for all tasks. The participants of our virtual reality experiment with ExplorViz rated the realized gestures for translation, rotation, and selection as highly usable. However, our zooming gesture was less favored.Conclusion: The results backup our claim that our hierarchical and multi-level approach enhances the current state of the art in landscape and application visualization for better software system comprehension, including new forms of interaction with physical models and virtual reality."
