title,abstract
Review article - Comparison of development methodologies in web applications,"AbstractContextWeb applications development is at its peak due to the advance of technological trends and the constant dependence of the Internet. As a result of the needs of developers, new development methodologies have emerged. However, that does not mean that companies always implement an optimal development process; instead, there are several disadvantages presented by an inadequate and not versatile methodologies.ObjectiveThe aim is to compare web development methodologies based on dynamic features presented during the life cycle to identify their use, relevance, and characteristics. The process employing is an SLR and field research to Ecuadorian development companies.MethodThe method used is a systematic literature review (SLR) for the identification of characteristics and processes of development methodologies. Additionally, a survey of Ecuadorian web application developers was implemented to assess the importance of using a method during the project.ResultsThe literature review exhibited as a result that UWE and OOHDM have greater flexibility than other methodologies before dynamic environments during the web development process. On the other hand, within field research was obtained that companies use different software development methods than those assessed in the study (hybrid methodologies). However, within the range of companies using the compared methodologies, UWE is the most selected.ConclusionsEach methodology holds particular features and employment environment, which makes them useful in specific conditions. Through the field research, it is possible to conclude that most of the companies use different methodologies than the evaluated ones; thus, the process is guided by hybrids methods or models based on experience. On the other hand, through the SLR, we identified UWE as the most suitable methodology for web development under dynamic environments, such as the size of the company, the need to modify the requirements, or the knowledge that the development team has about the process."
Research article - Software trustworthiness evaluation model based on a behaviour trajectory matrix,"AbstractContextSoftware trustworthiness is a highly important research topic. Trustworthiness evaluation based on factors that affect software behaviour is conducted mainly according to the influence degrees of these factors on the software behaviour to evaluate trustworthiness. As a result, minimization of the interference of human factors is considered.ObjectiveIn this study, to ensure the objectivity of evaluating the trustworthiness of software behaviour, a software trustworthiness evaluation model based on a behaviour trajectory matrix, namely, BTBM-TM was proposed.MethodCheckpoints were set up in the trajectory of the software behaviour, and binary code was introduced to express the software behaviour trajectory tree. The scenario information of the checkpoints was acquired, and used to construct behaviour trajectory matrices, which were used to represent the behaviour trajectory. The behaviour trajectory matrices were transformed into grayscale images, which were used to train the deep residual network (ResNet) to classify the software behaviour. The trained deep residual network was used to categorize the current software behaviour, and the cosine similarity algorithm was used to calculate the deviation degree of the software behaviour trajectory; to perform a dual evaluation of the trustworthiness of software behaviour.ResultsThe behaviour trajectory information of the Model class of 300 cycles was used to evaluate the trustworthiness of the mine-sweeping game. The trustworthiness evaluation results of the software behaviour of the scheme proposed in this paper (BTBM-TM) were compared with those of the schemes from references [6] and [10]. The accuracies of the schemes from [6] and [10] are lower than that of the BTBM-TM scheme.ConclusionsThe trajectory of software behaviour is represented by a matrix and converted into a grayscale image, whose processing method is used to evaluate the trustworthiness of software behaviour more objectively and accurately."
Research article - Better together: Comparing vulnerability prediction models,"AbstractContextVulnerability Prediction Models (VPMs) are an approach for prioritizing security inspection and testing to find and fix vulnerabilities. VPMs have been created based on a variety of metrics and approaches, yet widespread adoption of VPM usage in practice has not occurred. Knowing which VPMs have strong prediction and which VPMs have low data requirements and resources usage would be useful for practitioners to match VPMs to their project’s needs. The low density of vulnerabilities compared to defects is also an obstacle for practical VPMs.ObjectiveThe goal of the paper is to help security practitioners and researchers choose appropriate features for vulnerability prediction through a comparison of Vulnerability Prediction Models.MethodWe performed replications of VPMs on Mozilla Firefox with 28,750 source code files featuring 271 vulnerabilities using software metrics, text mining, and crash data. We then combined features from each VPM and reran our classifiers.ResultsWe improved the F-score of the best VPM (.20 to 0.28) by combining features from three types of VPMs and using Naive Bayes as the classifier. The strongest features in the combined model were the number of times a file was involved in a crash, the number of outgoing calls from a file, and the string “nullptr”.ConclusionOur results indicate that further work is needed to develop new features for input into classifiers. In addition, new analytic approaches for VPMs are needed for VPMs to be useful in practical situations, due to the low density of vulnerabilities in software (less than 1% for our dataset)."
Research article - How are distributed bugs diagnosed and fixed through system logs?,"AbstractContextDistributed systems are the backbone of today’s computing ecosystems. Debugging distributed bugs is crucial and challenging. There are still many unknowns about debugging real-world distributed bugs, especially through system logs.ObjectiveThis paper aims to provide a comprehensive study of how system logs can help diagnose and fix distributed bugs in practice.MethodThe study was carried out with three core research questions (RQs): How to identify failures in distributed bugs through logs? How to find and utilize bug-related log entries to figure out the root causes? How are distributed bugs fixed and how are logs and patches related? To answer these questions, we studied 106 real-world distributed bugs randomly sampled from five widely used distributed systems, and manually checked the bug report, the log, the patch, the source code and other related information for each of these bugs.ResultsSeven findings are observed and the main findings include: (1) For only about half of the distributed bugs, the failures are indicated by FATAL or ERROR log entries. FATAL are not always fatal, and INFO could be fatal. (2) For more than half of the studied bugs, root-cause diagnosis relies on log entries that are not part of the failure symptoms. (3) One third of the studied bugs are fixed by eliminating end symptoms instead of root causes. Finally, a distributed bug dataset with the in-depth analysis has been released to the research community.ConclusionThe findings in our study reveal the characteristics of distributed bugs, the differences from debugging single-machine system bugs, and the usages and limitations of existing logs. Our study also provides guidance and opportunities for future research on distributed bug diagnosis, fixing, and log analysis and enhancement."
"Research article - A fine-grained requirement traceability evolutionary algorithm: Kromaia, a commercial video game case study","AbstractContext:Commercial video games usually feature an extensive source code and requirements that are related to code lines from multiple methods. Traceability is vital in terms of maintenance and content update, so it is necessary to explore such search spaces properly.Objective:This work presents and evaluates CODFREL (Code Fragment-based Requirement Location), our approach to fine-grained requirement traceability, which lies in an evolutionary algorithm and includes encoding and genetic operators to manipulate code fragments that are built from source code lines. We compare it with a baseline approach (Regular-LSI) by configuring both approaches with different granularities (code lines / complete methods).Method:We evaluated our approach and Regular-LSI in the Kromaia video game case study, which is a commercial video game released on PC and PlayStation 4. The approaches are configured with method and code line granularity and work on 20 requirements that are provided by the development company. Our approach and Regular-LSI calculate similarities between requirements and code fragments or methods to propose possible solutions and, in the case of CODFREL, to guide the evolutionary algorithm.Results:The results, which compare code line and method granularity configurations of CODFREL with different granularity configurations of Regular-LSI, show that our approach outperforms Regular-LSI in precision and recall, with values that are 26 and 8 times better, respectively, even though it does not achieve the optimal solutions. We make an open-source implementation of CODFREL available.Conclusions:Since our approach takes into consideration key issues like the source code size in commercial video games and the requirement dispersion, it provides better starting points than Regular-LSI in the search for solution candidates for the requirements. However, the results and the influence of domain-specific language on them show that more explicit knowledge is required to improve such results."
Research article - Adequate vs. inadequate test suite reduction approaches,"AbstractContext: Regression testing is an important activity that allows ensuring the correct behavior of a system after changes. As the system grows, the time and resources to perform regression testing increase. Test Suite Reduction (TSR) approaches aim to speed up regression testing by removing obsolete or redundant test cases. These approaches can be classified as adequate or inadequate. Adequate TSR approaches reduce test suites and completely preserve test requirements (e.g., covered statements) of the original test suites. Inadequate TSR approaches do not preserve test requirements. The percentage of satisfied test requirements indicates the inadequacy level.Objective: We compare some state-of-the-art adequate and inadequate TSR approaches with respect to the size of reduced test suites and their fault-detection capability. We aim to increase our body of knowledge on TSR approaches by comparing: (i) well-known traditional adequate TSR approaches; (ii) their inadequate variants; and (iii) several variants of a novel Clustering-Based (CB) approach for (adequate and inadequate) TSR.Method: We conducted an experiment to compare adequate and inadequate TSR approaches. This comparison is founded on a public dataset containing information on real faults.Results: The most important findings from our experiment can be summarized as follows: (i) there is not an inadequate TSR approach that outperforms the others;(ii) some inadequate variants of the CB approach, and few traditional inadequate approaches, outperform the adequate ones in terms of reduction in test suite size with a negligible effect on fault-detection capability; and (iii) the CB approach is less sensitive than the other inadequate approaches, that is, variations in the inadequacy level have small effect on reduction in test suite size and on loss in fault-detection capability.Conclusions: These findings imply that inadequate TSR approaches and especially the CB approach might be appealing because they lead to a greater reduction in test suite size (with respect to the adequate ones) at the expense of a small loss in fault-detection capability."
Research article - A systematic literature review of machine learning techniques for software maintainability prediction,"AbstractContextSoftware maintainability is one of the fundamental quality attributes of software engineering. The accurate prediction of software maintainability is a significant challenge for the effective management of the software maintenance process.ObjectiveThe major aim of this paper is to present a systematic review of studies related to the prediction of maintainability of object-oriented software systems using machine learning techniques. This review identifies and investigates a number of research questions to comprehensively summarize, analyse and discuss various viewpoints concerning software maintainability measurements, metrics, datasets, evaluation measures, individual models and ensemble models.MethodThe review uses the standard systematic literature review method applied to the most common computer science digital database libraries from January 1991 to July 2018.ResultsWe survey 56 relevant studies in 35 journals and 21 conference proceedings. The results indicate that there is relatively little activity in the area of software maintainability prediction compared with other software quality attributes. CHANGE maintenance effort and the maintainability index were the most commonly used software measurements (dependent variables) employed in the selected primary studies, and most made use of class-level product metrics as the independent variables. Several private datasets were used in the selected studies, and there is a growing demand to publish datasets publicly. Most studies focused on regression problems and performed k-fold cross-validation. Individual prediction models were employed in the majority of studies, while ensemble models relatively rarely.ConclusionBased on the findings obtained in this systematic literature review, ensemble models demonstrated increased accuracy prediction over individual models, and have been shown to be useful models in predicting software maintainability. However, their application is relatively rare and there is a need to apply these, and other models to an extensive variety of datasets with the aim of improving the accuracy and consistency of results."
Research article - Intelligent software engineering in the context of agile software development: A systematic literature review,"AbstractCONTEXT: Intelligent Software Engineering (ISE) refers to the application of intelligent techniques to software engineering. We define an “intelligent technique” as a technique that explores data (from digital artifacts or domain experts) for knowledge discovery, reasoning, learning, planning, natural language processing, perception or supporting decision-making.OBJECTIVE: The purpose of this study is to synthesize and analyze the state of the art of the field of applying intelligent techniques to Agile Software Development (ASD). Furthermore, we assess its maturity and identify adoption risks.METHOD: Using a systematic literature review, we identified 104 primary studies, resulting in 93 unique studies. RESULTS: We identified that there is a positive trend in the number of studies applying intelligent techniques to ASD. Also, we determined that reasoning under uncertainty (mainly, Bayesian network), search-based solutions, and machine learning are the most popular intelligent techniques in the context of ASD. In terms of purposes, the most popular ones are effort estimation, requirements prioritization, resource allocation, requirements selection, and requirements management. Furthermore, we discovered that the primary goal of applying intelligent techniques is to support decision making. As a consequence, the adoption risks in terms of the safety of the current solutions are low. Finally, we highlight the trend of using explainable intelligent techniques.CONCLUSION: Overall, although the topic area is up-and-coming, for many areas of application, it is still in its infancy. So, this means that there is a need for more empirical studies, and there are a plethora of new opportunities for researchers."
Research article - An empirically evaluated checklist for surveys in software engineering,"AbstractContext: Over the past decade Software Engineering research has seen a steady increase in survey-based studies, and there are several guidelines providing support for those willing to carry out surveys. The need for auditing survey research has been raised in the literature. Checklists have been used both to conduct and to assess different types of empirical studies, such as experiments and case studies.Objective: To operationalize the assessment of survey studies by means of a checklist. To fulfill such goal, we aim to derive a checklist from standards for survey research and further evaluate the appropriateness of the checklist in the context of software engineering research.Method: We systematically aggregated knowledge from 12 methodological studies supporting survey-based research in software engineering. We identified the key stages of the survey process and its recommended practices through thematic analysis and vote counting. We evaluated the checklist by applying it to existing surveys and analyzed the results. Thereafter, we gathered the feedback of experts (the surveys’ authors) on our analysis and used the feedback to improve the survey checklist.Results: The evaluation provided insights regarding limitations of the checklist in relation to its understanding and objectivity. In particular, 19 of the 38 checklist items were improved according to the feedback received from experts.Conclusion: The proposed checklist is appropriate for auditing survey reports as well as a support tool to guide ongoing research with regard to the survey design process. A discussion on how to use the checklist and what its implications are for research practice is also provided."
Research article - Towards assisting developers in API usage by automated recovery of complex temporal patterns,"AbstractContextDespite the many advantages, the use of external libraries through their APIs remains difficult because of the usage patterns and constraints that are hidden or not properly documented. Existing work provides different techniques to recover API usage patterns from client programs in order to help developers use those libraries. However, most of these techniques produce patterns that generally do not involve temporal properties.ObjectiveIn this paper, we discuss the problem of temporal usage patterns recovery and propose an algorithm to solve it. We also discuss how the obtained patterns can be used at different stages of client development.MethodWe address the recovery of temporal API usage patterns as an optimization problem and solve it using a genetic-programming algorithm.ResultsOur evaluation on different APIs shows that the proposed algorithm allows to derive non-trivial temporal usage that are useful and generalizable to new API clients.ConclusionRecovering API usage temporal patterns helps client developers to use APIs in an appropriate way. In addition to potentially improve productivity, such patterns also helps preventing errors that result from an incorrect use of the APIs."
