title,abstract
Research article - Recommending tags for pull requests in GitHub,"AbstractContextIn GitHub, contributors make code changes, then create and submit pull requests to projects. Tags are a simple and effective way to attach additional information to pull requests and facilitate their organization. However, little effort has been devoted to study pull requests’ tags in GitHub.ObjectiveOur objective in this paper is to propose an approach which automatically recommends tags for pull requests in GitHub.MethodWe make a survey on the usage of tags in pull requests. Survey results show that tags are useful for developers to track, search or classify pull requests. But some respondents think that it is difficult to choose right tags and keep consistency of tags. 60.61% of respondents think that a tag recommendation tool is useful. In order to help developers choose tags, we propose a method FNNRec which uses feed-forward neural network to analyze titles, description, file paths and contributors.ResultsWe evaluate the effectiveness of FNNRec on 10 projects containing 68,497 tagged pull requests. The experimental results show that on average, FNNRec outperforms approach TagDeepRec and TagMulRec by 62.985% and 24.953% in terms of F1−score@3, respectively.ConclusionFNNRec is useful to find appropriate tags and improve tag setting process in GitHub."
Research article - Integrated framework for incorporating sustainability design in software engineering life-cycle: An empirical study,"AbstractContext:Owing to the critical role of software-intensive systems in society, software engineers have the accountability to consider sustainability as a goal while structuring a software system. However, there are no practical guidelines providing a tangible decomposition of the sustainability aspect. Moreover, there are limited quantifiable methods to support sustainable design and analysis.Objectives:The purpose of this study is to help software practitioners to take sustainability into account by providing systematic guidelines for the software engineering process. We propose a framework that presents a meta model to decompose sustainability requirements and an assessment approach to evaluate sustainability achievements.Method:This work presents an integrated framework that combines a goal-based approach, scenario-based approach, and feature modeling to gather sustainability related requirements and corresponding features. For sustainability assessment, software analysis and machine learning techniques are utilized to analyze software products based on sustainability metrics and criteria.Results and Conclusions:The empirical study conducted with participants from academia and industry revealed that the proposed framework improves participant’s ability to consider sustainability aspect in their software engineering tasks through focusing on requirements, design, and evaluation. With the provided sustainability meta-model, the participants could extract more stakeholders, requirements, and features in shorter time. Moreover, the empirical study result also demonstrated that this study is capable to indicate specific scenarios that should be redesigned to improve the sustainability achievements level."
Research article - Architectural decision-making as a financial investment: An industrial case study,"AbstractContextMaking architectural decisions is a crucial task but also very difficult, considering the scope of the decisions and their impact on quality attributes. To make matters worse, architectural decisions need to combine both technical and business factors, which are very dissimilar by nature.ObjectivesWe provide a cost-benefit approach and supporting tooling that treats architectural decisions as financial investments by: (a) combining both technical and business factors; and (b) transforming the involved factors into currency, allowing their uniform aggregation. Apart from illustrating the method, we validate both the proposed approach and the tool, in terms of fitness for purpose, usability, and potential limitations.MethodTo validate the approach, we have performed a case study in a software development company, in the domain of low-energy embedded systems. We employed triangulation in the data collection phase of the case study, by performing interviews, focus groups, an observational session, and questionnaires.ResultsThe results of the study suggested that the proposed approach: (a) provides a structured process for systematizing decision-making; (b) enables the involvement of multiple stakeholders, distributing the decision-making responsibility to more knowledgeable people; (c) uses monetized representations that are important for assessing decisions in a unified manner; and (d) enables decision reuse and documentation.ConclusionsThe results of the study suggest that architectural decision-making can benefit from treating this activity as a financial investment. The various benefits that have been identified from mixing financial and technological aspects are well-accepted from industrial stakeholders."
Research article - Governance and Management of Green IT: A Multi-Case Study,"AbstractContextThe changes that are taking place with respect to environmental sensitivity are forcing organizations to adopt a new approach to this problem. Implementing sustainability initiatives has become a priority for the social and environmental awareness of organizations that want to stay ahead of the curve. One of the business areas that has, more than others, proven to be a vital asset and a potential ally of the environment, is the area of Information Technology (IT). Through this area, Green IT practices advocate sustainability in and by IT. However, organizations have a significant handicap in this regard, due to the lack of specific Green IT standards and frameworks that help them carry out this type of sustainability practices.ObjectiveWe have developed the “Governance and Management Framework for Green IT” (GMGIT), which establishes the necessary characteristics to implement Green IT in organizations, from the point of view of the governance and management of this area. After developing and validating a first version of this framework, we have performed a set of improvements, obtaining the GMGIT 2.0, which we want to validate.MethodWe have conducted a series of empirical validations at international level based on case studies, whose characteristics and results are presented in this study.ResultsThe results of this multi-case study show an example of the current situation of organizations in Green IT, as well as the resolution of problems encountered during the validations conducted with the GMGIT 1.0.ConclusionThe findings obtained demonstrate the usefulness, applicability, and validity of the framework when implementing, auditing, and improving Green IT in organizations in a systematic and progressive manner."
Research article - Statement frequency coverage: A code coverage criterion for assessing test suite effectiveness,"AbstractContext:Software testing is a pivotal activity in the development of high-quality software. As software is evolving through its life cycle, the need for a fault-revealing criterion assessing the effectiveness of the test suite grows. Over the years, researchers have proposed coverage-based criteria, including statement and branch coverage, to solve this issue. In literature, the effectiveness of such criteria is attested in terms of their correlations with the mutation score.Objective:In this paper, we aim at proposing a coverage-based criterion named statement frequency coverage, which outperforms statement and branch coverage in terms of correlation with mutation score.Method:To this end, we incorporated the frequency of executed statements into statement coverage and created a coverage-based criterion for assessing test suite effectiveness. Statement frequency coverage assigns a continuous value to a statement whose value is proportional to the number of times executed during test execution. We evaluated our approach on 22 real-world Python projects with more than 118 000 source lines of code (without blank lines, comments, and test cases) and 21 000 test cases through measuring the correlation between statement frequency coverage and corresponding mutation score.Results:The results show that statement frequency coverage outperforms statement and branch coverage criteria. The correlation between it and the corresponding mutation score is higher than the correlation of statement and branch coverage with their mutation score. The results also show that unlike statement and branch coverage, there is no statistical difference between statement frequency coverage and mutation score.Conclusion:Statement frequency coverage is a better choice compared to statement and branch coverage in assessing test suite effectiveness in the real-world setting. Furthermore, we demonstrate that although statement frequency coverage subsumes statement coverage, it is incomparable to branch coverage under the adequate test suite condition."
Research article - A unified framework for declarative debugging and testing,"AbstractContext:Debugging is the most challenging and time consuming task in software development. However, it is not properly integrated in the software development cycle, because the result of so much effort is not available in further iterations of the cycle, and the debugging process itself does not benefit from the outcome of other phases such as testing.Objective:We propose to integrate debugging and testing within a single unified framework where each phase generates useful information for the other and the outcomes of each phase are reused.Method:We consider a declarative debugging setting that employs tests to automatically entail the validity of some subcomputations, thus decreasing the time and effort needed to find a bug. Additionally, the debugger stores as new tests the information collected from the user during the debugging phase. This information becomes part of the program test suite, and can be used in future debugging sessions, and also as regression tests.Results:We define a general framework where declarative debugging establishes a bidirectional collaboration with testing. The new setting preserves the properties of the underlying declarative debugging framework (weak completeness and soundness) while generating test cases that can be used later in other debugging sessions or even in other cycles of the software development. The proposed framework is general enough to be instantiated to very different programming languages: Erlang (functional), Java (imperative, object-oriented), and SQL (data query); and the experimental results obtained for Erlang programs validate the effectiveness of the framework.Conclusion:We propose a general unified framework for debugging and testing that simplifies each phase and maximizes the reusability of the outcomes in the different phases of the software development cycle, therefore reducing the overall effort."
Short communication - The effectiveness of data augmentation in code readability classification,"AbstractContext: Training deep learning models for code readability classification requires large datasets of quality pre-labeled data. However, it is almost always time-consuming and expensive to acquire readability data with manual labels.Objective: We thus propose to introduce data augmentation approaches to artificially increase the size of training set, this is to reduce the risk of overfitting caused by the lack of readability data and further improve the classification accuracy as the ultimate goal.Method: We create transformed versions of code snippets by manipulating original data from aspects such as comments, indentations, and names of classes/methods/variables based on domain-specific knowledge. In addition to basic transformations, we also explore the use of Auxiliary Classifier GANs to produce synthetic data.Results: To evaluate the proposed approach, we conduct a set of experiments. The results show that the classification performance of deep neural networks can be significantly improved when they are trained on the augmented corpus, achieving a state-of-the-art accuracy of 87.38%.Conclusion:We consider the findings of this study as primary evidence of the effectiveness of data augmentation in the field of code readability classification."
Research article - What skills do IT companies look for in new developers? A study with Stack Overflow jobs,"AbstractContext: There is a growing demand for information on how IT companies look for candidates to their open positions. Objective: This paper investigates which hard and soft skills are more required in IT companies by analyzing the description of 20,000 job opportunities. Method: We applied open card sorting to perform a high-level analysis on which types of hard skills are more requested. Further, we manually analyzed the most mentioned soft skills. Results: Programming languages are the most demanded hard skills. Communication, collaboration, and problem-solving are the most demanded soft skills. Conclusion: We recommend developers to organize their resumé according to the positions they are applying. We also highlight the importance of soft skills, as they appear in many job opportunities."
Research article - On the influence of model fragment properties on a machine learning-based approach for feature location,"AbstractContext:Leveraging machine learning techniques to address feature location on models has been gaining attention. Machine learning techniques empower software product companies to take advantage of the knowledge and the experience to improve the performance of the feature location process. Most of the machine learning-based works for feature location on models report the machine learning techniques and the tuning parameters in detail. However, these works focus on the size and the distribution of the data sets, neglecting the properties of their contents.Objective:In this paper, we analyze the influence of three model fragment properties (density, multiplicity, and dispersion) on a machine learning-based approach for feature location.Method:The analysis of these properties is based on an industrial case provided by CAF, a worldwide provider of railway solutions. The test cases were evaluated through a machine learning technique that uses different subsets of a knowledge base to learn how to locate unknown features.Results:Results show that the density and dispersion properties have a direct impact on the results. In our case study, the model fragments with extra-small density values achieve results with up to 43% more precision, 41% more recall, 42% more F-measure, and 0.53 more Matthews Correlation Coefficient (MCC) than the model fragments with other density values. On the other hand, the model fragments with extra-small and small dispersion values achieve results with up to 53% more precision, 52% more recall, 52% more F-measure, and 0.57 more MCC than the model fragments with other dispersion values.Conclusions:The analysis of the results shows that both density and dispersion properties significantly influence the results. These results can serve not only to improve the reports by means of the model fragment properties, but also to be able to compare machine learning-based feature location approaches fairly improving the feature location results."
Research article - COSTE: Complexity-based OverSampling TEchnique to alleviate the class imbalance problem in software defect prediction,"AbstractContext:Generally, there are more non-defective instances than defective instances in the datasets used for software defect prediction (SDP), which is referred to as the class imbalance problem. Oversampling techniques are frequently adopted to alleviate the problem by generating new synthetic defective instances. Existing techniques generate either near-duplicated instances which result in overgeneralization (high probability of false alarm, pf) or overly diverse instances which hurt the prediction model’s ability to find defects (resulting in low probability of detection, pd). Furthermore, when existing oversampling techniques are applied in SDP, the effort needed to inspect the instances with different complexity is not taken into consideration.Objective:In this study, we introduce Complexity-based OverSampling TEchnique (COSTE), a novel oversampling technique that can achieve low pf and high pd simultaneously. Meanwhile, COSTE also performs better in terms of Norm(popt) and ACC, two effort-aware measures that consider the testing effort.Method:COSTE combines pairs of defective instances with similar complexity to generate synthetic instances, which improves the diversity within the data, maintains the ability of prediction models to find defects, and takes the different testing effort needed for different instances into consideration. We conduct experiments to compare COSTE with Synthetic Minority Oversampling TEchnique, Borderline-SMOTE, Majority Weighted Minority Oversampling TEchnique and MAHAKIL.Results:The experimental results on 23 releases of 10 projects show that COSTE greatly improves the diversity of the synthetic instances without compromising the ability of prediction models to find defects. In addition, COSTE outperforms the other oversampling techniques under the same testing effort. The statistical analysis indicates that COSTE’s ability to outperform the other oversampling techniques is significant under the statistical Wilcoxon rank sum test and Cliff’s effect size.Conclusion:COSTE is recommended as an efficient alternative to address the class imbalance problem in SDP."
Research article - Evaluating the effects of similar-class combination on class integration test order generation,"AbstractContext:In integration testing, the order in which classes are integrated and tested is significant for the construction of test stubs. With the existing approaches, it is usually difficult to generate the sub-optimal test orders for real applications, which have large numbers of classes.Objective:There exist moderately large numbers of classes in software systems, which is one of the main factors that complicate the generation of class integration test order (CITO). The main objectives of this study are reducing the problem space for CITO generation, and minimizing the stubbing cost of the generated test orders.Method:The approach proposed in this study is based on the hypothesis that similar-class combination can remove class dependencies and yield a smaller problem space. Identical class dependence and symmetric classes are the two main properties that are used to identify similar classes. In addition, a new cycle-breaking algorithm is introduced to minimize the stubbing cost of the generated test orders, which fully considers the two factors (number of test stubs and the corresponding stubbing complexity) that affect the overall stubbing cost. Empirical experiments are conducted on nine open-source Java programs to evaluate the performance of the proposed approach.Results:With similar-class combination, the proposed approach reduced the numbers of classes and class dependencies by over 10% and 6%, respectively, for six programs. Moreover, for four programs, the proposed approach reduced the number of cycles among class dependencies by more than 20%. The cycle-breaking algorithm achieved reduction of more than 13% in the stubbing cost, thus outperforming other competing techniques.Conclusions:The proposed method relies on the two aforementioned important properties to identify similar classes, and these properties are known to significantly improve the performance of CITO generation. The results obtained in this study confirmed the capability of the proposed approach in terms of minimizing the number of classes and class dependencies in programs. It outperformed other competing methods in minimizing the stubbing costs of the generated test orders."
Research article - Archetypes of delay: An analysis of online developer conversations on delayed work items in IBM Jazz,"AbstractContext.A widely adopted methodology, agile software development provides enhanced flexibility to actively adjust a project scope. In agile teams, particularly in distributed environment, developers interact, manage requirements knowledge, and coordinate primarily in online collaboration tools. Developer conversations become invaluable sources to track and understand developers’ interactions around implementation of requirements, as well as the progress of implementation relative to the project scope and the planned iterations in agile projects. Although extensive research around iteration planning exists, there is a lack of research that leverages developer conversation data to understand delays in implementing planned requirements in agile projects.Objective.By using developer conversations in a large agile project at IBM, this work aims to analyze conversation in work items (WIs) that are delayed and derive patterns that suggest reasons for delay in the project.Method.We conducted a case study of the IBM Jazz project, and used thematic analysis to code the developer conversations as time-series, and cluster analysis to identify patterns that differentiated the evolution of discussions in WIs that were late vs. not late in the project.Results.We identified six main patterns of WI delay. Through semantic analysis of developer conversations within particular clusters we were able to explain the reasons for delays in each pattern. In comparison to non-late WIs, we find that the major reason for delay is a lack of frequent communication associated with a poor project management of WIs. Similarly, non-late tasks more often delegate to children tasks to accelerate the implementation of requirements, in addition to processing requests quickly to resolve bottlenecks in implementation.Conclusion.Our study complements existing research in bringing evidence that developer conversations are a useful resource that can highlight delays in requirement implementation, as well as recommend patterns in the dynamics of developers interactions relevant to such delays."
Research article - A Method to Estimate Software Strategic Indicators in Software Development: An Industrial Application,"AbstractContextExploiting software development related data from software-development intensive organizations to support tactical and strategic decision making is a challenge. Combining data-driven approaches with expert knowledge has been highlighted as a sensible approach for leading software-development intensive organizations to rightful decision-making improvements. However, most of the existing proposals lack of important aspects that hinders their industrial uptake such as: customization guidelines to fit the proposals to other contexts and/or automatic or semi-automatic data collection support for putting them forward in a real organization. As a result, existing proposals are rarely used in the industrial context.ObjectiveSupport software-development intensive organizations with guidance and tools for exploiting software development related data and expert knowledge to improve their decision making.MethodWe have developed a novel method called SESSI (Specification and Estimation of Software Strategic Indicators) that was articulated from industrial experiences with Nokia, Bittium, Softeam and iTTi in the context of Q-Rapids European project following a design science approach. As part of the industrial summative evaluation, we performed the first case study focused on the application of the method.ResultsWe detail the phases and steps of the SESSI method and illustrate its application in the development of ModelioNG, a software product of Modeliosoft development firm.ConclusionThe application of the SESSI method in the context of ModelioNG case study has provided us with useful feedback to improve the method and has evidenced that applying the method was feasible in this context."
