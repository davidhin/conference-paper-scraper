title,abstract
Research article - Is deep learning better than traditional approaches in tag recommendation for software information sites?,"AbstractContextInspired by the success of deep learning in other domains, this new technique been gaining widespread recent interest in being applied to diverse data analysis problems in software engineering. Many deep learning models, such as CNN, DBN, RNN, LSTM and GAN, have been proposed and recently applied to software engineering tasks including effort estimation, vulnerability analysis, code clone detection, test case selection, requirements analysis and many others. However, there is a perception that applying deep learning is a ”silver bullet” if it can be applied to a software engineering data analysis problem.ObjectThis motivated us to ask the question as to whether deep learning is better than traditional approaches in tag recommendation task for software information sites.MethodIn this paper we test this question by applying both the latest deep learning approaches and some traditional approaches on tag recommendation task for software information sites. This is a typical Software Engineering automation problem where intensive data processing is required to link disparate information to assist developers. Four different deep learning approaches – TagCNN, TagRNN, TagHAN and TagRCNN – are implemented and compared with three advanced traditional approaches – EnTagRec, TagMulRec, and FastTagRec.ResultsOur comprehensive experimental results show that the performance of these different deep learning approaches varies significantly. The performance of TagRNN and TagHAN approaches are worse than traditional approaches in tag recommendation tasks. The performance of TagCNN and TagRCNN approaches are better than traditional approaches in tag recommendation tasks.ConclusionTherefore, using appropriate deep learning approaches can indeed achieve better performance than traditional approaches in tag recommendation tasks for software information sites."
"Research article - Formal Quality of Service assurances, ranking and verification of cloud deployment options with a probabilistic model checking method","AbstractContext: Existing software workbenches allow for the deployment of cloud applications across a variety of Infrastructure-as-a-Service (IaaS) providers. The expected workload, Quality of Service (QoS) and Non-Functional Requirements (NFRs) must be considered before an appropriate infrastructure is selected. However, this decision-making process is complex and time-consuming. Moreover, the software engineer needs assurances that the selected infrastructure will lead to an adequate QoS of the application.Objective: The goal is to develop a new method for selection of an optimal cloud deployment option, that is, an infrastructure and configuration for deployment and to verify that all hard and as many soft QoS requirements as possible will be met at runtime.Method: A new Formal QoS Assurances Method (FoQoSAM), which relies on stochastic Markov models is introduced to facilitate an automated decision-making process. For a given workload, it uses QoS monitoring data and a user-related metric in order to automatically generate a probabilistic model. The probabilistic model takes the form of a finite automaton. It is further used to produce a rank list of cloud deployment options. As a result, any of the cloud deployment options can be verified by applying a probabilistic model checking approach.Results: Testing was performed by ranking deployment options for two cloud applications, File Upload and Video-conferencing. The FoQoSAM method was compared to a baseline Analytic Hierarchy Process (AHP). The results show that the first ranked cloud deployment options satisfy all hard and at least one of the soft requirements for both methods, however, the FoQoSAM method always satisfies at least an additional QoS requirement compared to the baseline AHP method.Conclusions: The proposed new FoQoSAM method is appropriate and can be used in decision-making when ranking and verifying cloud deployment options. Due to its practical utility it was integrated into the SWITCH workbench."
Research article - Deriving architectural models from requirements specifications: A systematic mapping study,"AbstractContextSoftware architecture design creates and documents the high-level structure of a software system. Such structure, expressed in architectural models, comprises software elements, relations among them, and properties of these elements and relations. Existing software architecture methods offer ways to derive architectural models from requirements specifications. These models must balance different forces that should be analyzed during this derivation process, such as those imposed by different application domains and quality attributes. Such balance is difficult to achieve, requiring skilled and experienced architects.ObjectThe purpose of this paper is to provide a comprehensive overview of the existing methods to derive architectural models from requirements specifications and offer a research roadmap to challenge the community to address the identified limitations and open issues that require further investigation.MethodTo achieve this goal, we performed a systematic mapping study following the good practices from the Evidence-Based Software Engineering field.ResultsThis study resulted in 39 primary studies selected for analysis and data extraction, from the 2575 initially retrieved.ConclusionThe major findings indicate that current architectural derivation methods rely heavily on the architects’ tacit knowledge (experience and intuition), do not offer sufficient support for inexperienced architects, and lack explicit evaluation mechanisms. These and other findings are synthesized in a research roadmap which results would benefit researchers and practitioners."
Short communication - On the need to update systematic literature reviews,"AbstractContextMany Systematic Literature Reviews (SLRs) were performed in the recent past, but just a few are being updated. Keeping SLRs updated is essential to prolong their lifespan.ObjectiveTo give a picture about how SLRs are being updated and what researchers think about SLRs updates.MethodIn this work, we present a Systematic Mapping (SM) study about SLRs updates and a survey with EBSE researchers that published their SLRs between 2011 and 2015.ResultsWe included 22 studies in the SM, where 15 changed some artifact from the original study, including changes in research questions. We obtained 28 answers in our survey with SLRs authors that, in general, consolidate interpretations retrieved from the SM, but some answers did not.ConclusionSLRs may lose their impact over the years. Identifying actions to keep them updated is of great importance to SLR research field."
"Research article - Euphoria: A Scalable, event-driven architecture for designing interactions across heterogeneous devices in smart environments","AbstractContext: From personal mobile and wearable devices to public ambient displays, our digital ecosystem has been growing with a large variety of smart sensors and devices that can capture and deliver insightful data to connected applications, creating thus the need for new software architectures to enable fluent and flexible interactions in such smart environments.Objective: We introduce Euphoria, a new software architecture design and implementation that enables easy prototyping, deployment, and evaluation of adaptable and flexible interactions across heterogeneous devices in smart environments.Method: We designed Euphoria by following the requirements of the ISO/IEC 25010:2011 standard on Software Quality Requirements and Evaluation applied to the specific context of smart environments.Results: To demonstrate the adaptability and flexibility of Euphoria, we describe three application scenarios for contexts of use involving multiple users, multiple input/output devices, and various types of smart environments, as follows: (1) wearable user interfaces and whole-body gesture input for interacting with public ambient displays, (2) multi-device interactions in physical-digital spaces, and (3) interactions on smartwatches for a connected car application scenario. We also perform a technical evaluation of Euphoria regarding the main factors responsible for the magnitudes of the request-response times for producing, broadcasting, and consuming messages inside the architecture. We deliver the source code of Euphoria free to download and use for research purposes.Conclusion: By introducing Euphoria and discussing its applicability, we hope to foster advances and developments in new software architecture initiatives for our increasingly complex smart environments, but also to readily support implementations of novel interactive systems and applications for smart environments of all kinds."
Research article - A new benchmark for evaluating pattern mining methods based on the automatic generation of testbeds,"AbstractContextMining patterns is one of the most attractive topics in the field of software design. Knowledge about the number, type, and location of pattern instances is crucial to understand the original design decisions. Several techniques and tools have been presented in the literature for mining patterns in a software system. However, evaluating the quality of the detection results is usually done manually or subjectively. This can significantly affect the evaluation results. Therefore, a fair comparison of the quality of the various mining methods is not possible.ObjectiveThis paper describes a new benchmark to evaluate pattern mining methods in source code or design. Our work aims at overcoming the challenges faced in benchmarking in pattern detection. The proposed benchmark is comprehensive, fair, and objective, with a repeatable evaluation process.MethodOur proposed benchmark is based on automatic generation of testbeds using graph theory. The generated testbeds are Java source codes and their corresponding class diagrams in which various types of patterns and their variants are inserted in different locations. The generated testbeds differ in their levels of complexity and full information is available on the utilized patterns.ResultsThe results show that our proposed benchmark is able to evaluate the pattern mining methods quantitatively and objectively. Also, it can be used to compare pattern mining methods in a fair and repeatable manner.ConclusionsBased on our findings, it can be argued that benchmarking in the pattern mining field is significantly less mature than topics such as presenting a new detection method. Therefore, special attention is needed in the pattern evaluation topic. Our proposed benchmark is a step towards achieving a comparative understanding of the effectiveness of detection methods and demonstrating their strengths and weaknesses."
Research article - Mining software repositories for adaptive change commits using machine learning techniques,"AbstractContextVersion Control Systems, such as Subversion, are standard repositories that preserve all of the maintenance changes undertaken to source code artifacts during the evolution of a software system. The documented data of the version history are organized as commits; however, these commits do not keep a tag that would identify the purpose of the relevant undertaken change of a commit, thus, there is rarely enough detail to clearly direct developers to the changes associated with a specific type of maintenance.ObjectiveThis work examines the version histories of an open source system to automatically classify version commits into one of two categories, namely adaptive commits and non-adaptive commits.MethodWe collected the commits from the version history of three open source systems, then we obtained eight different code change metrics related to, for example, the number of changed statements, methods, hunks, and files. Based on these change metrics, we built a machine learning approach to classify whether a commit was adaptive or not.ResultsIt is observed that code change metrics can be indicative of adaptive maintenance activities. Also, the classification findings show that the machine learning classifier developed has approximately 75% prediction accuracy within labeled change histories.ConclusionThe proposed method automates the process of examining the version history of a software system and identifies which commits to the system are related to an adaptive maintenance task. The evaluation of the method supports its applicability and efficiency. Although the evaluation of the proposed classifier on unlabeled change histories shows that it is not much better than the random guessing in terms of F-measure, we feel that our classifier would serve as a better basis for developing advanced classifiers that have predictive power of adaptive commits without the need of manual efforts."
Research article - A model of requirements engineering in software startups,"AbstractContextOver the past 20 years, software startups have created many products that have changed human life. Since these companies are creating brand-new products or services, requirements are difficult to gather and highly volatile. Although scientific interest in software development in this context has increased, the studies on requirements engineering in software startups are still scarce and mostly focused on elicitation activities.ObjectiveThis study overcomes this gap by answering how requirements engineering practices are performed in this context.MethodWe conducted a grounded theory study based on 17 interviews with software startups practitioners.ResultsWe constructed a model to show that software startups do not follow a single set of practices but, instead, build a custom process, changed throughout the development of the company, combining different practices according to a set of influences (Founders, Software Development Manager, Developers, Market, Business Model and Startup Ecosystem).ConclusionOur findings show that requirements engineering activities in software startups are similar to those in agile teams, but some steps vary as a consequence of the lack of an accessible customer."
Research article - Empirical evaluation and proposals for bands-based COSMIC early estimation methods,"AbstractBackground. In the early phases of software development projects, thorough application of the COSMIC functional size measurement method may require more time and effort than available. Thus, early approximate methods have been proposed for estimating the COSMIC functional size of an application, instead of measuring it.Objective. The goal of this paper is to empirically evaluate the accuracy of the COSMIC early size estimation methods that are based on evaluations at the functional process level, for which historical data are available. The goal is to provide practitioners with empirical evidence on the accuracy of these methods.Method. We evaluated the Average Functional Process and the Equal Size Bands methods. We also proposed and evaluated two new approaches for defining bands in the Fixed Size Classification method. The estimation was performed by applying these methods to a set of software applications for which the data necessary to perform estimations were available, having been previously measured according to the standard COSMIC method.Results. Our analyses show that the Average Functional Process method generally provides estimates that are reasonable for early and quick sizing, but in some cases its estimation errors are too large to be acceptable. On the contrary, the methods using bands can provide quite accurate estimates. We determine the level of accuracy that can be obtained based on the type of method used, the number of bands used, and the quantitative characterization of the ability to classify each functional process in the correct band.Conclusions. The Average Functional Process method may be unreliable, as it occasionally yields quite large errors. Organizations using bands-based methods cannot just follow the prescribed estimation process: they need to properly train people in charge of classifying functional processes in the correct size band."
