title,abstract
Research article - Comparing the comprehensibility of requirements models: An experiment replication,"AbstractContextThere are several requirements modeling approaches with differences, for instance, in perspective, abstraction levels, modeling focus, and representation. Therefore, comparing and selecting a requirements modeling approach can be a difficult task. An important requirement for an approach is model comprehensibility.ObjectiveThis paper replicates the experiment executed by Hadar et al. (2013) to compare the comprehensibility of Tropos requirements models and Use case models, along with the effort to comprehend the model, and the productivity.MethodThis replication varies the operationalization, protocol, population, and experimenters, addressing some future works proposed by the original experiment. Only one application domain of the original experiment was considered, but the same questionnaire was used to evaluate model comprehensibility. In addition to the two models of the original experiment, we also considered another Use case template.ResultsDifferently from the original experiment, the results of this replication indicate no difference in model comprehensibility and effort between Tropos and the two Use case models considered. As in the original experiment, the results indicate no difference in productivity.ConclusionsDifferences in the experiment setting may explain the different results from the original experiment. Yet, it may be difficult to compare requirements approaches with complementary strengths and limitations as the requirements models must have equivalent content and complexity."
Research article - IgS-wBSRM: A time-aware Web Service QoS monitoring approach in dynamic environments,"AbstractContextQuality of Service(QoS) is an important criterion to measure the quality of third-party web services. However, it is always affected by different environmental factors. Consequently, how to monitor web service QoS timely and accurately in dynamic environments is an important problem.ObjectiveOur article aims to design a novel Web service QoS monitoring approach which can be used in dynamic environments.MethodTo achieve the above objective, we propose a novel weighted naïve Bayesian runtime monitoring approach based on information gain theory and sliding window mechanism, called IgS-wBSRM. IgS-wBSRM initializes the weights of different environmental factors according to training samples collected. Then, according to information entropy and information gain theory, IgS-wBSRM reads the sample data flow in sequence, and calculates the information gain of each environmental impact factor after the arrival of new sample. It updates the initialized weights with TF-IDF algorithm in dynamic environments. In this way, IgS-wBSRM can correct the delay judgement, jitter noise and off-line constant problems in traditional monitoring approaches. Furthermore, considering the timeliness of the training samples, IgS-wBSRM combines a sliding window mechanism to update the weights of each environmental impact factor, and it can eliminate the impact of the recent service state in the accumulated historical data.ResultsA set of dedicated experiments based on a real world data set and a simulated data set demonstrates that IgS-wBSRM can abandon the expiration information of historical data effectively, and can monitor QoS more accurately.ConclusionsThe overall effect of IgS-wBSRM is better than other QoS monitoring approaches. We suggest directions for follow-up work, e.g., exploring the influence of the size of the sliding window, considering multiple QoS attributes combined with data integration theory and applying IgS-wBSRM in other QoS areas."
Research article - Kontun: A Framework for recommendation of authentication schemes and methods,"AbstractContextThere are many techniques for performing authentication, such as text passwords and biometrics. Combining two factors into one technique is known as multi-factor authentication. The lack of a proper method for comparing and selecting these techniques for their implementation in software development processes is observed.ObjectiveThe article presents a recommendation Framework proposal for comparing and selecting authentication techniques in a software development process.MethodKnowledge from academy is obtained through a systematic literature review and experience from industry is gathered using a survey and interviews. The results of these two techniques are used to generate a Framework proposal, which is validated afterwards, through an expert panel and the case study method.ResultsA recommendation Framework is generated, which recommends the most appropriate authentication schemes and methods for software applications based on criteria identified in literature and industry, categorized by usability, security and costs, plus the context for which the application is intended. The Framework's validity is ascertained by confirming that its recommendations are on line with those on industry, based in the results from the developed case studies. A tool prototype was created in order to help using the Framework in software development processes.ConclusionThe proposed Framework helps to cover the observed gap in literature, helping software developers to compare and select the most appropriate authentication techniques for their applications."
Research article - Software metrics thresholds calculation techniques to predict fault-proneness: An empirical comparison,"AbstractContext: Nowadays, fault-proneness prediction is an important field of software engineering. It can be used by developers and testers to prioritize tests. This would allow a better allocation of resources, reducing testing time and costs, and improving the effectiveness of software testing. Non-supervised fault-proneness prediction models, especially thresholds-based models, can easily be automated and give valuable insights to developers and testers on the classification performed.Objective: In this paper, we investigated three thresholds calculation techniques that can be used for fault-proneness prediction: ROC Curves, VARL (Value of an Acceptable Risk Level) and Alves rankings. We compared the performance of these techniques with the performance of four machine learning and two clustering based models.Method: Threshold values were calculated on a total of twelve different public datasets: eleven from the PROMISE Repository and another based on the Eclipse project. Thresholds-based models were then constructed using each thresholds calculation technique investigated. For comparison, results were also computed for supervised machine learning and clustering based models. Inter-dataset experimentation between different systems and versions of a same system was performed.Results: Results show that ROC Curves is the best performing method among the three thresholds calculation methods investigated, closely followed by Alves Rankings. VARL method didn’t give valuable results for most of the datasets investigated and was easily outperformed by the two other methods. Results also show that thresholds-based models using ROC Curves outperformed machine learning and clustering based models.Conclusion: The best of the three thresholds calculation techniques for fault-proneness prediction is ROC Curves, but Alves Rankings is a good choice too. In fact, the advantage of Alves Rankings over ROC Curves technique is that it is completely unsupervised and can therefore give pertinent threshold values when fault data is not available."
Research article - Maintaining accurate web usage models using updates from activity diagrams,"AbstractContextMarkov operational profile (Markov OP) is a type of usage models for large applications involving state transitions. Such usage models not only help us ensure and maximize product reliability, but can also be used to understand user behavior, and fine-tune system performance and usability. Web usage models can be constructed based on actual usage of the application by target users recorded in existing web logs. Such models constructed before maintenance and evolution may not reflect actual usage of the updated application accurately. At this point, the updated web application has not been deployed yet, so that its actual usage data could not be collected to construct a new Markov OP needed to test the newly updated web application.ObjectiveThis paper aims at maintaining accurate Markov OP using updates derived from activity diagrams used in web application maintenance and evolution.MethodMarkov OP shares some common characteristics with activity diagrams which describe the application in terms of user activities. We develop a method to update the initial Markov OP by analyzing its differences with the activity diagrams.ResultsWe have applied our method in a web application to provide an initial validation of its applicability and effectiveness. After the deployment of the updated web application and new usage data became available, a new Markov OP was constructed. We quantified inaccuracies of the initial Markov OP and the updated Markov OP using the new Markov OP as the reference standard, and quantitatively demonstrated that our method improves the accuracy of the initial Markov OP for the updated web application.ConclusionOur new method provides an effective and practical way to maintain accurate Markov OP over web application maintenance and evolution using existing activity diagrams."
Research article - A framework for the recovery and visualization of system availability scenarios from execution traces,"AbstractContextDynamic analysis is typically concerned with the analysis of system functional aspects at run time. However, less work has been devoted to the dynamic analysis of software quality attributes. The recovery of availability scenarios from system execution traces is particularly important for critical systems to verify that the running implementation supports and complies with availability requirements, especially if the source code is not available (e.g., in legacy systems) and after the system has undergone several ad-hoc maintenance tasks.ObjectivePropose a dynamic analysis approach, along with tool support, to recover availability scenarios, from system execution traces running high availability features.MethodNative execution traces, collected from systems running high availability features, are pre-processed, filtered, merged, and segmented into execution phases. The segmented scenarios are then visualized, at a high level of abstraction, using the ITU-T standard Use Case Maps (UCM) language extended with availability annotations.ResultsThe applicability of our proposed approach has been demonstrated by implementing it as a prototype feature within the jUCMNav tool and by applying it to four real-world systems running high availability features. Furthermore, we have conducted an empirical study to prove that resulting UCM models improve the understandability of log files that contain high availability features.ConclusionWe have proposed a framework to filter, merge, segment, and visualize native log traces. The framework presents the following benefits: (1) it offers analysts the flexibility to specify what to include/exclude from an execution trace, (2) it provides a log segmentation method based on the type of information reported in the execution trace, (3) it uses the UCM language to visually describe availability scenarios at a high level of abstraction, and (4) it offers a scalable solution for the visualization problem through the use of the UCM stub-plug-in concept."
Research article - Software defect prediction using stacked denoising autoencoders and two-stage ensemble learning,"AbstractContextSoftware defect prediction (SDP) plays an important role in allocating testing resources reasonably, reducing testing costs, and ensuring software quality. However, software metrics used for SDP are almost entirely traditional features compared with deep representations (DPs) from deep learning. Although stacked denoising autoencoders (SDAEs) are powerful for feature learning and have been successfully applied in other fields, to the best of our knowledge, it has not been investigated in the field of SDP. Meanwhile, class-imbalance is still a pressing problem needing to be addressed.ObjectiveIn this paper, we propose a novel SDP approach, SDAEsTSE, which takes advantages of SDAEs and ensemble learning, namely the proposed two-stage ensemble (TSE).MethodOur method mainly includes two phases: the deep learning phase and two-stage ensemble (TSE) phase. We first use SDAEs to extract the DPs from the traditional software metrics, and then a novel ensemble learning approach, TSE, is proposed to address the class-imbalance problem.ResultsExperiments are performed on 12 NASA datasets to demonstrate the effectiveness of DPs, the proposed TSE, and SDAEsTSE, respectively. The performance is evaluated in terms of F-measure, the area under the curve (AUC), and Matthews correlation coefficient (MCC). Generally, DPs, TSE, and SDAEsTSE contribute to significantly higher performance compared with corresponding traditional metrics, classic ensemble methods, and benchmark SDP models.ConclusionsIt can be concluded that (1) deep representations are promising for SDP compared with traditional software metrics, (2) TSE is more effective for addressing the class-imbalance problem in SDP compared with classic ensemble learning methods, and (3) the proposed SDAEsTSE is significantly effective for SDP."
Research article - An empirical study to improve software security through the application of code refactoring,"AbstractContextCode bad smells indicate design flaws that can degrade the quality of software and can potentially lead to the introduction of faults. They can be eradicated by applying refactoring techniques. Code bad smells that impact the security perspective of software should be detected and removed from their code base. However, the existing literature is insufficient to support this claim and there are few studies that empirically investigate bad smells and refactoring opportunities from a security perspective.ObjectiveIn this paper, we investigate how refactoring can improve the security of an application by removing code bad smell.MethodWe analyzed three different code bad smells in five software systems. First, the identified code bad smells are filtered against security attributes. Next, the object-oriented design and security metrics are calculated for the five investigated systems. Later, refactoring is applied to remove security-related code bad smells. The correctness of detection and refactoring of investigated code smells are then validated. Finally, both traditional object-oriented and security metrics are again calculated after removing bad smells to assess its impact on the design and security attributes of systems.ResultsWe found ‘feature envy’ to be the most abundant security bad smell in investigated projects. The ‘move method’ and ‘move field’ are commonly applied refactoring techniques because of the abundance of feature envy.ConclusionThe results of security metrics indicate that refactoring helps improve the security of an application without compromising the overall quality of software systems."
Research article - Developing an agent-based simulation model of software evolution,"AbstractContextIn attempt to simulate the factors that affect the software evolution behaviour and possibly predict it, several simulation models have been developed recently. The current system dynamic (SD) simulation model of software evolution process was built based on actor-network theory (ANT) of software evolution by using system dynamic environment, which is not a suitable environment to reflect the complexity of ANT theory. In addition the SD model has not been investigated for its ability to represent the real-world process of software evolution.ObjectivesThis paper aims to re-implements the current SD model to an agent-based simulation environment ‘Repast’ and checks the behaviour of the new model compared to the existing SD model. It also aims to investigate the ability of the new Repast model to represent the real-world process of software evolution.Methodsa new agent-based simulation model is developed based on the current SD model's specifications and then tests similar to the previous model tests are conducted in order to perform a comparative evaluation between of these two results. In addition an investigation is carried out through an interview with an expert in software development area to investigate the model's ability to represent real-world process of software evolution.ResultsThe Repast model shows more stable behaviour compared with the SD model. Results also found that the evolution health of the software can be calibrated quantitatively and that the new Repast model does have the ability to represent real-world processes of software evolution.ConclusionIt is concluded that by applying a more suitable simulation environment (agent-based) to represent ANT theory of software evolution, that this new simulation model will show more stable bahaviour compared with the previous SD model; And it will also shows the ability to represent (at least quantatively) the real-world aspect of software evolution."
Research article - Technical debt and agile software development practices and processes: An industry practitioner survey,"AbstractContext: Contemporary software development is typically conducted in dynamic, resource-scarce environments that are prone to the accumulation of technical debt. While this general phenomenon is acknowledged, what remains unknown is how technical debt specifically manifests in and affects software processes, and how the software development techniques employed accommodate or mitigate the presence of this debt.Objectives: We sought to draw on practitioner insights and experiences in order to classify the effects of agile method use on technical debt management, given the popularity and perceived success of agile methods. We explore the breadth of practitioners’ knowledge about technical debt; how technical debt is manifested across the software process; and the perceived effects of common agile software development practices and processes on technical debt. In doing so, we address a research gap in technical debt knowledge and provide novel and actionable managerial recommendations.Method: We designed, tested and executed a multi-national survey questionnaire to address our objectives, receiving 184 responses from practitioners in Brazil, Finland, and New Zealand.Results: Our findings indicate that: 1) Practitioners are aware of technical debt, although, there was under utilization of the concept, 2) Technical debt commonly resides in legacy systems, however, concrete instances of technical debt are hard to conceptualize which makes it problematic to manage, 3) Queried agile practices and processes help to reduce technical debt; in particular, techniques that verify and maintain the structure and clarity of implemented artifacts (e.g., Coding standards and Refactoring) positively affect technical debt management.Conclusions: The fact that technical debt instances tend to have characteristics in common means that a systematic approach to its management is feasible. However, notwithstanding the positive effects of some agile practices on technical debt management, competing stakeholders’ interests remain a concern."
Research article - Systematic literature review on agile practices in global software development,"AbstractContextDeveloping software in distributed development environments exhibits coordination, control and communication challenges. Agile practices, which demand frequent communication and self-organization between remote sites, are increasingly found in global software development (GSD) to mitigate said challenges.ObjectiveWe aim to provide detailed insight into what is reported on the successful application of agile practices in GSD from 1999 to 2016 and also identify the most frequently applied agile practices and reported distribution scenarios. We further strive to uncover research opportunities and gaps in the field of agile GSD.MethodWe build our systematic literature review on top of a previous review, which investigated studies published between 1999 and 2009, and extend the review by years 2010–2016, for which we conduct both a quantitative and a qualitative analysis.ResultsOur results show that the majority of the cases studied is global and involves complex distribution scenarios with Scrum or combined Scrum/Extreme Programming being the most used agile methods. Key results include that in contrast to 1999–2009, where four Extreme Programming practices were among the ten most frequently used agile practices, in 2010–2016 Scrum is in the center of agile GSD implementations with eight Scrum-based practices in the top ten agile practices used in GSD.ConclusionAgile GSD is a maturing research field with higher quality contributions and a greater variety of publication types and methods from 2010 to 2016 than before from 1999 to 2009. However, researchers need to report full empirical contextual details of their studied cases in order to improve the generalizability of results and allow the future creation of stronger frameworks to drive the implementation of agile practices in GSD."
