title,abstract
Short communication - Leveraging Small Sample Learning for Business Process Management,"AbstractContext: Tool support for business process management (BPM) is improving more and more. Often, machine learning techniques are used to recognize certain execution patterns, to optimize workflows and to observe or predict processes. Frequently, many organisations cannot meet the fundamental prerequisites of machine learning methods since less data is recorded and therefore available for analysis. Most machine learning techniques rely on big and sufficient data source sets that can be analyzed. Small Sample Learning (SSL) tackles the issue of implementing machine learning methods in environments where only quantitatively insufficient datasets are available. These methods are strongly tailored to computer vision or natural language processing problems, which is why they are still neglected in the BPM area.Objective: This paper motivates the use of SSL methods in the BPM area and fosters a research stream that is concerned with the transferability to and the application of these methods in the BPM area.Method: We propose a concept for leveraging SSL methods in BPM and illustrate the idea exemplarly in the field of process mining.Results: Reasons for the need of SSL methods in the BPM area and a conceptual approach for transferring existing SSL methods to the BPM area. The feasibility of our apprach is shown by a brief overview of a primary study leveraging SSL methods for process prediction.Conclusions: Many areas of process mining or BPM in general depend on a sufficient amount of (training) data. Often small and medium sized companies lack ”big data”, which is why advantages of machine learning and data analysis in the context of BPM cannot be applied. Existing methods that deal with insufficient data are very domain-specific and must be transferred to the process mining area respectively the BPM area."
Review article - A systematic review of scheduling approaches on multi-tenancy cloud platforms,"AbstractContext:Scheduling in cloud is complicated as a result of multi-tenancy. Diverse tenants have different requirements, including service functions, response time, QoS and throughput. Diverse tenants require different scheduling capabilities, resource consumption and competition. Multi-tenancy scheduling approaches have been developed for different service models, such as Software as a Service (SaaS), Platform as a service (PaaS), Infrastructure as a Service (IaaS), and Database as a Service (DBaaS).Objective:In this paper, we survey the current landscape of multi-tenancy scheduling, laying out the challenges and complexity of software engineering where multi-tenancy issues are involved. This study emphasises scheduling policies, cloud provisioning and deployment with regards to multi-tenancy issues. We conduct a systematic literature review of research studies related to multi-tenancy scheduling approaches on cloud platforms determine the primary scheduling approaches currently used and the challenges for addressing key multi-tenancy scheduling issues.Method:We adopted a systematic literature review method to search and review many major journal and conference papers on four major online electronic databases, which address our four predefined research questions. Defining inclusion and exclusion criteria was the initial step before extracting data from the selected papers and deriving answers addressing our enquiries.Results:Finally, 53 papers were selected, of which 62 approaches were identified. Most of these methods are developed without cloud layers’ limitation (43.40%) and on SaaS, most of scheduling approaches are oriented to framework design (43.75%).Conclusion:The results have demonstrated most of multi-tenancy scheduling solutions can work at any delivery layer. With the difference of tenants’ requirements and functionalities, the choice of cloud service delivery models is changed. Based on our study, designing a multi-tenancy scheduling framework should consider the following 3 factors: computing, QoS and storage resource. One of the potential research foci of multi-tenancy scheduling approaches is on GPU scheduling."
Review article - Big Data analytics in Agile software development: A systematic mapping study,"AbstractContext:Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity.Objective:Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA).Method:As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019.Results:In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics.Conclusions:As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives."
Research article - Identifying method-level mutation subsumption relations using Z3,"AbstractContext:Mutation analysis is a popular but costly approach to assess the quality of test suites. One recent promising direction in reducing costs of mutation analysis is to identify redundant mutations, i.e., mutations that are subsumed by some other mutations. A previous approach found redundant mutants manually through truth tables but it cannot be applied to all mutations. Another work derives them using automatic test suite generators but it is a time consuming task to generate mutants and tests, and to execute tests.Objective:This article proposes an approach to discover redundant mutants by proving subsumption relations among method-level mutation operators using weak mutation testing.Method:We conceive and encode a theory of subsumption relations in the Z3 theorem prover for 37 mutation targets (mutations of an expression or statement).Results:We automatically identify and prove a number of subsumption relations using Z3, and reduce the number of mutations in a number of mutation targets. To evaluate our approach, we modified MuJava to include the results of 24 mutation targets and evaluate our approach in 125 classes of 5 large open source popular projects used in prior work. Our approach correctly discards mutations in 75.93% of the cases, and reduces the number of mutations by 71.38%.Conclusions:Our approach offers a good balance between the effort required to derive subsumption relations and the effectiveness for the targets considered in our evaluation in the context of strong mutation testing."
Research article - Spectral clustering based mutant reduction for mutation testing,"AbstractContext:Mutation testing techniques, which attempt to construct a set of so-called mutants by seeding various faults into the software under test, have been widely used to generate test cases as well as to evaluate the effectiveness of a test suite. Its popularity in practice is significantly hindered by its high cost, majorly caused by the large number of mutants generated by the technique.Objective:It is always a challenging task to reduce the number of mutants while preserving the effectiveness of mutation testing. In this paper, we make use of an intelligent technique, namely spectral clustering, to improve the efficacy of mutant reduction.Method:First of all, we give a family of definitions and the method to calculate the distance between mutants according to the weak mutation testing criteria. Then we propose a mutant reduction method based on spectral clustering (SCMT), including the determination method of the number of clusters, spectral clustering of mutants, and selection of representative mutants.Results:The experimental studies based on 12 object programs show that the new approach can significantly reduce the number of mutants without jeopardizing the performance of mutation testing. As compared with other benchmark techniques, the new approach based on weak mutation testing criteria cannot only consistently deliver high effectiveness of mutation testing, but also help significantly reduce the time-cost of mutation testing.Conclusion:It is clearly demonstrated that the use of spectral clustering can help enhance the cost-effectiveness of mutation testing. The research reveals some potential research directions for not only mutation testing but also the broad area of software testing."
Research article - Using mutual information to test from Finite State Machines: Test suite selection,"AbstractContext:Mutual Information is an information theoretic measure designed to quantify the amount of similarity between two random variables ranging over two sets. In this paper, we adapt this concept and show how it can be used to select a good test suite to test from a Finite State Machine (FSM) based on a maximise diversity approach.Objective:The main goal of this paper is to use Mutual Information in order to select test suites to test from FSMs and evaluate whether we obtain better results, concerning the quality of the selected test suite, than current state-of-the-art measures.Method:First, we defined our scenario. We considered the case where we receive two (or more) test suites and we have to choose between them. We were interested in this scenario because it is a recurrent case in regression testing. Second, we defined our notion based on Mutual Information: Biased Mutual Information. Finally, we carried out experiments in order to evaluate the measure.Results:We obtained experimental evidence that demonstrates the potential value of the measure. We also showed that the time needed to compute the measure is negligible when compare to the time needed to apply extra testing. We compared our measure with a state-of-the-art test selection measure and showed that our proposal outperforms it. Finally, we have compared our measure with a notion of transition coverage. Our experiments showed that our measure is slightly worse than transition coverage, as expected, but its computation is 10 times faster.Conclusion:Our experiments showed that Biased Mutual Information is a good measure for selecting test suites, outperforming the current state-of-the-art measure, and having a (negative) correlation to fault coverage. Therefore, we can conclude that our new measure can be used to select the test suite that is likely to find more faults. As a result, it has the potential to be used to automate test generation."
Research article - Validating class integration test order generation systems with Metamorphic Testing,"AbstractContext:Previous studies proposed different kinds of approaches for class integration test order generation, and corresponding systems can be implemented based on these approaches. Such class integration test order generation systems can facilitate the process of software integration testing if they are implemented correctly.Objective:However, a test oracle problem exists in the class integration test order generation systems. Since these approaches for class integration test order generation normally deliver a local optimum rather than a global optimum, there are no practically feasible ways to validate their generated class integration test orders, that is, these implementation systems are untestable.Method:To address the test oracle problem, we apply Metamorphic Testing (MT) to validate class integration test order generation systems. Metamorphic Relations (MRs), which are the key components of MT, reason about relations between test outputs of a system. Five effective MRs are developed to ensure the quality of the class integration test order generation systems. In these proposed MRs, follow-up test inputs are generated by modifying classes or class dependencies in the source test inputs while some characteristics of the source test outputs are preserved, for example, the same class integration test order or the equal stubbing cost. Faults can be detected in systems if an individual MR is violated for certain tests.Results:Failure detection of MT has been successfully demonstrated in empirical experiments on three systems implementing different typical class integration test order generation approaches. More than 84% of faulty programs can be detected by all MRs, for three class integration test order generation systems under investigation.Conclusion:The experimental results show that the proposed MRs are able to systematically and effectively detect faults in class integration test order generation systems. This study explores a new application domain in MT and further extends its applications in Software Engineering."
Research article - Fast and curious: A model for building efficient monitoring- and decision-making frameworks based on quantitative data,"AbstractContext:Nowadays, the hype around artificial intelligence is at its absolute peak. Large amounts of data are collected every second of the day and a variety of tools exists to enable easy analysis of data. In practice, however, making meaningful use of it is way more challenging. For instance, affected stakeholders often struggle to specify their information needs and to interpret the results of such analyses.Objective:In this study we investigate how to enable continuous monitoring of information needs, and the generation of knowledge and insights for various stakeholders involved in the lifecycle of software-intensive products. The overarching goal is to support their decision making by providing relevant insights related to their area of responsibility.Methods:We implement multiple monitoring- and decision-making frameworks for six individual, real-world cases selected from three different platforms and covering four types of stakeholders. We compare the individual procedures to derive a generic process for instantiating such frameworks as well as a model to scale it up for multiple stakeholders.Results:For one, we discovered that information needs of stakeholders are often related to a limited subset of data sources and should be specified in stages. For another, stakeholders often benefit from sharing and reusing existing components among themselves in later phases. Specifically, we identify three types of reuse: (1) Data and knowledge, (2) tools and methods, and (3) concepts. As a result, key aspects of our model are iterative feedback and specification cycles as well as the reuse of appropriate components to speed up the instantiation process and maximize the efficiency of the model.Conclusion:Our results indicate that knowledge and insights can be generated much faster and stakeholders feel the benefits of the analysis very early on by iteratively specifying information needs and by systematically sharing and reusing knowledge, tools and concepts."
Research article - Industry-Academia research collaboration in software engineering: The Certus model,"AbstractContextResearch collaborations between software engineering industry and academia can provide significant benefits to both sides, including improved innovation capacity for industry, and real-world environment for motivating and validating research ideas. However, building scalable and effective research collaborations in software engineering is known to be challenging. While such challenges can be varied and many, in this paper we focus on the challenges of achieving participative knowledge creation supported by active dialog between industry and academia and continuous commitment to joint problem solving.ObjectiveThis paper aims to understand what are the elements of a successful industry-academia collaboration that enable the culture of participative knowledge creation.MethodWe conducted participant observation collecting qualitative data spanning 8 years of collaborative research between a software engineering research group on software V&V and the Norwegian IT sector. The collected data was analyzed and synthesized into a practical collaboration model, named the Certus Model.ResultsThe model is structured in seven phases, describing activities from setting up research projects to the exploitation of research results. As such, the Certus model advances other collaborations models from literature by delineating different phases covering the complete life cycle of participative research knowledge creation.ConclusionThe Certus model describes the elements of a research collaboration process between researchers and practitioners in software engineering, grounded on the principles of research knowledge co-creation and continuous commitment to joint problem solving. The model can be applied and tested in other contexts where it may be adapted to the local context through experimentation."
Research article - Do users care about ad’s performance costs? Exploring the effects of the performance costs of in-app ads on user experience,"AbstractContext: In-app advertising is the primary source of revenue for many mobile apps. The cost of advertising (ad cost) is non-negligible for app developers to ensure a good user experience and continuous profits. Previous studies mainly focus on addressing the hidden performance costs generated by ads, including consumption of memory, CPU, data traffic, and battery. However, there is no research on analyzing users’ perceptions of ads’ performance costs to our knowledge.Objective: To fill this gap and better understand the effects of performance costs of in-app ads on user experience, we conduct a study on analyzing user concerns about ads’ performance costs.Method: First, we propose RankMiner, an approach to quantify user concerns about specific app issues, including performance costs. Then, based on the usage traces of 20 subject apps, we measure the performance costs of ads. Finally, we conduct correlation analysis on the performance costs and quantified user concerns to explore whether users complain more for higher performance costs.Results: Our findings include the following: (1) RankMiner can quantify users’ concerns better than baselines by an improvement of 214% and 2.5% in terms of Pearson correlation coefficient (a metric for computing correlations between two variables) and NDCG score (a metric for computing accuracy in prioritizing issues), respectively. (2) The performance costs of the with-ads versions are statistically significantly larger than those of no-ads versions with negligible effect size; (3) Users are more concerned about the battery costs of ads, and tend to be insensitive to ads’ data traffic costs.Conclusion: Our study is complementary to previous work on in-app ads, and can encourage developers to pay more attention to alleviating the most user-concerned performance costs, such as battery cost."
"Research article - A methodical framework for service oriented architecture adoption: Guidelines, building blocks, and method fragments","AbstractContextRapidly-changing business requirements expect high business process flexibility that can be achieved using service oriented architecture (SOA). This requires enterprises to adopt SOA and assess their SOA adoption maturity to achieve continuous improvement. SOA realization demands service development with varying levels of granularity.ObjectivesThe research aims to develop a methodical framework for SOA realization based on Welke's SOA maturity model, a model that assumes a methodology dimension. The framework is concerned with formalizing knowledge on how to identify and shape the main building blocks of a method at each maturity level.MethodsThe research applies the principles of design science research and method engineering to develop a methodical framework for SOA realization.ResultsThe research identifies the gaps in SOA realization methods and illustrates how a methodical framework based on a maturity model facilitates the SOA adoption process. The evaluation results revealed that the framework would help enterprises to select method fragments required at each maturity level to accomplish business excellence.ConclusionThe implications of this research are twofold: from a theoretical perspective, the researchers or practitioners can use the results for further study. From a practical standpoint, enterprises can use the methodical guidelines to assess their current maturity level and select and implement the required method fragments from the method base provided in the proposed framework."
Research article - On the prediction of long-lived bugs: An analysis and comparative study using FLOSS projects,"AbstractContext:Software evolution and maintenance activities in today’s Free/Libre Open Source Software (FLOSS) rely primarily on information extracted from bug reports registered in bug tracking systems. Many studies point out that most bugs that adversely affect the user’s experience across versions of FLOSS projects are long-lived bugs. However, proposed approaches that support bug fixing procedures do not consider the real-world lifecycle of a bug, in which bugs are often fixed very fast. This may lead to useless efforts to automate the bug management process.Objective:This study aims to confirm whether the number of long-lived bugs is significantly high in popular open-source projects and to characterize the population of long-lived bugs by considering the attributes of bug reports. We also aim to conduct a comparative study evaluating the prediction accuracy of five well-known machine learning algorithms and text mining techniques in the task of predicting long-lived bugs.Methods:We collected bug reports from six popular open-source projects repositories (Eclipse, Freedesktop, Gnome, GCC, Mozilla, and WineHQ) and used the following machine learning algorithms to predict long-lived bugs: K-Nearest Neighbor, Naïve Bayes, Neural Networks, Random Forest, and Support Vector Machines.Results:Our results show that long-lived bugs are relatively frequent (varying from 7.2% to 40.7%) and have unique characteristics, confirming the need to study solutions to support bug fixing management. We found that the Neural Network classifier yielded the best results in comparison to the other algorithms evaluated.Conclusion:Research efforts regarding long-lived bugs are needed and our results demonstrate that it is possible to predict long-lived bugs with a high accuracy (around 70.7%) despite the use of simple prediction algorithms and text mining methods."
Research article - Multifaceted infrastructure for self-adaptive IoT systems,"AbstractBackground:Internet of Things (IoT) enables the interaction among objects to provide services to their users. Areas such as eHealth, smart energy, and smart buildings have been benefiting from the IoT potential. However, the development of IoT systems is still complex because it deals with a highly dynamic, volatile, and heterogeneous environment. These characteristics require discovering devices, managing these devices’ context, and self-adapt their behavior.Goal: In this work, we propose a self-adaptive IoT infrastructure to support multiple facets, i.e., the contextual discovery of smart objects, the context management, and the self-adaptation process of the development of these systems.Methods: We evaluated the proposed infrastructure by developing a smart building application with and without it. The evaluation focused on four issues: the feasibility of integrating the context management through middleware platforms with adaptation based on workflows in a request/response communication model, the impact of our infrastructure on the development of self-adaptive IoT systems considering cyclomatic complexity and coupling code metrics; the impact of using contextual filters on the orchestrator of self-adaptation; and the impact on the quality of the self-adaptation.Results: The results suggest that: (i) it is feasible to use the proposed infrastructure in the development of self-adaptive IoT systems; (ii) there is a reduction in the cyclomatic complexity and the coupling with our approach, (iii) there is a considerable decrease in the number of rules evaluated at runtime, (iv) our infrastructure reduces the execution time of the adaptations when using contextual filters, and (v) the self-adaptation process was effective when using the orchestrator of self-adaptations.Conclusion: With these results, we observed that the proposed multifaceted infrastructure could reduce the complexity related to the development of IoT systems, in addition to optimizing their self-adaptation process."
