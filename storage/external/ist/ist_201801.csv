title,abstract
Research article - MULTI: Multi-objective effort-aware just-in-time software defect prediction,"AbstractContext: Just-in-time software defect prediction (JIT-SDP) aims to conduct defect prediction on code changes, which have finer granularity. A recent study by Yang et al. has shown that there exist some unsupervised methods, which are comparative to supervised methods in effort-aware JIT-SDP.Objective: However, we still believe that supervised methods should have better prediction performance since they effectively utilize the gathered defect prediction datasets. Therefore we want to design a new supervised method for JIT-SDP with better performance.Method: In this article, we propose a multi-objective optimization based supervised method MULTI to build JIT-SDP models. In particular, we formalize JIT-SDP as a multi-objective optimization problem. One objective is designed to maximize the number of identified buggy changes and another object is designed to minimize the efforts in software quality assurance activities. There exists an obvious conflict between these two objectives. MULTI uses logistic regression to build the models and uses NSGA-II to generate a set of non-dominated solutions, which each solution denotes the coefficient vector for the logistic regression.Results: We design and conduct a large-scale empirical studies to compare MULTI with 43 state-of-the-art supervised and unsupervised methods under the three commonly used performance evaluation scenarios: cross-validation, cross-project-validation, and timewise-cross-validation. Based on six open-source projects with 227,417 changes in total, our experimental results show that MULTI can perform significantly better than all of the state-of-the-art methods when considering ACC and POPT performance metrics.Conclusion: By using multi-objective optimization, MULTI can perform significantly better than the state-of-the-art supervised and unsupervised methods in the three performance evaluation scenarios. The results confirm that supervised methods are still promising in effort-aware JIT-SDP."
Research article - A semi-automated approach for generating natural language requirements documents based on business process models,"AbstractContext: The analysis of requirements for business-related software systems is often supported by using business process models. However, the final requirements are typically still specified in natural language. This means that the knowledge captured in process models must be consistently transferred to the specified requirements. Possible inconsistencies between process models and requirements represent a serious threat for the successful development of the software system and may require the repetition of process analysis activities.Objective: The objective of this paper is to address the problem of inconsistency between process models and natural language requirements in the context of software development.Method: We define a semi-automated approach that consists of a process model-based procedure for capturing execution-related data in requirements models and an algorithm that takes these models as input for generating natural language requirements. We evaluated our approach in the context of a multiple case study with three organizations and a total of 13 software development projects.Results: We found that our approach can successfully generate well-readable requirements, which do not only positively contribute to consistency, but also to the completeness and maintainability of requirements. The practical use of our approach to identify a suitable subcontractor on the market in 11 of the 13 projects further highlights the practical value of our approach.Conclusion: Our approach provides a structured way to obtain high-quality requirements documents from process models and to maintain textual and visual representations of requirements in a consistent way."
Research article - Empirical evidence in follow the Sun software development: A systematic mapping study,"AbstractContextFollow the Sun (FTS) development is a special case of Global Software Development. It is applied in the context of global projects to reduce the software development life-cycle duration. A number of studies have attempted to aggregate a better understanding of FTS development, but it is still an immature research area.ObjectiveThis paper aims to investigate the existing empirical evidence about FTS research with a focus on identifying what research has been conducted in the area and which results have been obtained.MethodTo achieve this goal, we performed a systematic mapping study to answer our research questions: “Which FTS studies have been published in the literature?” and “What empirical support is provided for them?” We investigated papers published between 1990 and 2017. The synthesis was made through classifying the papers into different categories (research topics, research methods, conferences and journals venues for FTS research, and countries involved in FTS research).ResultsWe selected 57 papers using a predefined search strategy. The majority of the papers discussing FTS were published in the International Conference on Global Software Engineering (ICGSE). The main research topic addressed is processes and organization development for FTS. Case studies combined with the interview as a research sub-method is adopted in the most studies performed in FTS. The majority of the existing research and the most active researchers in this topic are from the United States and Brazil. However, India and the United States are the countries that appear most often in the studies conducted to investigate FTS.ConclusionOur findings suggest that FTS software development is an up-to-date research topic in Software Engineering. However, little information about FTS has been published over the last few years. The emergent need in this research is the development of evaluation research for testing FTS feasibility and effectiveness in practice."
Research article - Just enough semantics: An information theoretic approach for IR-based software bug localization,"AbstractContextSoftware systems are often shipped with defects. Whenever a bug is reported, developers use the information available in the associated report to locate source code fragments that need to be modified in order to fix the bug. However, as software systems evolve in size and complexity, bug localization can become a tedious and time-consuming process. To minimize the manual effort, contemporary bug localization tools utilize Information Retrieval (IR) methods for automated support. IR methods exploit the textual content of bug reports to automatically capture and rank relevant buggy source files.ObjectiveIn this paper, we propose a new paradigm of information-theoretic IR methods to support bug localization tasks in software systems. These methods, including Pointwise Mutual Information (PMI) and Normalized Google Distance (NGD), exploit the co-occurrence patterns of code terms in the software system to reveal hidden textual semantic dimensions that other methods often fail to capture. Our objective is establish accurate semantic similarity relations between source code and bug reports.MethodFive benchmark datasets from different application domains are used to conduct our analysis. The proposed methods are compared against classical IR methods that are commonly used in bug localization research.ResultsThe results show that information-theoretic IR methods significantly outperform classical IR methods, providing a semantically aware, yet, computationally efficient solution for bug localization in large and complex software systems. (A replication package is available at: http://seel.cse.lsu.edu/data/ist17.zip).ConclusionsInformation-theoretic co-occurrence methods provide “just enough semantics” necessary to establish relations between bug reports and code artifacts, achieving a balance between simple lexical methods and computationally-expensive semantic IR methods that require substantial amounts of data to function properly."
Review article - Applying system dynamics approach in software and information system projects: A mapping study,"AbstractContextSoftware and information system are everywhere and the projects involving them are becoming more complex. However, these projects performance patterns are not showing improvement or convergence over time. Additionally, there is a growing interest in modeling the complexities involved in such projects for evaluating long-term impacts, especially the dynamic dimension.ObjectiveThis study aims to analyze how the system dynamics approach has been used in the scientific literature to model complexity in software and information system projects.MethodThe research approach used was a mapping study that combined bibliometrics and content analysis to draw the scenario of the research literature related to software and information system projects, identifying patterns, evolution trends, and research gaps.ResultsThe results show the focus of the studies analyzed regarding the step of policy design and evaluation in the modeling process (46%), besides investigating software development projects (34%). This study also reveals that the most employed tools are simulations (78%) and the causal loop diagram (61%), but only 37% presented model equations. As for the software and information system projects success dimension, system quality has prevailed (73%).ConclusionThe mapping showed that there is a gap of studies exploring the implementation and post implementation phases of software and information systems. Few studies explored the social components; the majority of the studies focused on technical aspects and did not report the complete steps of system dynamics modeling development process. This lack of information hinders the reproduction of past results for expanding and developing new studies."
Review article - Test case prioritization approaches in regression testing: A systematic literature review,"AbstractContextSoftware quality can be assured by going through software testing process. However, software testing phase is an expensive process as it consumes a longer time. By scheduling test cases execution order through a prioritization approach, software testing efficiency can be improved especially during regression testing.ObjectiveIt is a notable step to be taken in constructing important software testing environment so that a system's commercial value can increase. The main idea of this review is to examine and classify the current test case prioritization approaches based on the articulated research questions.MethodSet of search keywords with appropriate repositories were utilized to extract most important studies that fulfill all the criteria defined and classified under journal, conference paper, symposiums and workshops categories. 69 primary studies were nominated from the review strategy.ResultsThere were 40 journal articles, 21 conference papers, three workshop articles, and five symposium articles collected from the primary studies. As for the result, it can be said that TCP approaches are still broadly open for improvements. Each approach in TCP has specified potential values, advantages, and limitation. Additionally, we found that variations in the starting point of TCP process among the approaches provide a different timeline and benefit to project manager to choose which approaches suite with the project schedule and available resources.ConclusionTest case prioritization has already been considerably discussed in the software testing domain. However, it is commonly learned that there are quite a number of existing prioritization techniques that can still be improved especially in data used and execution process for each approach."
Research article - Two sides of the same coin – how agile software development teams approach uncertainty as threats and opportunities,"AbstractContextUncertainty affects software development projects in many ways. Capabilities to manage uncertainty can determine the success or failure of entire projects and even companies. Against failure rates of software development projects, uncertainty is often viewed in a negative light. Its relevance as enabler of learning and innovation tends to be disregarded by scholars and practitioners.ObjectiveThis study extends research that empirically investigates approaches to dealing with different types of uncertainties. Acknowledging both the possibility for negative and positive consequences of uncertainty, we aim to expand existing knowledge and contribute to improving the practices to handle uncertainty in software development.MethodA multiple-case study including field observations and in-depth interviews with 42 professional software developers was performed with a sample of 11 agile software development teams from different Swiss companies.ResultsWe detail practices employed to mitigate threats while simultaneously allowing teams to remain open to opportunities. We establish a taxonomical classification of uncertainty in software development, which extends beyond often-addressed requirements uncertainty and additionally includes uncertainty related to resource availability and task specifications. Against the background of different uncertainty types, we discuss distinct practices and four overarching principles to approach uncertainty using strategies for threat avoidance versus opportunity realization. Specifically, we find that employed practices are guided by 1) uncertainty anticipation, 2) information accrual, 3) solution inspection, and 4) role-based coordination.ConclusionWe argue that approaches to dealing efficiently and effectively with uncertainty can be improved when teams distinguish between different types of uncertainty and treat them explicitly as threats versus opportunities. Whereas potential to seize opportunities is under-used, agile methods could evolve into better frameworks supporting the systematic search for innovation. We discuss the implications of our findings for agile development teams in particular and product developing organizations in general."
Review article - Factors influencing the understandability of process models: A systematic literature review,"AbstractContextProcess models are key in facilitating communication in organizations and in designing process-aware information systems. Organizations are facing increasingly larger and more complex processes, which pose difficulties to the understandability of process models. The literature reports several factors that are considered to influence the understandability of process models. However, these studies typically focus on testing of a limited set of factors. A work that collects, abstracts and synthesizes an in-depth summary of the current literature will help in developing the research in this field.ObjectiveWe conducted a systematic literature review (SLR) focusing on the empirical studies in the existing literature in order to better understand the state of the research on process model understandability, and identify the gaps and opportunities for future research.MethodWe searched the studies between the years 1995 and 2015 in established electronic libraries. Out of 1066 publications retrieved initially, we selected 45 publications for thorough analysis. We identified, analyzed and categorized factors that are considered to influence the understandability of process models as studied in the literature using empirical methods. We also analyzed the indicators that are used to quantify process model understandability.ResultsOur analysis identifies several gaps in the field, as well as issues of inconsistent findings regarding the effect of some factors, unbalanced emphasis on certain indicators, and methodological concerns.ConclusionsThe existing research calls for comprehensive empirical studies to contribute to a better understanding of the factors of process model understandability. Our study is a comprehensive source for researchers working on the understandability of process models and related fields, and a useful guide for practitioners aiming to generate understandable process models."
Research article - Are you smelling it? Investigating how similar developers detect code smells,"AbstractContextA code smell indicates a poor implementation choice that often worsens software quality. Thus, code smell detection is an elementary technique to identify refactoring opportunities in software systems. Unfortunately, there is limited knowledge on how similar two or more developers detect smells in code. In particular, few studies have investigated if developers agree or disagree when recognizing a smell and which factors can influence on such (dis)agreement.ObjectiveWe perform a broader study to investigate how similar the developers detect code smells. We also analyze whether certain factors related to the developers’ profiles concerning background and experience may influence such (dis)agreement. Moreover, we analyze if the heuristics adopted by developers on detecting code smells may influence on their (dis)agreement.MethodWe conducted an empirical study with 75 developers who evaluated instances of 15 different code smell types. For each smell type, we analyzed the agreement among the developers and we assessed the influence of 6 different factors on the developers’ evaluations. Altogether more than 2700 evaluations were collected, resulting in substantial quantitative and qualitative analyses.ResultsThe results indicate that the developers presented a low agreement on detecting all 15 smell types analyzed in our study. The results also suggest that factors related to background and experience did not have a consistent influence on the agreement among the developers. On the other hand, the results show that the agreement was consistently influenced by specific heuristics employed by developers.ConclusionsOur findings reveal that the developers detect code smells in significantly different ways. As a consequence, these findings introduce some questions concerning the results of previous studies that did not consider the different perceptions of developers on detecting code smells. Moreover, our findings shed light towards improving state-of-the-art techniques for accurate, customized detection of code smells."
Research article - Ensuring the semantic correctness of a BAUML artifact-centric BPM,"AbstractContextUsing models to represent business processes provides several advantages, such as facilitating the communication between the stakeholders or being able to check the correctness of the processes before their implementation. In contrast to traditional process modeling approaches, the artifact-centric approach treats data as a key element of the process, also considering the tasks or activities that are performed in it.ObjectiveThis paper presents a way to verify and validate the semantic correctness of an artifact-centric business process model defined using a combination of UML and OCL models - a BAUML model.MethodWe achieve our goal by presenting several algorithms that encode the initial models into first-order logic, which then allows to use an existing satisfiability checking tool to determine their correctness.ResultsAn approach to verify and validate an artifact-centric BPM specified in BAUML, which uses a combination of UML and OCL models. To do this, we provide a method to translate all BAUML components into a set of logic formulas. The result of this translation ensures that the only changes allowed are those specified in the model, and that those changes are taking place according the order established by the model. Having obtained this logic representation, these models can be validated by any existing reasoning method able to deal with negation of derived predicates. Moreover, we show how to automatically generate the relevant tests to validate the models. We also show the feasibility of our approach by implementing a prototype tool and applying it to a running example.ConclusionIt is feasible to ensure the semantic correctness of an artifact-centric business process model in practice."
Review article - A systematic review of requirements change management,"AbstractContextSoftware requirements are often not set in concrete at the start of a software development project; and requirements changes become necessary and sometimes inevitable due to changes in customer requirements and changes in business rules and operating environments; hence, requirements development, which includes requirements changes, is a part of a software process. Previous work has shown that failing to manage software requirements changes well is a main contributor to project failure. Given the importance of the subject, there's a plethora of research work that discuss the management of requirements change in various directions, ways and means. An examination of these works suggests that there's a room for improvement.ObjectiveIn this paper, we present a systematic review of research in Requirements Change Management (RCM) as reported in the literature.MethodWe use a systematic review method to answer four key research questions related to requirements change management. The questions are: (1) What are the causes of requirements changes? (2) What processes are used for requirements change management? (3) What techniques are used for requirements change management? and (4) How do organizations make decisions regarding requirements changes? These questions are aimed at studying the various directions in the field of requirements change management and at providing suggestions for future research work.ResultsThe four questions were answered; and the strengths and weaknesses of existing techniques for RCM were identified.ConclusionsThis paper has provided information about the current state-of-the-art techniques and practices for RCM and the research gaps in existing work. Benefits, risks and difficulties associated with RCM are also made available to software practitioners who will be in a position of making better decisions on activities related to RCM. Better decisions will lead to better planning which will increase the chance of project success."
Research article - An empirical study on the impact of refactoring activities on evolving client-used APIs,"AbstractContextRefactoring is recognized as an effective practice to maintain evolving software systems. For software libraries, we study how library developers refactor their Application Programming Interfaces (APIs), especially when it impacts client users by breaking an API of the library.ObjectiveOur work aims to understand how clients that use a library API are affected by refactoring activities. We target popular libraries that potentially impact more library client users.MethodWe distinguish between library APIs based on their client-usage (referred to as client-used APIs) in order to understand the extent to which API breakages relate to refactorings. Our tool-based approach allows for a large-scale study across eight libraries (i.e., totaling 183 consecutive versions) with around 900 clients projects.ResultsWe find that library maintainers are less likely to break client-used API classes. Quantitatively, we find that refactoring activities break less than 37% of all client-used APIs. In a more qualitative analysis, we show two documented cases of where non-refactoring API breaking changes are motivated other maintenance issues (i.e., bug fix and new features) and involve more complex refactoring operations.ConclusionUsing our automated approach, we find that library developers are less likely to break APIs and tend to break client-used APIs when performing maintenance issues."
Research article - RINGA: Design and verification of finite state machine for self-adaptive software at runtime,"AbstractContextIn recent years, software environments such as the cloud and Internet of Things (IoT) have become increasingly sophisticated, and as a result, development of adaptable software has become very important. Self-adaptive software is appropriate for today's needs because it changes its behavior or structure in response to a changing environment at runtime. To adapt to changing environments, runtime verification is an important requirement, and research that integrates traditional verification with self-adaptive software is in high demand.ObjectiveModel checking is an effective static verification method for software, but existing problems at runtime remain unresolved. In this paper, we propose a self-adaptive software framework that applies model checking to software to enable verification at runtime.MethodThe proposed framework consists of two parts: the design of self-adaptive software using a finite state machine and the adaptation of the software during runtime. For the first part, we propose two finite state machines for self-adaptive software called the self-adaptive finite state machine (SA-FSM) and abstracted finite state machine (A-FSM). For the runtime verification part, a self-adaptation process based on a MAPE (monitoring, analyzing, planning, and executing) loop is implemented.ResultsWe performed an empirical evaluation with several model-checking tools (i.e., NuSMV and CadenceSMV), and the results show that the proposed method is more efficient at runtime. We also investigated a simple example application in six scenarios related to the IoT environment. We implemented Android and Arduino applications, and the results show the practical usability of the proposed self-adaptive framework at runtime.ConclusionsWe proposed a framework for integrating model checking with a self-adaptive software lifecycle. The results of our experiments showed that the proposed framework can achieve verify self-adaptation software at runtime."
Review article - A systematic literature review of software requirements reuse approaches,"AbstractContextEarly software reuse is considered as the most beneficial form of software reuse. Hence, previous research has focused on supporting the reuse of software requirements.ObjectiveThis study aims to identify and investigate the current state of the art with respect to (a) what requirement reuse approaches have been proposed, (b) the methods used to evaluate the approaches, (c) the characteristics of the approaches, and (d) the quality of empirical studies on requirements reuse with respect to rigor and relevance.MethodWe conducted a systematic review and a combination of snowball sampling and database search have been used to identify the studies. The rigor and relevance scoring rubric has been used to assess the quality of the empirical studies. Multiple researchers have been involved in each step to increase the reliability of the study.ResultsSixty-nine studies were identified that describe requirements reuse approaches. The majority of the approaches used structuring and matching of requirements as a method to support requirements reuse and text-based artefacts were commonly used as an input to these approaches. Further evaluation of the studies revealed that the majority of the approaches are not validated in the industry. The subset of empirical studies (22 in total) was analyzed for rigor and relevance and two studies achieved the maximum score for rigor and relevance based on the rubric. It was found that mostly text-based requirements reuse approaches were validated in the industry.ConclusionFrom the review, it was found that a number of approaches already exist in literature, but many approaches are not validated in industry. The evaluation of rigor and relevance of empirical studies show that these do not contain details of context, validity threats, and the industrial settings, thus highlighting the need for the industrial evaluation of the approaches."
Research article - Adapting automated test generation to GUI testing of industry applications,"AbstractContextAutomated test generation promises to improve the effectiveness of software testing and to reduce the involved manual effort. While automated test generation has been successfully applied for code-level API testing, it has not found widespread adoption in practice for testing of graphical user interfaces. Tools for test generation do not support GUI testing out-of-the-box but require dedicated extensions.ObjectiveThis paper explores the applicability of automated test generation for testing GUIs of industry applications. We propose a test adapter approach to bridge the gap between automated test generation tools and industry applications.MethodA multiple case study was conducted in which automated test generation with test adapters has been applied at the unit, integration, and system test level in three industry projects from two different companies.ResultsAutomated test generation via test adapters could be applied at all test levels. It has led to an increase of coverage as well as the detection of new defects that were not found by preceding testing activities in the projects. While test adapters can easily be implemented at the unit test level, their complexity and the corresponding effort for providing adapter implementations rises at higher test levels.ConclusionTest adapters can be used for applying automated test generation for testing GUIs of industry applications. They bridge the gap between automated test generation tools and industry applications. The development of test adapters requires no tool-specific knowledge and can be performed by members of the development team."
Research article - A semi-automated framework for the identification and estimation of Architectural Technical Debt: A comparative case-study on the modularization of a software component,"AbstractContextResearch and industry's attention has been focusing on developing systems that enable fast time to market in the short term, but would assure a sustainable delivery of business value and maintenance operations in the long run. A related phenomenon has been identified in Architectural Technical Debt: if the system architecture is sub-optimal for long-term business goals, it might need to be refactored. A key property of the system assuring long-term goals is its modularity, or else the degree to which components are decoupled: such property allows the product to be evolved without costly changes pervading the whole system. However, understanding the business benefits of refactoring to achieve modularity is not trivial, especially for large refactorings involving substantial architectural changes.ObjectiveThe aim of this study was to develop a technique to identify Architectural Technical Debt in the form of a non-modularized component and to quantify the convenience of its repayment.MethodWe have conducted a single, embedded case study in a large company, comparing a component before and after it was refactored to achieve modularity. We have developed a holistic framework for the semi-automated identification and estimation of Architectural Technical Debt in the form of non-modularized components. We then evaluate the technique reporting a comparative study of the difference in maintenance and development costs in two coexisting systems, one including the refactored component and one including the non-refactored one.ResultsThe main contributions are a measurement system for the identification of the Architectural Technical Debt according to the stakeholders’ goals, a mathematical relationship for calculating and quantifying its interest in terms of extra-effort spent in additional development and maintenance, and an overall decision framework to assess the benefit of refactoring. We also report context-specific results that show the estimated benefits of refactoring the specific case of Architectural Technical Debt.ConclusionWe found that it is possible to identify this kind of Architectural Technical Debt and to quantify its repayment convenience. Thanks to the developed framework, it was possible to estimate that the Architectural Technical Debt present in the component was causing substantial continuous extra-effort, and that the modularization would be repaid in several months of development and maintenance."
