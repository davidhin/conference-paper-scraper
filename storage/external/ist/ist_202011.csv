title,abstract
Review article - Crowdsourced software testing: A systematic literature review,"AbstractContextCrowdsourced software testing (CST) refers to the use of crowdsourcing techniques in the domain of software testing. CST is an emerging area with its applications rapidly increasing in the last decade.ObjectiveA comprehensive review on CST has been conducted to determine the current studies aiming to improve and assess the value of using CST as well as the challenges identified by these evaluation studies.MethodWe conducted a systematic literature review on CST by searching six popular databases. We identified 50 primary studies that passed our quality assessment criteria and defined two research questions covering the aim of the study.ResultsThere are three main process activities that the current literature aims to improve, namely selection of suitable testers, reporting of defects, and validation of submitted defects. In addition, there are 23 CST evaluation studies and most of them involve a large group and real crowd. These studies have identified 27 different challenges encountered during the application of crowdsourcing in software testing.ConclusionsThe improvements achieved for the specific process activities in CST help explore other unexplored process activities. Similarly, knowing the characteristics of the evaluation studies can direct us on what other studies are worth investigating. Additionally, many of the challenges identified by the evaluation studies represent research problems that need better understanding and alternative solutions. This research also offers opportunities for practitioners to understand and apply new solutions proposed in the literature and the variations between them. Moreover, it provides awareness to the related parties regarding the challenges reported in the literature, which they may encounter during CST tasks."
Research article - BITA*: Business-IT alignment framework of multiple collaborating organisations,"AbstractContextBusinesses today must collaborate in a coordinated fashion. To collaborate, they must align their business processes and IT by complying to a common reference architecture. The common reference architecture that addresses their specific collaboration requirements is generally an adaptation of an existing generic reference architecture. However, a design framework for adapting reference architectures is lacking.ObjectiveIn this paper we propose a design framework for aligning business processes and IT across diverse collaborating organisations in order to derive a more specific reference architecture from a generic one.MethodWe developed the design framework using the guidelines of ISO/IEC/IEEE standard for modelling design viewpoints and validated it in a real-life business case study.ResultsWe developed an architectural design framework which we call BITA* that is composed of three coherent architectural design viewpoints. The BP2BP alignment viewpoint provides alignment modelling abstractions for business analysts to be used to align business collaboration processes. The IT2IT alignment viewpoint provides alignment modelling abstractions for software architects to be used to align distributed IT systems. The BP2IT alignment viewpoint provides alignment modelling abstractions for interdisciplinary teams of business and IT specialists for aligning the mapping of business collaboration processes and the underlying distributed IT. The modelling abstractions are applied in a case study to derive a reference architecture for meat supply chain transparency systems.ConclusionA key challenge in developing the design framework is the difficulty of comparing models of business processes and IT that come from diverse organisations. Our main contribution is the set of modelling abstractions, which enabled us to represent business processes and IT in a uniform and comparable manner, and the systematic approach for applying the modelling abstractions. The framework is applied in the agri-food sector and needs to be evaluated further in multiple case studies from various application domains."
Research article - From software architecture to analysis models and back: Model-driven refactoring aimed at availability improvement,"AbstractContextWith the ever-increasing evolution of software systems, their architecture is subject to frequent changes due to multiple reasons, such as new requirements. Appropriate architectural changes driven by non-functional requirements are particularly challenging to identify because they concern quantitative analyses that are usually carried out with specific languages and tools. A considerable number of approaches have been proposed in the last decades to derive non-functional analysis models from architectural ones. However, there is an evident lack of automation in the backward path that brings the analysis results back to the software architecture.ObjectiveIn this paper, we propose a model-driven approach to support designers in improving the availability of their software systems through refactoring actions.MethodThe proposed framework makes use of bidirectional model transformations to map UML models onto Generalized Stochastic Petri Nets (GSPN) analysis models and vice versa. In particular, after availability analysis, our approach enables the application of model refactoring, possibly based on well-known fault tolerance patterns, aimed at improving the availability of the architectural model.ResultsWe validated the effectiveness of our approach on an Environmental Control System. Our results show that the approach can generate: (i) an analyzable availability model from a software architecture description, and (ii) valid software architecture models back from availability models. Finally, our results highlight that the application of fault tolerance patterns significantly improves the availability in each considered scenario.ConclusionThe approach integrates bidirectional model transformation and fault tolerance techniques to support the availability-driven refactoring of architectural models. The results of our experiment showed the effectiveness of the approach in improving the software availability of the system."
Research article - A microservice composition approach based on the choreography of BPMN fragments,"AbstractContextMicroservices must be composed to provide users with complex and elaborated functionalities. It seems that the decentralized nature of microservices makes a choreography style more appropriate to achieve such cooperation, where lighter solutions based on asynchronous events are generally used. However, a microservice composition based on choreography distributes the flow logic of the composition among microservices making further analysis and updating difficult, i.e. there is not a big picture of the composition that facilitates these tasks. Business Process Model and Notation (BPMN) is the OMG standard developed to represent Business Processes (BPs), being widely used to define the big picture of such compositions. However, BPMN is usually considered in orchestration-based solutions, and orchestration can be a drawback to achieve the decoupling pursued by a microservice architecture.ObjectiveDefining a microservice composition approach that allows us to create a composition in a BPMN model, which facilitates further analysis for taking engineering decisions, and execute them through an event-based choreography to have a high degree of decoupling and independence among microservices.MethodWe followed a research methodology for information systems that consists of a 5-step process: awareness of the problem, suggestion, development, evaluation, and conclusion.ResultsWe presented a microservice composition approach based on the choreography of BPMN fragments. On the one hand, we propose to describe the big picture of the composition with a BPMN model, providing a valuable mechanism to analyse it when engineering decisions need to be taken. On the other hand, this model is split into fragments in order to be executed through an event-based choreography form, providing the high degree of decoupling among microservices demanded in this type of architecture. This composition approach is supported by a microservice architecture defined to achieve that both descriptions of a composition (big picture and split one) coexist. A realization of this architecture in Java/Spring technology is also presented.ConclusionsThe evaluation that is done to our work allows us to conclude that the proposed approach for composing microservices is more efficient than solutions based on ad-hoc development."
Research article - Type slicing: An accurate object oriented slicing based on sub-statement level dependence graph,"AbstractContextProgram slicing is very useful in program analysis and software engineering. It computes the slice, which is a part of program and contains all the statements related to the given slicing criterion. The more accurate a slicing technique could be, the smaller the slice is.ObjectiveThis paper aims to improve the current slicing accuracy for object-oriented programs. The slicing accuracy is mainly related to three factors, the dependency graph (which extracts the inner relationships of source code), the slicing criterion (which determines the slicing requirement), and the slicing algorithm (which computes the slice for the criterion from the dependency graph).MethodOur method consists of three parts. First, we present a Sub-Statement Level Dependence Graph (SSLDG), which computes finer-grained dependences for object-oriented programs than mostly-used statement level graph. Second, we present a new type slicing criterion called (Sub-statement) Type Slicing Criterion (STSC), which supports the user to specify not only the statement and object variable, but also the type of object among its polymorphic types. At last, a corresponding slicing algorithm called (Sub-statement) Type Slicing (STS) is designed to perform the slicing process.ResultsWe implement STS on Java programs as MyJavaSlicer, and run it with ten open source projects and random slicing criteria. The results show that STS slicing algorithm as well as SSLDG would make slices 35.90% smaller than traditional two-phase slicing; additionally using STSC would make the slices 48.4% further smaller; STSC also helps traditional two-phase slices reduced by 56.90%.ConclusionsSTS could provide more accurate slices than traditional two-phase slicing, and it also runs faster on most cases; STSC helps specify the slicing requirement, and roughly halves the size of slices for both slicing algorithms."
Research article - Automated model-based performance analysis of software product lines under uncertainty,"AbstractContext: A Software Product Line (SPL) can express the variability of a system through the specification of configuration options. Evaluating performance characteristics, such as the system response time and resource utilization, of a software product is challenging, even more so in the presence of uncertain values of the attributes.Objective: The goal of this paper is to automate the generation of performance models for software products derived from the feature model by selection heuristics. We aim at obtaining model-based predictive results to quantify the correlation between the features, along with their uncertainties, and the system performance. This way, software engineers can be informed on the performance characteristics before implementing the system.Method: We propose a tool-supported framework that, starting from a feature model annotated with performance-related characteristics, derives Queueing Network (QN) performance models for all the products of the SPL. Model-based performance analysis is carried out on the models obtained by selecting the products that show the maximum and minimum performance-based costs.Results: We applied our approach to almost seven thousand feature models including more than one hundred and seventy features. The generation of QN models is automatically performed in much less than one second, whereas their model-based performance analysis embeds simulation delays and requires about six minutes on average.Conclusion: The experimental results confirm that our approach can be effective on a variety of systems for which software engineers may be provided with early insights on the system performance in reasonably short times. Software engineers are supported in the task of understanding the performance bounds that may encounter when (de)selecting different configuration options, along with their uncertainties."
Research article - PostFinder: Mining Stack Overflow posts to support software developers,"AbstractContext – During the development of complex software systems, programmers look for external resources to understand better how to use specific APIs and to get advice related to their current tasks. Stack Overflow provides developers with a broader insight into API usage as well as useful code examples. Given the circumstances, tools and techniques for mining Stack Overflow are highly desirable. Objective – In this paper, we introduce PostFinder, an approach that analyzes the project under development to extract suitable context, and allows developers to retrieve messages from Stack Overflow being relevant to the API function calls that have already been invoked. Method – PostFinder augments posts with additional data to make them more exposed to queries. On the client side, it boosts the context code with various factors to construct a query containing information needed for matching against the stored indexes. Multiple facets of the data available are used to optimize the search process, with the ultimate aim of recommending highly relevant SO posts. Results – The approach has been validated utilizing a user study involving a group of 12 developers to evaluate 500 posts for 50 contexts. Experimental results indicate the suitability of PostFinder to recommend relevant Stack Overflow posts and concurrently show that the tool outperforms a well-established baseline. Conclusions – We conclude that PostFinder can be deployed to assist developers in selecting relevant Stack Overflow posts while they are programming as well as to replace the module for searching posts in a code-to-code search engine."
Research article - Cloud applications monitoring: An industrial study,"AbstractContextModern software systems employ large IT infrastructures hosted in on-premise clouds or using “rented” cloud resources from specific vendors. The unifying force across any cloud strategy is incremental product and application improvement against conservation of those resources. This is where monitoring of cloud applications becomes a key assetObjectiveTo shed light over the status of monitoring practices in industry, we study: (a) monitoring practices and tools adoption in industry; (b) size and complexity of industrial monitoring problems; (c) the role of software architecture and software process with respect to monitoring strategies.MethodWe conduct mixed-methods empirical research featuring interviews and a web survey featuring 140+ practitioners from over 70 different organizations.ResultsEven if the market makes available a significant set of monitoring tools, our results show a rather unappealing picture of industrial monitoring: (a) industrial decision-makers do not perceive monitoring as a key asset even though the downtime of their applications correlates heavily with the level of automation and responsiveness enabled by monitoring; (b) monitoring is done with crude technology, mostly MySQL querying or similar (e.g., Nagios); finally, (c) incidents are discovered by clients rather than application owners.ConclusionWe conclude that the road toward the industrial adoption of cutting-edge monitoring technology is still one of the less travelled, presumably in connection to the considerable investment required. Furthermore, the lack of industrial cloud monitoring standards does not help in addressing the proliferation of multiple tool combinations, with varying effectiveness. Further research should be invested in looking into and addressing these major concerns."
Research article - Towards automatically generating block comments for code snippets,"AbstractCode commenting is a common programming practice of practical importance to help developers review and comprehend source code. There are two main types of code comments for a method: header comments that summarize the method functionality located before a method, and block comments that describe the functionality of the code snippets within a method. Inspired by the effectiveness of deep learning techniques in the NLP field, many studies focus on using the machine translation model to automatically generate comment for the source code. Because the data set of block comments is difficult to collect, current studies focus more on the automatic generation of header comments than that of block comments. However, block comments are important for program comprehension due to their explanation role for the code snippets in a method. To fill the gap, we have proposed an approach that combines heuristic rules and learning-based method to collect a large number of comment-code pairs from 1,032 open source projects in our previous study. In this paper, we propose a reinforcement learning-based method, RL-BlockCom, to automatically generate block comments for code snippets based on the collected comment-code pairs. Specifically, we utilize the abstract syntax tree (i.e., AST) of a code snippet to generate a token sequence with a statement-based traversal way. Then we propose a composite learning model, which combines the actor-critic algorithm of reinforcement learning with the encoder-decoder algorithm, to generate block comments. On the data set of the comment-code pairs, the BLEU-4 score of our method is 24.28, which outperforms the baselines and state-of-the-art in comment generation."
Research article - The impact of using a domain language for an agile requirements management,"AbstractContext: The development of software systems is a complex activity because of its nature and the management of its construction. It is challenging to create and follow a plan. Moreover, budget overrun is a common consequence of this situation. Agile methods, like Scrum, help to mitigate this problem using incremental and iterative development. Agile methods jump start new developments, but it is difficult to be agile after several months when the software has to deal with many requirements that are scattered and tangled across several User Stories written in different Sprints. Objective: In this paper, we propose a traceability approach anchored on an index structure to access specific User Stories from a large set. Our proposed strategy has the goal to consolidate the information dispersed in different User Stories into a particular lexicon: The Language Extended Lexicon (LEL). Method: The proposed approach consists of a set of rules which extract the information dispersed in the User Stories and organize it in symbols of the Lexicon. Thus, the Lexicon supplies a consolidated and organized structure to mitigate the problem of tangled information that generates lack of traceability among different sprints. Results: We assessed how the Lexicon built by our approach improves everyday activities related to requirement management. The assessment is based on a quantitative evaluation with 36 subjects. Conclusion: The approach presents benefits for requirement tracing in agile methodologies supported by the preliminary results of the evaluation. We have developed an application (a prototype) that automates the LEL derivation rules from a set of User Stories."
Research article - Effectiveness of Kotlin vs. Java in android app development tasks,"AbstractContext:Kotlin is a new programming language representing an alternative to Java; they both target the same JVM and can safely coexist in the same application. Kotlin is advertised as capable to solve several known limitations of Java. Recent surveys show that Kotlin achieved a relevant diffusion among Java developers. Goal:We planned to empirically assess a few typical promises of Kotlin w.r.t. known Java’s limitations, in terms of development effectiveness, maintainability, and ease of development. Method:Our experiment involved 27 teams of 4 people each that completed a set of maintenance tasks (both defect correction and feature addition) on Android apps written in either Java or Kotlin. In addition to the number of fixed defects, effort, and code size, we collected, though a questionnaire, the participants’ perceptions about the avoidance of known pitfalls. Results:We did not observe any significant difference in terms of maintainability between the two languages.We found a significant difference regarding the amount of code written, which constitutes evidence of better conciseness of Kotlin. Concerning ease of development, the frequency of NullPointerExceptions reported by the subjects was significantly lower when developing in Kotlin. On the other hand, no significant difference was found in the occurrence of other common Java pitfalls. Finally, the IDE support was deemed better for Java than Kotlin. Conclusions:Some of the promises of Kotlin to be a ”better Java” have been confirmed by our empirical assessment. Evidence suggests that the effort in transitioning to Kotlin can provide some advantages to Java developers, especially regarding code conciseness.Our results may serve as the basis for further investigations on the properties of the language."
Research article - Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions,"AbstractBackground: Developing and maintaining large scale machine learning (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems.Objective: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges.Method: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four quality attributes: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment.Results: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions.Conclusion: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in ML system development practice and the lack of solutions point to directions for future research."
Research article - Guidelines for the search strategy to update systematic literature reviews in software engineering,"AbstractContextSystematic Literature Reviews (SLRs) have been adopted within Software Engineering (SE) for more than a decade to provide meaningful summaries of evidence on several topics. Many of these SLRs are now potentially not fully up-to-date, and there are no standard proposals on how to update SLRs in SE.ObjectiveThe objective of this paper is to propose guidelines on how to best search for evidence when updating SLRs in SE, and to evaluate these guidelines using an SLR that was not employed during the formulation of the guidelines.MethodTo propose our guidelines, we compare and discuss outcomes from applying different search strategies to identify primary studies in a published SLR, an SLR update, and two replications in the area of effort estimation. These guidelines are then evaluated using an SLR in the area of software ecosystems, its update and a replication.ResultsThe use of a single iteration forward snowballing with Google Scholar, and employing as a seed set the original SLR and its primary studies is the most cost-effective way to search for new evidence when updating SLRs. Furthermore, the importance of having more than one researcher involved in the selection of papers when applying the inclusion and exclusion criteria is highlighted through the results.ConclusionsOur proposed guidelines formulated based upon an effort estimation SLR, its update and two replications, were supported when using an SLR in the area of software ecosystems, its update and a replication. Therefore, we put forward that our guidelines ought to be adopted for updating SLRs in SE."
"Research article - The Symposium on Search-Based Software Engineering: Past, Present and Future","AbstractContextSearch-Based Software Engineering (SBSE) is the research field where Software Engineering (SE) problems are modelled as search problems to be solved by search-based techniques. The Symposium on Search Based Software Engineering (SSBSE) is the premier event on SBSE, which had its 11th edition in 2019.ObjectiveIn order to better understand the characteristics and evolution of papers published at SSBSE, this work reports results from a mapping study targeting the proceedings of all SSBSE editions. Despite the existing mapping studies on SBSE, our contribution in this work is to provide information to researchers and practitioners willing to enter the SBSE field, being a source of information to strengthen the symposium, guide new studies, and motivate new collaboration among research groups.MethodA systematic mapping study was conducted with a set of four research questions, in which 134 studies published in all editions of SSBSE, dated from 2009 to 2019, were evaluated. In a fifth question, 32 papers published in the challenge track were summarised.ResultsThroughout the years, 290 authors from 25 countries have contributed to the main track of the symposium, with the collaboration of at least two institutions in 46.3% of the papers. SSBSE papers have got substantial external visibility, as most citations are from different venues. The SE tasks addressed by SSBSE are mostly related to software testing, software debugging, software design, and maintenance. Evolutionary algorithms are present in 75% of the papers, being the most common search technique. The evaluation of the SBSE approaches usually includes industrial systems.ConclusionsSSBSE has helped increase the popularity of SBSE in the SE research community and has played an important role on making SBSE mature. There are still problems and challenges to be addressed in the SBSE field, which can be tackled by SSBSE authors in further studies."
