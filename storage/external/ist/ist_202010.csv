title,abstract
Review article - NLP-assisted software testing: A systematic mapping of the literature,"AbstractContextTo reduce manual effort of extracting test cases from natural-language requirements, many approaches based on Natural Language Processing (NLP) have been proposed in the literature. Given the large amount of approaches in this area, and since many practitioners are eager to utilize such techniques, it is important to synthesize and provide an overview of the state-of-the-art in this area.ObjectiveOur objective is to summarize the state-of-the-art in NLP-assisted software testing which could benefit practitioners to potentially utilize those NLP-based techniques. Moreover, this can benefit researchers in providing an overview of the research landscape.MethodTo address the above need, we conducted a survey in the form of a systematic literature mapping (classification). After compiling an initial pool of 95 papers, we conducted a systematic voting, and our final pool included 67 technical papers.ResultsThis review paper provides an overview of the contribution types presented in the papers, types of NLP approaches used to assist software testing, types of required input requirements, and a review of tool support in this area. Some key results we have detected are: (1) only four of the 38 tools (11%) presented in the papers are available for download; (2) a larger ratio of the papers (30 of 67) provided a shallow exposure to the NLP aspects (almost no details).ConclusionThis paper would benefit both practitioners and researchers by serving as an “index” to the body of knowledge in this area. The results could help practitioners utilizing the existing NLP-based techniques; this in turn reduces the cost of test-case design and decreases the amount of human resources spent on test activities. After sharing this review with some of our industrial collaborators, initial insights show that this review can indeed be useful and beneficial to practitioners."
Research article - Engineering human-in-the-loop interactions in cyber-physical systems,"AbstractContext: Cyber-Physical Systems (CPSs) are gradually and widely introducing autonomous capabilities into everything. However, human participation is required to accomplish tasks that are better performed with humans (often called human-in-the-loop). In this way, human-in-the-loop solutions have the potential to handle complex tasks in unstructured environments, by combining the cognitive skills of humans with autonomous systems behaviors.Objective: The objective of this paper is to provide appropriate techniques and methods to help designers analyze and design human-in-the-loop solutions. These solutions require interactions that engage the human, provide natural and understandable collaboration, and avoid disturbing the human in order to improve human experience.Method: We have analyzed several works that identified different requirements and critical factors that are relevant to the design of human-in-the-loop solutions. Based on these works, we have defined a set of design principles that are used to build our proposal. Fast-prototyping techniques have been applied to simulate the designed human-in-the-loop solutions and validate them.Results: We have identified the technological challenges of designing human-in-the-loop CPSs and have provided a method that helps designers to identify and specify how the human and the system should work together, focusing on the control strategies and interactions required.Conclusions: The use of our approach facilitates the design of human-in-the-loop solutions. Our method is practical at earlier stages of the software life cycle since it allows domain experts to focus on the problem and not on the solution."
Research article - Refactoring effect on internal quality attributes: What haven’t they told you yet?,"AbstractContextCode refactoring was conceived for enhancing code structures, often in terms of internal quality attributes such as cohesion and coupling. Developers may have to apply multiple refactoring operations to achieve the expected enhancement. Re-refactoring occurs whenever one or more refactoring operations are performed on a previously refactored code element. The literature often assumes each single refactoring improves rather than worsens internal quality attributes, while re-refactoring implies further improvements. Unfortunately, quantitative evidence on this matter is scarce if not nonexistent.ObjectiveThis paper extends a large quantitative study about the refactoring effect on internal quality attributes with new insights, plus an unprecedented re-refactoring effect analysis. We particularly investigate if re-refactoring operations are more effective in improving attributes when compared to single operations.MethodWe analyzed 23 open software projects with 29,303 refactoring operations, from which nearly 50% constitute re-refactorings. We assessed five attributes: cohesion, complexity, coupling, inheritance, and size. We combined descriptive analysis and statistical tests to deeply understand the effect of both refactoring and re-refactoring on each attribute.ResultsContrary to current knowledge, our study revealed that 90% of refactoring operations, and 100% of re-refactoring operations, were applied to code elements with at least one critical attribute. Critical attribute is an attribute whose metrics used for computing it have anomalous values, e.g. high coupling. Most operations (65%) improve attributes presumably associated with the refactoring type applied; the other operations (35%) keep those attributes unaffected. Whenever refactoring and re-refactoring operations are applied without additional changes, i.e., root-canal refactoring, attributes tend to improve or at least not worsen. Surprisingly, if these operations occur with additional changes such as feature additions, i.e., floss refactoring, they mostly improve rather than worsen attributes.ConclusionsBesides revealing the effect of refactoring and re-refactoring on each attribute, we derived insights on leveraging the current refactoring practices."
Research article - Using simulated annealing for locating array construction,"AbstractContextCombinatorial interaction testing is known to be an efficient testing strategy for computing and information systems. Locating arrays are mathematical objects that are useful for this testing strategy, as they can be used as a test suite that permits fault localization as well as fault detection. In this application, each row of an array is used as an individual test.ObjectiveThis paper proposes an algorithm for constructing locating arrays with a small number of rows. Testing cost increases as the number of tests increases; thus the problem of finding locating arrays of small sizes is of practical importance.MethodThe proposed algorithm uses simulated annealing, a meta-heuristic algorithm, to find locating array of a given size. The whole algorithm repeatedly executes the simulated annealing algorithm with the input array size being dynamically varied.ResultsExperimental results show (1) that the proposed algorithm is able to construct locating arrays for problem instances of large sizes and (2) that, for problem instances for which nontrivial locating arrays are known, the algorithm is often able to generate locating arrays that are smaller than or at least equal to the known arrays.ConclusionBased on the results, we conclude that the proposed algorithm can produce small locating arrays and scale to practical problems."
"Research article - Early prediction of quality of service using interface-level metrics, code-level metrics, and antipatterns","AbstractContext: With the current high trends of deploying and using web services in practice, effective techniques for maintaining high quality of Service are becoming critical for both service providers and subscribers/users. Service providers want to predict the quality of service during early stages of development before releasing them to customers. Service clients consider the quality of service when selecting the best one satisfying their preferences in terms of price/budget and quality between the services offering the same features. The majority of existing studies for the prediction of quality of service are based on clustering algorithms to classify a set of services based on their collected quality attributes. Then, the user can select the best service based on his expectations both in terms of quality and features. However, this assumption requires the deployment of the services before being able to make the prediction and it can be time-consuming to collect the required data of running web services during a period of time. Furthermore, the clustering is only based on well-known quality attributes related to the services performance after deployment. Objective: In this paper, we start from the hypothesis that the quality of the source code and interface design can be used as indicators to predict the quality of service attributes without the need to deploy or run the services by the subscribers. Method: We collected training data of 707 web services and we used machine learning to generate association rules that predict the quality of service based on the interface and code quality metrics, and antipatterns. Results: The empirical validation of our prediction techniques shows that the generated association rules have strong support and high confidence which confirms our hypothesis that source code and interface quality metrics/antipatterns are correlated with web service quality attributes which are response time, availability, throughput, successability, reliability, compliance, best practices, latency, and documentation. Conclusion: To the best of our knowledge, this paper represents the first study to validate the correlation between interface metrics, source code metrics, antipatterns and quality of service. Another contribution of our work consists of generating association rules between the code/interface metrics and quality of service that can be used for prediction purposes before deploying new releases."
Research article - Understanding the relationship of conflict and success in software development projects,"AbstractContextSoftware development incorporates numerous people with diverse expertise and expectations. This makes conflict a common phenomenon in software development. Besides human causes, many conflicts in software development root in the tools and processes. Moreover, the growing role of software in any type of system is increasing the heterogeneity in software projects. The number and variety of tools and processes are increasing. Nevertheless, the relationship between conflicts, particularly rooted in non-human elements, and software project success is still unclear.ObjectiveWe aim to understand the impact of conflict on the success of software development projects for different types of conflict and different environments. Particularly, we distinguish between human-rooted conflict (HRC) and non-human-rooted conflict (NHRC). Moreover, we investigate whether organization size and team size moderate the impact of conflict on software project success.MethodsFirst, we conduct a survey and analyze it using structural equation modeling (SEM) to investigate any correlation between conflict and software project success. Second, we explore the reasons behind the relationship between conflict and software project success by conducting 13 semi-structured expert interviews.ResultsHRC is always a threat to software project success for any organization or team size. Based on the interviews, resolving an HRC is regularly problematic. On the other hand, NHRC is negatively correlated with software project success only in corporate organizations and small teams. High coordination overhead and dependency on tools and processes make NHRC more influential in corporate organizations. In contrast, overlooking non-human elements and lack of experienced individuals in smaller teams make them more vulnerable to NHRC.ConclusionWhile the detrimental impact of HRC is constant for software project success, NHRC can be controlled efficiently. Corporate organizations need to frequently improve the non-human elements in the development. Smaller teams should expect tools and processes to be significantly influential in their success."
Research article - Investigating the relationship between personalities and agile team climate of software professionals in a telecom company,"AbstractContextPrevious research found that the performance of a team not only depends on the team personality composition, but also on the interactive effects of team climate. Although investigation on personalities associated with software development has been an active research area over the past decades, there has been very limited research in relation to team climate.ObjectiveOur study investigates the association between the five factor model personality traits (openness to experience, conscientiousness, extraversion, agreeableness and neuroticism) and the factors related to team climate (team vision, participative safety, support for innovation and task orientation) within the context of agile teams working in a telecom company.MethodA survey was used to gather data on personality characteristics and team climate perceptions of 43 members from eight agile teams. The data was initially used for correlation analysis; then, regression models were developed for predicting the personality traits related to team climate perception.ResultsWe observed a statistically significant positive correlation between openness to experience and support for innovation (r = 0.31). Additionally, agreeableness was observed to be positively correlated with overall team climate (r = 0.35). Further, from regression models, we observed that personality traits accounted to less than 15% of the variance in team climate.ConclusionA person's ability to easily get along with team members (agreeableness) has a significant positive influence on the perceived level of team climate. Results from our regression analysis suggest that further data may be needed, and/or there are other human factors, in addition to personality traits, that should also be investigated with regard to their relationship with team climate. Overall, the relationships identified in our study are likely to be applicable to organizations within the telecommunications domain that use scrum methodology for software development."
Research article - Recommending refactorings via commit message analysis,"AbstractContextThe purpose of software restructuring, or refactoring, is to improve software quality and developer productivity.ObjectivePrior studies have relied mainly on static and dynamic analysis of code to detect and recommend refactoring opportunities, such as code smells. Once identified, these smells are fixed by applying refactorings which then improve a set of quality metrics. While this approach has value and has shown promising results, many detected refactoring opportunities may not be related to a developer’s current context and intention. Recent studies have shown that while developers document their refactoring intentions, they may miss relevant refactorings aligned with their rationale.MethodIn this paper, we first identify refactoring opportunities by analyzing developer commit messages and check the quality improvements in the changed files, then we distill this knowledge into usable context-driven refactoring recommendations to complement static and dynamic analysis of code.ResultsThe evaluation of our approach, based on six open source projects, shows that we outperform prior studies that apply refactorings based on static and dynamic analysis of code alone.ConclusionThis study provides compelling evidence of the value of using the information contained in existing commit messages to recommend future refactorings."
Research article - Neural networks learn to detect and emulate sorting algorithms from images of their execution traces,"AbstractContextRecent advancements in the applicability of neural networks across a variety of fields, such as computer vision, natural language processing and others, have re-sparked an interest in program induction methods. (Kitzelman [1], Gulwani et al. [2] or Kant [3].)ProblemWhen performing a program induction task, it is not feasible to search across all possible programs that map an input to an output because the number of possible combinations or sequences of instructions is too high: at least an exponential growth based on the generated program length. Moreover, there does not exist a general framework to formulate such program induction tasks and current computational limitations do not allow a very wide range of machine learning applications in the field of computer programs generation.ObjectiveIn this study, we analyze the effectiveness of execution traces as learning representations for neural network models in a program induction set-up. Our goal is to generate visualizations of program execution dynamics, specifically of sorting algorithms, and to apply machine learning techniques on them to capture their semantics and emulate their behavior using neural networks.MethodWe begin by classifying images of execution traces for algorithms working on a finite array of numbers, such as various sorting and data structures algorithms. Next we experiment with detecting sub-program patterns inside the trace sequence of a larger program. The last step is to predict future steps in the execution of various sorting algorithms. More specifically, we try to emulate their behavior by observing their execution traces. We also discuss generalizations to other classes of programs, such as 1-D cellular automata.ResultsOur experiments show that neural networks are capable of modeling the mechanisms underlying simple algorithms if enough execution traces are provided as data. We compare the performance of our program induction model with other similar experimental results from Graves et al. [4] and Vinyals et al. [5]. We were also able to demonstrate that sorting algorithms can be treated both as images displaying spatial patterns, as well as sequential instructions in a domain specific language, such as swapping two elements. We tested our approach on three types of increasingly harder tasks: detection, recognition and emulation.ConclusionsWe demonstrate that simple algorithms can be modelled using neural networks and provide a method for representing specific classes of programs as either images or sequences of instructions in a domain-specific language, such that a neural network can learn their behavior. We consider the complexity of various set-ups to arrive at some improvements based on the data representation type. The insights from our experiments can be applied for designing better models of program induction."
Research article - An empirical evaluation of the use of models to improve the understanding of safety compliance needs,"AbstractContextCritical systems in application domains such as automotive, railway, aerospace, and healthcare are required to comply with safety standards. The understanding of the safety compliance needs specified in these standards can be difficult from their text. A possible solution is to use models.ObjectiveWe aim to evaluate the use of models to understand safety compliance needs.MethodWe have studied the effectiveness, efficiency, and perceived benefits in understanding these needs, with models and with the text of safety standards, by means of an experiment. The standards considered are DO-178C and EN 50128. We use SPEM-like diagrams to graphically represent the models.ResultsThe mean effectiveness of 20 undergraduate students in understanding the needs and the mean efficiency were higher with models (22% and 38%, respectively), and the difference is statistically significant (p-value ≤ 0.02). Most of the students agreed upon the ease of understanding the structure of safety compliance needs with models when compared to the text, but on average, the students were undecided about whether the models are easy to understand or easier to understand than the text.ConclusionsThe results allow us to claim that the use of models can improve the understanding of safety compliance needs. Nonetheless, there seems to be room for improvement in relation to the perceived benefits. It must be noted that our conclusions may differ if the subjects were experienced practitioners."
Research article - Requirements elicitation methods based on interviews in comparison: A family of experiments,"AbstractContextThere are several methods to elicit requirements through interviews between an end-user and a team of software developers. The choice of the best method in this context is usually on subjective developers’ preferences instead of objective reasons. There is a lack of empirical evaluations of methods to elicit requirements that help developers to choose the most suitable one.ObjectiveThis paper designs and conducts a family of experiments to compare three methods to elicit requirements: Unstructured Interviews, where there is no specific protocol or artifacts; Joint Application Design (JAD), where each member of the development team has a specific role; Paper Prototyping, where developers contrast the requirements with the end-user through prototypes.MethodThe experiment is a between-subjects design with next response variables: number of requirements, time, diversity, completeness, quality and performance. The experiment consists of a maximum of 4 rounds of interviews between students that play the role of developers and an instructor that plays the role of client. Subjects had to elaborate a requirements specification document as results of the interviews. We recruited 167 subjects in 4 replications in 3 years. Subjects were gathered in development teams of 6 developers at most, and each team was an experimental unit.ResultsWe found some significant differences. Paper Prototyping yields the best results to elicit as many requirements as possible, JAD requires the highest time to report the requirements and the least overlapping, and Unstructured Interviews yields the highest overlapping and the lowest time to report the requirements.ConclusionsPaper Prototyping is the most suitable for eliciting functional requirements, JAD is the most suitable for non-functional requirements and to avoid overlapping, Unstructured Interviews is the fastest but with poor quality in the results."
Research article - A statistical pattern based feature extraction method on system call traces for anomaly detection,"AbstractContextIn host-based anomaly detection, feature extraction on the system call traces is important to build an effective anomaly detection model. Different kinds of feature extraction methods are recently proposed and most of them aim at preserving the positional information of the system calls within a trace. These extracted features are generally named from system calls, therefore, cannot be used directly in the case of cross platform applications. In addition, some of these feature extraction methods are very costly to implement.ObjectiveThis paper presents a new feature extraction method. It aims at extracting features that are irrelevant to the names of system calls. The samples represented by the extracted features can be directly used in the case of cross platform applications. In addition, this method is lightweight in that the feature values are not expensive to compute.MethodThe proposed method firstly transforms the system calls in a trace into frequency sequences of n-grams and then explores a fixed number of statistical features on the frequency sequences. The extracted features are irrelevant to the names/indexes of system calls on a platform. The calculation of feature values works on the frequency sequences rather than on system call sequences. These feature vectors built on the training set with only normal data are then used to train a one class classification model for anomaly detection.ResultsWe compared our method with four previously proposed feature extraction methods on system call traces. When used on the same platform, even though our method does not always obtain the highest AUC, overall, it performs better than all the compared methods. When testing on cross platform, it performs the best among all compared methods.ConclusionThe features extracted by our method are platform-independent and are suitable for anomaly detection across platforms."
Research article - Effort-Aware semi-Supervised just-in-Time defect prediction,"AbstractContextSoftware defect prediction is an important technique that can help practitioners allocate their quality assurance efforts. In recent years, just-in-time (JIT) defect prediction has attracted considerable interest, as it enables developers to identify risky changes at check-in time.ObjectiveMany studies have conducted research from supervised and unsupervised perspectives. A model that does not rely on label information would be preferred. However, the performance of unsupervised models proposed by previous studies in the classification scenario was unsatisfactory due to the lack of supervised information. Furthermore, most supervised models fail to outperform simple unsupervised models in the ranking scenario. To overcome this weakness, we conduct research from the semi-supervised perspective that only requires a small quantity of labeled data for training.MethodIn this paper, we propose a semi-supervised model for JIT defect prediction named Effort-Aware Tri-Training (EATT), which is an effort-aware method using a greedy strategy to rank changes. We compare EATT with the state-of-the-art supervised and unsupervised models with respect to different labeled rate.ResultsThe experimental results on six open-source projects demonstrate that EATT outperforms existing supervised and unsupervised models for effort-aware JIT defect prediction, and has similar or superior performance in classifying defect-inducing changes.ConclusionThe results show that EATT can not only achieve high classification accuracy as supervised models, but also offer more practical value than other compared models from the perspective of the effort needed to review changes."
Research article - Efficient feature extraction model for validation performance improvement of duplicate bug report detection in software bug triage systems,"AbstractContextThere are many duplicate bug reports in the semi-structured software repository of various software bug triage systems. The duplicate bug report detection (DBRD) process is a significant problem in software triage systems.ObjectiveThe DBRD problem has many issues, such as efficient feature extraction to calculate similarities between bug reports accurately, building a high-performance duplicate detector model, and handling continuous real-time queries. Feature extraction is a technique that converts unstructured data to structured data. The main objective of this study is to improve the validation performance of DBRD using a feature extraction model.MethodThis research focuses on feature extraction to build a new general model containing all types of features. Moreover, it introduces a new feature extractor method to describe a new viewpoint of similarity between texts. The proposed method introduces new textual features based on the aggregation of term frequency and inverse document frequency of text fields of bug reports in uni-gram and bi-gram forms. Further, a new hybrid measurement metric is proposed for detecting efficient features, whereby it is used to evaluate the efficiency of all features, including the proposed ones.ResultsThe validation performance of DBRD was compared for the proposed features and state-of-the-art features. To show the effectiveness of our model, we applied it and other related studies to DBRD of the Android, Eclipse, Mozilla, and Open Office datasets and compared the results. The comparisons showed that our proposed model achieved (i) approximately 2% improvement for accuracy and precision and more than 4.5% and 5.9% improvement for recall and F1-measure, respectively, by applying the linear regression (LR) and decision tree (DT) classifiers and (ii) a performance of 91%−99% (average ~97%) for the four metrics, by applying the DT classifier as the best classifier.ConclusionOur proposed features improved the validation performance of DBRD concerning runtime performance. The pre-processing methods (primarily stemming) could improve the validation performance of DBRD slightly (up to 0.3%), but rule-based machine learning algorithms are more useful for the DBRD problem. The results showed that our proposed model is more effective both for the datasets for which state-of-the-art approaches were effective (i.e., Mozilla Firefox) and those for which state-of-the-art approaches were less effective (i.e., Android). The results also showed that the combination of all types of features could improve the validation performance of DBRD even for the LR classifier with less validation performance, which can be implemented easily for software bug triage systems. Without using the longest common subsequence (LCS) feature, which is effective but time-consuming, our proposed features could cover the effectiveness of LCS with lower time-complexity and runtime overhead. In addition, a statistical analysis shows that the results are reliable and can be generalized to other datasets or similar classifiers."
Research article - Simplifying the Search of npm Packages,"AbstractContextCode reuse, generally done through software packages, allows developers to reduce time-to-market and improve code quality. The npm ecosystem is a Node.js package management system which contains more than 700 K Node.js packages and to help developers find high-quality packages that meet their needs, npms developed a search engine to rank Node.js packages in terms of quality, popularity, and maintenance. However, the current ranking mechanism for npms tends to be arbitrary and contains many different equations, which increases complexity and computation.ObjectiveThe goal of this paper is to empirically improve the efficiency of npms by simplifying the used components without impacting the current npms package ranks.MethodWe use feature selection methods with the aim of simplifying npms’ equations. We remove the features that do not have a significant effect on the package’s rank. Then, we study the impact of the simplified npms on the packages’ rank, the amount of resources saved compared to the original npms, and the performance of the simplified npms as npm evolves.ResultsOur findings indicate that (1) 31% of the unique variables of npms’ equation can be removed without breaking the original packages’ ranks; (2) The simplified npms, on average, preserves the overlapping of the packages by 98% and the ranking of those packages by 97%; (3) Using the simplified npms saves 10% of packages scoring time and more than 1.47 million network requests on each scoring run; (4) As the npm evolve through a period of 12 months, the simplified-npms was able to achieve results similar to the original npms.ConclusionOur results show that the simplified npms preserves the original ranks of packages and is more efficient than the original npms. We believe that using our approach, helps the npms community speed up the scoring process by saving computational resources and time."
