title,abstract
Research article - Duplex output software effort estimation model with self-guided interpretation,"AbstractContextSoftware effort estimation (SEE) plays a key role in predicting the effort needed to complete software development task. However, the conclusion instability across learners has affected the implementation of SEE models. This instability can be attributed to the lack of an effort classification benchmark that software researchers and practitioners can use to facilitate and interpret prediction results.ObjectiveTo ameliorate the conclusion instability challenge by introducing a classification and self-guided interpretation scheme for SEE.MethodWe first used the density quantile function to discretise the effort recorded in 14 datasets into three classes (high, low and moderate) and built regression models for these datasets. The results of the regression models were an effort estimate, termed output 1, which was then classified into an effort class, termed output 2. We refer to the models generated in this study as duplex output models as they return two outputs. The introduced duplex output models trained with the leave-one-out cross validation and evaluated with MAE, BMMRE and adjusted R2, can be used to predict both the software effort and the class of software effort estimate. Robust statistical tests (Welch's t-test and Kruskal-Wallis H-test) were used to examine the statistical significant differences in the models’ prediction performances.ResultsWe observed the following: (1) the duplex output models not only predicted the effort estimates, they also offered a guide to interpreting the effort expended; (2) incorporating the genetic search algorithm into the duplex output model allowed the sampling of relevant features for improved prediction accuracy; and (3) ElasticNet, a hybrid regression, provided superior prediction accuracy over the ATLM, the state-of-the-art baseline regression.ConclusionThe results show that the duplex output model provides a self-guided benchmark for interpreting estimated software effort. ElasticNet can also serve as a baseline model for SEE."
Research article - Understanding metric-based detectable smells in Python software: A comparative study,"AbstractContextCode smells are supposed to cause potential comprehension and maintenance problems in software development. Although code smells are studied in many languages, e.g. Java and C#, there is a lack of technique or tool support addressing code smells in Python.ObjectiveDue to the great differences between Python and static languages, the goal of this study is to define and detect code smells in Python programs and to explore the effects of Python smells on software maintainability.MethodIn this paper, we introduced ten code smells and established a metric-based detection method with three different filtering strategies to specify metric thresholds (Experience-Based Strategy, Statistics-Based Strategy, and Tuning Machine Strategy). Then, we performed a comparative study to investigate how three detection strategies perform in detecting Python smells and how these smells affect software maintainability with different detection strategies. This study utilized a corpus of 106 Python projects with most stars on GitHub.ResultsThe results showed that: (1) the metric-based detection approach performs well in detecting Python smells and Tuning Machine Strategy achieves the best accuracy; (2) the three detection strategies discover some different smell occurrences, and Long Parameter List and Long Method are more prevalent than other smells; (3) several kinds of code smells are more significantly related to changes or faults in Python modules.ConclusionThese findings reveal the key features of Python smells and also provide a guideline for the choice of detection strategy in detecting and analyzing Python smells."
Review article - Authentication schemes and methods: A systematic literature review,"AbstractContextThere is a great variety of techniques for performing authentication, like the use of text passwords or smart cards. Some techniques combine others into one, which is known as multi-factor authentication. There is an interest in knowing existing authentication techniques, including those aimed at multi-factor authentication, and the frameworks that can be found in literature that are used to compare and select these techniques according to different criteria.ObjectiveThis article aims to gather the existing knowledge on authentication techniques and ways to discern the most effective ones for different contexts.MethodA systematic literature review is performed in order to gather existing authentication techniques proposed in literature and ways to compare and select them in different contexts. A total of 515 single-factor and 442 multi-factor authentication techniques have been found. Furthermore, 17 articles regarding comparison and selection criteria for authentication techniques and 8 frameworks that help in such a task are discussed.ResultsA great variety of single-factor techniques has been found and smart card-based authentication was shown to be the most researched technique. Similarly, multi-factor techniques combine the different single-factor techniques found and the combination of text-passwords and smart cards is the most researched technique. Usability, security and costs are the most used criteria for comparing and selecting authentication schemes, whereas the context is given an important remark as well. No framework among the ones found analyzed in detail both single-factor and multi-factor authentication techniques for the decision-making process.ConclusionThe review shows that a vast research has been done for authentication techniques, although its use in some contexts has not been researched as much. The lack of works regarding the comparison and selection of authentication techniques is observed."
Research article - Automatic feasible transition path generation from UML state chart diagrams using grouping genetic algorithms,"AbstractContextTransition coverage testing, a testing technique using state charts of Unified Modeling Language (UML), requires generation of transition paths that cover all transitions. However, if a generated path is infeasible due to internal variables then the transition will not be executed according to the input sequence, resulting in a test failure. Thus, feasible transition paths (FTPs) must be generated to run transition coverage tests.ObjectiveSeveral automatic transition path generation studies have been conducted using genetic algorithms (GAs), but when generating a transition path using a GA, the dependent transition pairs serve as distractions. Furthermore, counter problems that require repeated execution of dependent transitions (for example, to test a failing operation of an automatic teller machine, the password must be incorrect three times) make FTP generation more difficult.MethodIn this study, to address these issues, an automatic FTP generation method using a grouping GA (GGA) is described. Considering the characteristics of the problem, modification of the original GGA is proposed. A chromosome initialized using information from the state chart, and generating transition coverage, satisfied FTP while adjusting the length of the chromosome.ResultsAn experiment using the `inres initiator' state chart and the ‘ATM’ state chart generated FTPs successfully. In the case of the inres initiator state chart, the proposed GGA was shown to be capable of generating FTPs with a 100% success rate. In the case of the ATM state chart, the proposed GGA was shown to be capable of generating FTPs with a 100% success rate, by setting the maximum number of generations.ConclusionThe proposed GGA can be applied effectively to transition coverage testing using UML state charts, and can generate test paths suitable for testing purposes by setting the genetic parameter value and the maximum number of generations."
Research article - Feature-family-based reliability analysis of software product lines,"AbstractContextVerification techniques are being applied to ensure that software systems achieve desired quality levels and fulfill functional and non-functional requirements. However, applying these techniques to software product lines is challenging, given the exponential blowup of the number of products. Current product-line verification techniques leverage symbolic model checking and variability information to optimize the analysis, but still face limitations that make them costly or infeasible. In particular, state-of-the-art verification techniques for product-line reliability analysis are enumerative which hinders their applicability, given the latent exponential blowup of the configuration space.ObjectiveThe objectives of this paper are the following: (a) we present a method to efficiently compute the reliability of all configurations of a compositional or annotation-based software product line from its UML behavioral models, (b) we provide a tool that implements the proposed method, and (c) we report on an empirical study comparing the performance of different reliability analysis strategies for software product lines.MethodWe present a novel feature-family-based analysis strategy to compute the reliability of all products of a (compositional or annotation-based) software product line. The feature-based step of our strategy divides the behavioral models into smaller units that can be analyzed more efficiently. The family-based step performs the reliability computation for all configurations at once by evaluating reliability expressions in terms of a suitable variational data structure.ResultsOur empirical results show that our feature-family-based strategy for reliability analysis outperforms, in terms of time and space, four state-of-the-art strategies (product-based, family-based, feature-product-based, and family-product-based) for the same property. It is the only one that could be scaled to a 220-fold increase in the size of the configuration space.ConclusionOur feature-family-based strategy leverages both feature- and family-based strategies by taming the size of the models to be analyzed and by avoiding the products enumeration inherent to some state-of-the-art analysis methods."
Review article - Assumptions and their management in software development: A systematic mapping study,"AbstractContextAssumptions are constantly made by stakeholders or generated automatically in software development. However, there is a lack of systematic analysis and comprehensive understanding of the research and practice regarding assumptions and their management.ObjectiveThis work aims to explore and analyze the state of the art on assumptions and their management in software development.MethodA systematic mapping study that covers the literature from January 2001 to December 2015 on assumptions and their management in software development.Results134 studies were included: (1) The studies were published in 94 venues, which indicates that assumptions and their management has been a broad topic in software engineering. (2) Only 21 studies defined the assumption concept. (3) Most assumptions are made for or related to the artifacts in requirements engineering and software design, which demonstrates that assumptions should be managed from the early phases of software development. (4) Much effort has been put on Assumptions Making, Description, and Evaluation. Assumptions Maintenance received moderate attention. More than half of the tools identified aim to support assume-guarantee reasoning. For the other tools, most of them can be used to support Assumptions Description. (5) All the identified types of stakeholders are involved in Assumptions Making, followed by Evaluation and Description. Stakeholders involved in requirements engineering, software design, and software construction play a central role in assumptions management. (6) The main challenge is the difficulty of performing assumptions management activities in software development. (7) The identified assumptions management approaches, tools, benefits, and lessons learned are limited to their specific contexts (e.g., context of use). (8) Most of the negative consequences are caused by invalid or implicit assumptions.ConclusionsThis work provides researchers and practitioners with a reflection of the past fifteen years of research and practice on assumptions and their management in software development."
Review article - A systematic mapping study about socio-technical congruence,"AbstractContextLack of coordination may create significant problems between work teams, this problem is even most critical when team workers are geographically distributed as it results in cost increases and delays in the projects. There exists a technique called Socio-Technical Congruence (STC) that aims at helping to measure and control the coordination level existing in an organization at their different levels.ObjectiveThe objective of this paper is to carry out a systematic mapping of the field of socio-technical congruence. The aspects of particular interest for this article are: Socio-Technical Congruence definition, different ways to measure it, available tools to measure or that can help to measure it, the areas of application, its benefits and the case studies that analyze the effects Socio-Technical Congruence has on the organization as regards the products quality and the improvements in performance in the long term development, in an attempt to characterize the state of the art of this field identifying gaps and opportunities for further research. Therefore, companies could use this work as a starting point to apply STC measures in their work teams.ResultsThis paper presents the results of a systematic mapping of Literature about Socio-Technical Congruence (STC) in order to investigate and classify the existing articles and conferences about the subject, as well as summarizing the most important aspects in regards to provide a general overview about the existing studies.ConclusionsAfter analyzing the 40 papers found, we can conclude that there is no one standard measure of socio-technical congruence, although most take the proposal by Kwan et al. applying adaptations and improvements on it as regards the environment that it will be focused on. In general, most case studies talk about the benefits of STC control in organizations. However, only one paper focus on global software development where the problems of communication, coordination and control are an important risk. Moreover, there are only a few papers that explore the risks of excessively overloading users with coordination iterations when controlling STC. In fact, no case study to examine these risks and their effect on developers’ productivity has been found. The small number of studies found on STC, together with the research gaps we have pointed out, suggest that further investigation on socio-technical congruence is required."
Research article - Perspectives on usability guidelines for smartphone applications: An empirical investigation and systematic literature review,"AbstractContextSeveral usability guidelines have been proposed to improve the usability of smartphone apps. These guidelines can be classified into three disjoint sets: platform specific guidelines, genre specific guidelines, and generic guidelines. However, smartphone applications are usually developed for multiple platforms and targeted for a variety of users. Hence the usefulness of existing guidelines is severally limited.ObjectiveThis study aims to develop a comprehensive list of usability guidelines suitable for multiple platforms and genres of smartphone applications.MethodA controlled experiment was conducted, and it highlighted that even popular and established apps have usability problems. In order to identify different perspectives on usability a systematic literature review was conducted.ResultsSystematic literature review resulted in 148 studies that proposed a total of 359 usability guidelines. These guidelines were condensed into 25 guidelines in 7 categories by removing redundancy, repetition and similarity through a sequential and iterative process. Finally, usefulness of the proposed classification of guidelines is established by mapping these to usability issues identified earlier."
"Research article - An architecture, system engineering, and acquisition approach for space system software resiliency","AbstractContextSoftware-intensive space systems can harbor defects and vulnerabilities that may enable external adversaries or malicious insiders to disrupt or disable system functions, risking mission compromise or loss. Mitigating this risk demands a sustained focus on the security and resiliency of the system architecture including software, hardware, and other components.ObjectiveIn this paper we offer methodical approaches for improving space system resiliency through software architecture design, system engineering, and increased software security, thereby reducing the risk of latent software defects and vulnerabilities.MethodWe conducted a systematic review of existing architectural practices, standards, security and coding practices, various threats, defects, and vulnerabilities that impact space systems from hundreds of relevant publications and interviews of subject matter experts. We expanded on the system-level body of knowledge for resiliency and identified a new software architecture framework and acquisition methodology to improve the resiliency of space systems from a software perspective with an emphasis on the early phases of the systems engineering life cycle. This methodology involves seven steps: 1) Define technical resiliency requirements, 1a) Identify standards/policy for software resiliency, 2) Develop a request for proposal (RFP)/statement of work (SOW) for resilient space systems software, 3) Define software resiliency goals for space systems, 4) Establish software resiliency quality attributes, 5) Perform architectural tradeoffs and identify risks, 6) Conduct architecture assessments as part of the procurement process, and 7) Ascertain space system software architecture resiliency metrics.ResultsData illustrates that software vulnerabilities can lead to opportunities for malicious cyber activities, which could degrade the space mission capability for its user community. Reducing the number of vulnerabilities by improving architecture and software system engineering practices can contribute to making space systems more resilient.ConclusionSince cyber-attacks [1] are enabled by shortfalls in software, robust software engineering practices and an architectural design are foundational to resiliency, which is a quality that allows the system to take a hit to a critical component and recover in a known, bounded, and generally acceptable period of time. To achieve software resiliency for space systems, acquirers and suppliers must identify relevant factors and systems engineering practices to apply across the life cycle, in software requirements analysis, architecture development, design, implementation, verification and validation, and maintenance phases."
Research article - A tuned version of genetic algorithm for efficient test suite generation in interactive t-way testing strategy,"AbstractContextTo improve the quality and correctness of a software product it is necessary to test different aspects of the software system. Among different approaches for software testing, combinatorial testing along with covering array is a proper testing method. The most challenging problem in combinatorial testing strategies like t-way, is the combinatorial explosion which considers all combinations of input parameters. Many evolutionary and meta-heuristic strategies have been proposed to address and mitigate this problem.ObjectiveGenetic Algorithm (GA) is an evolutionary search-based technique that has been used in t-way interaction testing by different approaches. Although useful, all of these approaches can produce test suite with small interaction strengths (i.e. t ≤ 6). Additionally, most of them suffer from expensive computations. Even though there are other strategies which use different meta-heuristic algorithms to solve these problems, in this paper, we propose an efficient uniform and variable t-way minimal test suite generation approach to address these problems using GA, called Genetic Strategy (GS).MethodBy changing the bit structure and accessing test cases quickly, GS improves performance of the fitness function. These adjustments and reduction of the complexities of GA in the proposed GS decreases the test suite size and increases the speed of test suite generation up tot=20.ResultsTo evaluate the efficiency and performance of the proposed GS, various experiments are performed on different set of benchmarks. Experimental results show that not only GS supports higher interaction strengths in comparison with the existing GA-based strategies, but also its supported interaction strength is higher than most of other AI-based and computational-based strategies.ConclusionFurthermore, experimental results show that GS can compete against the existing (both AI-based and computational-based) strategies in terms of efficiency and performance in most of the case studies."
Research article - How to ask for technical help? Evidence-based guidelines for writing questions on Stack Overflow,"AbstractContextThe success of Stack Overflow and other community-based question-and-answer (Q&A) sites depends mainly on the will of their members to answer others’ questions. In fact, when formulating requests on Q&A sites, we are not simply seeking for information. Instead, we are also asking for other people's help and feedback. Understanding the dynamics of the participation in Q&A communities is essential to improve the value of crowdsourced knowledge.ObjectiveIn this paper, we investigate how information seekers can increase the chance of eliciting a successful answer to their questions on Stack Overflow by focusing on the following actionable factors: affect, presentation quality, and time.MethodWe develop a conceptual framework of factors potentially influencing the success of questions in Stack Overflow. We quantitatively analyze a set of over 87 K questions from the official Stack Overflow dump to assess the impact of actionable factors on the success of technical requests. The information seeker reputation is included as a control factor. Furthermore, to understand the role played by affective states in the success of questions, we qualitatively analyze questions containing positive and negative emotions. Finally, a survey is conducted to understand how Stack Overflow users perceive the guideline suggestions for writing questions.ResultsWe found that regardless of user reputation, successful questions are short, contain code snippets, and do not abuse with uppercase characters. As regards affect, successful questions adopt a neutral emotional style.ConclusionWe provide evidence-based guidelines for writing effective questions on Stack Overflow that software engineers can follow to increase the chance of getting technical help. As for the role of affect, we empirically confirmed community guidelines that suggest avoiding rudeness in question writing."
Research article - Calculating completeness of software project scope definition,"AbstractContextSoftware project plan is the basis of the project execution, and its quality depends on completeness of software scope definition. A method is required that should gauge the completeness of different aspects of scope definition, thus, providing guidance to practitioners to reconsider areas that have not been defined well.ObjectiveThis paper aims to evaluate completeness of software project scope definition. It identifies a detailed list of different aspects of software project scope definition, and builds a method where numerical score can be assigned to a scope definition.MethodA detailed list of different elements of software project scope definition is identified through literature. These elements are then empirically evaluated through an electronic survey, conducted with the industrial experts. Once finalized, these elements are ranked and assigned weights to systematically build a scorecard that is used to calculate scope definition score. Evaluation of the proposed method is done through a series of formal experiments.ResultsEvaluation results suggest that the proposed method is useful not only in calculating completeness of software projects scope definition, but it also serves as a guide for practitioners to determine specific aspects that require further consideration."
Review article - The contribution that empirical studies performed in industry make to the findings of systematic reviews: A tertiary study,"AbstractContextSystematic reviews can provide useful knowledge for software engineering practice, by aggregating and synthesising empirical studies related to a specific topic.ObjectiveWe sought to assess how far the findings of systematic reviews addressing practice-oriented topics have been derived from empirical studies that were performed in industry or that used industry data.MethodWe drew upon and augmented the data obtained from a tertiary study that performed a systematic review of systematic reviews published in the period up to the end of 2015, seeking to identify those with findings that are relevant for teaching and practice. For the supplementary analysis reported here, we then examined the profiles of the primary studies as reported in each systematic review.ResultsWe identified 48 systematic reviews as candidates for further analysis. The many differences that arise between systematic reviews, together with the incompleteness of reporting for these, mean that our counts should be treated as indicative rather than definitive. However, even when allowing for problems of classification, the findings from the majority of these systematic reviews were predominantly derived from using primary studies conducted in industry. There was also an emphasis upon the use of case studies, and a number of the systematic reviews also made some use of weaker ‘experience’ or even ‘opinion’ papers.ConclusionsPrimary studies from industry play an important role as inputs to systematic reviews. Using more rigorous industry-based primary studies can give greater authority to the findings of the systematic reviews, and should help with the creation of a corpus of sound empirical data to support evidence-informed decisions."
Research article - Contextualizing spectrum-based fault localization,"AbstractContext: Fault localization is among the most expensive tasks in software development. Spectrum-based fault localization (SFL) techniques seek to pinpoint faulty program elements (e.g., statements), by sorting them only by their suspiciousness scores. Developers tend to fall back on another debugging strategy if they do not find the bug in the first positions of a suspiciousness list.Objective: In this study, we assess techniques to contextualize code inspection whose goal is two-fold: to provide guidance during fault localization, and to improve the effectiveness of SFL techniques in classifying bugs within the first picks. Code Hierarchy (CH) and Integration Coverage-based Debugging (ICD) techniques provide a search roadmap—a list of methods—that guide the developer toward faults. CH assigns a method with the highest suspiciousness score of its blocks, and ICD captures method call relationships from testing to establish the roadmap. Two new filtering strategies—Fixed Budget (FB) and Level Score (LS)—are combined with ICD and CH for reducing the amount of blocks to inspect in each method.Method: We evaluated the effectiveness of ICD, CH, FB, LS, and a suspiciousness block list (BL) on 62 bugs from 7 real programs.Results: ICD and CH using FB found more faults inspecting less blocks than BL with statistical significance. More than 50% of the faults were found inspecting at most 10 blocks using ICD-FB and CH-FB. Moreover, ICD and CH located 70% of the faults by inspecting, at most, 4 methods.Conclusions: These results suggest that the contextualization provided by roadmaps and filtering strategies is useful for guiding developers toward faults and improves the performance of SFL techniques."
