title,abstract
Research article - Model-based test suite generation for graph transformation system using model simulation and search-based techniques,"AbstractContextTest generation by model checking is a useful technique in model-based testing that allows automatic generation of test cases from models by utilizing the counter-examples/witnesses produced through a model checker. However, generating redundant test cases and state space explosion problem are two major obstacles to transfer this technique into industrial practice.ObjectiveAn idea to cope with these challenges consists in an intelligent model checking for exploring only a portion of the state space according to the test objectives. Motivated by this idea, we propose an approach that exploits meta-heuristic algorithms to adapt a model checker when used for integration testing of systems formally specified by graph transformations.MethodThis method is not based on model checking algorithms, but rather uses the modeling and simulation features of the underlying model checker. In the proposed approach, a population of test suites that each of which is a set of paths on the state space, is evolved towards satisfying the all def-use test objectives. Consequently, a test suite with high coverage is generated.ResultsTo assess the efficiency of our approach, it is implemented in GROOVE, an open source toolset for designing and model checking graph transformation systems. Empirical results based on some case studies, confirm a significant improvement in terms of coverage, speed and memory usage, in comparison with the state of the art techniques.ConclusionOur analysis reveals that intelligent model checking can appropriately address the challenges of traditional model-checking-assisted testing. We further conclude that graph transformation specification is an efficient modeling solution to behavioral testing and graph transformation tools have a great potential for developing a model-based testing tool."
Short communication - Software feature refinement prioritization based on online user review mining,"AbstractContextOnline software reviews have provided a wealth of user feedback on software applications. User reviews along with ratings have been influential in a series of software engineering tasks e.g. software maintenance and release planning.ObjectiveOur research aims to assist managers in prioritizing features to be refined in next release from the perspective of enhancing user ratings via mining online reviews.MethodWe first extract software features from user reviews and determine their probability distribution in each review with LDA. Then the ground truth rating of each feature is estimated by linear regression under the assumption that the software functionality rating is a convex combination of all feature ratings weighted by their distribution probabilities over the review. Finally, we formalize feature refinement prioritization as an optimization problem which maximizes user group’s rating on the software functionality under the constraint of development budget.ResultsThe proposed approach can use topic model to jointly extract features from user reviews semi-supervisedly and determine each feature’s weight in each user’s rating on the software functionality. The estimated ground truth ratings of all features reveal how reviewer group evaluate those features. Finally, we provide an illustrative example to demonstrate the key idea of our framework.ConclusionOur proposed framework is general to various software products with mass user reviews and semi-automatic without much human efforts and intervention. The framework’s interpretability helps managers better understand user feedback on the software functionality and make feature refinement plan for the upcoming releases."
Review article - A survey on software testability,"AbstractContextSoftware testability is the degree to which a software system or a unit under test supports its own testing. To predict and improve software testability, a large number of techniques and metrics have been proposed by both practitioners and researchers in the last several decades. Reviewing and getting an overview of the entire state-of-the-art and state-of-the-practice in this area is often challenging for a practitioner or a new researcher.ObjectiveOur objective is to summarize the body of knowledge in this area and to benefit the readers (both practitioners and researchers) in preparing, measuring and improving software testability.MethodTo address the above need, the authors conducted a survey in the form of a systematic literature mapping (classification) to find out what we as a community know about this topic. After compiling an initial pool of 303 papers, and applying a set of inclusion/exclusion criteria, our final pool included 208 papers (published between 1982 and 2017).ResultsThe area of software testability has been comprehensively studied by researchers and practitioners. Approaches for measurement of testability and improvement of testability are the most-frequently addressed in the papers. The two most often mentioned factors affecting testability are observability and controllability. Common ways to improve testability are testability transformation, improving observability, adding assertions, and improving controllability.ConclusionThis paper serves for both researchers and practitioners as an “index” to the vast body of knowledge in the area of testability. The results could help practitioners measure and improve software testability in their projects. To assess potential benefits of this review paper, we shared its draft version with two of our industrial collaborators. They stated that they found the review useful and beneficial in their testing activities. Our results can also benefit researchers in observing the trends in this area and identify the topics that require further investigation."
Research article - A systematic mapping study of infrastructure as code research,"AbstractContext: Infrastructure as code (IaC) is the practice to automatically configure system dependencies and to provision local and remote instances. Practitioners consider IaC as a fundamental pillar to implement DevOps practices, which helps them to rapidly deliver software and services to end-users. Information technology (IT) organizations, such as GitHub, Mozilla, Facebook, Google and Netflix have adopted IaC. A systematic mapping study on existing IaC research can help researchers to identify potential research areas related to IaC, for example defects and security flaws that may occur in IaC scripts.Objective: The objective of this paper is to help researchers identify research areas related to infrastructure as code (IaC) by conducting a systematic mapping study of IaC-related research.Method: We conduct our research study by searching five scholar databases. We collect a set of 31,498 publications by using seven search strings. By systematically applying inclusion and exclusion criteria, which includes removing duplicates and removing non-English and non peer-reviewed publications, we identify 32 publications related to IaC. We identify topics addressed in these publications by applying qualitative analysis.Results: We identify four topics studied in IaC-related publications: (i) framework/tool for infrastructure as code; (ii) adoption of infrastructure as code; (iii) empirical study related to infrastructure as code; and (iv) testing in infrastructure as code. According to our analysis, 50.0% of the studied 32 publications propose a framework or tool to implement the practice of IaC or extend the functionality of an existing IaC tool.Conclusion: Our findings suggest that framework or tools is a well-studied topic in IaC research. As defects and security flaws can have serious consequences for the deployment and development environments in DevOps, we observe the need for research studies that will study defects and security flaws for IaC."
Research article - Impact of model notations on the productivity of domain modelling: An empirical study,"AbstractContextThe intensive use of models is a cornerstone of the Model-Driven Engineering (MDE) paradigm and its claimed gains in productivity. However, in order to maximize these productivity gains, it is important to adequately select the modeling formalism to be used. Unfortunately, the MDE community still lacks empirical data to support such choice.ObjectiveThis paper aims at contributing to filling this gap by reporting an empirical study in which two types of domain model notations, graphical vs. textual, are compared regarding their efficiency and effectiveness during the creation of domain models.MethodA quasi-experiment was designed in which 127 participants were randomly classified in four groups. Then, each group was randomly assigned to a different combination of notation and application. All the participants were students enrolled in the 6th semester of the Computer Engineering degree at the University of Alicante. The statistical procedure applied was a two-factor multivariate analysis of variance (two-way MANOVA).ResultsThe data shows a statistically significant effect of notation type on the efficiency and effectiveness of domain modelling activities, independently from the application being modelled.ConclusionThe joint examination of our results and those of previous studies suggests that, in MDE, different tasks call for different types of notations. Therefore, MDE environments should offer both textual and graphical notations, and assist developers in selecting the most suitable one depending on the task being carried out. In particular, our data suggest that domain model creation tasks are better supported by graphical notations. To augment the validity of the conclusions of this paper, the experiment should be replicated with different subject profiles, notations, domain model sizes, tasks and application types."
Research article - Little’s law based validation framework for load testing,"AbstractContext: Performance is a key quality consideration for large-scale software systems which supports thousands of concurrent users. Load testing is an integral part of the development lifecycle and is used to address performance issues before deploying the system in production. But, how do we validate the output reported by load testing tools? Little’s Law is useful for validating the accuracy of load testing output. Though IT industry is flooded with various enterprise and open source tools for load testing, but they do not offer support for validation of its result using Little’s Law. Manual validation is time intensive and infeasible with increase in the complexity of testing scripts.Objective: In this paper we provide a Little’s law-based validation framework which will enable the researchers and industry practitioners to validate load testing results. The implementation of the framework is also demonstrated.Method: To understand the constructs commonly used in load test scripts, we analysed scripts of two open source benchmark applications and eight large-scale software systems used in industry. We found that transactions are arranged using control flow patterns like sequential, loop and conditional. Based on the analysis, we devised the framework.Results: The efficacy of the proposed framework is successfully evaluated on two systems - an open source Dell DVD Store benchmarking application and a real-world large scale system used in industry. The framework is independent of load testing tool used and can be used with complex testing scripts.Conclusions There are no known frameworks or inbuilt support in existing load testing tools for guiding practitioners on applying Little’s Law using output generated by tools. We address this significant gap by providing a framework which combines information from test scripts/reports and Little’s Law to determine whether the results are valid. The provided implementation can be easily integrated with existing load testing tools."
Research article - Live programming in practice: A controlled experiment on state machines for robotic behaviors,"AbstractContextLive programming environments are gaining momentum across multiple programming languages. A tenet of live programming is a development feedback cycle, resulting in faster development practices. Although practitioners of live programming consider it a positive inclusion in their workflow, no in-depth investigations have yet been conducted on its benefits in a realistic scenario, nor using complex API.ObjectiveThis paper carefully studies the advantage of using live programming in defining nested state machines for robot behaviors. We analyzed two important aspects of developing robotic behaviors using these machines: program comprehension and program writing. We analyzed both development practices in terms of speed and accuracy.MethodWe conducted two controlled experiments, one for program comprehension and another for program writing. We measured the speed and accuracy of randomized assigned participants on completing programming tasks, against a baseline.ResultsIn a robotic behavior context, we found that a live programming system for nested state machine programs does not significantly outperform a non-live language in program comprehension nor in program writing in terms of speed and accuracy. However, the feedback of test subjects indicates their preference for the live programming system.ConclusionsThe results of this work seem to contradict the studies of live programming in other areas, even while participants still favor using live programming techniques. We learned that the complex API chosen in this work has a strong negative influence on the results. To the best of our knowledge, this is the first in-depth live programming experiment in a complex domain."
Review article - Machine learning techniques for code smell detection: A systematic literature review and meta-analysis,"AbstractBackground: Code smells indicate suboptimal design or implementation choices in the source code that often lead it to be more change- and fault-prone. Researchers defined dozens of code smell detectors, which exploit different sources of information to support developers when diagnosing design flaws. Despite their good accuracy, previous work pointed out three important limitations that might preclude the use of code smell detectors in practice: (i) subjectiveness of developers with respect to code smells detected by such tools, (ii) scarce agreement between different detectors, and (iii) difficulties in finding good thresholds to be used for detection. To overcome these limitations, the use of machine learning techniques represents an ever increasing research area.Objective: While the research community carefully studied the methodologies applied by researchers when defining heuristic-based code smell detectors, there is still a noticeable lack of knowledge on how machine learning approaches have been adopted for code smell detection and whether there are points of improvement to allow a better detection of code smells. Our goal is to provide an overview and discuss the usage of machine learning approaches in the field of code smells.Method: This paper presents a Systematic Literature Review (SLR) on Machine Learning Techniques for Code Smell Detection. Our work considers papers published between 2000 and 2017. Starting from an initial set of 2456 papers, we found that 15 of them actually adopted machine learning approaches. We studied them under four different perspectives: (i) code smells considered, (ii) setup of machine learning approaches, (iii) design of the evaluation strategies, and (iv) a meta-analysis on the performance achieved by the models proposed so far.Results: The analyses performed show that God Class, Long Method, Functional Decomposition, and Spaghetti Code have been heavily considered in the literature. Decision Trees and Support Vector Machines are the most commonly used machine learning algorithms for code smell detection. Models based on a large set of independent variables have performed well. JRip and Random Forest are the most effective classifiers in terms of performance. The analyses also reveal the existence of several open issues and challenges that the research community should focus on in the future.Conclusion: Based on our findings, we argue that there is still room for the improvement of machine learning techniques in the context of code smell detection. The open issues emerged in this study can represent the input for researchers interested in developing more powerful techniques."
Research article - The current state of software license renewals in the I.T. industry,"AbstractContextThe software industry has changed significantly in the 21st century; no longer is it dominated by organizations seeking to sell products directly to customers, instead most multinational organizations nowadays provide services via licensing agreements. These licenses are for a fixed-duration; and hence, the question of their renewal becomes of paramount importance for the selling organization’s revenue.ObjectiveDespite its financial impact, the topic of license renewal strategies, processes, tools, and support receives very limited attention in the research literature. Hence, it is believed that an interesting research question is: What is the state of current industrial practice in this essential field?MethodTo initially explore the topic of license renewals, this paper implements the Grounded theory method. To implement the method, semi-structured, cross-sectional, anonymous, selfreported interviews are carried out with 20 professionals from multiple organizations, later the Constant Comparative Method is used to analyse the collected data.ResultsThis paper presents a synthesized picture of the current industrial practice of the end-to-end software license renewal process. Alongside, it also identifies a set of challenges and risk factors that impact on renewal decisions of customers, hence on the overall revenue of seller organizations. Finally, using structured brainstorming techniques, this paper identifies 11 future research directions, that can help organizations with the mitigation of the risks in the license renewal process.ConclusionIt is concluded that lack of effective communication among stakeholders, the absence of customer trust, and scarcity of value generated from purchased licenses are among the primary drivers that influence renewal decisions. Also, there is a need to invest in intelligent automation along with artificial intelligence enabled analytics in order to enhance customer satisfaction."
