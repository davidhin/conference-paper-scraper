title,abstract
Short communication - Performance metamorphic testing: A Proof of concept,"AbstractContextPerformance testing is a challenging task mainly due to the lack of test oracles, i.e. mechanisms to decide whether the performance of a program is acceptable or not because of a bug. Metamorphic testing enables the generation of test cases in the absence of an oracle by exploiting the so–called metamorphic relations between the inputs and outputs of multiple executions of the program under test. In the last two decades, metamorphic testing has been successfully used to detect functional faults in different domains. However, its applicability to performance testing remains unexplored.ObjectiveWe propose the application of metamorphic testing to reveal performance failures.MethodWe define Performance Metamorphic Relations (PMRs) as expected relations between performance measurements of multiple executions of the program under test. These relations can be turned into assertions for the automated detection of performance bugs, removing the need for complex benchmarks and domain experts guidance. As a further benefit, PMRs can be turned into fitness functions to guide search–based techniques on the generation of test data.ResultsThe feasibility of the approach is illustrated through an experimental proof of concept in the context of the automated analysis of feature models.ConclusionThe results confirm the potential of metamorphic testing, in combination with search-based techniques, to automate the detection of performance bugs."
Research article - Automatic generation of predictive monitors from scenario-based specifications,"AbstractContextUnpredictability and uncertainty about future evolutions of both the system and its environment may easily compromise the behavior of the system. The subsequent software failures can have serious consequences. When dealing with open environments, run-time monitoring is one of the most promising techniques to detect software failures. Several monitoring approaches have been proposed in the last years; however, they suffer from two main limitations. First, they provide limited information to be exploited at run-time for early detecting and managing situations that most probably will lead to failures. Second, they mainly rely on logic-based specifications, whose intrinsic complexity may hamper the use of these monitoring approaches in industrial contexts.ObjectiveIn order to address these two limitations, this paper proposes a novel approach, called PREDIMO (PREDIctive MOnitoring). The approach starts from scenario-based specifications, automatically generates predictive monitors called MAs (Multi-valued Automata), which take into account the actual status and also the possible evolution of both system and environment in the near future, and enables the definition of precise strategies to prevent failures. More specifically, the generated monitors evaluate the specified properties and return one of the seven different values representing the degree of controllability of the system and the distance of the potential incoming failure. The translation from scenario-based specifications to MAs preserves the semantics of the starting specification.MethodWe use the design and creation research methodology to design an innovative approach that fills highlighted gaps of state-of-the-art approaches. The validation of the approach is performed through a large experimentation with OSGi (Open Service Gateway Initiative) applications.ResultsWe present a novel language to specify the properties to be monitored. Then, we present a novel approach to automatically generate predictive monitors from the specified properties.ConclusionThe overall approach is tool supported and a large experimentation demonstrates its feasibility and usability."
Research article - What factors influence the reviewer assignment to pull requests?,"AbstractContextWhen external contributors want to collaborate with an open-source project, they can send a pull request to the project core team. Subsequently, a core team member is assigned to review the pull request. Recently, some techniques for recommending reviewers to analyze pull requests were proposed. However, they replicate previous (not always desired) patterns and do not provide the rationale behind the recommendation.ObjectiveThis paper aims at understanding the factors that influence on assigning reviewers to pull requests and evaluating the extent of this influence.MethodTo reach this goal, we mined association rules from 22,523 pull requests belonging to 3 projects hosted on GitHub. In addition to showing such patterns, we also present a qualitative analysis that explains why the patterns arose.ResultsIn summary our results indicate that (i) some reviewers always analyze smaller pull requests, with few commits and few files; (ii) some reviewers frequently (up to 58% of the times) analyze pull requests filed by inexperienced requesters; (iii) some reviewers have more chances (up to 25 times) to analyze pull requests filed by requesters of their acquaintance; and (iv) some reviewers have more chances (up to 20 times) to analyze pull requests containing files that they have recently changed.ConclusionIn particular, the results allow for the following conclusions: (i) factors such as number of commits and files in the pull request may influence the reviewers assignment; (ii) factors regarding the requester profile may influence on reviewer allocation; (iii) the social relationship between requester and reviewer exert influence on pull request evaluation, that is, when the reviewer knows the requester, his or her chances of evaluating such contributions may increase; and (iv) factors such as ownership and locality of pull request artifacts are important predictors for the reviewer. Furthermore, we point out that, besides identifying influence factors related to pull request reviewer, the adopted approach allowed us to quantify the extent of that influence via support, confidence, and lift metrics."
Research article - Feature interaction in software product line engineering: A systematic mapping study,"AbstractContext: Software product lines (SPL) engineering defines a set of systems that share common features and artifacts to achieve high productivity, quality, market agility, low time to market, and cost. An SPL product is derived from a configuration of features which need to be compounded together without violating their particular specifications. While it is easy to identify the behavior of a feature in isolation, specifying and resolving interactions among features may not be a straightforward task. The feature interaction problem has been a challenging subject for decades.Objective: This study aims at surveying existing research on feature interaction in SPL engineering in order to identify common practices and research trends.Method: A systematic mapping study was conducted with a set of seven research questions, in which the 35 studies found are mainly classified regarding the feature interaction solution presented: detection, resolution and general analysis.Results: 43% of the papers deal with feature interaction at early phases of a software lifecycle. The remaining is shared among the other categories: source code detection, resolution and analysis. For each category, it was also identified the main strategies used to deal with interactions.Conclusions: The findings can help to understand the needs in feature interaction for SPL engineering, and highlight aspects that still demand an additional investigation. For example, often strategies are partial and only address specific points of a feature interaction investigation."
Research article - A comparison of automated training-by-example selection algorithms for Evidence Based Software Engineering,"AbstractContextStudy search and selection is central to conducting Evidence Based Software Engineering (EBSE) research, including Systematic Literature Reviews and Systematic Mapping Studies. Thus, selecting relevant studies and excluding irrelevant studies, is critical. Prior research argues that study selection is subject to researcher bias, and the time required to review and select relevant articles is a target for optimization.ObjectiveThis research proposes two training-by-example classifiers that are computationally simple, do not require extensive training or tuning, ensure inclusion/exclusion consistency, and reduce researcher study selection time: one based on Vector Space Models (VSM), and a second based on Latent Semantic Analysis (LSA).MethodAlgorithm evaluation is accomplished through Monte-Carlo Cross-Validation simulations, in which study subsets are randomly chosen from the corpus for training, with the remainder classified by the algorithm. The classification results are then assessed for recall (a measure of completeness), precision (a measure of exactness) and researcher efficiency savings (reduced proportion of corpus studies requiring manual review as a result of algorithm use). A second smaller simulation is conducted for external validation.Results and conclusionsVSM algorithms perform better in recall; LSA algorithms perform better in precision. Recall improves with larger training sets with a higher proportion of truly relevant studies. Precision improves with training sets with a higher portion of irrelevant studies, without a significant impact from the training set size. The algorithms reduce the influence of researcher bias and are found to significantly improve researcher efficiency.To improve recall, the findings recommend VSM and a large training set including as many truly relevant studies as possible. If precision and efficiency are most critical, the findings suggest LSA and a training set including a large proportion of truly irrelevant studies."
Research article - What is wrong with topic modeling? And how to fix it using search-based software engineering,"AbstractContextTopic modeling finds human-readable structures in unstructured textual data. A widely used topic modeling technique is Latent Dirichlet allocation. When running on different datasets, LDA suffers from “order effects”, i.e., different topics are generated if the order of training data is shuffled. Such order effects introduce a systematic error for any study. This error can relate to misleading results; specifically, inaccurate topic descriptions and a reduction in the efficacy of text mining classification results.ObjectiveTo provide a method in which distributions generated by LDA are more stable and can be used for further analysis.MethodWe use LDADE, a search-based software engineering tool which uses Differential Evolution (DE) to tune the LDA’s parameters. LDADE is evaluated on data from a programmer information exchange site (Stackoverflow), title and abstract text of thousands of Software Engineering (SE) papers, and software defect reports from NASA. Results were collected across different implementations of LDA (Python+Scikit-Learn, Scala+Spark) across Linux platform and for different kinds of LDAs (VEM, Gibbs sampling). Results were scored via topic stability and text mining classification accuracy.ResultsIn all treatments: (i) standard LDA exhibits very large topic instability; (ii) LDADE’s tunings dramatically reduce cluster instability; (iii) LDADE also leads to improved performances for supervised as well as unsupervised learning.ConclusionDue to topic instability, using standard LDA with its “off-the-shelf” settings should now be depreciated. Also, in future, we should require SE papers that use LDA to test and (if needed) mitigate LDA topic instability. Finally, LDADE is a candidate technology for effectively and efficiently reducing that instability."
Research article - Assessing the impact of the awareness level on a co-operative game,"AbstractContextWhen playing a co-operative game, being aware of your collaborators (where they are playing, what they are doing, the abilities they have, etc.) is essential for achieving the game's goals. This led to the definition of Gamespace Awareness in order to guide in the identification of the awareness needs in the form of a compilation of the awareness elements that a co-operative game should feature.ObjectiveGamespace Awareness does not establish how much awareness information players must be provided with. This constitutes the main motivation for this work: to assess the impact of different levels of Gamespace Awareness elements on a co-operative game.MethodA multiplayer action game was developed that supports three different awareness configurations, each one featuring different awareness levels (high, medium and low). The impact of these awareness levels was measured as regards game score, time, players’ happiness while playing, enjoyment and perceived usefulness. Several techniques such as subjective surveys and facial expression analysis were used to measure these factors.ResultsThe analysis of the results shows that the higher the awareness, the better the game score. However, the highest level of player happiness was not achieved with the most awareness-enabled configuration; we found that the players’ enjoyment depends not only on their awareness level but also on their expertise level. Finally, the awareness elements related to the present and the future were the most useful, as could be expected in a multiplayer action game.ConclusionsThe results showed that the medium level awareness obtained the best results. We therefore concluded that a certain level of awareness is necessary, but that excessive awareness could negatively affect the game experience."
Research article - A domain-specific language to visualize software evolution,"AbstractContextAccurately relating code authorship to commit frequency over multiple software revisions is a complex task. Most of the navigation tools found in common source code versioning clients are often too rigid to formulate specific queries and adequately present results of such queries. Questions related to evolution asked by software engineers are therefore challenging at answering using common Git clients.ObjectiveThis paper explores the use of stacked adjacency matrices and a domain specific language to produce tailored interactive visualizations for software evolution exploration. We are able to support some classical software evolution tasks using short and concise scripts using our language.MethodWe propose a domain-specific language to stack adjacency matrices and produce scalable and interactive visualizations. Our language and visualizations are evaluated using two independent controlled experiments and closely observing participants.ResultsWe made the following findings: (i) participants are able to express sophisticated queries using our domain-specific language and visualizations, (ii) participants perform better than GitHub’s visualizations to answer a set of questions.ConclusionOur visual and scripting environment performs better than GitHub’s visualizations at extracting software evolution information."
"Research article - Synchronised visualisation of software process and product artefacts: Concept, design and prototype implementation","AbstractContextMost prior software visualisation (SV) research has focused primarily on making aspects of intangible software product artefacts more evident. While undoubtedly useful, this focus has meant that software process visualisation has received far less attention.ObjectiveThis paper presents Conceptual Visualisation, a novel SV approach that builds on the well-known CodeCity metaphor by situating software code artefacts alongside their software development processes, in order to link and synchronise these typically separate components.MethodWhile the majority of prior SV research has focused on re-presenting what is already available in the code (i.e., the implementation) or information derived from it (i.e., various metrics), the presented approach instead makes the design concepts and original developers’ intentions – both significant sources of information in terms of software development and maintenance – readily and contextually available in a visualisation environment that tightly integrates the code artefacts with their original functional requirements and development activity.ResultsOur approach has been implemented in a prototype tool called ScrumCity with the proof of concept being demonstrated using six real-world open source systems. A preliminary case study has further been carried out with real world data.ConclusionConceptual Visualisation, as implemented in ScrumCity, shows early promise in enabling developers and managers (and potentially other stakeholders) to traverse and explore multiple aspects of software product and process artefacts in a synchronised manner, achieving traceability between the two."
Review article - Leveraging organizational climate theory for understanding industry-academia collaboration,"AbstractContextIndustry-academia collaboration (IAC) in the field of software engineering is widely discussed in the literature, highlighting its importance and benefits. However, along with the benefits, academic researchers face challenges while performing empirical studies in industry, risking their success. Awareness of these challenges and the importance of addressing them has recently grown, and became the center of discussion in several publication venues.ObjectiveIn this paper, we aim to address one of the key challenges affecting the success of IAC: stakeholder involvement. To this end, we propose a vision for leveraging organizational climate theory toward an effective management of IAC in software engineering research. Organizational climate is defined as the organization's priorities as perceived by its employees and was found to be an effective means of predicting employee behavior.MethodTo provide a basis and motivation for our vision, we conducted a literature review, focused on the workshop series of CESI, Conducting Empirical Studies in Industry, in order to elicit the relevant reported challenges of IAC, and to analyze them through the lens of the organizational climate theory.ResultsEmergent categories of the elicited challenges of IAC are related to the two basic components that determine the emergence of organizational climate: management commitment and communication. This result demonstrates that analyzing stakeholder involvement-related challenges of IAC through the lens of organizational climate theory provides an indication of the climate components that should be enhanced in order to address these challenges.ConclusionThe above analysis lays the foundation for our vision that organizational climate may serve as an effective means of addressing the discussed challenges. We propose that developing measures of organizational research collaboration climate and deploying respective interventions for improvement would be instrumental for enhancing stakeholder involvement in IAC. We further propose a research outline toward fulfilling these potential contributions."
"Research article - Recruitment, engagement and feedback in empirical software engineering studies in industrial contexts","AbstractContextResearch carried out in industrial contexts are recognized as important to the advancement of software engineering knowledge and practice. However, several challenges present themselves in the three key phases of research carried out in industrial contexts, recruitment, engagement and feedback.ObjectiveThe aim of this paper is to report the challenges related to each of the three phases of research carried out in industrial contexts, and the associated solutions we have found useful from our combined body of industrial empirical software engineering research studies spanning four case studies, five grounded theory studies, seven survey studies and two quasi-experimental studies involving a total of over 400 industrial participants in the past decade.MethodWe designed an instrument to gather details of our studies carried out in industrial contexts and performed thematic analysis to synthesise and draw out the most prominent challenges faced.ResultsWe present a set of recommendations around study design, conduct and reporting to try and mitigate some of these challenges as they apply specifically to industrial empirical research.ConclusionThese recommendations can guide researchers, novice and experienced, working in close collaboration with industry stakeholders to make the most of their industrial software engineering research."
